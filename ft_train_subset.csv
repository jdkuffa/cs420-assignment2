cleaned_method,target_block,tokens_in_method
"def _resolve_lib_imported_symbols(self, lib, imported_libs, generic_refs):
    """"""Resolve the imported symbols in a library.""""""
    for symbol in lib.elf.imported_symbols:
        imported_lib = self._find_exported_symbol(symbol, imported_libs)
        if not imported_lib:
            lib.unresolved_symbols.add(symbol)
        else:
            lib.linked_symbols[symbol] = imported_lib
            if generic_refs:
                ref_lib = generic_refs.refs.get(imported_lib.path)
                if not ref_lib or not symbol in ref_lib.exported_symbols:
                    lib.imported_ext_symbols[imported_lib].add(symbol)
",if generic_refs :,189
"def make_docs_directory(output_dir, name):
    if not isdir(pjoin(output_dir, name)):
        subprocess.run([""mkdir"", pjoin(output_dir, name)], stdout=subprocess.PIPE)
    for i in range(10):
        if not isdir(pjoin(output_dir, name, str(i))):
            subprocess.run(
                [""mkdir"", pjoin(output_dir, name, str(i))], stdout=subprocess.PIPE
            )
","if not isdir ( pjoin ( output_dir , name , str ( i ) ) ) :",121
"def assert_results(self, results, activities, msg=""""):
    activity_ids = []
    extra_context = []
    for result in results:
        if hasattr(result, ""serialization_id""):
            activity_ids.append(result.serialization_id)
        else:
            activity_ids.append(result)
        if hasattr(result, ""extra_context""):
            extra_context.append(result.extra_context)
    compare_lists(activity_ids, [a.serialization_id for a in activities], msg)
    if extra_context:
        self.assertEquals([a.extra_context for a in activities], extra_context)
","if hasattr ( result , ""extra_context"" ) :",164
"def for_file(cls, filename: str, modname: str) -> ""ModuleAnalyzer"":
    if (""file"", filename) in cls.cache:
        return cls.cache[""file"", filename]
    try:
        with tokenize.open(filename) as f:
            obj = cls(f, modname, filename, decoded=True)
            cls.cache[""file"", filename] = obj
    except Exception as err:
        if "".egg"" + path.sep in filename:
            obj = cls.cache[""file"", filename] = cls.for_egg(filename, modname)
        else:
            raise PycodeError(""error opening %r"" % filename, err) from err
    return obj
","if "".egg"" + path . sep in filename :",170
"def merge_dicts(source: Dict, destination: Dict) -> Dict:
    for key, value in source.items():
        if isinstance(value, dict):
            # get node or create one
            node = destination.setdefault(key, {})
            merge_dicts(value, node)
        else:
            destination[key] = value
    return destination
","if isinstance ( value , dict ) :",92
"def _escape_attrib_c14n(text):
    # escape attribute value
    try:
        if ""&"" in text:
            text = text.replace(""&"", ""&amp;"")
        if ""<"" in text:
            text = text.replace(""<"", ""&lt;"")
        if '""' in text:
            text = text.replace('""', ""&quot;"")
        if ""\t"" in text:
            text = text.replace(""\t"", ""&#x9;"")
        if ""\n"" in text:
            text = text.replace(""\n"", ""&#xA;"")
        if ""\r"" in text:
            text = text.replace(""\r"", ""&#xD;"")
        return text
    except (TypeError, AttributeError):
        _raise_serialization_error(text)
","if ""\t"" in text :",188
"def get_oldest(class_name):
    """"""Get the oldest object for a specific class name""""""
    for cls, wdict in six.iteritems(live_refs):
        if cls.__name__ == class_name:
            if not wdict:
                break
            return min(six.iteritems(wdict), key=itemgetter(1))[0]
",if cls . __name__ == class_name :,90
"def recursive_rm(*patterns):
    """"""Recursively remove a file or matching a list of patterns.""""""
    for root, subdirs, subfiles in os.walk(u"".""):
        root = os.path.normpath(root)
        if root.startswith("".git/""):
            continue
        for file in subfiles:
            for pattern in patterns:
                if fnmatch.fnmatch(file, pattern):
                    safe_remove(os.path.join(root, file))
        for dir in subdirs:
            for pattern in patterns:
                if fnmatch.fnmatch(dir, pattern):
                    safe_rmtree(os.path.join(root, dir))
","if fnmatch . fnmatch ( file , pattern ) :",170
"def _methods(ctx, cls, inst):
    while cls.Attributes._wrapper and len(cls._type_info) > 0:
        (cls,) = cls._type_info.values()
    if cls.Attributes.methods is not None:
        for k, v in cls.Attributes.methods.items():
            is_shown = True
            if v.when is not None:
                is_shown = v.when(inst, ctx)
            if is_shown:
                yield k, v
",if v . when is not None :,130
"def save(self):
    for file_field in self.files:
        file = self.cleaned_data[file_field]
        self.cleaned_data[file_field] = default_storage.save(file.name, file)
    for name in settings.CONFIG:
        current = getattr(config, name)
        new = self.cleaned_data[name]
        if isinstance(new, str):
            new = normalize_newlines(new)
        if (
            conf.settings.USE_TZ
            and isinstance(current, datetime)
            and not timezone.is_aware(current)
        ):
            current = timezone.make_aware(current)
        if current != new:
            setattr(config, name, new)
","if isinstance ( new , str ) :",192
"def order_authors(self, entry):
    sort_authors = entry.author_sort.split(""&"")
    authors_ordered = list()
    error = False
    ids = [a.id for a in entry.authors]
    for auth in sort_authors:
        results = (
            self.session.query(Authors)
            .filter(Authors.sort == auth.lstrip().strip())
            .all()
        )
        # ToDo: How to handle not found authorname
        if not len(results):
            error = True
            break
        for r in results:
            if r.id in ids:
                authors_ordered.append(r)
    if not error:
        entry.authors = authors_ordered
    return entry
",if r . id in ids :,195
"def describe_images(self, Filters=None, Owners=None):
    images = []
    for image in self.mock_ec2_images:
        if not (Owners is None or image.get(""ImageOwnerAlias"") in Owners):
            continue
        if Filters and not _matches_image_filters(image, Filters):
            continue
        images.append(deepcopy(image))
    return dict(Images=images)
","if Filters and not _matches_image_filters ( image , Filters ) :",107
"def prefetch_related(self, *args):
    try:
        for arg in args:
            if isinstance(arg, str):
                arg = FastPrefetch.make_from_field(model=self.model, field_name=arg)
            elif isinstance(arg, Prefetch):
                arg = FastPrefetch.make_from_prefetch(arg, self.model)
            if not isinstance(arg, FastPrefetch):
                raise Exception(""Must be FastPrefetch object"")
            if arg.field in self.prefetches:
                raise Exception(""Prefetch for field '%s' already exists."")
            self.prefetches[arg.field] = arg
    except Exception as e:  # noqa
        traceback.print_exc()
    return self
","if not isinstance ( arg , FastPrefetch ) :",196
"def niap_scan(rule, extensions, paths, apath, ignore_paths=None):
    """"""NIAP scan.""""""
    try:
        if not apath:
            apath = """"
        options = {
            ""choice_rules"": rule,
            ""alternative_path"": apath,
            ""choice_extensions"": extensions,
            ""ignore_paths"": ignore_paths,
            ""show_progress"": False,
        }
        scanner = Scanner(options, paths)
        res = scanner.scan()
        if res:
            return res[""choice_matcher""]
    except Exception:
        logger.exception(""NIAP scan"")
    return {}
",if res :,176
"def secret_generator(self, string, *args, **kwargs):
    # There may be multiple strings on the same line
    results = self.regex.findall(string)
    for result in results:
        # To accommodate changing self.regex, due to different filetypes
        if isinstance(result, tuple):
            result = result[1]
        entropy_value = self.calculate_shannon_entropy(result)
        if entropy_value > self.entropy_limit:
            yield result
","if isinstance ( result , tuple ) :",121
"def encode(obj, encoding=""utf-8"", errors=""strict""):
    encoder = __encoder(encoding)
    if encoder:
        result = encoder(obj, errors)
        if not (isinstance(result, tuple) and len(result) == 2):
            raise TypeError(""encoder must return a tuple (object, integer)"")
        return result[0]
","if not ( isinstance ( result , tuple ) and len ( result ) == 2 ) :",86
"def greetings(request):
    if request.method == ""POST"":
        form = HelloForm(request.POST)
        if form.is_valid():
            first_name = form.cleaned_data[""fname""]
            last_name = form.cleaned_data[""lname""]
            return HttpResponse(""Hello, {} {}"".format(first_name, last_name))
        else:
            return render(request, ""home.html"")
    return render(request, ""home.html"")
",if form . is_valid ( ) :,125
"def logic():
    while 1:
        yield a
        var = 0
        i = len(a) - 1
        while i >= 0:
            if a[i] == 1:
                var += 1
            i -= 1
        out.next = var
",if a [ i ] == 1 :,74
"def reconfigure(self, user_config) -> None:
    if user_config:
        if self.is_function:
            raise ValueError(
                ""argument func_or_class must be a class to use user_config""
            )
        elif not hasattr(self.callable, BACKEND_RECONFIGURE_METHOD):
            raise RayServeException(
                ""user_config specified but backend ""
                + self.backend_tag
                + "" missing ""
                + BACKEND_RECONFIGURE_METHOD
                + "" method""
            )
        reconfigure_method = getattr(self.callable, BACKEND_RECONFIGURE_METHOD)
        reconfigure_method(user_config)
",if self . is_function :,190
"def convert_charged_to_paritally_charged_and_partially_refunded(apps, schema_editor):
    PaymentModel = apps.get_model(""payment"", ""Payment"")
    for payment in PaymentModel.objects.all():
        if payment.charge_status == CHARGED:
            if is_fully_charged(payment):
                payment.charge_status = ChargeStatus.FULLY_CHARGED
            elif (
                payment.transactions.filter(
                    kind=TransactionKind.REFUND, is_success=True
                ).first()
                is not None
            ):
                payment.charge_status = ChargeStatus.PARTIALLY_REFUNDED
            else:
                payment.charge_status = ChargeStatus.PARTIALLY_CHARGED
            payment.save()
",if payment . charge_status == CHARGED :,196
"def _connect_loop(self, retry):
    # Iterate through the hosts a full cycle before starting over
    status = None
    for host, port in self.client.hosts:
        if self.client._stopped.is_set():
            status = STOP_CONNECTING
            break
        status = self._connect_attempt(host, port, retry)
        if status is STOP_CONNECTING:
            break
    if status is STOP_CONNECTING:
        return STOP_CONNECTING
    else:
        raise ForceRetryError(""Reconnecting"")
",if self . client . _stopped . is_set ( ) :,133
"def Query(host, port, req):
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    resp = b""""
    try:
        sock.connect((host, int(port)))
        sock.sendall(req)
        while True:
            data = sock.recv(8096)
            if not data:
                return resp
            else:
                resp += data
    except socket.error:
        return resp
    finally:
        sock.close()
",if not data :,133
"def execute(self, arbiter, props):
    if ""name"" in props:
        watcher = self._get_watcher(arbiter, props[""name""])
        if props.get(""waiting""):
            resp = TransformableFuture()
            resp.set_upstream_future(watcher.start())
            resp.set_transform_function(lambda x: {""info"": x})
            return resp
        return watcher.start()
    else:
        return arbiter.start_watchers()
","if props . get ( ""waiting"" ) :",128
"def _select_algs(alg_type, algs, possible_algs, none_value=None):
    """"""Select a set of allowed algorithms""""""
    if algs == ():
        return possible_algs
    elif algs:
        result = []
        for alg_str in algs:
            alg = alg_str.encode(""ascii"")
            if alg not in possible_algs:
                raise ValueError(""%s is not a valid %s algorithm"" % (alg_str, alg_type))
            result.append(alg)
        return result
    elif none_value:
        return [none_value]
    else:
        raise ValueError(""No %s algorithms selected"" % alg_type)
",if alg not in possible_algs :,180
"def fetch(self, mutagen_file):
    for frame in mutagen_file.tags.getall(self.key):
        if frame.desc.lower() == self.description.lower():
            if self.key == ""USLT"":
                return frame.text
            try:
                return frame.text[0]
            except IndexError:
                return None
",if frame . desc . lower ( ) == self . description . lower ( ) :,98
"def setup_widgets(self):
    Editor.setup_widgets(self)
    # Get app version
    app_ver = ""(unknown version)""
    try:
        import pkg_resources, scc
        if scc.__file__.startswith(pkg_resources.require(""sccontroller"")[0].location):
            app_ver = ""v"" + pkg_resources.require(""sccontroller"")[0].version
    except:
        # pkg_resources is not available or __version__ file missing
        # There is no reason to crash on this.
        pass
    # Display version in UI
    self.builder.get_object(""lblVersion"").set_label(app_ver)
","if scc . __file__ . startswith ( pkg_resources . require ( ""sccontroller"" ) [ 0 ] . location ) :",160
"def clean_email(self):
    email = self.cleaned_data.get(""email"")
    if email:
        if self.user.email != email:
            unique = User.objects.filter(email__iexact=email).count()
            if unique > 0:
                raise forms.ValidationError(
                    u""An user with this email address already exists.""
                )
    return email
",if unique > 0 :,105
"def forwards(self, orm):
    from sentry.constants import RESERVED_TEAM_SLUGS
    from sentry.models import slugify_instance
    try:
        superuser = orm[""sentry.User""].objects.filter(is_superuser=True)[0]
    except IndexError:
        return
    for project in orm[""sentry.Project""].objects.filter(team__isnull=True):
        if not project.owner:
            project.owner = superuser
        project.team = orm[""sentry.Team""](
            name=project.name,
            owner=project.owner,
        )
        slugify_instance(project.team, project.team.name, reserved=RESERVED_TEAM_SLUGS)
        project.team.save()
        project.save()
",if not project . owner :,196
"def list_cb(cards):
    for c in cards.values():
        if c[""proplist""][""device.string""] == device[""Address""]:
            self.devices[device[""Address""]] = c
            self.generate_menu(device)
            return
","if c [ ""proplist"" ] [ ""device.string"" ] == device [ ""Address"" ] :",66
"def _extract_tags_from_span(attr):
    if attr is None:
        return {}
    tags = {}
    for attribute_key, attribute_value in attr.items():
        if isinstance(attribute_value, (int, bool, float)):
            value = str(attribute_value)
        elif isinstance(attribute_value, str):
            res, _ = check_str_length(str_to_check=attribute_value)
            value = res
        else:
            logging.warning(""Could not serialize tag %s"", attribute_key)
            continue
        tags[attribute_key] = value
    return tags
","if isinstance ( attribute_value , ( int , bool , float ) ) :",161
"def visit(z, dirname, names):
    for name in names:
        path = os.path.normpath(os.path.join(dirname, name))
        if os.path.isfile(path):
            p = path[len(base_dir) + 1 :]
            if not dry_run:
                z.write(path, p)
            log.debug(""adding '%s'"" % p)
",if os . path . isfile ( path ) :,105
"def validate(self):
    if ""lb_method"" in self.resource:
        lb_method = self.resource[""lb_method""]
        if lb_method not in [""ROUND_ROBIN"", ""LEAST_CONNECTIONS"", ""SOURCE_IP""]:
            raise ValueError(
                ""The lb_method attribute must be ""
                ""either ROUND_ROBIN, LEAST_CONNECTIONS ""
                ""or SOURCE_IP""
            )
    if ""protocol"" in self.resource:
        protocol = self.resource[""protocol""]
        if protocol not in [""TCP"", ""HTTP"", ""HTTPS""]:
            raise ValueError(""The type attribute must be "" ""either TCP, HTTP or HTTPS"")
    return True
","if lb_method not in [ ""ROUND_ROBIN"" , ""LEAST_CONNECTIONS"" , ""SOURCE_IP"" ] :",181
"def compute_adjlist(self, sp_adj, max_degree=32):
    """"""Transfer sparse adjacent matrix to adj-list format""""""
    num_data = sp_adj.shape[0]
    adj = num_data + np.zeros((num_data + 1, max_degree), dtype=np.int32)
    for v in range(num_data):
        neighbors = np.nonzero(sp_adj[v, :])[1]
        len_neighbors = len(neighbors)
        if len_neighbors > max_degree:
            neighbors = np.random.choice(neighbors, max_degree, replace=False)
            adj[v] = neighbors
        else:
            adj[v, :len_neighbors] = neighbors
    return adj
",if len_neighbors > max_degree :,184
"def _fix_paused_jobs_sorting(self, jobs):
    for i, job in enumerate(jobs):
        if job.next_run_time is not None:
            if i > 0:
                paused_jobs = jobs[:i]
                del jobs[:i]
                jobs.extend(paused_jobs)
            break
",if i > 0 :,91
"def random_init(self):
    self.conv1.weight.data.normal_(0, math.sqrt(2.0 / (7 * 7 * 64)))
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            m.weight.data.normal_(0, math.sqrt(2.0 / n))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.InstanceNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
","elif isinstance ( m , nn . InstanceNorm2d ) :",195
"def parseUrlEncoded(self, cad):
    dicc = []
    if cad == """":
        dicc.append(Variable("""", None))
    for i in cad.split(""&""):
        if i:
            var_list = i.split(""="", 1)
            if len(var_list) == 1:
                dicc.append(Variable(var_list[0], None))
            elif len(var_list) == 2:
                dicc.append(Variable(var_list[0], var_list[1]))
    self.variables = dicc
",if i :,147
"def get_environment_with_overrides(overrides):
    env = os.environ.copy()
    for key in overrides:
        if overrides[key] is None and key in env:
            del env[key]
        else:
            assert isinstance(overrides[key], str)
            if key.upper() == ""PATH"":
                update_system_path(env, overrides[key])
            else:
                env[key] = overrides[key]
    return env
",if overrides [ key ] is None and key in env :,125
"def create_tuple_column(spec, column_by_spec_getter):
    brackets = 0
    column_begin = 0
    inner_spec = get_inner_spec(spec)
    nested_columns = []
    for i, x in enumerate(inner_spec + "",""):
        if x == "","":
            if brackets == 0:
                nested_columns.append(inner_spec[column_begin:i])
                column_begin = i + 1
        elif x == ""("":
            brackets += 1
        elif x == "")"":
            brackets -= 1
        elif x == "" "":
            if brackets == 0:
                column_begin = i + 1
    return TupleColumn([column_by_spec_getter(x) for x in nested_columns])
",if brackets == 0 :,195
"def _send_launch_message(self, app_id, force_launch=False, callback_function=False):
    if force_launch or self.app_id != app_id:
        self.logger.info(""Receiver:Launching app %s"", app_id)
        self.app_to_launch = app_id
        self.app_launch_event.clear()
        self.app_launch_event_function = callback_function
        self.launch_failure = None
        self.send_message({MESSAGE_TYPE: TYPE_LAUNCH, APP_ID: app_id})
    else:
        self.logger.info(""Not launching app %s - already running"", app_id)
        if callback_function:
            callback_function()
",if callback_function :,189
"def _needs_init(self, code, name):
    if code in self.init_params:
        if hasattr(self, name):
            _log.warning(
                ""Even though the %r attribute is set, it will be ""
                ""overwritten during initialization because 'init_params' ""
                ""contains %r"",
                name,
                code,
            )
        return True
    if not hasattr(self, name):
        return True
    return False
","if hasattr ( self , name ) :",132
"def extra_backend_context(self):
    """"""Specific metadata to be included when passed to backends.""""""
    context = {}
    if self.in_response_to:
        original = self.in_response_to
        context[""in_response_to""] = original.id
        if ""external_id"" in original.fields:
            context[""external_id""] = original.fields[""external_id""]
    return context
","if ""external_id"" in original . fields :",103
"def OnReplaceAll(self, id, code):
    control = _GetControl(None)
    if control is not None:
        control.SetSel(0)
        num = 0
        if self.DoFindNext() == FOUND_NORMAL:
            lastSearch.replaceText = self.editReplaceText.GetWindowText()
            while _ReplaceIt(control) == FOUND_NORMAL:
                num = num + 1
        win32ui.SetStatusText(""Replaced %d occurrences"" % num)
        if num > 0 and not self.butKeepDialogOpen.GetCheck():
            self.DestroyWindow()
",if self . DoFindNext ( ) == FOUND_NORMAL :,157
"def dict(x):
    a = [""(""]
    b = None
    ks = sorted(x.keys())
    for i in ks:
        v = x[i]
        f = Encoder.encoder(v)
        if f:
            v = f(v)
            if isinstance(v, (str, basestring)):
                if b:
                    a.append("","")
                a.append(Encoder.string(i))
                a.append("":"")
                a.append(v)
                b = True
    a.append("")"")
    return """".join(a)
",if b :,159
"def _load_repositories(self, structure):
    """"""Load other image repositories into this local repo""""""
    if ""repositories"" not in structure:
        return False
    loaded_repositories = []
    for imagerepo in structure[""repositories""]:
        for tag in structure[""repositories""][imagerepo]:
            if imagerepo and tag:
                loaded_repo = self._load_image(structure, imagerepo, tag)
                if loaded_repo:
                    loaded_repositories.extend(loaded_repo)
    return loaded_repositories
",if loaded_repo :,141
"def is_valid_relation(self, parent, sub, created=False):
    if sub.is_isolated():
        return {
            ""error"": _(
                ""Isolated instances may not be added or removed from instances groups via the API.""
            )
        }
    if self.parent_model is InstanceGroup:
        ig_obj = self.get_parent_object()
        if ig_obj.controller_id is not None:
            return {
                ""error"": _(
                    ""Isolated instance group membership may not be managed via the API.""
                )
            }
    return None
",if ig_obj . controller_id is not None :,163
"def durationActual(self, dA):
    self._checkFrozen()
    if isinstance(dA, DurationTuple):
        self._durationActual = dA
    elif isinstance(dA, Duration):
        if len(dA.components) > 1:
            dA = copy.deepcopy(dA)
            dA.consolidate()
        self._durationActual = DurationTuple(dA.type, dA.dots, dA.quarterLength)
    elif isinstance(dA, str):
        self._durationActual = durationTupleFromTypeDots(dA, dots=0)
",if len ( dA . components ) > 1 :,149
"def get_encoding(klass, name, diff=None):
    cid2unicode = klass.encodings.get(name, klass.std2unicode)
    if diff:
        cid2unicode = cid2unicode.copy()
        cid = 0
        for x in diff:
            if isinstance(x, int):
                cid = x
            elif isinstance(x, PSLiteral):
                try:
                    cid2unicode[cid] = name2unicode(x.name)
                except KeyError:
                    pass
                cid += 1
    return cid2unicode
","if isinstance ( x , int ) :",154
"def __getitem__(self, name):
    if name in Quality.DOWNLOADED + Quality.SNATCHED + Quality.SNATCHED_PROPER:
        status, quality = Quality.splitCompositeStatus(name)
        if quality == Quality.NONE:
            return self.statusStrings[status]
        else:
            return (
                self.statusStrings[status]
                + "" (""
                + Quality.qualityStrings[quality]
                + "")""
            )
    else:
        return self.statusStrings[name]
",if quality == Quality . NONE :,155
"def _set(self, function):
    try:
        self._tokens = function
        self._next = six.next(self._tokens)
    except IOError as exc:
        error = str(exc)
        if error.count(""]""):
            self.error.set(error.split(""]"")[1].strip())
        else:
            self.error.set(error)
        self._tokens = Tokeniser._off
        self._next = []
        return self.error.set(""issue setting the configuration parser"")
    except StopIteration:
        self._tokens = Tokeniser._off
        self._next = []
        return self.error.set(""issue setting the configuration parser, no data"")
    return True
","if error . count ( ""]"" ) :",179
"def is_valid(self):
    # Some actions do not require comments.
    action = self.helper.actions.get(self.data.get(""action""))
    if action:
        if not action.get(""comments"", True):
            self.fields[""comments""].required = False
        if action.get(""versions"", False):
            self.fields[""versions""].required = True
    result = super(ReviewForm, self).is_valid()
    if result:
        self.helper.set_data(self.cleaned_data)
    return result
","if not action . get ( ""comments"" , True ) :",136
"def lambda_handler(event, context):
    try:
        if debug:
            print(""Connect to Redshift: %s"" % host)
        conn = pg8000.connect(
            database=database,
            user=user,
            password=password,
            host=host,
            port=port,
            ssl=ssl,
        )
    except:
        print(""Redshift Connection Failed: exception %s"" % sys.exc_info()[1])
        return ""Failed""
    if debug:
        print(""Succesfully Connected Redshift Cluster"")
    # Collect Query Monitoring Rule metrics and Publish to SNS topic
    query_redshift(conn)
    conn.close()
    return ""Finished""
",if debug :,184
"def parse(self, source, parser=None):
    close_source = False
    if not hasattr(source, ""read""):
        source = open(source, ""rb"")
        close_source = True
    try:
        if not parser:
            parser = XMLParser(target=TreeBuilder())
        while 1:
            data = source.read(65536)
            if not data:
                break
            parser.feed(data)
        self._root = parser.close()
        return self._root
    finally:
        if close_source:
            source.close()
",if close_source :,155
"def set_metadata_keywords(self, toc_id, kws):
    with self._lock:
        old_fpath = self._lookup(toc_id)
        new_fpath = old_fpath.rsplit(self.colon, 1)[0]
        flags = """".join(sorted([k[0] for k in kws]))
        if flags:
            new_fpath += ""%s2,%s"" % (self.colon, flags)
            if new_fpath != old_fpath:
                os.rename(
                    os.path.join(self._path, old_fpath),
                    os.path.join(self._path, new_fpath),
                )
                self._toc[toc_id] = new_fpath
",if new_fpath != old_fpath :,191
"def check_health(self):
    ""Check the health of the connection with a PING/PONG""
    if self.health_check_interval and time() > self.next_health_check:
        try:
            self.send_command(""PING"", check_health=False)
            if nativestr(self.read_response()) != ""PONG"":
                raise ConnectionError(""Bad response from PING health check"")
        except (ConnectionError, TimeoutError):
            self.disconnect()
            self.send_command(""PING"", check_health=False)
            if nativestr(self.read_response()) != ""PONG"":
                raise ConnectionError(""Bad response from PING health check"")
","if nativestr ( self . read_response ( ) ) != ""PONG"" :",177
"def ValidateStopLongitude(self, problems):
    if self.stop_lon is not None:
        value = self.stop_lon
        try:
            if not isinstance(value, (float, int)):
                self.stop_lon = util.FloatStringToFloat(value, problems)
        except (ValueError, TypeError):
            problems.InvalidValue(""stop_lon"", value)
            del self.stop_lon
        else:
            if self.stop_lon > 180 or self.stop_lon < -180:
                problems.InvalidValue(""stop_lon"", value)
",if self . stop_lon > 180 or self . stop_lon < - 180 :,152
"def get_closest(cls, *locale_codes: str) -> ""Locale"":
    """"""Returns the closest match for the given locale code.""""""
    for code in locale_codes:
        if not code:
            continue
        code = code.replace(""-"", ""_"")
        parts = code.split(""_"")
        if len(parts) > 2:
            continue
        elif len(parts) == 2:
            code = parts[0].lower() + ""_"" + parts[1].upper()
        if code in _supported_locales:
            return cls.get(code)
        if parts[0].lower() in _supported_locales:
            return cls.get(parts[0].lower())
    return cls.get(_default_locale)
",if code in _supported_locales :,184
"def insistent_bucket_delete(conn, bucket_name, keys):
    bucket = conn.lookup(bucket_name)
    if bucket:
        # Delete key names passed by the test code.
        _delete_keys(bucket, keys)
    while True:
        try:
            conn.delete_bucket(bucket_name)
        except boto.exception.S3ResponseError as e:
            if e.status == 404:
                # Create not yet visible, but it just happened above:
                # keep trying.  Potential consistency.
                continue
            else:
                raise
        break
",if e . status == 404 :,161
"def echo(socket, address):
    print(""New connection from %s:%s"" % address)
    socket.sendall(b""Welcome to the echo server! Type quit to exit.\r\n"")
    # using a makefile because we want to use readline()
    rfileobj = socket.makefile(mode=""rb"")
    while True:
        line = rfileobj.readline()
        if not line:
            print(""client disconnected"")
            break
        if line.strip().lower() == b""quit"":
            print(""client quit"")
            break
        socket.sendall(line)
        print(""echoed %r"" % line)
    rfileobj.close()
",if not line :,168
"def parse_messages(self, data):
    buf = BytesIO(data)
    msgs = []
    while buf.tell() < buf.len:
        token = buf.read(1)
        if token == b""+"":
            continue
        if token == b""-"":
            raise NotImplementedError(""Resend packet"")
        if token == b""$"":
            packet_data = b""""
            c = buf.read(1)
            while c != b""#"":
                packet_data += c
                c = buf.read(1)
            checksum = buf.read(2)
            if checksum != self.compute_checksum(packet_data):
                raise ValueError(""Incorrect checksum"")
            msgs.append(packet_data)
    return msgs
","if token == b""+"" :",200
"def _cancel_job_by_id(api: CustomObjectsApi, namespace: str, job_id: str):
    try:
        api.delete_namespaced_custom_object(
            **_crd_args(namespace),
            name=_job_id_to_resource_name(job_id),
        )
    except client.ApiException as e:
        if e.status == 404:
            return None
        else:
            raise
",if e . status == 404 :,117
"def __next__(self):
    try:
        self.dt = advance_iterator(self.gen)
    except StopIteration:
        if self.genlist[0] is self:
            heapq.heappop(self.genlist)
        else:
            self.genlist.remove(self)
            heapq.heapify(self.genlist)
",if self . genlist [ 0 ] is self :,94
"def addPoint(self, x, y):
    if self._n > 0:
        if self._last != 0:
            pctChange = (y - self._last) / self._last
        else:
            pctChange = 0
        self._sum_pct_change += pctChange
        self._sum_pct_change_abs += abs(pctChange)
        self._window.append((x, pctChange))
    self._n += 1
    self._last = y
    if len(self._window) == self._windowSize:
        self.removePoint(*self._window.popleft())
",if self . _last != 0 :,149
"def html_rewriter(self):
    embed_rules = {}
    link_rules = {}
    for rule in self.converter_rules:
        if isinstance(rule, EmbedTypeRule):
            embed_rules[rule.embed_type] = rule.handler.expand_db_attributes
        elif isinstance(rule, LinkTypeRule):
            link_rules[rule.link_type] = rule.handler.expand_db_attributes
    return MultiRuleRewriter([LinkRewriter(link_rules), EmbedRewriter(embed_rules)])
","elif isinstance ( rule , LinkTypeRule ) :",124
"def get_entities(f):
    entities = []
    for line in f:
        if not line.strip() or line[0] != ""T"":
            continue
        parts = line.split(""\t"")[1].split()
        entity_type = parts[0]
        char_offsets = "" "".join(parts[1:])
        for start_end in char_offsets.split("";""):
            start, end = start_end.split("" "")
            entities += [Entity((int(start), int(end)), entity_type)]
    return entities
","if not line . strip ( ) or line [ 0 ] != ""T"" :",139
"def _return_dbus_error(invocation, data):
    if isinstance(data, DbusError):
        invocation.return_dbus_error(data.name, data.message)
    else:
        if isinstance(data, Exception):
            et, ev, etb = sys.exc_info()
            if ev is data:
                message = """".join(traceback.format_exception(et, ev, etb))
            else:
                message = """".join(traceback.format_exception_only(data.__class__, data))
        else:
            message = str(data)
        invocation.return_error_literal(
            Gio.dbus_error_quark(), Gio.DBusError.FAILED, message
        )
","if isinstance ( data , Exception ) :",191
"def connect_skipped_ops(context):
    nn_spec = context.builder.nn_spec
    for layer in nn_spec.layers:
        for i, inp_name in enumerate(layer.input):
            if inp_name in context.skip_map_names:
                layer.input[i] = context.skip_map_names[inp_name]
        for i, out_name in enumerate(layer.output):
            if out_name in context.skip_map_names:
                layer.output[i] = context.skip_map_names[out_name]
",if inp_name in context . skip_map_names :,148
"def check_splitter(command):
    """"""Check xld or shntool installed""""""
    try:
        env = os.environ.copy()
        if ""xld"" in command:
            env[""PATH""] += os.pathsep + ""/Applications""
        elif headphones.CONFIG.CUE_SPLIT_FLAC_PATH:
            command = os.path.join(headphones.CONFIG.CUE_SPLIT_SHNTOOL_PATH, ""shntool"")
        devnull = open(os.devnull)
        subprocess.Popen(
            [command], stdout=devnull, stderr=devnull, env=env
        ).communicate()
    except OSError as e:
        if e.errno == os.errno.ENOENT:
            return False
    return True
",elif headphones . CONFIG . CUE_SPLIT_FLAC_PATH :,189
"def apply_env_options(self):
    ""apply options passed through environment variables""
    env_options = filter(self.is_flower_envvar, os.environ)
    for env_var_name in env_options:
        name = env_var_name.replace(self.ENV_VAR_PREFIX, """", 1).lower()
        value = os.environ[env_var_name]
        try:
            option = options._options[name]
        except KeyError:
            option = options._options[name.replace(""_"", ""-"")]
        if option.multiple:
            value = [option.type(i) for i in value.split("","")]
        else:
            value = option.type(value)
        setattr(options, name, value)
",if option . multiple :,190
"def file_reader(file_path: str):
    length = int(os.stat(file_path).st_size)
    offset = 0
    while offset < length:
        bytes_to_read = min((length - offset), MAX_BLOB_SIZE - 1)
        if not bytes_to_read:
            break
        blob_bytes = await asyncio.get_event_loop().run_in_executor(
            None, read_bytes, file_path, offset, bytes_to_read
        )
        yield blob_bytes
        offset += bytes_to_read
",if not bytes_to_read :,145
"def is_macro_call(form, ctx):
    if rt.seq_QMARK_(form) is true and isinstance(rt.first(form), symbol.Symbol):
        name = rt.name(rt.first(form))
        if resolve_local(ctx, name):
            return None
        var = resolve_var(ctx, rt.first(form))
        if var and var.is_defined():
            val = var.deref()
            if isinstance(val, code.BaseCode) and val.is_macro():
                return val
    return None
","if isinstance ( val , code . BaseCode ) and val . is_macro ( ) :",143
"def read_password():
    while True:
        first = getpass.getpass(""password: "")
        if len(first) < 8:
            print(""Passwords must be at least eight characters."")
            continue
        second = getpass.getpass("" (again): "")
        if first != second:
            print(""Passwords not the same. Try again."")
            continue
        break
    return first
",if first != second :,101
"def _unpack_opargs(code):
    extended_arg = 0
    for i in range(0, len(code), 2):
        op = code[i]
        if op >= HAVE_ARGUMENT:
            arg = code[i + 1] | extended_arg
            extended_arg = (arg << 8) if op == EXTENDED_ARG else 0
        else:
            arg = None
        yield (i, op, arg)
",if op >= HAVE_ARGUMENT :,111
"def test_client_path(datasette, prefix, expected_path):
    original_base_url = datasette._settings[""base_url""]
    try:
        if prefix is not None:
            datasette._settings[""base_url""] = prefix
        response = await datasette.client.get(""/asgi-scope"")
        path = response.json()[""path""]
        assert path == expected_path
    finally:
        datasette._settings[""base_url""] = original_base_url
",if prefix is not None :,122
"def to_dict(self):
    ret = {}
    for attr in self.__slots__:
        value = getattr(self, attr, None)
        if value is None:
            continue
        if isinstance(value, dict) and len(value) == 0:
            continue
        ret[attr] = value
    ret[""type""] = int(self.type)
    if self.emoji:
        ret[""emoji""] = self.emoji.to_dict()
    return ret
","if isinstance ( value , dict ) and len ( value ) == 0 :",120
"def test_opdm_to_ohdm_mapping():
    db = opdm_to_ohdm_mapping(6)
    for dbe in db:
        assert isinstance(dbe, DualBasisElement)
        assert set(dbe.primal_tensors_names) == {""ck"", ""kc""}
        if len(dbe.primal_tensors_names) == 4:
            assert np.allclose(dbe.primal_coeffs, 0.5)
            assert np.isclose(dbe.dual_scalar, 0.0)
        elif len(dbe.primal_tensors_names) == 2:
            assert np.allclose(dbe.primal_coeffs, 1.0)
            assert np.isclose(dbe.dual_scalar, 1.0)
",if len ( dbe . primal_tensors_names ) == 4 :,196
"def replaceDerived(startSite=self):
    if not allDerived:
        return
    for derivedSite in startSite.derivation.chain():
        for subsite in derivedSite.recurse(streamsOnly=True, includeSelf=True):
            if subsite in target.sites:
                subsite.replace(target, replacement, recurse=recurse, allDerived=False)
",if subsite in target . sites :,94
"def decode(self, session_data):
    encoded_data = base64.b64decode(force_bytes(session_data))
    try:
        # could produce ValueError if there is no ':'
        hash, pickled = encoded_data.split(b"":"", 1)
        expected_hash = self._hash(pickled)
        if not constant_time_compare(hash.decode(), expected_hash):
            raise SuspiciousOperation(""Session data corrupted"")
        else:
            return pickle.loads(pickled)
    except Exception:
        # ValueError, SuspiciousOperation, unpickling exceptions. If any of
        # these happen, just return an empty dictionary (an empty session).
        return {}
","if not constant_time_compare ( hash . decode ( ) , expected_hash ) :",170
"def _change_case(col: str, case_type: str) -> str:
    """"""Change case of a column name.""""""
    case_types = [""preserve"", ""upper"", ""lower"", ""snake""]
    if case_type.lower() not in case_types:
        raise JanitorError(f""case_type must be one of: {case_types}"")
    if case_type.lower() != ""preserve"":
        if case_type.lower() == ""upper"":
            col = col.upper()
        elif case_type.lower() == ""lower"":
            col = col.lower()
        elif case_type.lower() == ""snake"":
            col = _camel2snake(col)
    return col
","elif case_type . lower ( ) == ""snake"" :",182
"def eval_sexp_str(sexp_str):
    sr = SexpReader(InputPort(StringIO(sexp_str)))
    while True:
        sexp = sr.get_sexp()
        if sexp is None:
            break
        if sexp is EOF:
            break
        if sexp is COMMENT:
            continue
        py_ast = translator.translate_sexp(tuple_it(sexp))
        if py_ast is not None:
            code = compile(py_ast, ""<string>"", ""exec"")
            if code is not None:
                exec(code, global_env)
",if py_ast is not None :,163
"def refresh(self):
    """"""Get default value from a boto3 call for free tier instance type.""""""
    if not self.value:
        scheduler = self.pcluster_config.get_section(""cluster"").get_param_value(
            ""scheduler""
        )
        if scheduler:
            self.value = (
                ""optimal"" if scheduler == ""awsbatch"" else get_default_instance_type()
            )
",if scheduler :,111
"def split_row(row):
    new_row_0 = []
    new_row_1 = []
    for index, el in enumerate(row):
        if index in target_columns_0:
            new_row_0.append(el)
        elif index in target_columns_1:
            new_row_1.append(el)
        else:
            new_row_0.append(el)
            new_row_1.append(el)
    return [new_row_0, new_row_1]
",if index in target_columns_0 :,138
"def connector(sender, instance, created, **kw):
    # Only email new announcements in a group. We don't want to email everyone.
    if created and instance.group:
        from kitsune.announcements.tasks import send_group_email
        now = datetime.now()
        if instance.is_visible():
            send_group_email.delay(instance.pk)
        elif now < instance.show_after:
            send_group_email.delay(instance.pk, eta=instance.show_after)
",elif now < instance . show_after :,133
"def addon_requirements(ctx):
    """"""Install all addon requirements.""""""
    for directory in os.listdir(settings.ADDON_PATH):
        path = os.path.join(settings.ADDON_PATH, directory)
        requirements_file = os.path.join(path, ""requirements.txt"")
        if os.path.isdir(path) and os.path.isfile(requirements_file):
            print(""Installing requirements for {0}"".format(directory))
            ctx.run(
                pip_install(requirements_file, constraints_file=CONSTRAINTS_PATH),
                echo=True,
            )
    print(""Finished installing addon requirements"")
",if os . path . isdir ( path ) and os . path . isfile ( requirements_file ) :,169
"def _get_command_help(self, commands):
    help_str = """"
    for name, mod in commands.items():
        mod_help = self._get_help(mod)
        if not mod_help:
            help_str += f""{name}\n""
        else:
            help_str += f""{name} - {mod_help}\n""
    return help_str.strip()
",if not mod_help :,103
"def stringify(self, token_indices):
    # Used in metric reporter to convert from tokens to string
    res = """"
    if hasattr(self, ""vocab""):
        res = "" "".join([self.vocab._vocab[index] for index in token_indices])
        if hasattr(self, ""tokenizer""):
            if hasattr(self.tokenizer, ""decode""):
                res = self.tokenizer.decode(res)
    return res
","if hasattr ( self . tokenizer , ""decode"" ) :",106
"def load_info(cls, path, load_model_if_required=True) -> dict:
    load_path = path + cls.model_info_name
    try:
        return load_pkl.load(path=load_path)
    except:
        if load_model_if_required:
            model = cls.load(path=path, reset_paths=True)
            return model.get_info()
        else:
            raise
",if load_model_if_required :,115
"def to_dict(self):
    d = super().to_dict()
    d[""type""] = ""select""
    d[""data""] = []
    if self.user and self.security:
        for a in self.user.addresses:
            if not a.deleted:
                d[""data""].append(
                    {""label"": a.desc, ""value"": self.security.encode_id(a.id)}
                )
    return d
",if not a . deleted :,118
"def to_string(self):
    """"""Get the string code for this asset. Even for remote assets.""""""
    if self._source_str is None:
        if callable(self._source):
            self._source_str = self._source()
            if not isinstance(self._source_str, str):
                t = ""Source function of asset %r did not return a str, but a %s.""
                raise ValueError(t % (self.name, self._source.__class__.__name__))
        elif self._remote:
            self._source_str = self._get_from_url(self._source)
        else:  # pragma: no cover
            assert False, ""This should not happen""
    return self._source_str
","if not isinstance ( self . _source_str , str ) :",185
"def get_command_modules_paths(include_prefix=False):
    glob_pattern = os.path.normcase(
        ""/src/command_modules/{}*/setup.py"".format(COMMAND_MODULE_PREFIX)
    )
    for path in glob.glob(get_repo_root() + glob_pattern):
        folder = os.path.dirname(path)
        name = os.path.basename(folder)
        if not include_prefix:
            name = name[len(COMMAND_MODULE_PREFIX) :]
        yield name, folder
",if not include_prefix :,133
"def remove_firewall_rule(
    self,
    rule=""allowedprogram"",
    name=""Firewall"",
    mode=""ENABLE"",
    program=sys.executable,
    **kwargs
):
    netsh_args = {""program"": program}
    netsh_args.update(kwargs)
    try:
        if _run_netsh_cmd(""firewall delete %s"" % rule, netsh_args):
            if name in self._rules:
                del self._rules[name]
            return True
        else:
            return False
    except:
        return None
","if _run_netsh_cmd ( ""firewall delete %s"" % rule , netsh_args ) :",149
"def check_ini_installed():
    """"""Raise if no GNOME Shell ini file for Quod Libet is found""""""
    quodlibet_installed = False
    for path in get_gs_provider_files():
        try:
            with open(path, ""rb"") as handle:
                data = handle.read().decode(""utf-8"", ""replace"")
                if SearchProvider.BUS_NAME in data:
                    quodlibet_installed = True
                    break
        except EnvironmentError:
            pass
    if not quodlibet_installed:
        raise PluginImportException(
            _(""No GNOME Shell search provider for "" ""Quod Libet installed."")
        )
",if SearchProvider . BUS_NAME in data :,182
"def delete_rules(chain_name, rules):
    log.debug(f""deleting rules from {chain_name}: {rules}"")
    table = iptc.Table(iptc.Table.FILTER)
    with iptables_txn(table):
        chain = iptc.Chain(table, chain_name)
        for potential_rule in chain.rules:
            if Rule.from_iptc(potential_rule) in rules:
                chain.delete_rule(potential_rule)
",if Rule . from_iptc ( potential_rule ) in rules :,119
"def __gt__(self, other):
    if isinstance(other, tuple):
        return super().__gt__(other)
    try:
        if self.atEnd == 1 and other != INFINITY:
            return True
        elif self.atEnd == 1:
            return False
        else:
            return self.offset > other
    except ValueError:
        return NotImplemented
",elif self . atEnd == 1 :,98
"def test_related_objects_include_hidden_local_only(self):
    result_key = ""get_all_related_objects_with_model_hidden_local""
    for model, expected in TEST_RESULTS[result_key].items():
        objects = [
            (field, self._model(model, field))
            for field in model._meta.get_fields(
                include_hidden=True, include_parents=False
            )
            if field.auto_created and not field.concrete
        ]
        self.assertEqual(
            sorted(self._map_names(objects), key=self.key_name),
            sorted(expected, key=self.key_name),
        )
",if field . auto_created and not field . concrete,183
"def _cpu_stats(self):
    import psutil
    percents = psutil.cpu_percent(None, True)
    stats = {}
    if self._cpu_percent_init:
        i = 0
        for percent in percents:
            stats[""sys/cpu%i/util"" % i] = percent / 100
            i += 1
        if percents:
            stats[""sys/cpu/util""] = sum(percents) / len(percents) / 100
    self._cpu_percent_init = True
    return stats
",if percents :,130
"def select(self, limit=0):
    """"""Match all tags under the targeted tag.""""""
    if limit < 1:
        limit = None
    for child in self.get_descendants(self.tag):
        if self.match(child):
            yield child
            if limit is not None:
                limit -= 1
                if limit < 1:
                    break
",if limit < 1 :,100
"def iterate(self, handle):
    """"""Iterate over the records in the XML file.""""""
    parser = self.parser
    content_handler = parser.getContentHandler()
    records = content_handler.records
    BLOCK = self.BLOCK
    while True:
        if len(records) > 1:
            # Then at least the first record is finished
            record = records.pop(0)
            yield record
        # Read in another block of the file...
        text = handle.read(BLOCK)
        if not text:
            break
        parser.feed(text)
    # We have reached the end of the XML file;
    # send out the remaining records
    yield from records
    records.clear()
    parser.close()
",if len ( records ) > 1 :,184
"def append(self, frame, ts=None):
    assert ts is None or len(self._frame_ts) == 0 or ts > self._frame_ts[-1]
    self._frames.append(frame)
    self._next_frame_to_read = len(self._frames)
    if ts is None:
        if len(self._frame_ts) > 0:
            self._frame_ts.append(self._frame_ts[-1] + 1000.0 / self.fps)
        else:
            self._frame_ts.append(0.0)
    else:
        self._frame_ts.append(ts)
",if len ( self . _frame_ts ) > 0 :,157
"def map_volumes_to_size(volumes):
    primary_manifestations = {}
    for volume in volumes:
        if volume.node_id == self.volume_service.node_id:
            # FLOC-1240 non-primaries should be added in too
            path = volume.get_filesystem().get_path()
            primary_manifestations[path] = (
                volume.name.dataset_id,
                volume.size.maximum_size,
            )
    return primary_manifestations
",if volume . node_id == self . volume_service . node_id :,133
"def setUp(self):
    super(CompleterTest, self).setUp()
    # directories must end with os.sep for completer to
    # search inside the directory for possible completions
    if self.tempdir[-1] != os.sep:
        self.tempdir += os.sep
    self.paths = []  # type: List[str]
    # create some files and directories in temp_dir
    for c in string.ascii_lowercase:
        path = os.path.join(self.tempdir, c)
        self.paths.append(path)
        if ord(c) % 2:
            filesystem.mkdir(path)
        else:
            with open(path, ""w""):
                pass
",if ord ( c ) % 2 :,174
"def pretty_list(items, cols=3):
    text = []
    width = 24
    col_width = u""{"" + u"":<"" + str(width) + u""} ""
    for i, lang in enumerate(items):
        lang = lang.decode(u""utf-8"")
        if len(lang) > width:
            lang = lang[: width - 3] + ""...""
        text.append(u""{:>3}. "".format(i + 1))
        text.append(col_width.format(lang))
        if (i + 1) % cols == 0:
            text.append(u""\n"")
    return u"""".join(text)
",if ( i + 1 ) % cols == 0 :,165
"def to_representation(self, obj):
    serializer_class = self.get_sub_serializer(obj)
    if serializer_class:
        serializer = serializer_class(instance=obj, context=self.context)
        # preserve links for list view
        if self.parent:
            serializer.parent = self.parent
            serializer.polymorphic_base = self
            # capabilities prefetch is only valid for these models
            if isinstance(obj, (JobTemplate, WorkflowJobTemplate)):
                serializer.capabilities_prefetch = self._capabilities_prefetch
            else:
                serializer.capabilities_prefetch = None
        return serializer.to_representation(obj)
    else:
        return super(UnifiedJobTemplateSerializer, self).to_representation(obj)
","if isinstance ( obj , ( JobTemplate , WorkflowJobTemplate ) ) :",193
"def state_DEVAD(self, mdio):
    if self.devad == -1:
        self.devad = 0
        if self.clause45:
            prtad = [""PRTAD: %02d"" % self.portad, ""PRT"", ""P""]
        else:
            prtad = [""PHYAD: %02d"" % self.portad, ""PHY"", ""P""]
        self.putff([2, prtad])
        self.ss_frame_field = self.samplenum
    self.devad_bits -= 1
    self.devad |= mdio << self.devad_bits
    if not self.devad_bits:
        self.state = ""TA""
",if self . clause45 :,183
"def colorDiffRow(self, c, row, v):
    if not row:
        return
    baseval = row[self.basenum + 1]
    for c in self.columns[1:]:
        v = c.getValue(row)
        if v != baseval:
            if baseval is None:
                return ""green""  # addition
            elif v is None:
                return ""red""  # deletion
            else:
                return ""yellow""  # difference
",if baseval is None :,131
"def _get_ntp_entity(self, peer_type):
    ntp_entities = {}
    command = ""show ntp peers""
    output = self._send_command(command)
    for line in output.splitlines():
        # Skip first two lines and last line of command output
        if line == """" or ""-----"" in line or ""Peer IP Address"" in line:
            continue
        elif IPAddress(len(line.split()[0])).is_unicast:
            peer_addr = line.split()[0]
            ntp_entities[peer_addr] = {}
        else:
            raise ValueError(""Did not correctly find a Peer IP Address"")
    return ntp_entities
","if line == """" or ""-----"" in line or ""Peer IP Address"" in line :",176
"def getExpandedPath(playerPath):
    if not os.path.isfile(playerPath):
        if os.path.isfile(playerPath + ""mplayer.exe""):
            playerPath += ""mplayer.exe""
            return playerPath
        elif os.path.isfile(playerPath + ""\\mplayer.exe""):
            playerPath += ""\\mplayer.exe""
            return playerPath
    if os.access(playerPath, os.X_OK):
        return playerPath
    for path in os.environ[""PATH""].split("":""):
        path = os.path.join(os.path.realpath(path), playerPath)
        if os.access(path, os.X_OK):
            return path
","elif os . path . isfile ( playerPath + ""\\mplayer.exe"" ) :",181
"def main():
    args = parser.parse_args()
    print_arguments(args)
    check_gpu(args.use_gpu)
    if args.profile:
        if args.use_gpu:
            with profiler.cuda_profiler(""cuda_profiler.txt"", ""csv"") as nvprof:
                train(args)
        else:
            with profiler.profiler(""CPU"", sorted_key=""total"") as cpuprof:
                train(args)
    else:
        train(args)
",if args . use_gpu :,134
"def add_actions(self, lst, sched_id):
    for a in lst:
        # First we look if we do not already have it, if so
        # do nothing, we are already working!
        if a.id in self.schedulers[sched_id][""actions""]:
            continue
        a.sched_id = sched_id
        a.status = ""queue""
        self.assign_to_a_queue(a)
","if a . id in self . schedulers [ sched_id ] [ ""actions"" ] :",114
"def _create_data_from_iob(data_path, separator):
    with open(data_path, encoding=""utf-8"") as input_file:
        columns = []
        for line in input_file:
            line = line.strip()
            if line == """":
                if columns:
                    yield columns
                columns = []
            else:
                for i, column in enumerate(line.split(separator)):
                    if len(columns) < i + 1:
                        columns.append([])
                    columns[i].append(column)
        if len(columns) > 0:
            yield columns
",if len ( columns ) > 0 :,177
"def _mock_set_magics(self):
    these_magics = _magics
    if self._mock_methods is not None:
        these_magics = _magics.intersection(self._mock_methods)
        remove_magics = set()
        remove_magics = _magics - these_magics
        for entry in remove_magics:
            if entry in type(self).__dict__:
                # remove unneeded magic methods
                delattr(self, entry)
    # don't overwrite existing attributes if called a second time
    these_magics = these_magics - set(type(self).__dict__)
    _type = type(self)
    for entry in these_magics:
        setattr(_type, entry, MagicProxy(entry, self))
",if entry in type ( self ) . __dict__ :,194
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 10:
            self.add_blob_key(d.getPrefixedString())
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,92
"def restart_with_reloader(argv):
    args = [sys.executable] + argv
    while True:
        new_environ = os.environ.copy()
        new_environ[""RUN_MAIN_CELERY""] = ""true""
        exit_code = subprocess.call(args, env=new_environ)
        if exit_code != 3:
            return exit_code
",if exit_code != 3 :,96
"def batch_load_fn(self, keys: Iterable[K]) -> Promise[List[R]]:
    with opentracing.global_tracer().start_active_span(
        self.__class__.__name__
    ) as scope:
        span = scope.span
        span.set_tag(opentracing.tags.COMPONENT, ""dataloaders"")
        results = self.batch_load(keys)
        if not isinstance(results, Promise):
            return Promise.resolve(results)
        return results
","if not isinstance ( results , Promise ) :",119
"def set_path(self):
    """"""Set consoles PYTHONPATH if changed by the user""""""
    from spyderlib.widgets.externalshell import pythonshell
    for sw in self.shellwidgets:
        if isinstance(sw, pythonshell.ExternalPythonShell):
            if sw.is_interpreter and sw.is_running():
                sw.path = self.main.get_spyder_pythonpath()
                sw.shell.path = sw.path
",if sw . is_interpreter and sw . is_running ( ) :,114
"def testCopyAndDeepcopy(self):
    # Test copying all objects defined in this module
    import copy
    import sys
    import types
    for part in sys.modules[self.__module__].__dict__:
        match = False
        for skip in [""_"", ""__"", ""Test"", ""Exception"", ""MotionType""]:
            if part.startswith(skip) or part.endswith(skip):
                match = True
        if match:
            continue
        obj = getattr(sys.modules[self.__module__], part)
        # noinspection PyTypeChecker
        if callable(obj) and not isinstance(obj, types.FunctionType):
            copy.copy(obj)
            copy.deepcopy(obj)
",if part . startswith ( skip ) or part . endswith ( skip ) :,174
"def list_themes(v=False):
    """"""Display the list of the themes""""""
    for t, l in themes():
        if not v:
            t = os.path.basename(t)
        if l:
            if v:
                print(t + ("" (symbolic link to `"" + l + ""')""))
            else:
                print(t + ""@"")
        else:
            print(t)
",if l :,113
"def _hasIPv6(self):  # pragma: nocover
    if not socket.has_ipv6:
        return False
    try:
        socket.getaddrinfo(
            ""::1"",
            0,
            socket.AF_UNSPEC,
            socket.SOCK_STREAM,
            socket.IPPROTO_TCP,
            socket.AI_PASSIVE | socket.AI_ADDRCONFIG,
        )
        return True
    except socket.gaierror as e:
        # Check to see what the error is
        if e.errno == socket.EAI_ADDRFAMILY:
            return False
        else:
            raise e
",if e . errno == socket . EAI_ADDRFAMILY :,170
"def check_distribution_strategy():
    if tf.distribute.has_strategy():
        strategy = tf.distribute.get_strategy()
        if not isinstance(strategy, _SUPPORTED_STRATEGIES):
            raise ValueError(
                ""Sonnet optimizers are not compatible with `{}`. ""
                ""Please use one of `{}` instead."".format(
                    strategy.__class__.__name__,
                    ""`, `"".join(s.__name__ for s in _SUPPORTED_STRATEGIES),
                )
            )
","if not isinstance ( strategy , _SUPPORTED_STRATEGIES ) :",132
"def _process_attributes(self, node, items):
    attributes = []
    for child in items:
        if child.tag in (tags.attribute, tags.attributeGroup, tags.anyAttribute):
            attribute = self.process(child, node)
            attributes.append(attribute)
        else:
            raise self._create_error(""Unexpected tag `%s`"" % (child.tag), node)
    return attributes
","if child . tag in ( tags . attribute , tags . attributeGroup , tags . anyAttribute ) :",105
"def _verify_signature(
    self, payload, signing_input, header, signature, key="""", algorithms=None
):
    alg = header.get(""alg"")
    if algorithms is not None and alg not in algorithms:
        raise InvalidAlgorithmError(""The specified alg value is not allowed"")
    try:
        alg_obj = self._algorithms[alg]
        key = alg_obj.prepare_key(key)
        if not alg_obj.verify(signing_input, key, signature):
            raise InvalidSignatureError(""Signature verification failed"")
    except KeyError:
        raise InvalidAlgorithmError(""Algorithm not supported"")
","if not alg_obj . verify ( signing_input , key , signature ) :",147
"def visit_Import(self, node):
    # type: (ast.Import) -> None
    for child in node.names:
        if isinstance(child, ast.alias):
            import_name = child.name
            if import_name == self._SDK_PACKAGE:
                self._set_inferred_type_for_name(import_name, Boto3ModuleType())
    self.generic_visit(node)
","if isinstance ( child , ast . alias ) :",105
"def GeneratePageMetatadata(self, proc):
    address_space = self.session.GetParameter(""default_address_space"")
    for map in proc.task.map.hdr.walk_list(""links.next"", include_current=False):
        start = map.links.start
        end = map.links.end
        # Skip the entire region.
        if end < self.plugin_args.start:
            continue
        # Done.
        if start > self.plugin_args.end:
            break
        for vaddr in utils.xrange(start, end, 0x1000):
            if self.plugin_args.start <= vaddr <= self.plugin_args.end:
                yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))
",if start > self . plugin_args . end :,200
"def subhelp_macro(self, macro_name):
    if macro_name in self.macros.keys():
        macro_def = self.macros[macro_name]
        if ""\n"" in macro_def:
            self.log(""Macro '"" + macro_name + ""' defined as:"")
            self.log(self.macros[macro_name] + ""----------------"")
        else:
            self.log(""Macro '"" + macro_name + ""' defined as: '"" + macro_def + ""'"")
    else:
        self.logError(""Macro '"" + macro_name + ""' is not defined"")
","if ""\n"" in macro_def :",147
"def get_path_pairs(folder, split_f):
    img_paths = []
    mask_paths = []
    with open(split_f, ""r"") as lines:
        for line in tqdm(lines):
            ll_str = re.split(""\t"", line)
            imgpath = os.path.join(folder, ll_str[0].rstrip())
            maskpath = os.path.join(folder, ll_str[1].rstrip())
            if os.path.isfile(maskpath):
                img_paths.append(imgpath)
                mask_paths.append(maskpath)
            else:
                print(""cannot find the mask:"", maskpath)
    return img_paths, mask_paths
",if os . path . isfile ( maskpath ) :,186
"def instant_defaults_listener(target, args, kwargs):
    for key, column in sa.inspect(target.__class__).columns.items():
        if hasattr(column, ""default"") and column.default is not None:
            if callable(column.default.arg):
                setattr(target, key, column.default.arg(target))
            else:
                setattr(target, key, column.default.arg)
",if callable ( column . default . arg ) :,107
"def stat_f(filename, ignore_EACCES=False):
    """"""Call os.stat(), but don't die if the file isn't there. Returns None.""""""
    try:
        return os.stat(filename)
    except OSError as e:
        if e.errno in (errno.ENOENT, errno.ENOTDIR):
            return None
        if ignore_EACCES and e.errno == errno.EACCES:
            return None
        raise
","if e . errno in ( errno . ENOENT , errno . ENOTDIR ) :",114
"def __init__(self, noteList):
    super().__init__()
    self._noteList = []
    for value in noteList:
        if value is None:
            self._noteList.append(None)
        elif isinstance(value, str):
            self._noteList.append(note.Note(value))
        else:
            try:
                if value.isClassOrSubclass([note.Note, pitch.Pitch]):
                    self._noteList.append(value)
            except (AttributeError, NameError):
                self._noteList.append(None)
","if value . isClassOrSubclass ( [ note . Note , pitch . Pitch ] ) :",153
"def place(btn):
    """"""get item from stack, and put it in the right place""""""
    if len(remove_stack) == 0:
        app.bell()
    else:
        f = remove_stack.pop()
        if btn == p[0]:
            f.grid(row=0, column=0, sticky=""NEWS"")
        elif btn == p[1]:
            f.grid(row=0, column=1, sticky=""NEWS"")
        elif btn == p[2]:
            f.grid(row=1, column=0, sticky=""NEWS"")
        elif btn == p[3]:
            f.grid(row=1, column=1, sticky=""NEWS"")
",elif btn == p [ 1 ] :,177
"def _apply_color_overlay(self, layer, color, shape, alpha):
    for effect in layer.effects.find(""coloroverlay""):
        color, shape_e = draw_solid_color_fill(layer.bbox, effect.value)
        color = paste(self._viewport, layer.bbox, color, 1.0)
        if shape_e is None:
            shape_e = np.ones((self.height, self.width, 1), dtype=np.float32)
        else:
            shape_e = paste(self._viewport, layer.bbox, shape_e)
        opacity = effect.opacity / 100.0
        self._apply_source(
            color, shape * shape_e, alpha * shape_e * opacity, effect.blend_mode
        )
",if shape_e is None :,197
"def processTopTree(self, p, justOneFile=False):
    current = p.copy()
    for p in current.self_and_parents():
        h = p.h
        if h.startswith(""@rst"") and not h.startswith(""@rst-""):
            self.processTree(
                p,
                ext=None,
                toString=False,
                justOneFile=justOneFile,
            )
            break
    else:
        self.processTree(
            current,
            ext=None,
            toString=False,
            justOneFile=justOneFile,
        )
    g.blue(""done"")
","if h . startswith ( ""@rst"" ) and not h . startswith ( ""@rst-"" ) :",180
"def __mul__(self, factor):
    f1 = self._value
    f2 = factor._value
    # Make sure that f2 is the smallest, to speed up the loop
    if f2 > f1:
        f1, f2 = f2, f1
    if self.irr_poly in (f1, f2):
        return _Element(0)
    mask1 = 2 ** 128
    v, z = f1, 0
    while f2:
        if f2 & 1:
            z ^= v
        v <<= 1
        if v & mask1:
            v ^= self.irr_poly
        f2 >>= 1
    return _Element(z)
",if f2 & 1 :,174
"def get_blacklist(self, guild: Optional[discord.Guild] = None) -> Set[int]:
    async with self._access_lock:
        ret: Set[int]
        gid: Optional[int] = guild.id if guild else None
        if gid in self._cached_blacklist:
            ret = self._cached_blacklist[gid].copy()
        else:
            if gid is not None:
                ret = set(await self._config.guild_from_id(gid).blacklist())
            else:
                ret = set(await self._config.blacklist())
            self._cached_blacklist[gid] = ret.copy()
        return ret
",if gid in self . _cached_blacklist :,177
"def to_internal_value(self, data):
    try:
        if isinstance(data, (list, tuple)):
            return super(StringListBooleanField, self).to_internal_value(data)
        elif data in NullBooleanField.TRUE_VALUES:
            return True
        elif data in NullBooleanField.FALSE_VALUES:
            return False
        elif data in NullBooleanField.NULL_VALUES:
            return None
        elif isinstance(data, str):
            return self.child.run_validation(data)
    except TypeError:
        pass
    self.fail(""type_error"", input_type=type(data))
",elif data in NullBooleanField . TRUE_VALUES :,159
"def execute():
    for wo in frappe.db.sql(
        """"""select name from `tabWork Order` where docstatus < 2"""""", as_dict=1
    ):
        work_order = frappe.get_doc(""Work Order"", wo.name)
        if work_order.operations:
            work_order.flags.ignore_validate_update_after_submit = True
            work_order.calculate_time()
            work_order.save()
",if work_order . operations :,120
"def return_load_handlers(filename, del_file_after_loading=True):
    if os.path.isfile(filename) and os.path.getsize(filename) > 0:
        with open(filename, ""rb"") as file:
            if apihelper.CUSTOM_SERIALIZER is None:
                handlers = pickle.load(file)
            else:
                handlers = apihelper.CUSTOM_SERIALIZER.load(file)
        if del_file_after_loading:
            os.remove(filename)
        return handlers
",if apihelper . CUSTOM_SERIALIZER is None :,139
"def setUp(self):
    BaseTestCase.setUp(self)
    self.rawData = []
    self.dataByKey = {}
    for i in range(1, 11):
        unicodeCol = u""Unicode \u3042 %d"" % i
        fixedCharCol = (u""Fixed Unicode %d"" % i).ljust(40)
        if i % 2:
            nullableCol = u""Nullable %d"" % i
        else:
            nullableCol = None
        dataTuple = (i, unicodeCol, fixedCharCol, nullableCol)
        self.rawData.append(dataTuple)
        self.dataByKey[i] = dataTuple
",if i % 2 :,165
"def upgrade(ver, session):
    if ver is None:
        columns = table_columns(""make_rss"", session)
        if ""rsslink"" not in columns:
            log.info(""Adding rsslink column to table make_rss."")
            table_add_column(""make_rss"", ""rsslink"", String, session)
        ver = 0
    return ver
","if ""rsslink"" not in columns :",92
"def setlist(self):
    self.list.LDelRow(0, 0)
    self.list.LSetDrawingMode(0)
    if self.contents:
        self.list.LAddRow(len(self.contents), 0)
        for i in range(len(self.contents)):
            v = repr(self.contents[i][0])
            if self.contents[i][1]:
                v = v + '""' + self.contents[i][1] + '""'
            self.list.LSetCell(v, (0, i))
    self.list.LSetDrawingMode(1)
    self.list.LUpdate(self.wid.GetWindowPort().visRgn)
",if self . contents [ i ] [ 1 ] :,180
"def onStatusChanged(self, provider, status, extras):
    if self.root.on_status:
        s_status = ""unknown""
        if status == 0x00:
            s_status = ""out-of-service""
        elif status == 0x01:
            s_status = ""temporarily-unavailable""
        elif status == 0x02:
            s_status = ""available""
        self.root.on_status(""provider-status"", ""{}: {}"".format(provider, s_status))
",elif status == 0x02 :,130
"def weight_initialization(self):
    for m in self.modules():
        if isinstance(m, ME.MinkowskiBatchNorm):
            nn.init.constant_(m.bn.weight, 1)
            nn.init.constant_(m.bn.bias, 0)
","if isinstance ( m , ME . MinkowskiBatchNorm ) :",69
"def _find_best_version(self):
    versions_by_size = order_versions_in_descending_order(self.available_versions)
    if self.version == ""latest"":
        self.version = versions_by_size[0]
    version_constraints = get_version_constraints(self.version)
    num_of_matches = 0
    for version in versions_by_size:
        for version_constraint in version_constraints:
            if not version_constraint.versions_matching(version):
                break
            else:
                num_of_matches += 1
        if num_of_matches == len(version_constraints):
            return version
        else:
            num_of_matches = 0
    return ""latest""
",if not version_constraint . versions_matching ( version ) :,192
"def _nickname(self, ctx: commands.Context, *, nickname: str = None):
    """"""Sets [botname]'s nickname.""""""
    try:
        if nickname and len(nickname) > 32:
            await ctx.send(
                _(""Failed to change nickname. Must be 32 characters or fewer."")
            )
            return
        await ctx.guild.me.edit(nick=nickname)
    except discord.Forbidden:
        await ctx.send(_(""I lack the permissions to change my own nickname.""))
    else:
        await ctx.send(_(""Done.""))
",if nickname and len ( nickname ) > 32 :,152
"def execute(self):
    if self._qr is None:
        if self._tuples:
            ResultWrapper = TuplesQueryResultWrapper
        elif self._dicts:
            ResultWrapper = DictQueryResultWrapper
        else:
            ResultWrapper = NaiveQueryResultWrapper
        self._qr = ResultWrapper(self.model_class, self._execute(), None)
    return self._qr
",if self . _tuples :,97
"def to_match_ip(value):
    if ""/"" in value:
        (ip_addr, ip_mask) = value.split(""/"")
        if ip_mask.isdigit():
            ip = netaddr.ip.IPNetwork(value)
            ip_addr = str(ip.ip)
            ip_mask = str(ip.netmask)
        return ip_addr, ip_mask
    return value
",if ip_mask . isdigit ( ) :,104
"def correct(self):
    for A in self.circles:
        intersects = False
        for B in self.circles:
            if A != B:
                radsq = (A.r + B.r) * (A.r + B.r)
                d = A.sqdist_o(B)
                if radsq > d:
                    intersects = True
                    break
        if not intersects:
            A.x = A.ox
            A.y = A.oy
",if A != B :,145
"def py_info_clear(self):
    """"""""""""
    py_info_folder = self.py_info_at
    with py_info_folder:
        for filename in py_info_folder.path.iterdir():
            if filename.suffix == "".json"":
                with py_info_folder.lock_for_key(filename.stem):
                    if filename.exists():
                        filename.unlink()
","if filename . suffix == "".json"" :",111
"def _dom_node(self, domroot):
    element = domroot.createElement(self.tagname)
    for attribute, value in self.attlist():
        if isinstance(value, list):
            value = "" "".join([serial_escape(""%s"" % (v,)) for v in value])
        element.setAttribute(attribute, ""%s"" % value)
    for child in self.children:
        element.appendChild(child._dom_node(domroot))
    return element
","if isinstance ( value , list ) :",116
"def remove_empty_lines(content):
    try:
        if content[0] == ""\n"":
            content = content[1 : content.rfind(""\n"")]
        if content[-1] == ""\n"":
            content = content[: content.rfind(""\n"")]
    except IndexError:
        pass
    return content
","if content [ 0 ] == ""\n"" :",81
"def expandNode(self, node):
    event = self.getEvent()
    parents = [node]
    while event:
        token, cur_node = event
        if cur_node is node:
            return
        if token != END_ELEMENT:
            parents[-1].appendChild(cur_node)
        if token == START_ELEMENT:
            parents.append(cur_node)
        elif token == END_ELEMENT:
            del parents[-1]
        event = self.getEvent()
",if token != END_ELEMENT :,130
"def iter_pairs(items, wrap, repeat=False):
    if not items:
        return
    while True:
        for i0, i1 in zip(items[:-1], items[1:]):
            yield i0, i1
        if wrap:
            yield items[-1], items[0]
        if not repeat:
            return
",if wrap :,90
"def fix_value_info(value):
    num_fixed = 0
    if value.type.HasField(""tensor_type""):
        shape = value.type.tensor_type.shape
        if shape:
            dim = shape.dim[0]
            if fix_dim(dim):
                num_fixed += 1
    return num_fixed
",if fix_dim ( dim ) :,90
"def isMasterFromActiveLogins(self, chat_id):
    with self.bot.database as conn:
        cur = conn.cursor()
        cur.execute(""select count(1) from telegram_logins where uid = ?"", [chat_id])
        res = cur.fetchone()
        if res[0] == 1:
            return True
        return False
",if res [ 0 ] == 1 :,91
"def wrapper(cached=True, reset=False):
    nonlocal cached_python_dir
    if not cached or not cached_python_dir or reset:
        python_dir = os.environ.get(""_PYTHON_DIR_"") or load_settings(lazy=True).get(
            ""python_dir""
        )
        if python_dir:  # no cov
            if python_dir == ""isolated"":
                python_dir = PYTHON_DIR_ISOLATED
            elif python_dir == ""shared"":
                python_dir = PYTHON_DIR_SHARED
        else:  # no cov
            python_dir = PYTHON_DIR_SHARED
        cached_python_dir = python_dir
    return cached_python_dir
","if python_dir == ""isolated"" :",183
"def _handle_call(self, call):
    if call is None or self.type is not MessageType.call:
        self.call = None
        return
    # we get the participant source from the mentions array or
    # the author
    participants = []
    for uid in map(int, call.get(""participants"", [])):
        if uid == self.author.id:
            participants.append(self.author)
        else:
            user = utils.find(lambda u: u.id == uid, self.mentions)
            if user is not None:
                participants.append(user)
    call[""participants""] = participants
    self.call = CallMessage(message=self, **call)
",if user is not None :,185
"def extract(self, im: np.ndarray, debug_save_name=None):
    with fluid.dygraph.guard():
        if debug_save_name is not None:
            np.savez(debug_save_name, im)
        im = n2p(im)
        output_features = self.net.extract_backbone_features(im)
        # Store the raw backbone features which are input to estimator
        output = TensorList([layer.numpy() for layer in output_features])
        return output
",if debug_save_name is not None :,130
"def parseNode(self, node):
    for child in node.childNodes:
        if child.nodeType == ELEMENT_NODE:
            if child.tagName == ""Old"":
                try:
                    self.parseOld(child.childNodes[0].nodeValue)
                except IndexError:
                    raise ValidationError(""Old cannot be empty"", ""??"")
            elif child.tagName == ""New"":
                try:
                    self.parseNew(child.childNodes[0].nodeValue)
                except IndexError:
                    raise ValidationError(""New cannot be empty"", ""??"")
","if child . tagName == ""Old"" :",162
"def _compute_bounds_2d(vals):
    minval = np.inf
    maxval = -np.inf
    for i in range(vals.shape[0]):
        for j in range(vals.shape[1]):
            v = vals[i][j]
            if not np.isnan(v):
                if v < minval:
                    minval = v
                if v > maxval:
                    maxval = v
    return minval, maxval
",if v > maxval :,122
"def truncate(s, n):
    """"""Return s truncated to n characters.""""""
    if len(s) <= n:
        return s
    else:
        s2 = s[: n - 3] + ""...(%s)"" % len(s)
        if s.endswith(""\n""):
            return s2 + ""\n""
        else:
            return s2
","if s . endswith ( ""\n"" ) :",92
"def draw(self):
    try:
        if self.env.cmd.show_obj.draw_bookmarks:
            self._draw_bookmarks()
    except AttributeError:
        if self.need_clear:
            self.win.erase()
            self.need_redraw = True
            self.need_clear = False
        DisplayableContainer.draw(self)
",if self . need_clear :,98
"def _parse_struct_fields(inner):
    fields = []
    field_tuples = _split_struct_fields(inner)
    for (name, value) in field_tuples:
        field = {}
        field[""name""] = name
        simple_type, inner = _parse_type(value)
        field[""type""] = simple_type
        if inner:
            field.update(_parse_complex(simple_type, inner))
        fields.append(field)
    return fields
",if inner :,123
"def on_task_learn(self, task, config):
    config = self.prepare_config(config)
    with Session() as session:
        for entry in task.all_entries:
            if entry.state not in config[""state""]:
                continue
            entry[""digest_task""] = task.name
            entry[""digest_state""] = entry.state
            session.add(DigestEntry(list=config[""list""], entry=entry))
","if entry . state not in config [ ""state"" ] :",115
"def async_post_installation(self):
    """"""Run post installation steps.""""""
    if self.data.config_flow:
        if self.data.full_name != ""hacs/integration"":
            await self.reload_custom_components()
        if self.data.first_install:
            self.pending_restart = False
            return
    self.pending_restart = True
","if self . data . full_name != ""hacs/integration"" :",97
"def generate_copyright(template, lang=""go""):
    if lang in [""Python"", ""shell""]:
        LANG_COMMENT_MARK = ""#""
    else:
        LANG_COMMENT_MARK = ""//""
    lines = template.split(NEW_LINE_MARK)
    BLANK = "" ""
    ans = LANG_COMMENT_MARK + BLANK + COPYRIGHT_HEADER + NEW_LINE_MARK
    for lino, line in enumerate(lines):
        if lino == 0 or lino == 1 or lino == len(lines) - 1:
            continue
        if len(line) == 0:
            BLANK = """"
        else:
            BLANK = "" ""
        ans += LANG_COMMENT_MARK + BLANK + line + NEW_LINE_MARK
    return ans + ""\n""
",if lino == 0 or lino == 1 or lino == len ( lines ) - 1 :,195
"def test_get_first_child(self, model):
    data = [
        (""2"", ""21""),
        (""21"", None),
        (""23"", ""231""),
        (""231"", None),
    ]
    for desc, expected in data:
        node = model.objects.get(desc=desc).get_first_child()
        if expected is None:
            assert node is None
        else:
            assert node.desc == expected
            assert type(node) == model
",if expected is None :,130
"def test_failures(self):
    for idx, doc in enumerate(JSONDOCS):
        idx = idx + 1
        if idx in SKIPS:
            json.loads(doc)
            continue
        try:
            json.loads(doc)
        except ValueError:
            pass
        else:
            self.fail(""Expected failure for fail%d.json: %r"" % (idx, doc))
",if idx in SKIPS :,109
"def parse_download_link(self, line, in_download):
    """"""Parse Flutter SDK download links""""""
    url = None
    in_download = False
    if ""Flutter "" in line:
        p = re.search(r""Flutter\s(\S+)"", line)
        if p is not None:
            in_download = True
    if in_download:
        with suppress(AttributeError):
            url = (
                ""https://storage.googleapis.com/flutter_infra/releases/stable/linux/""
                + ""flutter_linux_v{}-stable.tar.xz"".format(p.group(1))
            )
    return ((url, None), in_download)
",if p is not None :,180
"def yields(self):
    for h4 in self.soup.findAll(""h4""):
        for strong in h4.findAll(""strong""):
            raw_yield = strong.text
            for word in raw_yield.split():
                if word.isdigit():
                    yields = word
    return get_yields(""{} servings"".format(yields))
",if word . isdigit ( ) :,91
"def __iter__(self) -> Iterator[List[SampledData]]:
    for batch in self.data_loader:
        # batch : List[Tensor[N, C, H, W]]
        # images_batch : Tensor[N, C, H, W]
        # image : Tensor[C, H, W]
        images = [image for images_batch in batch for image in images_batch]
        if not images:
            continue
        if self.shuffle:
            random.shuffle(images)
        yield from self._produce_data(images)
",if self . shuffle :,139
"def set_cookie(self, cookie):
    """"""Set a cookie, without checking whether or not it should be set.""""""
    c = self._cookies
    self._cookies_lock.acquire()
    try:
        if cookie.domain not in c:
            c[cookie.domain] = {}
        c2 = c[cookie.domain]
        if cookie.path not in c2:
            c2[cookie.path] = {}
        c3 = c2[cookie.path]
        c3[cookie.name] = cookie
    finally:
        self._cookies_lock.release()
",if cookie . path not in c2 :,148
"def product(self, term, ordinal):
    dims = term._pyro_dims
    for dim in sorted(ordinal, reverse=True):
        pos = dims.find(dim)
        if pos != -1:
            key = ""product"", self._hash_by_id(term), dim
            if key in self._cache:
                term = self._cache[key]
            else:
                term = term.sum(pos)
                dims = dims.replace(dim, """")
                self._cache[key] = term
                term._pyro_dims = dims
    return term
",if key in self . _cache :,160
"def set_inputs(self, num_inputs):
    if num_inputs == len(self.inputs):
        return
    if num_inputs < len(self.inputs):
        while num_inputs < len(self.inputs):
            self.inputs.remove(self.inputs[-1])
    if num_inputs > len(self.inputs):
        if ""X"" not in self.inputs:
            self.inputs.new(""SvStringsSocket"", ""X"")
        if ""Y"" not in self.inputs and num_inputs == 2:
            self.inputs.new(""SvStringsSocket"", ""Y"")
        self.change_prop_type(None)
","if ""X"" not in self . inputs :",166
"def setCornerWidget(self, cornerWidget, cornerWidgetExpanded=False):
    if self.__cornerWidget is not None:
        self.removeChild(self.__cornerWidget)
    if cornerWidget is not None:
        if cornerWidgetExpanded and self.__headerLayout.stretch(1) == 1:
            self.__headerLayout.setStretch(1, 0)
        elif self.__headerLayout.stretch(1) == 0:
            self.__headerLayout.setStretch(1, 1)
        stretch = 1 if cornerWidgetExpanded else 0
        self.__headerLayout.addWidget(cornerWidget._qtWidget(), stretch)
        self.__cornerWidget = cornerWidget
        self.__cornerWidget._applyVisibility()
",elif self . __headerLayout . stretch ( 1 ) == 0 :,180
"def stop(self, kill=False, timeout=15, _=False):
    term = False
    start_time = time.time()
    timeout *= self._context.timeout_multiplier
    while self._handle and self._is_running():
        if kill:
            self._handle.kill()
        elif not term:
            self._handle.terminate()
            term = True
        time.sleep(1)
        if not kill and time.time() - start_time > timeout:
            kill = True
    if self._log:
        self._log.close()
",if kill :,147
"def filter_after(res_list, after_time):
    after_time = time_parse(after_time)
    new_res_list = []
    for res in res_list:
        if ""time_last"" in res:
            if res[""time_last""] > after_time:
                new_res_list.append(res)
        elif ""zone_time_last"" in res:
            if res[""zone_time_last""] > after_time:
                new_res_list.append(res)
        else:
            new_res_list.append(res)
    return new_res_list
","elif ""zone_time_last"" in res :",163
"def create(self, user, repo, add=False):
    try:
        group = self.gl.groups.search(user)
        data = {""name"": repo}
        if group:
            data[""namespace_id""] = group[0].id
        self.gl.projects.create(data=data)
    except GitlabCreateError as err:
        if (
            json.loads(err.response_body.decode(""utf-8""))[""message""][""name""][0]
            == ""has already been taken""
        ):
            raise ResourceExistsError(""Project already exists."") from err
        else:
            raise ResourceError(""Unhandled error."") from err
    if add:
        self.add(user=user, repo=repo, tracking=self.name)
",if group :,195
"def __init__(self, value, timestamp, attachments):
    self._value = value
    self._timestamp = timestamp
    if attachments is None:
        raise TypeError(""attachments should not be empty"")
    for key, value in attachments.items():
        if key is None or not isinstance(key, str):
            raise TypeError(
                ""attachment key should not be "" ""empty and should be a string""
            )
        if value is None or not isinstance(value, str):
            raise TypeError(
                ""attachment value should not be "" ""empty and should be a string""
            )
    self._attachments = attachments
","if value is None or not isinstance ( value , str ) :",158
"def valid_stream_name_or_error(name: str):
    try:
        if not name:
            raise Exception(""Stream name cannot be blank."")
        parsed = URL.parse(name)
        if parsed.has_channel:
            raise Exception(
                ""Stream names cannot start with '@' symbol. This is reserved for channels claims.""
            )
        if not parsed.has_stream or parsed.stream.name != name:
            raise Exception(""Stream name has invalid characters."")
    except (TypeError, ValueError):
        raise Exception(""Invalid stream name."")
",if not name :,144
"def openoutputfile(self, options, fulloutputpath):
    """"""Opens the output file.""""""
    if self.isarchive(options.output, ""output""):
        outputstream = options.outputarchive.openoutputfile(fulloutputpath)
        if outputstream is None:
            self.warning(
                ""Could not find where to put %s in output ""
                ""archive; writing to tmp"" % fulloutputpath
            )
            return StringIO()
        return outputstream
    else:
        return super(ArchiveConvertOptionParser, self).openoutputfile(
            options, fulloutputpath
        )
",if outputstream is None :,161
"def _response(self, request_number, t, *arg):
    msg = Message()
    msg.add_int(request_number)
    for item in arg:
        if isinstance(item, long):
            msg.add_int64(item)
        elif isinstance(item, int):
            msg.add_int(item)
        elif isinstance(item, (string_types, bytes_types)):
            msg.add_string(item)
        elif type(item) is SFTPAttributes:
            item._pack(msg)
        else:
            raise Exception(""unknown type for {!r} type {!r}"".format(item, type(item)))
    self._send_packet(t, msg)
","if isinstance ( item , long ) :",181
"def _real_start(self):
    try:
        if check_user(self.config[""user""]):
            self.action_log_info(""Starting up"")
            self.verify_hash_type()
            self.minion.tune_in()
            if self.minion.restart:
                raise SaltClientError(""Minion could not connect to Master"")
    except (KeyboardInterrupt, SaltSystemExit) as error:
        self.action_log_info(""Stopping"")
        if isinstance(error, KeyboardInterrupt):
            log.warning(""Exiting on Ctrl-c"")
            self.shutdown()
        else:
            log.error(error)
            self.shutdown(error.code)
","if check_user ( self . config [ ""user"" ] ) :",182
"def _mount(path, ftype, root=None):
    mpt = None
    if ftype == ""block"":
        mpt = tempfile.mkdtemp()
        if not __salt__[""mount.mount""](mpt, path):
            os.rmdir(mpt)
            return None
    elif ftype == ""dir"":
        return path
    elif ftype == ""file"":
        if ""guestfs.mount"" in __salt__:
            util = ""guestfs""
        elif ""qemu_nbd.init"" in __salt__:
            util = ""qemu_nbd""
        else:
            return None
        mpt = __salt__[""mount.mount""](path, device=root, util=util)
        if not mpt:
            return None
    return mpt
","elif ""qemu_nbd.init"" in __salt__ :",200
"def Visit_not_test(self, node):  # pylint: disable=invalid-name
    # not_test ::= 'not' not_test | comparison
    for child in node.children:
        self.Visit(child)
        if isinstance(child, pytree.Leaf) and child.value == ""not"":
            _AppendTokenSubtype(child, format_token.Subtype.UNARY_OPERATOR)
","if isinstance ( child , pytree . Leaf ) and child . value == ""not"" :",97
"def get_required_glibc(self) -> str:
    """"""Returns the required glibc version for this ELF file.""""""
    with contextlib.suppress(AttributeError):
        return self._required_glibc  # type: ignore
    version_required = """"
    for lib in self.needed.values():
        for version in lib.versions:
            if not version.startswith(""GLIBC_""):
                continue
            version = version[6:]
            if parse_version(version) > parse_version(version_required):
                version_required = version
    self._required_glibc = version_required
    return version_required
","if not version . startswith ( ""GLIBC_"" ) :",160
"def doFilter(self, filters, **args):
    plug_fils = {
        key[5:]: value for (key, value) in filters.items() if key.startswith(""plug_"")
    }
    filters_ = []
    for plugin in self.getWebPlugins():
        try:
            filter_ = plugin.doFilter(plug_fils, **args)
            if filter_:
                if type(filter_) is dict:
                    filters_.append(filter_)
                elif type(filter_) is list:
                    filters_.extend(filter_)
        except Exception as e:
            print(""[!] Plugin %s failed on applying filters!"" % plugin.getName())
            print(""[!]  -> %s"" % e)
    return filters_
",if type ( filter_ ) is dict :,197
"def test_all_managers_are_different(self):
    # all tree managers should be different. otherwise, possible infinite recursion.
    seen = {}
    for model in apps.get_models():
        if not issubclass(model, MPTTModel):
            continue
        tm = model._tree_manager
        if id(tm) in seen:
            self.fail(
                ""Tree managers for %s and %s are the same manager""
                % (model.__name__, seen[id(tm)].__name__)
            )
        seen[id(tm)] = model
",if id ( tm ) in seen :,149
"def build_list_params(self, params, items, label):
    if isinstance(items, six.string_types):
        items = [items]
    for index, item in enumerate(items):
        i = index + 1
        if isinstance(item, dict):
            for k, v in six.iteritems(item):
                params[label % (i, ""Name"")] = k
                if v is not None:
                    params[label % (i, ""Value"")] = v
        else:
            params[label % i] = item
","if isinstance ( item , dict ) :",144
"def tokenize_lines_fn(x):
    """"""Worker function to tokenize lines based on the tokenizer, and perform vocabulary lookup.""""""
    lines, tokenizer, vocab = x
    results = []
    for line in lines:
        if not line:
            break
        line = line.strip()
        # Empty lines are used as document delimiters
        if not line:
            results.append([])
        else:
            tokens = vocab[tokenizer(line)]
            if tokens:
                results.append(tokens)
    return results
",if tokens :,135
"def logConsumer(self):
    r = re.compile(r""dpkg-genchanges  >\.\./(.+\.changes)"")
    while True:
        stream, line = yield
        mo = r.search(line)
        if mo:
            self.setProperty(""deb-changes"", mo.group(1), ""DebPbuilder"")
",if mo :,88
"def teardown(self):
    if hasattr(self, ""mod_env""):
        if os.path.isfile(self.archive):
            os.remove(self.archive)
        else:
            shutil.rmtree(self.archive)
        self.archive = None
",if os . path . isfile ( self . archive ) :,69
"def __new__(cls, exceptions, result, requests):
    if len(result) != len(exceptions) != len(requests):
        raise ValueError(""Need result, exception and request for each error"")
    for e, req in zip(exceptions, requests):
        if not isinstance(e, BaseException) and e is not None:
            raise TypeError(""Expected an exception object, not '%r'"" % e)
        if not isinstance(req, TLRequest):
            raise TypeError(""Expected TLRequest object, not '%r'"" % req)
    if len(exceptions) == 1:
        return exceptions[0]
    self = BaseException.__new__(cls)
    self.exceptions = list(exceptions)
    self.results = list(result)
    self.requests = list(requests)
    return self
","if not isinstance ( req , TLRequest ) :",190
"def platformAttach(self, pid):
    self._connectSocket()
    self.attaching = True
    # Wait for the debug stub to stop the target
    while True:
        pkt = self._cmdTransact(""?"")
        if len(pkt) == 0:
            raise Exception(""Attach Response Error!"")
        if int(pkt[1:3], 16) == 0:
            import time
            time.sleep(0.1)
            self.platformSendBreak()
            pkt = self._cmdTransact(""?"")
        break
    self._sendPkt(""?"")
",if len ( pkt ) == 0 :,143
"def parse(filepath, variable):
    f = open(filepath, ""r"")
    for line in f:
        if line.startswith(""  ""):
            tokens = [t.strip() for t in line.split("":"")]
            if len(tokens) >= 2 and tokens[0].lower() == variable.lower():
                f.close()
                return "": "".join(tokens[1:])
    f.close()
",if len ( tokens ) >= 2 and tokens [ 0 ] . lower ( ) == variable . lower ( ) :,106
"def generateValueNode(self, node):
    result = super().generateValueNode(node)
    if result == """" or result.isspace():
        return '""""'
    else:
        if self.matchKeyword:  # don't quote search value on keyword field
            if self.CaseInSensitiveField:
                make_ci = self.makeCaseInSensitiveValue(result)
                result = make_ci.get(""value"")
                if make_ci.get(""is_regex""):  # Determine if still should be a regex
                    result = ""/%s/"" % result  # Regex place holders for regex
            return result
        else:
            return '""%s""' % result
",if self . matchKeyword :,175
"def to_dict(cls, arg):
    ""Generates dictionary from string or list of strings""
    if hasattr(arg, ""splitlines""):
        arg = arg.splitlines()
    if hasattr(arg, ""__reversed__""):
        result = {}
        for a in arg:
            a = a.strip()
            if a:
                key_val = a.split(None, 1)
                key = key_val[0]
                if len(key_val) > 1:
                    val = key_val[1]
                else:
                    val = """"
                result[key] = val
    else:
        result = arg
    return result
",if len ( key_val ) > 1 :,182
"def normalize(wtree):
    result = []
    for part in wtree[1:-1]:
        if isinstance(part, list):
            part = normalize(part)
            if part[0] == """":
                # Move the part content back at current level
                result += part[1:-1]
                continue
        elif not part:
            # Remove empty strings
            continue
        result.append(part)
    if not result:
        result = [""""]
    return [wtree[0]] + result + [wtree[-1]]
","if isinstance ( part , list ) :",147
"def parse_locked_sigs(sigfile_path):
    """"""Return <pn:task>:<hash> dictionary""""""
    sig_dict = {}
    with open(sigfile_path) as f:
        lines = f.readlines()
        for line in lines:
            if "":"" in line:
                taskkey, _, hashval = line.rpartition("":"")
                sig_dict[taskkey.strip()] = hashval.split()[0]
    return sig_dict
","if "":"" in line :",115
"def merge_configs(cfg, sec, args_dict):
    assert sec in CONFIG_SECS, ""invalid config section {}"".format(sec)
    sec_dict = getattr(cfg, sec.upper())
    for k, v in args_dict.items():
        if v is None:
            continue
        try:
            if hasattr(sec_dict, k):
                setattr(sec_dict, k, v)
        except:
            pass
    return cfg
",if v is None :,119
"def _mac_check_locale(locale_string):
    locale = None
    calendar = None
    currency = None
    div = locale_string.strip().split(""@"")
    LOG.debug(""Checking Locale %s"", "" "".join(div))
    locale = glocale.check_available_translations(div[0])
    if len(div) > 1:
        div = div[1].split("";"")
        for phrase in div:
            (name, value) = phrase.split(""="")
            if name == ""calendar"":
                calendar = glocale.check_available_translations(value)
            elif name == ""currency"":
                currency = value
    return (locale, calendar, currency)
","if name == ""calendar"" :",173
"def hilite(s, ok=True, bold=False):
    """"""Return an highlighted version of 's'.""""""
    if not term_supports_colors():
        return s
    else:
        attr = []
        if ok is None:  # no color
            pass
        elif ok:
            attr.append(""32"")  # green
        else:
            attr.append(""31"")  # red
        if bold:
            attr.append(""1"")
        return ""\x1b[%sm%s\x1b[0m"" % ("";"".join(attr), s)
",if bold :,151
"def get_hashes(self, ireq):
    key = key_from_ireq(ireq)
    existing_pin = self.existing_pins.get(key)
    if existing_pin and ireq_satisfied_by_existing_pin(ireq, existing_pin):
        if PIP_VERSION[:2] <= (20, 0):
            hashes = existing_pin.options.get(""hashes"", {})
        else:
            hashes = existing_pin.hash_options
        hexdigests = hashes.get(FAVORITE_HASH)
        if hexdigests:
            return {"":"".join([FAVORITE_HASH, hexdigest]) for hexdigest in hexdigests}
    return self.repository.get_hashes(ireq)
",if hexdigests :,183
"def get_verbosename(
    self,
    schema: s_schema.Schema,
    *,
    with_parent: bool = False,
) -> str:
    vn = super().get_verbosename(schema)
    if with_parent:
        subject = self.get_subject(schema)
        if subject is not None:
            pn = subject.get_verbosename(schema, with_parent=True)
            return f""{vn} of {pn}""
    return vn
",if subject is not None :,118
"def checkForPathDirections(font):
    headline(""Checking for path directions"")
    ok = True
    for layer, g in masterLayersIterator(font):
        firstPath = layer.paths[0]
        if firstPath and firstPath.direction != -1:
            ok = False
            if len(layer.paths) > 1:
                msg = ""Bad path order or direction.""
            else:
                msg = ""Bad path direction.""
            log(g.name, layer.name, msg)
    if ok:
        print(""OK"")
",if len ( layer . paths ) > 1 :,146
"def test_walk_prune(self):
    # Prune the search.
    all = []
    for root, dirs, files in self.walk(self.walk_path):
        all.append((root, dirs, files))
        # Don't descend into SUB1.
        if ""SUB1"" in dirs:
            # Note that this also mutates the dirs we appended to all!
            dirs.remove(""SUB1"")
    self.assertEqual(len(all), 2)
    self.assertEqual(all[0], (self.walk_path, [""SUB2""], [""tmp1""]))
    all[1][-1].sort()
    self.assertEqual(all[1], self.sub2_tree)
","if ""SUB1"" in dirs :",168
"def _get_params(self, params):
    from . import API_KEY
    if not API_KEY:
        raise APIKeyError
    api_dict = {""api_key"": API_KEY}
    if params:
        params.update(api_dict)
        for key, value in params.items():
            if isinstance(params[key], bool):
                params[key] = ""true"" if value is True else ""false""
    else:
        params = api_dict
    return params
","if isinstance ( params [ key ] , bool ) :",126
"def _conv_has_norm(module, sync_bn):
    for m in module.modules():
        if isinstance(m, ConvModule):
            if not m.with_norm:
                return False
            if sync_bn:
                if not isinstance(m.bn, SyncBatchNorm):
                    return False
    return True
",if not m . with_norm :,91
"def quads(self, quad):
    for s, p, o, c in super(Dataset, self).quads(quad):
        if c.identifier == self.default_context:
            yield (s, p, o, None)
        else:
            yield (s, p, o, c.identifier)
",if c . identifier == self . default_context :,80
"def handleMessages(self, msgs):
    for msg in msgs:
        if msg.has_key(""method"") and msg.has_key(""params""):
            if msg.has_key(""id""):
                if msg[""id""]:
                    self.handleRequest(msg)
                else:
                    self.handleNotification(msg)
            else:
                self.handleNotification(msg)
        elif msg.has_key(""result"") and msg.has_key(""error""):
            self.handleResponse(msg)
        else:  # unknown object
            self.sendResponse(None, InvalidJSONMessage())
            self.close()
","if msg . has_key ( ""id"" ) :",175
"def decode(self, probabilities) -> Prediction:
    last_char = self.blank
    chars = np.argmax(probabilities, axis=1)
    sentence = []
    for idx, c in enumerate(chars):
        if c != self.blank:
            if c != last_char:
                sentence.append((c, idx, idx + 1))
            else:
                _, start, end = sentence[-1]
                del sentence[-1]
                sentence.append((c, start, idx + 1))
        last_char = c
    return self.find_alternatives(probabilities, sentence, self.threshold)
",if c != last_char :,160
"def redrawRunning(self):
    """"""Forces the running frames to be redrawn with current values""""""
    # pylint: disable=broad-except
    try:
        items = self.findItems(""Running"", QtCore.Qt.MatchExactly, STATUS_COLUMN)
        if items:
            self.dataChanged(
                self.indexFromItem(items[0], RUNTIME_COLUMN),
                self.indexFromItem(items[-1], LASTLINE_COLUMN),
            )
    except Exception as e:
        list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))
",if items :,150
"def get_coords_of_dots(self):
    """"""Yiels the cordinates of every dot char in the world.""""""
    for y, line in enumerate(self.map):
        if line and line[0] == ""%"":
            continue
        for x, char in enumerate(line):
            if char.isDot():
                yield Pos(x, y)
","if line and line [ 0 ] == ""%"" :",93
"def consume(self):
    try:
        if self.data[self.p] == 10:  # \n
            self.line += 1
            self.charPositionInLine = 0
        else:
            self.charPositionInLine += 1
        self.p += 1
    except IndexError:
        # happend when we reached EOF and self.data[self.p] fails
        # just do nothing
        pass
",if self . data [ self . p ] == 10 :,111
"def maybe_schedule(s, relative=False, app=None):
    """"""Return schedule from number, timedelta, or actual schedule.""""""
    if s is not None:
        if isinstance(s, numbers.Number):
            s = timedelta(seconds=s)
        if isinstance(s, timedelta):
            return schedule(s, relative, app=app)
        else:
            s.app = app
    return s
","if isinstance ( s , timedelta ) :",105
"def test(self, text, s, e):
    ret = {}
    if self.lastMatch:
        n1 = len(self.lastMatch)
        n2 = e - s
        if n2 > n1:
            if n2 != n1 + 1:
                ret[s] = ""expected %d, %d found"" % (n1 + 1, n2)
    self.lastMatch = text[s:e]
    return ret
",if n2 > n1 :,116
"def _FillMetadata(self, vaddr, metadata):
    address_space = self.session.GetParameter(""default_address_space"")
    for type, _, addr in address_space.describe_vtop(vaddr):
        if type == ""pte"" and addr:
            metadata[""type""] = ""Valid""
            return self.profile._MMPTE(addr, vm=self.physical_address_space)
","if type == ""pte"" and addr :",102
"def quadruples_gen(s):
    t = []
    for c in s:
        res = table_a2b_hqx[ord(c)]
        if res == SKIP:
            continue
        elif res == FAIL:
            raise Error(""Illegal character"")
        elif res == DONE:
            yield t
            raise Done
        else:
            t.append(res)
        if len(t) == 4:
            yield t
            t = []
    yield t
",if res == SKIP :,135
"def renameGlyphOrderFile(filename, newNames, dryRun=False, print=print):
    lines = []
    didRename = False
    for line in readLines(filename):
        line = line.lstrip()
        if len(line) > 0 and line[0] != ""#"":
            newName = newNames.get(line)
            if newName is not None:
                didRename = True
                line = newName
        lines.append(line)
    if didRename:
        print(""Writing"", filename)
        if not dryRun:
            with open(filename, ""w"") as f:
                f.write(""\n"".join(lines))
","if len ( line ) > 0 and line [ 0 ] != ""#"" :",173
"def getfnl(startdir):
    filenamelist = []
    for root, dirs, files in os.walk(startdir):
        for fn in files:
            fullfn = os.path.join(root, fn)
            if fn.endswith("".xml""):
                try:
                    doc = ET.parse(fullfn)
                except Exception as e:
                    raise Exception(""Oops, bad XML in '%s': %s"" % (fullfn, e))
                rootelement = doc.getroot()
                # here we check if this xml file actually is a tool conf xml!
                if rootelement.tag == ""tool"":
                    filenamelist.append(fullfn)
    return filenamelist
","if rootelement . tag == ""tool"" :",191
"def find_nonterminal_transitions(self, C):
    trans = []
    for stateno, state in enumerate(C):
        for p in state:
            if p.lr_index < p.len - 1:
                t = (stateno, p.prod[p.lr_index + 1])
                if t[1] in self.grammar.Nonterminals:
                    if t not in trans:
                        trans.append(t)
    return trans
",if t not in trans :,123
"def validate_project_urls(metadata):
    probs = []
    for prurl in metadata.get(""project_urls"", []):
        name, url = prurl.split("","", 1)
        url = url.lstrip()
        if not name:
            probs.append(""No name for project URL {!r}"".format(url))
        elif len(name) > 32:
            probs.append(""Project URL name {!r} longer than 32 characters"".format(name))
        probs.extend(validate_url(url))
    return probs
",elif len ( name ) > 32 :,131
"def check_helper(self, fn, node, s):
    cct = self.CCTraverser(controller=self)
    for n in 1, 2:
        if self.test_kind is ""test"":
            g.trace(""===== PASS"", n)
        # Init this pass.
        self.file_name = fn
        self.indent = 0
        self.pass_n = n
        cct.visit(node)
    self.end_file()
","if self . test_kind is ""test"" :",120
"def initialize(self):
    try:
        if DEBUG_ON:
            print(""connect again"")
        self._mqttc = mqtt.Client(None)
        self._mqttc.on_message = self.mqtt_on_message
        self._mqttc.on_connect = self.mqtt_on_connect
        self._mqttc.on_subscribe = self.mqtt_on_subscribe
        self._mqttc.on_publish = self.mqtt_on_publish
        self._mqttc.on_disconnect = self.on_disconnect
        # Enable this line if you are doing the snip code, off stress
        # self._mqttc.loop_start()
    except TypeError:
        print(""Connect to mqtter error"")
        return
",if DEBUG_ON :,188
"def __init__(self, version, plugins):
    self.version = version
    self.brand = ""vanilla""
    self.plugins = []
    if plugins:
        parts = plugins.split("":"", 1)
        self.brand = parts[0].strip()
        if len(parts) == 2:
            self.plugins = [s.strip() for s in parts[1].split("";"")]
",if len ( parts ) == 2 :,98
"def __iter__(self):
    while True:
        keys = self.ns_range.make_datastore_query().Get(limit=self._batch_size)
        if not keys:
            break
        for key in keys:
            namespace = metadata.Namespace.key_to_namespace(key)
            self.ns_range = self.ns_range.with_start_after(namespace)
            yield namespace
",if not keys :,107
"def run(self):
    if self.check():
        print_success(""Target appears to be vulnerable."")
        path = self.valid_resource.format(self.filename)
        response = self.http_request(
            method=""GET"",
            path=path,
        )
        if response is None:
            print_error(""Error with reading response"")
            return
        if response.text:
            print_status(""Reading file: {}"".format(self.filename))
            print_info(response.text)
        else:
            print_error(""Exploit failed - empty response"")
    else:
        print_error(""Exploit failed - target seems to be not vulnerable"")
",if response is None :,180
"def pack_items(self, parent, items):
    if self._size > 0:
        self.ipr = self._size
    else:
        self.ipr = int(math.sqrt(max(1, len(items) - 1)) + 1)
        if len(items) == 6:
            self.ipr = 3  # Special (common) cases
        if len(items) == 8:
            self.ipr = 4  # Special (common) cases
    x, y = 0, 0
    for item in items:
        parent.attach(item.widget, x, y, 1, 1)
        x += 1
        if x >= self.ipr:
            x = 0
            y += 1
",if len ( items ) == 6 :,186
"def __call__(self, response, obj, headers):
    for message in obj:
        if message.message_text in [None, """", b""""]:
            continue
        content = message.message_text
        if (self.key_encryption_key is not None) or (self.resolver is not None):
            content = decrypt_queue_message(
                content,
                response,
                self.require_encryption,
                self.key_encryption_key,
                self.resolver,
            )
        message.message_text = self.decode(content, response)
    return obj
",if ( self . key_encryption_key is not None ) or ( self . resolver is not None ) :,162
"def make_arg(arg):
    if empty:
        return np.empty((1,) * max(1, arg.ndim), dtype=arg.dtype)
    else:
        if hasattr(arg, ""op"") and hasattr(arg.op, ""data""):
            arg = arg.op.data
        return arg[(0,) * max(1, arg.ndim)]
","if hasattr ( arg , ""op"" ) and hasattr ( arg . op , ""data"" ) :",91
"def walk_node(node, **kwargs):
    negated = kwargs[""negated""]
    if node.negated:
        negated = not negated
    for child in node.children:
        new_kwargs = kwargs.copy()
        new_kwargs[""negated""] = negated
        if not getattr(child, ""children"", []):
            leaf_callback(child, **new_kwargs)
        else:
            new_parent = trunk_callback(child, **new_kwargs)
            if new_parent:
                new_kwargs[""new_parent""] = new_parent
            walk_node(child, **new_kwargs)
",if new_parent :,158
"def _makeDict(instructionList):
    opcodeDict = {}
    mnemonicDict = {}
    for op, mnemonic, argBits, name, pops, pushes in instructionList:
        assert _mnemonicPat.match(mnemonic)
        mnemonicDict[mnemonic] = op, argBits, name
        if argBits:
            argoffset = op
            for i in range(1 << argBits):
                opcodeDict[op + i] = mnemonic, argBits, argoffset, name
        else:
            opcodeDict[op] = mnemonic, 0, 0, name
    return opcodeDict, mnemonicDict
",if argBits :,168
"def _validate_sid_uniqueness(self):
    sids = []
    for statement in self._statements:
        if ""Sid"" in statement:
            statementId = statement[""Sid""]
            if statementId:
                assert statementId not in sids
                sids.append(statementId)
","if ""Sid"" in statement :",83
"def dumpConnections(node):
    ""Helper function: dump connections to node""
    for intf in node.intfList():
        output("" %s:"" % intf)
        if intf.link:
            intfs = [intf.link.intf1, intf.link.intf2]
            intfs.remove(intf)
            output(intfs[0])
        else:
            output("" "")
",if intf . link :,113
"def _get_warnings(log_content, return_data):
    warnings = {}
    missing_files = find_missing_files(log_content)
    if missing_files:
        warnings[""missing_files""] = missing_files
    wrong_engine = find_wrong_renderer_warning(log_content)
    if wrong_engine:
        warnings[""wrong_engine""] = wrong_engine
    if warnings:
        if return_data.get(""warnings""):
            return_warnings = return_data.get(""warnings"")
            if return_warnings.get(""missing_files""):
                return_data[""warnings""][""missing_files""].extend(
                    warnings[""missing_files""]
                )
        else:
            return_data[""warnings""] = warnings
","if return_warnings . get ( ""missing_files"" ) :",194
"def add_hole(face):
    used = set()  # type: Set['Face']
    stack = [hedge.twin for hedge in face.inners]
    while stack:
        next_face = stack.pop().face
        if next_face in used:
            continue
        if id(face) == id(next_face):
            continue
        used.add(next_face)
        for loop_hedge in next_face.outer.loop_hedges:
            stack.append(loop_hedge.twin)
        if next_face.inners:
            del_hole(next_face)
",if next_face in used :,161
"def __init__(self, content=b"""", mimetype=None, code=200):
    self._iter = None
    self._is_str_iter = True
    self.content = content
    self.headers = HeaderDict()
    self.cookies = SimpleCookie()
    self.status_code = code
    defaults = self.defaults._current_obj()
    if not mimetype:
        mimetype = defaults.get(""content_type"", ""text/html"")
        charset = defaults.get(""charset"")
        if charset:
            mimetype = ""%s; charset=%s"" % (mimetype, charset)
    self.headers.update(defaults.get(""headers"", {}))
    self.headers[""Content-Type""] = mimetype
    self.errors = defaults.get(""errors"", ""strict"")
",if charset :,185
"def _store_parsers(
    parser, parser_keys, parser_values, sub_parser_keys, sub_parser_values
):
    for s in parser.subparsers.values():
        parser_keys.append(_get_parser_name(s))
        parser_values.append(s)
        if _is_group(s):
            for c in s.choices.values():
                sub_parser_keys.append(_get_parser_name(c))
                sub_parser_values.append(c)
                _store_parsers(
                    c, parser_keys, parser_values, sub_parser_keys, sub_parser_values
                )
",if _is_group ( s ) :,174
"def get(self):
    """"""Get next event from queue.""""""
    inputHookFunc = c_int.from_address(self.inputHookPtr).value
    Cevent = INPUT_RECORD()
    count = c_int(0)
    while 1:
        if inputHookFunc:
            call_function(inputHookFunc, ())
        status = self.ReadConsoleInputW(self.hin, byref(Cevent), 1, byref(count))
        if status and count.value == 1:
            e = event(self, Cevent)
            log_sock(ensure_unicode(e.keyinfo), ""keypress"")
            return e
",if status and count . value == 1 :,159
"def GetItemChildren(self, item=None, recursively=False):
    """"""Return the children of item as a list.""""""
    if not item:
        item = self.GetRootItem()
        if not item:
            return []
    children = []
    child, cookie = self.GetFirstChild(item)
    while child:
        children.append(child)
        if recursively:
            children.extend(self.GetItemChildren(child, True))
        child, cookie = self.GetNextChild(item, cookie)
    return children
",if not item :,135
"def _merge_inheritable_dicts_(cls, models):
    super(AuthModel, cls)._merge_inheritable_dicts_(models)
    for attr in cls._additional_inheritable_dict_attrs_:
        if isinstance(attr, tuple):
            attr_name = attr[0]
        else:
            attr_name = attr
        attrs = {}
        for model in models:
            if not issubclass(model, AuthModel):
                continue
            superattrs = getattr(model, attr_name)
            for k, v in superattrs.items():
                attrs[k] = v
        for k, v in getattr(cls, attr_name).items():
            attrs[k] = v
        setattr(cls, attr_name, attrs)
","if not issubclass ( model , AuthModel ) :",198
"def _initialize_weights(self) -> None:
    for m in self.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            import scipy.stats as stats
            X = stats.truncnorm(-2, 2, scale=0.01)
            values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)
            values = values.view(m.weight.size())
            with torch.no_grad():
                m.weight.copy_(values)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
","elif isinstance ( m , nn . BatchNorm2d ) :",191
"def _value_to_so_vector(value, ids=None):
    if not isinstance(value, collections.Sequence) or _is_str(value):
        raise TypeError(
            ""Expected list/sequence of SerializableObjects;""
            "" found type '{}'"".format(type(value))
        )
    av = AnyVector()
    for e in value:
        if not isinstance(e, SerializableObject):
            raise TypeError(
                ""Expected list/sequence of SerializableObjects;""
                "" found element of type '{}'"".format(type(e))
            )
        av.append(e)
    return PyAny(av)
","if not isinstance ( e , SerializableObject ) :",163
"def await_reservations(self, sc, status={}, timeout=600):
    """"""Block until all reservations are received.""""""
    timespent = 0
    while not self.reservations.done():
        logger.info(
            ""waiting for {0} reservations"".format(self.reservations.remaining())
        )
        # check status flags for any errors
        if ""error"" in status:
            sc.cancelAllJobs()
            sc.stop()
            sys.exit(1)
        time.sleep(1)
        timespent += 1
        if timespent > timeout:
            raise Exception(""timed out waiting for reservations to complete"")
    logger.info(""all reservations completed"")
    return self.reservations.get()
","if ""error"" in status :",199
"def generate_samples(self, data_dir, tmp_dir, unused_dataset_split):
    counter = 0
    done = False
    while not done:
        for frame_number, frame in enumerate(self.generate_stochastic_shape_instance()):
            if counter >= self.total_number_of_frames:
                done = True
                break
            yield {""frame"": frame, ""frame_number"": [frame_number]}
            counter += 1
",if counter >= self . total_number_of_frames :,118
"def dtype_to_ctype(dtype, with_fp_tex_hack=False):
    if dtype is None:
        raise ValueError(""dtype may not be None"")
    dtype = np.dtype(dtype)
    if with_fp_tex_hack:
        if dtype == np.float32:
            return ""fp_tex_float""
        elif dtype == np.float64:
            return ""fp_tex_double""
        elif dtype == np.complex64:
            return ""fp_tex_cfloat""
        elif dtype == np.complex128:
            return ""fp_tex_cdouble""
    return base_dtype_to_ctype(dtype)
",elif dtype == np . float64 :,167
"def __len__(self):
    if self.data is None:
        _fill_lock.acquire()
        try:
            if self.data is None:
                self._fill()
        finally:
            _fill_lock.release()
    return len(self.data)
",if self . data is None :,76
"def generate(self):
    sample_n = 0
    # in samples
    cycle_length = self.sample_rate / float(self.freq)
    midpoint = cycle_length * self.duty_cycle
    ascend_length = midpoint
    descend_length = cycle_length - ascend_length
    while True:
        cycle_position = sample_n % cycle_length
        if cycle_position < midpoint:
            yield (2 * cycle_position / ascend_length) - 1.0
        else:
            yield 1.0 - (2 * (cycle_position - midpoint) / descend_length)
        sample_n += 1
",if cycle_position < midpoint :,158
"def recurse(node):
    result = []  # collections.deque()
    if node is not None:
        if node.position < offset < node.endTimeHigh:
            result.extend(recurse(node.leftChild))
            # This currently requires timespans not elements, and list payloads...
            # TODO: Fix/disambiguate.
            for el in node.payload:
                if offset < self.elementEndTime(el, node):
                    result.append(el)
            result.extend(recurse(node.rightChild))
        elif offset <= node.position:
            result.extend(recurse(node.leftChild))
    return result
",if node . position < offset < node . endTimeHigh :,171
"def add_aggregates(self, queryset, display_fields):
    agg_funcs = {""Avg"": Avg, ""Min"": Min, ""Max"": Max, ""Count"": Count, ""Sum"": Sum}
    for display_field in display_fields:
        if display_field.aggregate:
            func = agg_funcs[display_field.aggregate]
            full_name = display_field.path + display_field.field
            queryset = queryset.annotate(func(full_name))
    return queryset
",if display_field . aggregate :,123
"def __init__(
    self, setting, newValue, settings=None, autoResolve=False, ignoreOld=False
):
    ActionConflict.__init__(self, setting, settings, autoResolve)
    self.Name = _(""button"")
    if not newValue:
        return
    newValue = newValue.lower()
    oldValue = self.Setting.Value.lower()
    badValues = [""disabled"", ""none""]
    if not ignoreOld:
        badValues.append(oldValue)
    if newValue in badValues:
        return
    for s in self.Settings:
        if s is setting:
            continue
        if s.Type == ""Button"":
            if s.Value.lower() == newValue:
                self.Conflicts.append(s)
","if s . Type == ""Button"" :",191
"def inner(**kwargs):
    coro = func(**kwargs)
    if coro is not None:
        task = asyncio.ensure_future(coro, loop=_loop)
        try:
            _loop.run_until_complete(task)
        except BaseException:
            # run_until_complete doesn't get the result from exceptions
            # that are not subclasses of `Exception`. Consume all
            # exceptions to prevent asyncio's warning from logging.
            if task.done() and not task.cancelled():
                task.exception()
            raise
",if task . done ( ) and not task . cancelled ( ) :,140
"def prune(model, amount=0.3):
    # Prune model to requested global sparsity
    import torch.nn.utils.prune as prune
    print(""Pruning model... "", end="""")
    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):
            prune.l1_unstructured(m, name=""weight"", amount=amount)  # prune
            prune.remove(m, ""weight"")  # make permanent
    print("" %.3g global sparsity"" % sparsity(model))
","if isinstance ( m , nn . Conv2d ) :",130
"def robot_fetch(path):
    """"""Check if robots can fetch a file.""""""
    for rule in kw[""robots_exclusions""]:
        robot = robotparser.RobotFileParser()
        robot.parse([""User-Agent: *"", ""Disallow: {0}"".format(rule)])
        if sys.version_info[0] == 3:
            if not robot.can_fetch(""*"", ""/"" + path):
                return False  # not robot food
        else:
            if not robot.can_fetch(""*"", (""/"" + path).encode(""utf-8"")):
                return False  # not robot food
    return True
",if sys . version_info [ 0 ] == 3 :,158
"def check_not_vislist_intent(self):
    syntaxMsg = (
        ""The intent that you specified corresponds to more than one visualization. ""
        ""Please replace the Vis constructor with VisList to generate a list of visualizations. ""
        ""For more information, see: https://lux-api.readthedocs.io/en/latest/source/guide/vis.html#working-with-collections-of-visualization-with-vislist""
    )
    for i in range(len(self._intent)):
        clause = self._intent[i]
        if isinstance(clause, str):
            if ""|"" in clause or ""?"" in clause:
                raise TypeError(syntaxMsg)
        if isinstance(clause, list):
            raise TypeError(syntaxMsg)
","if ""|"" in clause or ""?"" in clause :",186
"def __call__(self, data):
    if self._apply_shift:
        if not hasattr(data, ""coords""):
            raise Exception(""should quantize first using GridSampling3D"")
        if not isinstance(data.coords, torch.IntTensor):
            raise Exception(
                ""The pos are expected to be coordinates, so torch.IntTensor""
            )
        data.coords[:, :3] += (torch.rand(3) * 100).type_as(data.coords)
    return data
","if not isinstance ( data . coords , torch . IntTensor ) :",127
"def get_config(cls, config):
    out = {}
    with git_config.GitConfigParser(config, read_only=True) as config:
        section = 'gitrepo ""{}""'.format(cls.name)
        if config.has_section(section):
            for option in cls.config_options:
                if config.has_option(section, option):
                    out[option] = config.get(section, option)
    return out
",if config . has_section ( section ) :,116
"def checkComments(self, line):
    """"""Check to see if the line/lines is a comment.""""""
    if not self.inCommentBlock:
        if ""["" in line:
            if ""]"" not in line:
                self.inCommentBlock = True
            else:
                return ""Nextline""  # need to move on to the nextline after getting out of comment
    else:
        if ""]"" in line:
            if line.rfind(""["") > line.rfind(""]""):
                pass  # a comment block is closed but another is open.
            else:
                self.inCommentBlock = False
                return ""Nextline""  # need to move on to the nextline after getting out of comment
    return """"
","if ""["" in line :",188
"def done(res):
    if isinstance(res, failure.Failure):
        res.trap(expected_failure)
        if substring:
            self.failUnless(
                substring in str(res),
                ""%s: substring '%s' not in '%s'"" % (which, substring, str(res)),
            )
        # return the Failure for further analysis, but in a form that
        # doesn't make the Deferred chain think that we failed.
        return [res]
    else:
        self.fail(
            ""%s was supposed to raise %s, not get '%s'"" % (which, expected_failure, res)
        )
",if substring :,169
"def best_version_match(vers_num, item_dict):
    """"""Attempts to find the best match in item_dict for vers_num""""""
    vers_tuple = vers_num.split(""."")
    precision = 1
    while precision <= len(vers_tuple):
        test_vers = ""."".join(vers_tuple[0:precision])
        match_names = []
        for item in item_dict.keys():
            for item_version in item_dict[item]:
                if item_version.startswith(test_vers) and item not in match_names:
                    match_names.append(item)
        if len(match_names) == 1:
            return match_names[0]
        precision = precision + 1
    return None
",if len ( match_names ) == 1 :,189
"def validate_work_dir(s):
    dirs = s.split("":"")
    if len(dirs) != 2:
        print(
            ""work-dir %s is not in format local_abs_dir:container_abs_dir"" % s,
            file=sys.stderr,
        )
        sys.exit(1)
    for d in dirs:
        if not os.path.isabs(d):
            print(""%s is not an absolute path"" % d, file=sys.stderr)
            sys.exit(1)
",if not os . path . isabs ( d ) :,137
"def unknown_endtag(self, tag):
    if not tag in self.acceptable_elements:
        if tag in self.unacceptable_elements_with_end_tag:
            self.unacceptablestack -= 1
        if self.mathmlOK and tag in self.mathml_elements:
            if tag == ""math"" and self.mathmlOK:
                self.mathmlOK -= 1
        elif self.svgOK and tag in self.svg_elements:
            tag = self.svg_elem_map.get(tag, tag)
            if tag == ""svg"" and self.svgOK:
                self.svgOK -= 1
        else:
            return
    _BaseHTMLProcessor.unknown_endtag(self, tag)
",if self . mathmlOK and tag in self . mathml_elements :,189
"def __init__(self, message, response=None):
    # Unfortunately the store doesn't give us a consistent error response,
    # so we'll check the ones of which we're aware.
    with contextlib.suppress(AttributeError, JSONDecodeError):
        response_json = response.json()
        extra_error_message = """"
        if ""error_message"" in response_json:
            extra_error_message = response_json[""error_message""]
        elif ""message"" in response_json:
            extra_error_message = response_json[""message""]
        if extra_error_message:
            message += "": {}"".format(extra_error_message)
    super().__init__(response=response, message=message)
","if ""error_message"" in response_json :",175
"def compile_output(
    ir_set: irast.Set, *, ctx: context.CompilerContextLevel
) -> pgast.OutputVar:
    with ctx.new() as newctx:
        if newctx.expr_exposed is None:
            newctx.expr_exposed = True
        dispatch.visit(ir_set, ctx=newctx)
        path_id = ir_set.path_id
        if output.in_serialization_ctx(ctx) and newctx.stmt is newctx.toplevel_stmt:
            val = pathctx.get_path_serialized_output(ctx.rel, path_id, env=ctx.env)
        else:
            val = pathctx.get_path_value_output(ctx.rel, path_id, env=ctx.env)
    return val
",if output . in_serialization_ctx ( ctx ) and newctx . stmt is newctx . toplevel_stmt :,198
"def set_python_instance_state(self, instance, state):
    if hasattr(instance, ""__setstate__""):
        instance.__setstate__(state)
    else:
        slotstate = {}
        if isinstance(state, tuple) and len(state) == 2:
            state, slotstate = state
        if hasattr(instance, ""__dict__""):
            instance.__dict__.update(state)
        elif state:
            slotstate.update(state)
        for key, value in slotstate.items():
            setattr(object, key, value)
","if isinstance ( state , tuple ) and len ( state ) == 2 :",137
"def set_etc_timezone(
    tz, tz_file=None, tz_conf=""/etc/timezone"", tz_local=""/etc/localtime""
):
    util.write_file(tz_conf, str(tz).rstrip() + ""\n"")
    # This ensures that the correct tz will be used for the system
    if tz_local and tz_file:
        # use a symlink if there exists a symlink or tz_local is not present
        islink = os.path.islink(tz_local)
        if islink or not os.path.exists(tz_local):
            if islink:
                util.del_file(tz_local)
            os.symlink(tz_file, tz_local)
        else:
            util.copy(tz_file, tz_local)
    return
",if islink :,196
"def scan_resource_conf(self, conf):
    if ""rotation_period"" in conf.keys():
        time = force_int(conf[""rotation_period""][0][:-1])
        if time and ONE_DAY <= time <= NINETY_DAYS:
            return CheckResult.PASSED
    return CheckResult.FAILED
",if time and ONE_DAY <= time <= NINETY_DAYS :,81
"def _get_available_commands(stdout):
    commands_listing = False
    for line in stdout.split(""\n""):
        if line.startswith(""where <command> is one of:""):
            commands_listing = True
        elif commands_listing:
            if not line:
                break
            for command in line.split("", ""):
                stripped = command.strip()
                if stripped:
                    yield stripped
",if not line :,115
"def acquire_read_lock(self, wait=True):
    """"""Acquire the 'read' lock.""""""
    self.condition.acquire()
    try:
        # see if a synchronous operation is waiting to start
        # or is already running, in which case we wait (or just
        # give up and return)
        if wait:
            while self.current_sync_operation is not None:
                self.condition.wait()
        else:
            if self.current_sync_operation is not None:
                return False
        self.async_ += 1
        log.debug(""%s acquired read lock"", self)
    finally:
        self.condition.release()
    if not wait:
        return True
",if self . current_sync_operation is not None :,186
"def _linux_get_ifaces(self):
    ifaces = []
    procfs_path = ""/proc/net/dev""
    try:
        with open(procfs_path, ""r"") as f:
            lines = f.read().split(""\n"")
            for line in lines:
                # Only lines with colons contain interface names
                if "":"" in line:
                    fields = line.split("":"")
                    ifaces.append(fields[0].strip())
    except IOError as e:
        self.logger.error(
            ""Failed to open %s to enumerate interfaces: %s"" % (procfs_path, e.message)
        )
    return ifaces
",ifaces . append ( fields [ 0 ] . strip ( ) ),179
"def access_token(self):
    """"""WeChat component access token""""""
    access_token = self.session.get(""component_access_token"")
    if access_token:
        if not self.expires_at:
            # user provided access_token, just return it
            return access_token
        timestamp = time.time()
        if self.expires_at - timestamp > 60:
            return access_token
    self.fetch_access_token()
    return self.session.get(""component_access_token"")
",if not self . expires_at :,131
"def apply(self, s, item, evaluation):
    ""AppendTo[s_, item_]""
    if isinstance(s, Symbol):
        resolved_s = s.evaluate(evaluation)
        if not resolved_s.is_atom():
            result = Expression(""Set"", s, Expression(""Append"", resolved_s, item))
            return result.evaluate(evaluation)
    return evaluation.message(""AppendTo"", ""rvalue"", s)
",if not resolved_s . is_atom ( ) :,105
"def server_address(self):
    if self.server_address_ is None:
        self.lazy_init_lock_.acquire()
        try:
            if self.server_address_ is None:
                self.server_address_ = AddressPort()
        finally:
            self.lazy_init_lock_.release()
    return self.server_address_
",if self . server_address_ is None :,95
"def convert_body_to_file_like_object(params, **kwargs):
    if ""Body"" in params:
        if isinstance(params[""Body""], six.string_types):
            params[""Body""] = six.BytesIO(ensure_bytes(params[""Body""]))
        elif isinstance(params[""Body""], six.binary_type):
            params[""Body""] = six.BytesIO(params[""Body""])
","elif isinstance ( params [ ""Body"" ] , six . binary_type ) :",98
"def get_submodel_args_dict(args):
    submodel_argv = get_submodel_argv(args)
    result = {}
    i = 0
    while i < len(submodel_argv):
        arg = submodel_argv[i]
        next_arg = None if i == len(submodel_argv) - 1 else submodel_argv[i + 1]
        if next_arg and arg.startswith(""--""):
            if next_arg.startswith(""--""):
                result[arg[2:]] = True
            else:
                result[arg[2:]] = next_arg
                i += 1
        elif arg.startswith(""--""):
            result[arg[2:]] = True
        i += 1
    return result
","elif arg . startswith ( ""--"" ) :",188
"def dataStandart(data, dept, nominal_dept):
    """"""data from nasting to standart: TO container( objects( lists( floats, ), ), )""""""
    deptl = dept - 1
    output = []
    for object in data:
        if deptl >= nominal_dept:
            output.extend(dataStandart(object, deptl, nominal_dept))
        else:
            output.append(data)
            return output
    return output
",if deptl >= nominal_dept :,122
"def tamper(payload, **kwargs):
    danger_chars = string.punctuation + "" ""
    extra_danger_chars = (""_"", ""."")
    retval = """"
    for char in list(payload):
        if char not in danger_chars:
            retval += char
        elif char == extra_danger_chars[0]:
            retval += quote_plus(""%255F"")
        elif char == extra_danger_chars[1]:
            retval += quote_plus(""%252E"")
        else:
            retval += quote_plus(quote_plus(quote_plus(char)))
    return retval
",if char not in danger_chars :,150
"def hi_server(addr):
    while 1:
        ev, val = first(until_eol=True, sleep=3)
        if ev == ""sleep"":
            log.warn(""%s timeout!"" % time.asctime())
        else:
            send(""you said %s"" % val)
","if ev == ""sleep"" :",76
"def checkForNoiseProfile():
    global processingChain
    if sys.platform.startswith(""darwin""):
        # not currently supported
        processingChain = [[""lame"", ""rec.wav"", ""rec.mp3"", ""--noreplaygain"", ""--quiet""]]
    else:
        cmd = [""sox"", processingSrc, ""rec2.wav""]
        if os.path.exists(noiseProfile):
            cmd = cmd + [""noisered"", noiseProfile, NOISE_AMOUNT]
        processingChain[0] = cmd
",if os . path . exists ( noiseProfile ) :,128
"def pytest_collection_modifyitems(items):
    for item in items:
        if item.nodeid.startswith(""tests/contrib/funsor""):
            if ""stage"" not in item.keywords:
                item.add_marker(pytest.mark.stage(""funsor""))
            if ""init"" not in item.keywords:
                item.add_marker(pytest.mark.init(rng_seed=123))
            if ""test_pyroapi"" in item.nodeid and ""test_mean_field_ok"" in item.nodeid:
                item.add_marker(pytest.mark.xfail(reason=""not implemented""))
","if ""test_pyroapi"" in item . nodeid and ""test_mean_field_ok"" in item . nodeid :",160
"def find_element(self, action, suffix, parent):
    element = self.search_scopes(action + suffix)
    if element is not None:
        return element
    if action.endswith(""ByNextToken""):
        element = self.search_scopes(action[: -len(""ByNextToken"")] + suffix)
        if element is not None:
            return self.element_factory(action + suffix, element)
    return self.element_factory(action + suffix, parent)
",if element is not None :,114
"def _to_error(*lines):
    if len(lines) == 1:
        if isinstance(lines[0], (list, tuple)):
            lines = lines[0]
        elif not isinstance(lines[0], (str, unicode)):
            lines = [
                repr(lines[0]),
            ]
    return ""\n"".join(map(lambda x: _to_unicode(x, errors=""replace""), lines))
","elif not isinstance ( lines [ 0 ] , ( str , unicode ) ) :",109
"def primes(n):
    if n == 2:
        return [2]
    elif n < 2:
        return []
    s = list(range(3, n + 1, 2))
    mroot = n ** 0.5
    half = (n + 1) // 2 - 1
    i = 0
    m = 3
    while m <= mroot:
        if s[i]:
            j = (m * m - 3) // 2
            s[j] = 0
            while j < half:
                s[j] = 0
                j += m
        i = i + 1
        m = 2 * i + 3
    return [2] + [x for x in s if x]
",if s [ i ] :,184
"def Poll(
    generator: Callable[[], _T],
    condition: Callable[[_T], bool],
    interval: int = DEFAULT_POLL_INTERVAL,
    timeout: int = DEFAULT_POLL_TIMEOUT,
) -> _T:
    """"""Periodically calls generator function until a condition is satisfied.""""""
    started = time.time()
    while True:
        obj = generator()
        check_result = condition(obj)
        if check_result:
            return obj
        if timeout and (time.time() - started) > timeout:
            raise errors.PollTimeoutError(
                ""Polling on %s timed out after %ds."" % (obj, timeout)
            )
        time.sleep(interval)
",if check_result :,177
"def readlink(self, path):
    path = self._realpath(path)
    try:
        symlink = os.readlink(path)
    except OSError as e:
        return SFTPServer.convert_errno(e.errno)
    # if it's absolute, remove the root
    if os.path.isabs(symlink):
        if symlink[: len(self.ROOT)] == self.ROOT:
            symlink = symlink[len(self.ROOT) :]
            if (len(symlink) == 0) or (symlink[0] != ""/""):
                symlink = ""/"" + symlink
        else:
            symlink = ""<error>""
    return symlink
",if symlink [ : len ( self . ROOT ) ] == self . ROOT :,162
"def parse_drawings_on_layers(self, drawings, f_layer, b_layer):
    front = []
    back = []
    for d in drawings:
        if d[1].GetLayer() not in [f_layer, b_layer]:
            continue
        drawing = self.parse_drawing(d[1])
        if not drawing:
            continue
        if d[0] in [""ref"", ""val""]:
            drawing[d[0]] = 1
        if d[1].GetLayer() == f_layer:
            front.append(drawing)
        else:
            back.append(drawing)
    return {""F"": front, ""B"": back}
",if not drawing :,177
"def getExecutionCode(self, required):
    if self.useMatrixList:
        if ""translations"" in required:
            yield ""translations = self.toTranslations(matrices)""
        if ""rotations"" in required:
            yield ""rotations = self.toRotations(matrices)""
        if ""scales"" in required:
            yield ""scales = self.toScales(matrices)""
    else:
        if ""translation"" in required:
            yield ""translation = matrix.to_translation()""
        if ""rotation"" in required:
            yield ""rotation = matrix.to_euler()""
        if ""scale"" in required:
            yield ""scale = matrix.to_scale()""
","if ""translation"" in required :",173
"def load_files(self):
    """"""Try to load all files specified in arguments.""""""
    if self.filenames:
        for item in self.filenames:
            name_row_col = helpers.get_filename_cursor_pos(item)
            name = name_row_col[""name""]
            if os.path.isdir(name):
                continue
            # Avoid opening duplicate files
            if self.file_is_open(name):
                continue
            if not self.open_file(**name_row_col):
                self.new_file(name)
    # If nothing was loaded
    if not self.files:
        self.load_default()
",if self . file_is_open ( name ) :,177
"def _parse_config(self, config):
    cfg = {}
    for key, value in config.items():
        # If the value is a plain str, we will use the value
        # If the value is a dict, we will extract the name of an environment
        # variable stored under 'env' and optionally a default, stored under
        # 'default'.
        # (This is useful if the database relocates to a different host
        # during the lifetime of the LSH object)
        if isinstance(value, dict):
            if ""env"" in value:
                value = os.getenv(value[""env""], value.get(""default"", None))
        cfg[key] = value
    return cfg
","if ""env"" in value :",177
"def bits_per_symbol(self, value: int):
    if self.__bits_per_symbol != value:
        self.__bits_per_symbol = int(value)
        self._qad = None
        self.bits_per_symbol_changed.emit(self.__bits_per_symbol)
        if not self.block_protocol_update:
            self.protocol_needs_update.emit()
",if not self . block_protocol_update :,101
"def _feed_data(self, program, feed, feed_var_name, scope):
    # feed var to framework
    for op in program.global_block().ops:
        if op.desc.type() == ""feed"":
            feed_target_name = op.desc.output(""Out"")[0]
            cur_feed = feed[feed_target_name]
            if not isinstance(cur_feed, core.LoDTensor):
                cur_feed = _as_lodtensor(cur_feed, self.place)
            idx = op.desc.attr(""col"")
            core.set_feed_variable(scope, cur_feed, feed_var_name, idx)
        else:
            break
","if op . desc . type ( ) == ""feed"" :",183
"def rename_profile(self, profile, newname):
    """"""Rename a profile""""""
    if self.base.profiles.has_key(profile):
        self.base.profiles[newname] = self.base.profiles[profile]
        del self.base.profiles[profile]
        if profile == self.profile:
            self.profile = newname
",if profile == self . profile :,85
"def fsm():
    if tx_rst == 0:
        tx_bit.next = 1
        index.next = 0
        state.next = st.IDLE
    else:
        if state == st.IDLE:
            tx_bit.next = 1
            if tx_valid:  # a pulse
                state.next = st.START
        elif state == st.START:
            tx_bit.next = 0
            index.next = 7
            state.next = st.DATA
        elif state == st.DATA:
            tx_bit.next = tx_byte[index]
            if index == 0:
                state.next = st.IDLE
            else:
                index.next = index - 1
",elif state == st . START :,198
"def __call__(self, form, field):
    # databases allow multiple NULL values for unique columns
    if field.data is None:
        return
    try:
        obj = self.db_session.query(self.model).filter(self.column == field.data).one()
        if not hasattr(form, ""_obj"") or not form._obj == obj:
            if self.message is None:
                self.message = field.gettext(u""Already exists."")
            raise ValidationError(self.message)
    except NoResultFound:
        pass
","if not hasattr ( form , ""_obj"" ) or not form . _obj == obj :",138
"def pathdirs():
    """"""Convert sys.path into a list of absolute, existing, unique paths.""""""
    dirs = []
    normdirs = []
    for dir in sys.path:
        dir = os.path.abspath(dir or ""."")
        normdir = os.path.normcase(dir)
        if normdir not in normdirs and os.path.isdir(dir):
            dirs.append(dir)
            normdirs.append(normdir)
    return dirs
",if normdir not in normdirs and os . path . isdir ( dir ) :,116
"def test_string(self):
    w = lambda x: self.wbuf().write_string(x).write_flush()  # noqa
    r = lambda x: self.rbuf(x).read_string()  # noqa
    tc = [(""abc1"", ""00 00 00 04 61 62 63 31""), (b""abc2"", ""00 00 00 04 61 62 63 32"")]
    for p in tc:
        v = p[0]
        assert w(v) == self._b(p[1])
        if not isinstance(v, bytes):
            v = bytes(bytearray(v, ""utf-8""))
        assert r(self._b(p[1])) == v
","if not isinstance ( v , bytes ) :",167
"def skip_pp_directive(s, i):
    while i < len(s):
        if g.is_nl(s, i):
            if g.escaped(s, i):
                i = g.skip_nl(s, i)
            else:
                break
        elif g.match(s, i, ""//""):
            i = g.skip_to_end_of_line(s, i)
        elif g.match(s, i, ""/*""):
            i = g.skip_block_comment(s, i)
        else:
            i += 1
    return i
","elif g . match ( s , i , ""//"" ) :",161
"def finalize(self, callback):
    """"""Execute post wrap callback.""""""
    if self.view is not None:
        if not self.view.settings().get(""bracket_highlighter.busy"", False):
            callback()
        else:
            sublime.set_timeout(lambda: self.finalize(callback), 100)
","if not self . view . settings ( ) . get ( ""bracket_highlighter.busy"" , False ) :",81
"def _forceUserPrefs(self, prefs):
    prefsToSet = {
        ""tabWidth"": ""long"",
        ""indentWidth"": ""long"",
        ""useTabs"": ""bool"",
        ""useSmartTabs"": ""bool"",
    }
    for pref, kind in prefsToSet.iteritems():
        if not prefs.hasPrefHere(pref):
            if kind == ""long"":
                prefs.setLong(pref, prefs.getLong(pref))
            if kind == ""bool"":
                prefs.setBoolean(pref, prefs.getBoolean(pref))
","if kind == ""long"" :",149
"def wait(self, limit=None):
    running = True
    while running:
        it = self.consumer_set.iterconsume(limit=limit)
        if not it:
            break
        while True:
            try:
                it.next()
            except StopIteration:
                return
            except greenlet.GreenletExit:
                running = False
                break
            except Exception as e:
                LOG.exception(_(""Exception while processing consumer""))
                self.reconnect()
                # Break to outer loop
                break
",if not it :,161
"def countbox(self):
    self.box = [1000, 1000, -1000, -1000]
    for i in self.body:
        for x, y in i:
            if x < self.box[0]:
                self.box[0] = x
            if x > self.box[2]:
                self.box[2] = x
            if y < self.box[1]:
                self.box[1] = y
            if y > self.box[3]:
                self.box[3] = y
",if y < self . box [ 1 ] :,147
"def _GetKeys(self, key):
    res = []
    for path in self.cache[self.prefix]:
        if os.path.dirname(path) == key.value:
            sub_type, stat_entry = self.cache[self.prefix][path]
            if sub_type == ""Directory"":
                res.append(os.path.basename(stat_entry.pathspec.path))
    return sorted(res)
","if sub_type == ""Directory"" :",109
"def widget_attrs(self, widget):
    attrs = super(DecimalField, self).widget_attrs(widget)
    if isinstance(widget, NumberInput) and ""step"" not in widget.attrs:
        if self.decimal_places is not None:
            # Use exponential notation for small values since they might
            # be parsed as 0 otherwise. ref #20765
            step = str(Decimal(""1"") / 10 ** self.decimal_places).lower()
        else:
            step = ""any""
        attrs.setdefault(""step"", step)
    return attrs
",if self . decimal_places is not None :,141
"def list_inline_diff(oldlist, newlist, colors=None):
    if not colors:
        colors = init_colors(False)
    diff = simplediff.diff(oldlist, newlist)
    ret = []
    for change, value in diff:
        value = "" "".join(value)
        if change == ""="":
            ret.append(""'%s'"" % value)
        elif change == ""+"":
            item = ""{color_add}+{value}{color_default}"".format(value=value, **colors)
            ret.append(item)
        elif change == ""-"":
            item = ""{color_remove}-{value}{color_default}"".format(value=value, **colors)
            ret.append(item)
    return ""[%s]"" % ("", "".join(ret))
","if change == ""="" :",196
"def StartNanny(self, unresponsive_kill_period=None):
    # The nanny thread is a singleton.
    if NannyController.nanny is None:
        if unresponsive_kill_period is None:
            unresponsive_kill_period = config.CONFIG[""Nanny.unresponsive_kill_period""]
        NannyController.nanny = NannyThread(unresponsive_kill_period)
        NannyController.nanny.start()
",if unresponsive_kill_period is None :,111
"def format_params(params):
    items = list(params.items())
    for k, v in sorted(items, key=itemgetter(0), reverse=True):
        if isinstance(v, bytes):
            v = v.decode(""utf-8"")
        if isinstance(v, six.text_type):
            v = u'""{v}""'.format(v=v)
        yield u""{k}={v}"".format(k=k, v=v)
","if isinstance ( v , bytes ) :",113
"def _init_machine_secrets(self):
    try:
        self.cred.set_machine_account(LP_CTX)
    except NTSTATUSError as e:
        if e.args[0] == ntstatus.NT_STATUS_CANT_ACCESS_DOMAIN_INFO:
            return e.args[0]
        else:
            raise CallError(
                f""Failed to initialize machine account secrets: {e.args[1]}""
            )
    return ntstatus.NT_STATUS_SUCCESS
",if e . args [ 0 ] == ntstatus . NT_STATUS_CANT_ACCESS_DOMAIN_INFO :,131
"def _maybe_load_as_instance_attribute(self, node, obj, name):
    assert isinstance(obj, abstract.SimpleValue)
    if not isinstance(obj.cls, mixin.Class):
        return
    for base in obj.cls.mro:
        if isinstance(base, abstract.ParameterizedClass):
            base = base.base_cls
        if isinstance(base, abstract.PyTDClass):
            var = base.convert_as_instance_attribute(name, obj)
            if var is not None:
                if name in obj.members:
                    obj.members[name].PasteVariable(var, node)
                else:
                    obj.members[name] = var
                return
",if var is not None :,191
"def update_state(self):
    """"""Copy state to uA""""""
    self.state = {
        ""rows"": self.ui.min_rows.value(),
        ""sep"": self.ui.sep_txt.text(),
        ""start"": self.ui.start_txt.text(),
        ""end"": self.ui.end_txt.text(),
    }
    u = self.c.p.v.u
    if ""_lep"" in u:
        if ""csv"" in u[""_lep""]:
            u[""_lep""][""csv""].update(self.state)
        else:
            u[""_lep""][""csv""] = dict(self.state)
    else:
        u[""_lep""] = {""csv"": dict(self.state)}
","if ""csv"" in u [ ""_lep"" ] :",189
"def redraw_all_toplevels():
    """"""A hack to trigger redraws for all windows and widgets.""""""
    for widget in Gtk.Window.list_toplevels():
        if not widget.get_realized():
            continue
        if widget.is_active():
            widget.queue_draw()
            continue
        sensitive = widget.get_sensitive()
        widget.set_sensitive(not sensitive)
        widget.set_sensitive(sensitive)
",if widget . is_active ( ) :,116
"def save(self, *args, **kwargs):
    ""Process form""
    if self.instance:
        if self.is_valid():
            if self.cleaned_data[""location""]:
                self.instance.location = self.cleaned_data[""location""]
            if self.cleaned_data[""status""]:
                self.instance.status = self.cleaned_data[""status""]
            self.instance.save()
            if self.cleaned_data[""delete""]:
                if self.cleaned_data[""delete""] == ""delete"":
                    self.instance.delete()
                if self.cleaned_data[""delete""] == ""trash"":
                    self.instance.trash = True
                    self.instance.save()
","if self . cleaned_data [ ""location"" ] :",195
"def _get_kwargs(self):
    s = u""""
    if self.kwargs:
        s = u"", "".join(""%s=%r"" % (k, v) for k, v in self.kwargs.items())
        if self.args:
            s = u"", "" + s
    return s
",if self . args :,76
"def remove_trailing_spaces(editorWidget):
    cursor = editorWidget.textCursor()
    cursor.beginEditBlock()
    block = editorWidget.document().findBlockByLineNumber(0)
    while block.isValid():
        text = block.text()
        if text.endswith("" ""):
            cursor.setPosition(block.position())
            cursor.select(QTextCursor.LineUnderCursor)
            cursor.insertText(text.rstrip())
        block = block.next()
    cursor.movePosition(QTextCursor.End, QTextCursor.MoveAnchor)
    cursor.endEditBlock()
","if text . endswith ( "" "" ) :",152
"def gtk_on_select(self, selection):
    if self.interface.on_select:
        if self.interface.multiple_select:
            tree_model, tree_path = selection.get_selected_rows()
            if tree_path:
                tree_iter = tree_model.get_iter(tree_path[-1])
            else:
                tree_iter = None
        else:
            tree_model, tree_iter = selection.get_selected()
        # Covert the tree iter into the actual node.
        if tree_iter:
            node = tree_model.get(tree_iter, 0)[0]
        else:
            node = None
        self.interface.on_select(None, node=node)
",if self . interface . multiple_select :,198
"def advance(len=len):
    # If some successor has only one exit, must take it.
    # Else favor successors with fewer exits.
    candidates = []
    for i in succs[self.lastij]:
        e = len(succs[i])
        assert e > 0, ""else remove_from_successors() pruning flawed""
        if e == 1:
            candidates = [(e, i)]
            break
        candidates.append((e, i))
    else:
        candidates.sort()
    for e, i in candidates:
        if i != self.final:
            if remove_from_successors(i):
                self.lastij = i
                yield i
            add_to_successors(i)
",if i != self . final :,190
"def _remove_fs_obj(path):
    if os.path.exists(path):
        log.info(""DSC: Removing {0}"".format(path))
        if not __salt__[""file.remove""](path):
            error = ""Failed to remove {0}"".format(path)
            log.error(""DSC: {0}"".format(error))
            raise CommandExecutionError(error)
","if not __salt__ [ ""file.remove"" ] ( path ) :",101
"def _deploy_k8s_resource(self, yaml_data):
    for data in yaml_data:
        if data is None:
            continue
        kind = data.get(""kind"", None)
        name = data.get(""metadata"").get(""name"", None)
        namespace = data.get(""metadata"").get(""namespace"", None)
        logs = ""Deploy namespace={}, name={}, kind={}"".format(namespace, name, kind)
        logger.info(logs)
        if kind in self.support_namespace:
            self.create_func_dict.get(kind)(namespace, data)
        else:
            self.create_func_dict.get(kind)(data)
        time.sleep(3)
",if data is None :,180
"def _load_npy(self, imdb_path):
    self.db = np.load(imdb_path, allow_pickle=True)
    self.start_idx = 0
    if type(self.db) == dict:
        self.metadata = self.db.get(""metadata"", {})
        self.data = self.db.get(""data"", [])
    else:
        # TODO: Deprecate support for this
        self.metadata = {""version"": 1}
        self.data = self.db
        # Handle old imdb support
        if ""image_id"" not in self.data[0]:
            self.start_idx = 1
    if len(self.data) == 0:
        self.data = self.db
","if ""image_id"" not in self . data [ 0 ] :",184
"def _infere_context_data(self, path: str) -> str:
    """"""If this is a remote session, infere context data if any""""""
    if is_remote_session(self.view):
        window = self.view.window().id()
        try:
            interpreter = Market().get(window).interpreter
        except Exception as e:
            print(""while getting interp for Window ID {}: {}"".format(window, e))
            return path
        directory_map = interpreter.pathmap
        if directory_map is None:
            return path
        for local_dir, remote_dir in directory_map.items():
            if remote_dir in path:
                return path.replace(remote_dir, local_dir)
    return path
",if remote_dir in path :,194
"def generateCoveredList2(revlist=None):
    if not revlist:
        revlist = []
    covered_list = [x for x in revlist if x.outcome == Task.OUTCOME_COVERED]
    while len(covered_list):
        revlist = [x for x in revlist if x.outcome != Task.OUTCOME_COVERED]
        if len(revlist) > 0:
            return revlist
        newlist = _find_task_revdep_list(covered_list)
        revlist = list(set(revlist + newlist))
        covered_list = [x for x in revlist if x.outcome == Task.OUTCOME_COVERED]
    return revlist
",if len ( revlist ) > 0 :,178
"def asyncproxy(proxy, asynchronous=True, **kwargs):
    """"""convenience method to set proxy to asynchronous or sync mode.""""""
    if kwargs:
        kword = list(kwargs.keys())
        if kword != [""async""]:
            raise TypeError(
                ""asyncproxy() got an unexpected keyword argument '{:s}'"".format(
                    kword[0]
                )
            )
        asynchronous = kwargs[""async""]
    proxy._pyroAsync(asynchronous)
","if kword != [ ""async"" ] :",125
"def on_task_filter(self, task, config):
    if not config:
        return
    with Session() as session:
        for entry in task.entries:
            # Cache all new task entries
            if entry.get(""approved""):
                entry.accept(""entry is marked as approved"")
            elif not self._item_query(entry, task, session):
                log.verbose(""creating new pending entry %s"", entry)
                session.add(db.PendingEntry(task_name=task.name, entry=entry))
                entry.reject(""new unapproved entry, caching and waiting for approval"")
","if entry . get ( ""approved"" ) :",161
"def __eq__(self, other):
    if isinstance(other, Table):
        if self.headings != other.headings:
            return False
        for my_row, their_row in zip(self.rows, other.rows):
            if my_row != their_row:
                return False
    else:
        # -- ASSUME: table <=> raw data comparison
        other_rows = other
        for my_row, their_row in zip(self.rows, other_rows):
            if my_row != their_row:
                return False
    return True
",if my_row != their_row :,149
"def _dom_node(self, domroot):
    element = domroot.createElement(self.tagname)
    for attribute, value in self.attlist():
        if isinstance(value, ListType):
            value = "" "".join([serial_escape(""%s"" % v) for v in value])
        element.setAttribute(attribute, ""%s"" % value)
    for child in self.children:
        element.appendChild(child._dom_node(domroot))
    return element
","if isinstance ( value , ListType ) :",116
"def extractor(self, fname):
    fname = os.path.abspath(fname)
    outfile = os.path.splitext(fname)[0]
    try:
        fpout = open(outfile, ""wb"")
        gz = gzip.GzipFile(fname, ""rb"")
        while True:
            data = gz.read(self.BLOCK_SIZE)
            if data:
                fpout.write(data)
            else:
                break
        gz.close()
        fpout.close()
    except KeyboardInterrupt as e:
        raise e
    except Exception as e:
        return False
    return True
",if data :,161
"def pytest_pycollect_makeitem(collector, name, obj):
    """"""A pytest hook to collect asyncio coroutines.""""""
    if collector.funcnamefilter(name) and _is_coroutine(obj):
        item = pytest.Function.from_parent(collector, name=name)
        # Due to how pytest test collection works, module-level pytestmarks
        # are applied after the collection step. Since this is the collection
        # step, we look ourselves.
        transfer_markers(obj, item.cls, item.module)
        item = pytest.Function.from_parent(collector, name=name)  # To reload keywords.
        if ""asyncio"" in item.keywords:
            return list(collector._genfunctions(name, obj))
","if ""asyncio"" in item . keywords :",175
"def entry_from_named_pair(register_pairs, key):
    """"""Returns the entry in key given results provided by register_pairs""""""
    results = register_pairs.get(""results"")
    if results is None:
        raise RuntimeError(
            ""The dict argument does not have a 'results' entry. ""
            ""Must not have been created using 'register' in a loop""
        )
    for result in results:
        item = result.get(""item"")
        if item is not None:
            name = item.get(""name"")
            if name == key:
                return result[""content""]
    raise RuntimeError(
        ""There was no entry found in the dict that had an item with a name that matched {}"".format(
            key
        )
    )
",if item is not None :,196
"def _optimal_split(data, pattern):
    while data:
        match = re.search(pattern, data)
        if not match:
            break
        start, end = match.start(), match.end()
        if start:
            yield False, data[:start]
        yield True, data[start:end]
        data = data[end:]
    if data:
        yield False, data
",if not match :,106
"def will_execute_instruction_callback(self, state, pc, insn):
    world = state.platform
    with self.manticore.locked_context(""seen_rep"", dict) as reps:
        item = (
            world.current_transaction.sort == ""CREATE"",
            world.current_transaction.address,
            pc,
        )
        if item not in reps:
            reps[item] = 0
        reps[item] += 1
        if reps[item] > self.loop_count_threshold:
            state.abandon()
",if reps [ item ] > self . loop_count_threshold :,146
"def _check_drift_dyn_gen(drift):
    if not isinstance(drift, Qobj):
        if not isinstance(drift, (list, tuple)):
            raise TypeError(""drift should be a Qobj or a list of Qobj"")
        else:
            for d in drift:
                if not isinstance(d, Qobj):
                    raise TypeError(""drift should be a Qobj or a list of Qobj"")
","if not isinstance ( drift , ( list , tuple ) ) :",116
"def _get_or_create_tracker_id(class_def):
    with _DYNAMIC_CLASS_TRACKER_LOCK:
        class_tracker_id = _DYNAMIC_CLASS_TRACKER_BY_CLASS.get(class_def)
        if class_tracker_id is None:
            class_tracker_id = uuid.uuid4().hex
            _DYNAMIC_CLASS_TRACKER_BY_CLASS[class_def] = class_tracker_id
            _DYNAMIC_CLASS_TRACKER_BY_ID[class_tracker_id] = class_def
    return class_tracker_id
",if class_tracker_id is None :,153
"def all(self):
    regs = arch_to_regs[pwndbg.arch.current]
    retval = []
    for regset in (
        regs.pc,
        regs.stack,
        regs.frame,
        regs.retaddr,
        regs.flags,
        regs.gpr,
        regs.misc,
    ):
        if regset is None:
            continue
        elif isinstance(regset, (list, tuple)):
            retval.extend(regset)
        elif isinstance(regset, dict):
            retval.extend(regset.keys())
        else:
            retval.append(regset)
    return retval
","elif isinstance ( regset , ( list , tuple ) ) :",172
"def unpack_tarball(tar_filename, dest):
    print(""Unpacking %s into %s"" % (os.path.basename(tar_filename), dest))
    tar = tarfile.open(tar_filename)
    base_dir = None
    for member in tar:
        base_name = member.name.split(""/"")[0]
        if base_dir is None:
            base_dir = base_name
        else:
            if base_dir != base_name:
                print(""Unexpected path in %s: %s"" % (tar_filename, base_name))
    _extractall(tar, dest)
    tar.close()
    return os.path.join(dest, base_dir)
",if base_dir != base_name :,174
"def get(self, path, params={}):
    final_url = ""%s/public/api/customers/%s%s"" % (self.url, self.customer_id, path)
    try:
        response = requests.get(
            url=final_url, headers=self.auth_headers(), verify=self.verify_ssl
        )
        if response and response.status_code in [requests.codes.ok, requests.codes.bad]:
            return response.json()
    except requests.exceptions.RequestException as e:
        logger.exception(e)
","if response and response . status_code in [ requests . codes . ok , requests . codes . bad ] :",139
"def _hash_literal(self):
    self._expect(""{"", consume_whitespace=True)
    keyvals = {}
    while self._current() != ""}"":
        key = self._key()
        self._expect(""="", consume_whitespace=True)
        v = self._explicit_values()
        self._consume_whitespace()
        if self._current() != ""}"":
            self._expect("","")
            self._consume_whitespace()
        keyvals[key] = v
    self._expect(""}"")
    return keyvals
","if self . _current ( ) != ""}"" :",126
"def authenticate(self, identifier, secret):
    try:
        user = self.plugin_manager.hook.flaskbb_authenticate(
            identifier=identifier, secret=secret
        )
        if user is None:
            raise StopAuthentication(_(""Wrong username or password.""))
        self.plugin_manager.hook.flaskbb_post_authenticate(user=user)
        return user
    except StopAuthentication as e:
        self.plugin_manager.hook.flaskbb_authentication_failed(identifier=identifier)
        raise
    finally:
        try:
            self.session.commit()
        except Exception:
            logger.exception(""Exception while processing login"")
            self.session.rollback()
            raise
",if user is None :,177
"def compute_nullable_nonterminals(self):
    nullable = set()
    num_nullable = 0
    while True:
        for p in self.grammar.Productions[1:]:
            if p.len == 0:
                nullable.add(p.name)
                continue
            for t in p.prod:
                if t not in nullable:
                    break
            else:
                nullable.add(p.name)
        if len(nullable) == num_nullable:
            break
        num_nullable = len(nullable)
    return nullable
",if p . len == 0 :,154
"def test_cmd_query_iter(self):
    self._setup()
    stmt = ""SELECT 1; INSERT INTO %s VALUES (1),(2); SELECT 3""
    results = []
    try:
        for result in self.cnx.cmd_query_iter(stmt % self.table):
            results.append(result)
            if ""columns"" in result:
                results.append(self.cnx.get_rows())
    except NotImplementedError:
        # Some cnx are not implementing this
        if not isinstance(self.cnx, CMySQLConnection):
            raise
","if not isinstance ( self . cnx , CMySQLConnection ) :",148
"def test_looks_in_path(self):
    path_dirs = set(environ[""PATH""].split(os.path.pathsep))
    dirs = path_dirs - set(os.defpath.split(os.path.pathsep))
    for d in dirs:
        if os.path.isdir(d):
            for file_path in sorted(os.listdir(d)):
                p = os.path.join(d, file_path)
                if os.path.isfile(p) and os.access(p, os.X_OK):
                    print_d(""Testing %s"" % p)
                    self.failUnless(iscommand(p), msg=p)
                    return
",if os . path . isdir ( d ) :,180
"def show_consts(self, consts, level=0):
    indent = INDENT * level
    for i, obj in enumerate(consts):
        if isinstance(obj, types.CodeType):
            print(""%s%s (code object)"" % (indent, i))
            # RECURSIVE CALL.
            self.show_code(obj, level=level + 1)
        else:
            print(""%s%s %r"" % (indent, i, obj))
","if isinstance ( obj , types . CodeType ) :",118
"def _get_table(table_name):
    best_match = None
    best_score = 0
    for table in VSCtl._TABLES:
        score = VSCtl._score_partial_match(table.table_name, table_name)
        if score > best_score:
            best_match = table
            best_score = score
        elif score == best_score:
            best_match = None
    if best_match:
        return best_match
    elif best_score:
        vsctl_fatal('multiple table names match ""%s""' % table_name)
    else:
        vsctl_fatal('unknown table ""%s""' % table_name)
",if score > best_score :,172
"def test_re_escape(self):
    alnum_chars = string.ascii_letters + string.digits + ""_""
    p = """".join(chr(i) for i in range(256))
    for c in p:
        if c in alnum_chars:
            self.assertEqual(re.escape(c), c)
        elif c == ""\x00"":
            self.assertEqual(re.escape(c), ""\\000"")
        else:
            self.assertEqual(re.escape(c), ""\\"" + c)
        self.assertMatch(re.escape(c), c)
    self.assertMatch(re.escape(p), p)
",if c in alnum_chars :,160
"def get_setuptools_version():
    with open(""requirements.txt"") as f:
        for line in f:
            if line.startswith(""setuptools""):
                line = line.rstrip()
                if "">="" not in line:
                    raise ValueError('%s doesnt use "">=""' % line)
                _, version = line.split("">="")
                return version
","if "">="" not in line :",99
"def mouse_inside_delete_button():
    for idx, obj in enumerate(img_objects):
        if idx == selected_bbox:
            ind, x1, y1, x2, y2 = obj
            x1_c, y1_c, x2_c, y2_c = get_close_icon(x1, y1, x2, y2)
            if is_mouse_inside_points(x1_c, y1_c, x2_c, y2_c):
                return True
    return False
",if idx == selected_bbox :,138
"def check_sanity(self):
    super().check_sanity()
    if self.suicided:
        assert self.left is None
        assert self.right is None
    else:
        left = self.left
        if left.suicided:
            assert left.right is None
        else:
            assert left.right is self
        right = self.right
        if right.suicided:
            assert right.left is None
        else:
            assert right.left is self
",if left . suicided :,133
"def getbool(self, option, default=False):
    try:
        val = super(ServiceDef, self).get(self.name, option)
        if val.lower() == ""true"":
            val = True
        else:
            val = False
    except:
        val = default
    return val
","if val . lower ( ) == ""true"" :",82
"def get_op_number(self, prog):
    graph = IrGraph(core.Graph(prog.desc), for_test=False)
    quant_op_nums = 0
    op_nums = 0
    for op in graph.all_op_nodes():
        if op.name() in [""conv2d"", ""depthwise_conv2d"", ""mul""]:
            op_nums += 1
        elif ""fake_"" in op.name():
            quant_op_nums += 1
    return op_nums, quant_op_nums
","if op . name ( ) in [ ""conv2d"" , ""depthwise_conv2d"" , ""mul"" ] :",131
"def check_eol(filename):
    eol = u""\n""
    with open(filename, ""rb"") as f:
        d = f.read()
        if b""\r\n"" in d:
            eol = u""\r\n""
        elif b""\n"" in d:
            eol = u""\n""
        elif b""\r"" in d:
            eol = u""\r""
    return eol
","elif b""\r"" in d :",109
"def wait_for_parent_death(orig_parent_pid):
    while True:
        ppid = os.getppid()
        if ppid != orig_parent_pid:
            return
        # on some systems, getppid will keep returning
        # a dead pid, so check it for liveness
        try:
            os.kill(ppid, 0)
        except OSError:  # Probably ENOSUCH
            return
",if ppid != orig_parent_pid :,117
"def _get_project_vcf(x, suffix=""""):
    """"""Get our project VCF, either from the population or the variant batch file.""""""
    vcfs = _get_variant_file(x, (""population"", ""vcf""), suffix=suffix)
    if not vcfs:
        vcfs = _get_variant_file(
            x, (""vrn_file_batch"",), suffix=suffix, ignore_do_upload=True
        )
        if not vcfs and x.get(""variantcaller"") == ""ensemble"":
            vcfs = _get_variant_file(x, (""vrn_file"",), suffix=suffix)
    return vcfs
","if not vcfs and x . get ( ""variantcaller"" ) == ""ensemble"" :",155
"def check_parameter_types(params):
    for p in params:
        cl = p.__class__
        if cl not in CACHABLE_PARAM_TYPES:
            if cl in ITERABLES:
                check_parameter_types(p)
            elif cl is dict:
                check_parameter_types(p.items())
            else:
                raise UncachableQuery
",elif cl is dict :,107
"def convert(self, value, param, ctx):
    # Exact match
    if value in self.choices:
        return self.typ(value)
    if ctx is not None and ctx.token_normalize_func is not None:
        value = ctx.token_normalize_func(value)
        for choice in self.casted_choices:
            if ctx.token_normalize_func(choice) == value:
                return choice
    self.fail(
        ""invalid choice: %s. (choose from %s)"" % (value, "", "".join(self.choices)),
        param,
        ctx,
    )
",if ctx . token_normalize_func ( choice ) == value :,155
"def do_branch_rename_test(self, repo, q):
    st = monotonic()
    while monotonic() - st < 1:
        # Give inotify time to deliver events
        ans = repo.branch()
        if hasattr(q, ""__call__""):
            if q(ans):
                break
        else:
            if ans == q:
                break
        sleep(0.01)
    if hasattr(q, ""__call__""):
        self.assertTrue(q(ans))
    else:
        self.assertEqual(ans, q)
",if q ( ans ) :,143
"def make_dict(s: Sequence[T]) -> Dict[str, List[T]]:
    res: Dict[str, List[T]] = {}
    for a in s:
        k = key(a)
        ll = res.get(k, None)
        if ll is None:
            ll = []
            res[k] = ll
        ll.append(a)
    return res
",if ll is None :,102
"def collectAccidentalDisplayStatus(s_inner):
    post = []
    for e in s_inner.flat.notes:
        if e.pitch.accidental is not None:
            post.append(e.pitch.accidental.displayStatus)
        else:  # mark as not having an accidental
            post.append(""x"")
    return post
",if e . pitch . accidental is not None :,95
"def close(self):
    if self.housekeeper:
        self.housekeeper.stop.set()
        self.housekeeper.join()
        self.housekeeper = None
    if self.sock:
        sockname = None
        try:
            sockname = self.sock.getsockname()
        except (socket.error, OSError):
            pass
        try:
            self.sock.close()
            if type(sockname) is str:
                # it was a Unix domain socket, remove it from the filesystem
                if os.path.exists(sockname):
                    os.remove(sockname)
        except Exception:
            pass
        self.sock = None
    self.pool.close()
",if type ( sockname ) is str :,193
"def vars(self):
    ret = []
    if op.disklist:
        varlist = list(map(self.basename, op.disklist))
    else:
        varlist = []
        for name in self.discover:
            if self.diskfilter.match(name):
                continue
            if name not in blockdevices():
                continue
            varlist.append(name)
        #           if len(varlist) > 2: varlist = varlist[0:2]
        varlist.sort()
    for name in varlist:
        if name in self.discover:
            ret.append(name)
    return ret
",if self . diskfilter . match ( name ) :,180
"def computeShortcutModes(self, mode):
    if isinstance(mode, string_types):
        if mode == ""all"":
            mode = (""failing"", ""passing"", ""warnings"", ""exception"", ""cancelled"")
        elif mode == ""warnings"":
            mode = (""failing"", ""warnings"")
        else:
            mode = (mode,)
    return mode
","if mode == ""all"" :",93
"def content(self):
    """"""Content of the response, in bytes.""""""
    if self._content is False:
        # Read the contents.
        if self._content_consumed:
            raise RuntimeError(""The content for this response was already consumed"")
        if self.status_code == 0 or self.raw is None:
            self._content = None
        else:
            self._content = b"""".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""""
    self._content_consumed = True
    # don't need to release the connection; that's been handled by urllib3
    # since we exhausted the data.
    return self._content
",if self . status_code == 0 or self . raw is None :,161
"def _get_key_attrs(self, keydir, keyname, keysize, user):
    if not keydir:
        if ""gen_keys_dir"" in self.opts:
            keydir = self.opts[""gen_keys_dir""]
        else:
            keydir = self.opts[""pki_dir""]
    if not keyname:
        if ""gen_keys"" in self.opts:
            keyname = self.opts[""gen_keys""]
        else:
            keyname = ""minion""
    if not keysize:
        keysize = self.opts[""keysize""]
    return keydir, keyname, keysize, user
","if ""gen_keys"" in self . opts :",160
"def find_command_from_list(cls, files: str) -> Optional[str]:
    for file in files:
        if os.path.isabs(file):
            if os.path.exists(file):
                return file
        else:
            path = find_executable(file)
            if path is not None:
                return os.path.abspath(path)
    return None
",if os . path . exists ( file ) :,105
"def build_list_params(self, params, items, label):
    if isinstance(items, basestring):
        items = [items]
    for index, item in enumerate(items):
        i = index + 1
        if isinstance(item, dict):
            for k, v in item.iteritems():
                params[label % (i, ""Name"")] = k
                if v is not None:
                    params[label % (i, ""Value"")] = v
        else:
            params[label % i] = item
",if v is not None :,138
"def do_commit_twophase(self, connection, xid, is_prepared=True, recover=False):
    if is_prepared:
        if recover:
            connection.execute(""ROLLBACK"")
        connection.execute(""COMMIT PREPARED '%s'"" % xid)
        connection.execute(""BEGIN"")
        self.do_rollback(connection.connection)
    else:
        self.do_commit(connection.connection)
",if recover :,110
"def __init__(self, language, filename, type, video, link, fps):
    super(YavkaNetSubtitle, self).__init__(language)
    self.filename = filename
    self.page_link = link
    self.type = type
    self.video = video
    self.fps = fps
    self.release_info = filename
    if fps:
        if video.fps and float(video.fps) == fps:
            self.release_info += "" <b>[{:.3f}]</b>"".format(fps)
        else:
            self.release_info += "" [{:.3f}]"".format(fps)
",if video . fps and float ( video . fps ) == fps :,156
"def test_all(self):
    # objects defined in the module should be in __all__
    expected = []
    for name in dir(socketserver):
        if not name.startswith(""_""):
            mod_object = getattr(socketserver, name)
            if getattr(mod_object, ""__module__"", None) == ""socketserver"":
                expected.append(name)
    self.assertCountEqual(socketserver.__all__, expected)
","if not name . startswith ( ""_"" ) :",106
"def _get_proxy(self, url):
    if self._proxy:
        proxy = self._proxy
        scheme = url.split("":"")[0] if url else None
        if scheme:
            if scheme in proxy:
                return proxy[scheme]
            scheme = scheme[0:-1]
            if scheme in proxy:
                return proxy[scheme]
    return None
",if scheme :,101
"def get_ip(self):
    if self.ip is None:
        regex = re.compile(r""^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$"")
        result = regex.match(self.host)
        if not result:
            self.ip = socket.gethostbyname(self.host)
        else:
            self.ip = self.host
    return self.ip
",if not result :,114
"def get_queryset(self, request, username, local_site_name=None, *args, **kwargs):
    try:
        local_site = self._get_local_site(local_site_name)
        if local_site:
            user = local_site.users.get(username=username)
            profile = user.get_profile()
        else:
            profile = Profile.objects.get(user__username=username)
        q = self.watched_resource.get_queryset(
            request, local_site_name=local_site_name, *args, **kwargs
        )
        q = q.filter(starred_by=profile)
        return q
    except Profile.DoesNotExist:
        return self.watched_resource.model.objects.none()
",if local_site :,197
"def _check_generated_stderr(self, stderr, n):
    expected = [
        r""\[stderr:\d+\]"",
        ""^stderr$"",
        ""^stderr2$"",
    ] * n
    self.assertNotIn(""\n\n"", stderr)
    lines = stderr.splitlines()
    self.assertEqual(len(lines), len(expected), stderr)
    for line, expect in zip(lines, expected):
        if isinstance(expect, str):
            expect = [expect]
        for ex in expect:
            assert re.search(ex, line) is not None, ""Expected %r in %r"" % (ex, line)
","if isinstance ( expect , str ) :",156
"def _validate_context_aware_object_factories(cls, context_aware_object_factories):
    if not context_aware_object_factories:
        return {}
    for alias, obj in context_aware_object_factories.items():
        cls._validate_alias(""context_aware_object_factories"", alias, obj)
        cls._validate_not_targets(""context_aware_object_factories"", alias, obj)
        if not callable(obj):
            raise TypeError(
                ""The given context aware object factory {alias!r} must be a callable."".format(
                    alias=alias
                )
            )
    return context_aware_object_factories.copy()
",if not callable ( obj ) :,181
"def _formatparam(param, value=None, quote=True):
    if value is not None and len(value) > 0:
        if isinstance(value, tuple):
            value = ""a""
        if quote or param:
            pass
        else:
            return ""%s=%s"" % (param, value)
    else:
        return param
",if quote or param :,93
"def process_data(self, data):
    host_to_ips = {}
    for interface, details in data.items():
        ips = []
        ip_details = details.get(self.ip_type)
        if not ip_details:
            continue
        ips.append(ip_details[""address""])
        if ""additional_ips"" in ip_details:
            ips.extend([ip[""address""] for ip in ip_details[""additional_ips""]])
        host_to_ips[interface] = ips
    return host_to_ips
","if ""additional_ips"" in ip_details :",138
"def _get_error_file(self):
    for error_handler in self._server_configuration.error_handlers or []:
        if not error_handler.error_code or error_handler.error_code == ""default"":
            return os.path.join(
                self._server_configuration.application_root, error_handler.file
            )
    else:
        return None
","if not error_handler . error_code or error_handler . error_code == ""default"" :",98
"def setComboValue(self, combo, value, t=""int""):
    for i in range(combo.count()):
        if t == ""int"" and value == combo.itemData(i):
            combo.setCurrentIndex(i)
            break
        if t == ""str"" and value == combo.itemData(i):
            combo.setCurrentIndex(i)
            break
","if t == ""int"" and value == combo . itemData ( i ) :",96
"def read_isbn(self, f):
    found = []
    for k, v in f.get_subfields([""a"", ""z""]):
        m = re_isbn_and_price.match(v)
        if not m:
            m = re_isbn.match(v)
        if not m:
            continue
        found.append(m.group(1))
    return found
",if not m :,104
"def open(self, name, mode=""r"", buffering=0):
    ap = self.abspath(name)
    self.log(""open {ap} with mode {m}\n"".format(ap=ap, m=mode))
    if ""r"" in mode:
        try:
            reader = ZipReader(self, ap, mode, buffering)
        except:
            raise errors.OperationFailure(""Not found!"")
        else:
            return reader
    elif ""w"" in mode:
        if ap in self.zf.namelist():
            self._update(remove=[ap])
        return ZipWriter(self, ap, mode, buffering)
    else:
        raise errors.OperationFailure(""Unsupported mode!"")
",if ap in self . zf . namelist ( ) :,177
"def chain(nested):
    """"""itertools.chain() but leaves strings untouched""""""
    for i in nested:
        if isinstance(i, (list, tuple)):
            yield from chain(i)
        elif isinstance(i, BlueprintGroup):
            yield from i.blueprints
        else:
            yield i
","if isinstance ( i , ( list , tuple ) ) :",82
"def __init__(self, ref, parent, sortkey, handle, secondary):
    if sortkey:
        self.name = sortkey
        # sortkey must be localized sort, so
        self.sortkey = glocale.sort_key(sortkey)
        if not self.sortkey:
            self.sortkey = glocale.sort_key("""")
    else:
        self.name = """"
        self.sortkey = glocale.sort_key("""")
    self.ref = ref
    self.handle = handle
    self.secondary = secondary
    self.parent = parent
    self.prev = None
    self.next = None
    self.children = []
",if not self . sortkey :,160
"def set_variant(self, variant):
    self.clear()
    if variant is not None:
        if isinstance(variant, Package):
            self.set_package(variant)
            return
        self.set_package(variant.parent)
        if variant.index is not None:
            self.allow_selection = True
            self.selectRow(variant.index)
            self.allow_selection = False
    self.variant = variant
","if isinstance ( variant , Package ) :",116
"def visit_entry(self, node):
    self.entry += 1
    if self.pep_table and self.entry == 2 and len(node) == 1:
        node[""classes""].append(""num"")
        p = node[0]
        if isinstance(p, nodes.paragraph) and len(p) == 1:
            text = p.astext()
            try:
                pep = int(text)
                ref = self.document.settings.pep_base_url + self.pep_url % pep
                p[0] = nodes.reference(text, text, refuri=ref)
            except ValueError:
                pass
","if isinstance ( p , nodes . paragraph ) and len ( p ) == 1 :",168
"def fill_config_with_default_values(config, default_values):
    for section in default_values.keys():
        if not config.has_section(section):
            config.add_section(section)
        for (opt, val) in default_values[section].items():
            if not config.has_option(section, opt):
                config.set(section, opt, f""{val}"")
","if not config . has_option ( section , opt ) :",105
"def _add_embeddings_internal(self, sentences: List[Sentence]):
    if type(sentences) is Sentence:
        sentences = [sentences]
    for embedding in self.embeddings:
        embedding.embed(sentences)
        # iterate over sentences
        for sentence in sentences:
            sentence: Sentence = sentence
            # if its a forward LM, take last state
            if embedding.is_forward_lm:
                sentence.set_embedding(
                    embedding.name,
                    sentence[len(sentence) - 1]._embeddings[embedding.name],
                )
            else:
                sentence.set_embedding(
                    embedding.name, sentence[0]._embeddings[embedding.name]
                )
    return sentences
",if embedding . is_forward_lm :,199
"def remove(files):
    """"""Remove files and directories""""""
    for f in files.split():
        try:
            if os.path.isdir(f) and not os.path.islink(f):
                shutil.rmtree(f)
            else:
                os.remove(f)
        except OSError:
            pass
",if os . path . isdir ( f ) and not os . path . islink ( f ) :,90
"def check_materials():
    for obj in bpy.data.objects:
        if obj.type == ""MESH"":
            if obj.TLM_ObjectProperties.tlm_mesh_lightmap_use:
                for slot in obj.material_slots:
                    mat = slot.material
                    if mat is None:
                        if bpy.context.scene.TLM_SceneProperties.tlm_verbose:
                            print(""MatNone"")
                        mat = bpy.data.materials.new(name=""Material"")
                        mat.use_nodes = True
                        slot.material = mat
                    nodes = mat.node_tree.nodes
","if obj . type == ""MESH"" :",196
"def __cleanup_file(filename):
    data = None
    dirty = False
    with open(filename, mode=""r"", newline="""") as file:
        data = file.read()
        if ""\r\n"" in data:
            data = data.replace(""\r\n"", ""\n"")
            dirty = True
        if ""\r"" in data:
            data = data.replace(""\r"", ""\n"")
            dirty = True
    if dirty:
        new_filename = filename + "".new""
        with open(new_filename, mode=""w"", newline=""\n"") as new_file:
            new_file.write(data)
        os.replace(new_filename, filename)
","if ""\r\n"" in data :",177
"def _ext_service(self, entity_id, typ, service, binding):
    try:
        srvs = self[entity_id][typ]
    except KeyError:
        return None
    if not srvs:
        return srvs
    res = []
    for srv in srvs:
        if ""extensions"" in srv:
            for elem in srv[""extensions""][""extension_elements""]:
                if elem[""__class__""] == service:
                    if elem[""binding""] == binding:
                        res.append(elem)
    return res
","if elem [ ""__class__"" ] == service :",142
"def _set_cache_value(self, key, value, type):
    """"""Used internally by the accessor properties.""""""
    if type is bool:
        if value:
            self[key] = None
        else:
            self.pop(key, None)
    else:
        if value is None:
            self.pop(key, None)
        elif value is True:
            self[key] = None
        else:
            self[key] = value
",if value :,124
"def lngettext(self, msgid1, msgid2, n):
    try:
        tmsg = self._catalog[(msgid1, self.plural(n))]
        if self._output_charset:
            return tmsg.encode(self._output_charset)
        return tmsg.encode(locale.getpreferredencoding())
    except KeyError:
        if self._fallback:
            return self._fallback.lngettext(msgid1, msgid2, n)
        if n == 1:
            return msgid1
        else:
            return msgid2
",if n == 1 :,144
"def colorformat(text):
    if text[0:1] == ""#"":
        col = text[1:]
        if len(col) == 6:
            return col
        elif len(col) == 3:
            return col[0] + ""0"" + col[1] + ""0"" + col[2] + ""0""
    elif text == """":
        return """"
    assert False, ""wrong color format %r"" % text
",elif len ( col ) == 3 :,111
"def TryMerge(self, d):
    while d.avail() > 0:
        tt = d.getVarInt32()
        if tt == 8:
            self.set_source(d.getVarInt32())
            continue
        if tt == 18:
            length = d.getVarInt32()
            tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
            d.skip(length)
            self.add_index_spec().TryMerge(tmp)
            continue
        if tt == 0:
            raise ProtocolBuffer.ProtocolBufferDecodeError
        d.skipData(tt)
",if tt == 0 :,169
"def getResults(self):
    if hasattr(self, ""outputFile""):
        return [os.path.join(self.dir, self.outputFile)]
    else:
        ignore_regexps = [re.compile(s) for s in getattr(self, ""ignorePatterns"", [])]
        results = []
        for root, _, files in os.walk(self.output_dir()):
            for name in files:
                path = os.path.join(root, name)
                if not any(r.search(path) for r in ignore_regexps):
                    results.append(path)
        return results
",if not any ( r . search ( path ) for r in ignore_regexps ) :,160
"def __cmp__(self, other):
    if self.numberOfContours <= 0:
        return cmp(self.__dict__, other.__dict__)
    else:
        if cmp(len(self.coordinates), len(other.coordinates)):
            return 1
        ctest = Numeric.alltrue(
            Numeric.alltrue(Numeric.equal(self.coordinates, other.coordinates))
        )
        ftest = Numeric.alltrue(Numeric.equal(self.flags, other.flags))
        if not ctest or not ftest:
            return 1
        return cmp(self.endPtsOfContours, other.endPtsOfContours) or cmp(
            self.program, other.instructions
        )
",if not ctest or not ftest :,180
"def time_single_gate_transpile(self):
    if self.local_qasm_simulator is None:
        self.single_gate_circuit.compile(""single_gate"")
    else:
        if self.has_compile:
            qiskit.compile(self.single_gate_circuit, self.local_qasm_simulator)
        else:
            circ = qiskit.compiler.transpile(
                self.single_gate_circuit, self.local_qasm_simulator
            )
            qiskit.compiler.assemble(circ, self.local_qasm_simulator)
",if self . has_compile :,162
"def update_sockets(self):
    socket_info = parse_sockets(self)
    if not socket_info[""inputs""]:
        return
    for k, v in socket_info.items():
        if not (k in {""inputs"", ""outputs""}):
            continue
        if not self.add_or_update_sockets(k, v):
            print(""failed to load sockets for "", k)
            return
        self.flush_excess_sockets(k, v)
    self.node_dict[hash(self)] = {}
    self.node_dict[hash(self)][""sockets""] = socket_info
    return True
","if not ( k in { ""inputs"" , ""outputs"" } ) :",158
"def test_ranks(self):
    """"""ranks lists are handled correctly""""""
    rank_user = create_test_user(""Visible"", ""visible@te.com"")
    for rank in Rank.objects.iterator():
        rank_user.rank = rank
        rank_user.save()
        rank_link = reverse(""misago:users-rank"", kwargs={""slug"": rank.slug})
        response = self.client.get(rank_link)
        if rank.is_tab:
            self.assertEqual(response.status_code, 200)
            self.assertContains(response, rank_user.get_absolute_url())
        else:
            self.assertEqual(response.status_code, 404)
",if rank . is_tab :,175
"def __repr__(self):
    result = [""<%s.%s"" % (self.__class__.__module__, self.__class__.__qualname__)]
    if not self.closed:
        result.append("" name=%r mode=%r"" % (self.name, self.mode))
        if self._compress_type != ZIP_STORED:
            result.append(
                "" compress_type=%s""
                % compressor_names.get(self._compress_type, self._compress_type)
            )
    else:
        result.append("" [closed]"")
    result.append("">"")
    return """".join(result)
",if self . _compress_type != ZIP_STORED :,154
"def _recover_tasks(self, agent_tasks):
    orphans = _drone_manager.get_orphaned_autoserv_processes()
    for agent_task in agent_tasks:
        agent_task.recover()
        if agent_task.monitor and agent_task.monitor.has_process():
            orphans.discard(agent_task.monitor.get_process())
        self.add_agent_task(agent_task)
    self._check_for_remaining_orphan_processes(orphans)
",if agent_task . monitor and agent_task . monitor . has_process ( ) :,123
"def cookie_added(self, cookie: QNetworkCookie):
    if self.check_cookie_domain(cookie):
        name = cookie.name().data().decode()
        value = cookie.value().data().decode()
        self.saved_cookies[name] = value
        for _name in self.required_cookies:
            if _name not in self.saved_cookies:
                break
        else:
            print(self.required_cookies, self.saved_cookies)
            self.succeed.emit(self.saved_cookies)
",if _name not in self . saved_cookies :,139
"def delete_object(self, handle_list, link_type):
    model = self._widget.get_model()
    if model:
        for o in model:
            if o[0] == link_type:
                data = pickle.loads(o[1]._obj)
                if data[2] in handle_list:
                    model.remove(o.iter)
",if data [ 2 ] in handle_list :,101
"def apply_changelog_batch(
    self,
    batch: Iterable[EventT],
    to_key: Callable[[Any], KT],
    to_value: Callable[[Any], VT],
) -> None:
    """"""Apply batch of events from changelog topic to this store.""""""
    for event in batch:
        key = event.message.key
        if key is None:
            raise TypeError(f""Changelog entry is missing key: {event.message}"")
        value = event.message.value
        if value is None:
            self._del(key)
        else:
            # keys/values are already JSON serialized in the message
            self._set(key, value)
",if key is None :,170
"def execute(self, app):
    n = self[""name""]
    if not n or n == ""default"":
        n = ""Bowl""
    bowl = Bowl(n)
    blocks = bowl.calc(self.fromMm(""D""), math.radians(self[""res""]), self[""pocket""])
    if len(blocks) > 0:
        active = app.activeBlock()
        if active == 0:
            active = 1
        app.gcode.insBlocks(active, blocks, ""Create BOWL"")
        app.refresh()
        app.setStatus(_(""Generated: BOWL""))
    else:
        app.setStatus(_(""Error: Check the Bowl and End Mill parameters""))
",if active == 0 :,181
"def check_token(self):
    if self.token_is_available():
        return True
    else:
        new_UID = input(""Censys API UID:"")
        new_secret = input(""Censys API SECRET"")
        self.uid = new_UID
        self.secret = new_secret
        if self.token_is_available():
            self.write_conf()
            return True
        else:
            logger.error(
                ""The shodan api token is incorrect. ""
                ""Please enter the correct api token.""
            )
            self.check_token()
",if self . token_is_available ( ) :,165
"def coords(cls, dataset, dim, ordered=False, expanded=False, edges=False):
    dim = dataset.get_dimension(dim, strict=True)
    if expanded:
        data = util.expand_grid_coords(dataset, dim)
        if edges and data.shape == dataset.data.shape:
            data = cls._infer_interval_breaks(data, axis=1)
            data = cls._infer_interval_breaks(data, axis=0)
        return data
    values = cls.values(dataset, dim, expanded=False)
    if edges:
        return cls._infer_interval_breaks(values)
    else:
        return values
",if edges and data . shape == dataset . data . shape :,165
