input,expected_if,predicted_if,code_bleu_score,bleu_4_score,exact_match
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True):     try:         return self._read(count, timeout)     except usb.USBError as e:         if DEBUG_COMM:             log.info(                 ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s""                 % (e.errno, e.strerror, e.message, repr(e))             )         <MASK>             return []         if ignore_non_errors and is_noerr(e):             return []         raise",if ignore_timeouts and is_timeout ( e ) :,if ignore_timeouts and not e . errno :,34.98330125272251,34.98330125272251,0.0
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None):     """"""cache hidden states into memory.""""""     if mem_len is None or mem_len == 0:         return None     else:         if reuse_len is not None and reuse_len > 0:             curr_out = curr_out[:reuse_len]         <MASK>             new_mem = curr_out[-mem_len:]         else:             new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]     new_mem.stop_gradient = True     return new_mem",if prev_mem is None :,if prev_mem is None :,100.00000000000004,100.00000000000004,1.0
def filtered(gen):     for example in gen:         example_len = length_fn(example)         # Checking max length boundary.         if max_length is not None:             <MASK>                 continue         # Checking min length boundary.         if min_length is not None:             if example_len < min_length:                 continue         # Within bounds.         yield example,if example_len > max_length :,if example_len > max_length :,100.00000000000004,100.00000000000004,1.0
"def search(self, query):     # ""Search.ashx?query="" + query + filterVal     if not query:         logger.debug(""Empty search query"")         return []     logger.debug('Searching TuneIn for ""%s""' % query)     args = ""&query="" + query     search_results = self._tunein(""Search.ashx"", args)     results = []     for item in self._flatten(search_results):         <MASK>             # Only return stations             self._stations[item[""guide_id""]] = item             results.append(item)     return results","if item . get ( ""type"" , """" ) == ""audio"" :","if item [ ""guide_id"" ] in self . _stations :",5.663311452337806,5.663311452337806,0.0
"def _check_script(self, script, directive):     for var in compile_script(script):         <MASK>             # Skip variable checks             return False         if var.can_contain("".""):             # Yay! Our variable can contain any symbols!             reason = (                 'At least variable ""${var}"" can contain untrusted user input'.format(                     var=var.name                 )             )             self.add_issue(directive=[directive] + var.providers, reason=reason)             return True     return False","if var . must_contain ( ""/"" ) :","if var . can_contain ( ""."" ) :",39.181891500702136,39.181891500702136,0.0
"def getAllDataLinkIDs():     linkDataIDs = set()     dataType = _forestData.dataTypeBySocket     for socketID, linkedIDs in _forestData.linkedSockets.items():         for linkedID in linkedIDs:             <MASK>  # check which one is origin/target                 linkDataIDs.add(                     (socketID, linkedID, dataType[socketID], dataType[linkedID])                 )             else:                 linkDataIDs.add(                     (linkedID, socketID, dataType[linkedID], dataType[socketID])                 )     return linkDataIDs",if socketID [ 1 ] :,if socketID == linkedID :,17.965205598154213,17.965205598154213,0.0
"def _stderr_supports_color():     try:         if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty():             if curses:                 curses.setupterm()                 <MASK>                     return True             elif colorama:                 if sys.stderr is getattr(                     colorama.initialise, ""wrapped_stderr"", object()                 ):                     return True     except Exception:         # Very broad exception handling because it's always better to         # fall back to non-colored logs than to break at startup.         pass     return False","if curses . tigetnum ( ""colors"" ) > 0 :",if sys . stderr is not None :,4.408194605881708,4.408194605881708,0.0
"def offsets(self):     offsets = {}     offset_so_far = 0     for name, ty in self.fields.items():         if isinstance(ty, SimTypeBottom):             l.warning(                 ""Found a bottom field in struct %s. Ignore and increment the offset using the default ""                 ""element size."",                 self.name,             )             continue         if not self._pack:             align = ty.alignment             <MASK>                 offset_so_far += align - offset_so_far % align         offsets[name] = offset_so_far         offset_so_far += ty.size // self._arch.byte_width     return offsets",if offset_so_far % align != 0 :,if align :,1.9758011175389976,0.0,0.0
"def Restore(self):     picker, obj = self._window, self._pObject     value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH)     if value is not None:         if issubclass(picker.__class__, wx.FileDialog):             <MASK>                 value = value[-1]         picker.SetPath(value)         return True     return False",if type ( value ) == list :,"if value [ 0 ] == ""-"" :",9.425159511373677,9.425159511373677,0.0
"def dt_s_tup_to_string(dt_s_tup):     dt_string = dt_s_tup[0]  # string for identifying the file to parse.     if dt_s_tup[1] > 0:  # if there are seasons in the model         <MASK>             dt_string = dt_string[:2] + ""s"" + dt_string[2:]         else:             dt_string = ""s"" + dt_string     return dt_string","if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string :",if dt_string [ 2 ] > 0 :,3.959981382356705,3.959981382356705,0.0
"def writer(stream, items):     sep = """"     for item in items:         stream.write(sep)         sep = "" ""         <MASK>             item = str(item)         if not PY3K:             if not isinstance(item, unicode):                 item = str(item)         stream.write(item)     stream.write(""\n"")","if not isinstance ( item , str ) :","if not isinstance ( item , str ) :",100.00000000000004,100.00000000000004,1.0
"def _get_result_keys(self, config):     result_key = config.get(""result_key"")     if result_key is not None:         <MASK>             result_key = [result_key]         result_key = [jmespath.compile(rk) for rk in result_key]         return result_key","if not isinstance ( result_key , list ) :","if isinstance ( result_key , list ) :",81.76129038784515,81.76129038784515,0.0
"def _download_build_artifacts(self, build: Dict[str, Any]) -> None:     arch = build[""arch_tag""]     snap_build = self._lp_load_url(build[""self_link""])     urls = snap_build.getFileUrls()     if not urls:         logger.error(f""Snap file not available for arch {arch!r}."")         return     for url in urls:         file_name = _get_url_basename(url)         self._download_file(url=url, dst=file_name)         <MASK>             logger.info(f""Snapped {file_name}"")         else:             logger.info(f""Fetched {file_name}"")","if file_name . endswith ( "".snap"" ) :","if arch == ""snap"" :",7.433761660133445,7.433761660133445,0.0
"def _add_custom_statement(self, custom_statements):     if custom_statements is None:         return     self.resource_policy[""Version""] = ""2012-10-17""     if self.resource_policy.get(""Statement"") is None:         self.resource_policy[""Statement""] = custom_statements     else:         if not isinstance(custom_statements, list):             custom_statements = [custom_statements]         statement = self.resource_policy[""Statement""]         if not isinstance(statement, list):             statement = [statement]         for s in custom_statements:             <MASK>                 statement.append(s)         self.resource_policy[""Statement""] = statement",if s not in statement :,"if s . startswith ( ""Statement"" ) :",9.287528999566801,9.287528999566801,0.0
"def display_failures_for_single_test(result: TestResult) -> None:     """"""Display a failure for a single method / endpoint.""""""     display_subsection(result)     checks = _get_unique_failures(result.checks)     for idx, check in enumerate(checks, 1):         message: Optional[str]         <MASK>             message = f""{idx}. {check.message}""         else:             message = None         example = cast(Case, check.example)  # filtered in `_get_unique_failures`         display_example(example, check.name, message, result.seed)         # Display every time except the last check         if idx != len(checks):             click.echo(""\n"")",if check . message :,if check . message :,100.00000000000004,100.00000000000004,1.0
"def build(opt):     dpath = os.path.join(opt[""datapath""], ""qangaroo"")     version = ""v1.1""     if not build_data.built(dpath, version_string=version):         print(""[building data: "" + dpath + ""]"")         <MASK>             # An older version exists, so remove these outdated files.             build_data.remove_dir(dpath)         build_data.make_dir(dpath)         # Download the data.         for downloadable_file in RESOURCES:             downloadable_file.download_file(dpath)         # Mark the data as built.         build_data.mark_done(dpath, version_string=version)",if build_data . built ( dpath ) :,if version_string in VERSIONS :,5.630400552901077,5.630400552901077,0.0
"def call(self, step_input, states):     new_states = []     for i in range(self.num_layers):         out, new_state = self.lstm_cells[i](step_input, states[i])         step_input = (             layers.dropout(                 out, self.dropout_prob, dropout_implementation=""upscale_in_train""             )             <MASK>             else out         )         new_states.append(new_state)     return step_input, new_states",if self . dropout_prob > 0.0,if new_state is None :,6.770186228657864,6.770186228657864,0.0
"def jupyter_progress_bar(min=0, max=1.0):     """"""Returns an ipywidget progress bar or None if we can't import it""""""     widgets = wandb.util.get_module(""ipywidgets"")     try:         <MASK>             # TODO: this currently works in iPython but it's deprecated since 4.0             from IPython.html import widgets  # type: ignore         assert hasattr(widgets, ""VBox"")         assert hasattr(widgets, ""Label"")         assert hasattr(widgets, ""FloatProgress"")         return ProgressWidget(widgets, min=min, max=max)     except (ImportError, AssertionError):         return None",if widgets is None :,"if hasattr ( widgets , ""ProgressWidget"" ) :",5.522397783539471,5.522397783539471,0.0
"def _record_event(self, path, fsevent_handle, filename, events, error):     with self.lock:         self.events[path].append(events)         <MASK>             if not os.path.exists(path):                 self.watches.pop(path).close()",if events | pyuv . fs . UV_RENAME :,if not error :,3.300991086751251,3.300991086751251,0.0
"def _get_v1_id_from_tags(self, tags_obj, tag):     """"""Get image id from array of tags""""""     if isinstance(tags_obj, dict):         try:             return tags_obj[tag]         except KeyError:             pass     elif isinstance(tags_obj, []):         try:             for tag_dict in tags_obj:                 <MASK>                     return tag_dict[""layer""]         except KeyError:             pass     return """"","if tag_dict [ ""name"" ] == tag :","if tag_dict [ ""layer"" ] :",44.360636895626136,44.360636895626136,0.0
"def query_lister(domain, query="""", max_items=None, attr_names=None):     more_results = True     num_results = 0     next_token = None     while more_results:         rs = domain.connection.query_with_attributes(             domain, query, attr_names, next_token=next_token         )         for item in rs:             <MASK>                 if num_results == max_items:                     raise StopIteration             yield item             num_results += 1         next_token = rs.next_token         more_results = next_token != None",if max_items :,if next_token != None :,7.267884212102741,7.267884212102741,0.0
"def filter(this, args):     array = to_object(this, args.space)     callbackfn = get_arg(args, 0)     arr_len = js_arr_length(array)     if not is_callable(callbackfn):         raise MakeError(""TypeError"", ""callbackfn must be a function"")     _this = get_arg(args, 1)     k = 0     res = []     while k < arr_len:         <MASK>             kValue = array.get(unicode(k))             if to_boolean(callbackfn.call(_this, (kValue, float(k), array))):                 res.append(kValue)         k += 1     return args.space.ConstructArray(res)",if array . has_property ( unicode ( k ) ) :,if k < arr_len :,3.9413751108533592,3.9413751108533592,0.0
"def every_one_is(self, dst):     msg = ""all members of %r should be %r, but the %dth is %r""     for index, item in enumerate(self._src):         if self._range:             if index < self._range[0] or index > self._range[1]:                 continue         error = msg % (self._src, dst, index, item)         <MASK>             raise AssertionError(error)     return True",if item != dst :,if error :,12.753667906901528,0.0,0.0
"def schedule_logger(job_id=None, delete=False):     if not job_id:         return getLogger(""fate_flow_schedule"")     else:         if delete:             with LoggerFactory.lock:                 try:                     for key in LoggerFactory.schedule_logger_dict.keys():                         <MASK>                             del LoggerFactory.schedule_logger_dict[key]                 except:                     pass             return True         key = job_id + ""schedule""         if key in LoggerFactory.schedule_logger_dict:             return LoggerFactory.schedule_logger_dict[key]         return LoggerFactory.get_schedule_logger(job_id)",if job_id in key :,if key in LoggerFactory . schedule_logger_dict :,5.604233375480572,5.604233375480572,0.0
"def Tokenize(s):     # type: (str) -> Iterator[Token]     for item in TOKEN_RE.findall(s):         # The type checker can't know the true type of item!         item = cast(TupleStr4, item)         if item[0]:             typ = ""number""             val = item[0]         elif item[1]:             typ = ""name""             val = item[1]         <MASK>             typ = item[2]             val = item[2]         elif item[3]:             typ = item[3]             val = item[3]         yield Token(typ, val)",elif item [ 2 ] :,if item [ 2 ] :,75.98356856515926,75.98356856515926,0.0
"def _read_data_from_all_categories(self, directory, config, categories):     lines = []     for category in categories:         data_file = os.path.join(directory, _DATASET_VERSION, category, config)         <MASK>             with open(data_file) as f:                 ls = f.read().split(""\n"")                 for l in ls[::-1]:                     if not l:                         ls.remove(l)                 lines.extend(ls)     return lines",if os . path . exists ( data_file ) :,if data_file :,11.141275535087015,11.141275535087015,0.0
"def find_handlers(self, forms):     handlers = {}     for form in forms.itervalues():         for action_name, _action_label in form.actions:             <MASK>                 handlers[action_name] = form             else:                 raise HandlerError(                     ""More than one form defines the handler %s"" % action_name                 )     return handlers",if action_name not in handlers :,if _action_label == self . _action_label :,6.754312828675707,6.754312828675707,0.0
"def get_story_task_completed_body(payload: Dict[str, Any]) -> Optional[str]:     action = get_action_with_primary_id(payload)     kwargs = {         ""task_description"": action[""description""],     }     story_id = action[""story_id""]     for ref in payload[""references""]:         <MASK>             kwargs[""name_template""] = STORY_NAME_TEMPLATE.format(                 name=ref[""name""],                 app_url=ref[""app_url""],             )     if action[""changes""][""complete""][""new""]:         return STORY_TASK_COMPLETED_TEMPLATE.format(**kwargs)     else:         return None","if ref [ ""id"" ] == story_id :","if ref [ ""id"" ] == story_id :",100.00000000000004,100.00000000000004,1.0
"def _create_valid_graph(graph):     nodes = graph.nodes()     for i in range(len(nodes)):         for j in range(len(nodes)):             <MASK>                 continue             edge = (nodes[i], nodes[j])             if graph.has_edge(edge):                 graph.del_edge(edge)             graph.add_edge(edge, 1)",if i == j :,if i == j :,100.00000000000004,100.00000000000004,1.0
"def _post_order(op):     if isinstance(op, tvm.tir.Allocate):         lift_stmt[-1].append(op)         return op.body     if isinstance(op, tvm.tir.AttrStmt):         <MASK>             lift_stmt[-1].append(op)             return op.body         if op.attr_key == ""virtual_thread"":             return _merge_block(lift_stmt.pop() + [op], op.body)         return op     if isinstance(op, tvm.tir.For):         return _merge_block(lift_stmt.pop() + [op], op.body)     raise RuntimeError(""not reached"")","if op . attr_key == ""storage_scope"" :","if op . attr_key == ""virtual_thread"" :",65.91844162499147,65.91844162499147,0.0
"def format_lazy_import(names):     """"""Formats lazy import lines""""""     lines = """"     for _, name, asname in names:         pkg, _, _ = name.partition(""."")         <MASK>             line = ""{pkg} = _LazyModule.load({pkg!r}, {mod!r})\n""         else:             line = ""{asname} = _LazyModule.load({pkg!r}, {mod!r}, {asname!r})\n""         lines += line.format(pkg=pkg, mod=name, asname=asname)     return lines",if asname is None :,if asname is None :,100.00000000000004,100.00000000000004,1.0
"def evaluateWord(self, argument):     wildcard_count = argument[0].count(""*"")     if wildcard_count > 0:         if wildcard_count == 1 and argument[0].startswith(""*""):             return self.GetWordWildcard(argument[0][1:], method=""endswith"")         if wildcard_count == 1 and argument[0].endswith(""*""):             return self.GetWordWildcard(argument[0][:-1], method=""startswith"")         else:             _regex = argument[0].replace(""*"", "".+"")             matched = False             for w in self.words:                 matched = bool(re.search(_regex, w))                 <MASK>                     break             return matched     return self.GetWord(argument[0])",if matched :,if not matched :,35.35533905932737,35.35533905932737,0.0
"def setup(self, ir: ""IR"", aconf: Config) -> bool:     if self.kind == ""ConsulResolver"":         self.resolve_with = ""consul""         <MASK>             self.post_error(""ConsulResolver is required to have a datacenter"")             return False     elif self.kind == ""KubernetesServiceResolver"":         self.resolve_with = ""k8s""     elif self.kind == ""KubernetesEndpointResolver"":         self.resolve_with = ""k8s""     else:         self.post_error(f""Resolver kind {self.kind} unknown"")         return False     return True","if not self . get ( ""datacenter"" ) :","if self . kind == ""datacenter"" :",19.331263581394154,19.331263581394154,0.0
"def get_success_url(self):     """"""Continue to the flow index or redirect according `?back` parameter.""""""     if ""back"" in self.request.GET:         back_url = self.request.GET[""back""]         <MASK>             back_url = ""/""         return back_url     return reverse(self.success_url)","if not is_safe_url ( url = back_url , allowed_hosts = { self . request . get_host ( ) } ) :",if back_url is None :,0.9428841577864372,0.9428841577864372,0.0
"def download_main(     download, download_playlist, urls, playlist, output_dir, merge, info_only ):     for url in urls:         if url.startswith(""https://""):             url = url[8:]         <MASK>             url = ""http://"" + url         if playlist:             download_playlist(                 url, output_dir=output_dir, merge=merge, info_only=info_only             )         else:             download(url, output_dir=output_dir, merge=merge, info_only=info_only)","if not url . startswith ( ""http://"" ) :","if url . startswith ( ""http://"" ) :",86.17038791239612,86.17038791239612,0.0
"def __str__(self):     buf = [""""]     if self.fileName:         buf.append(self.fileName + "":"")     if self.line != -1:         <MASK>             buf.append(""line "")         buf.append(str(self.line))         if self.column != -1:             buf.append("":"" + str(self.column))         buf.append("":"")     buf.append("" "")     return str("""").join(buf)",if not self . fileName :,if self . line != - 1 :,11.339582221952005,11.339582221952005,0.0
"def parse_bash_set_output(output):     """"""Parse Bash-like 'set' output""""""     if not sys.platform.startswith(""win""):         # Replace ""\""-continued lines in *Linux* environment dumps.         # Cannot do this on Windows because a ""\"" at the end of the         # line does not imply a continuation.         output = output.replace(""\\\n"", """")     environ = {}     for line in output.splitlines(0):         line = line.rstrip()         if not line:             continue  # skip black lines         item = _ParseBashEnvStr(line)         <MASK>             environ[item[0]] = item[1]     return environ",if item :,if item :,100.00000000000004,0.0,1.0
"def remove_selected(self):     """"""Removes selected items from list.""""""     to_delete = []     for i in range(len(self)):         if self[i].selected:             to_delete.append(i)     to_delete.reverse()     for i in to_delete:         self.pop(i)     if len(to_delete) > 0:         first_to_delete = to_delete[-1]         <MASK>             self[0].selected = True         elif first_to_delete > 0:             self[first_to_delete - 1].selected = True",if first_to_delete == 0 and len ( self ) > 0 :,if first_to_delete == 0 :,46.21246966669783,46.21246966669783,0.0
"def update(self, update_tracks=True):     self.enable_update_metadata_images(False)     old_album_title = self.metadata[""album""]     self.metadata[""album""] = config.setting[""nat_name""]     for track in self.tracks:         <MASK>             track.metadata[""album""] = self.metadata[""album""]         for file in track.linked_files:             track.update_file_metadata(file)     self.enable_update_metadata_images(True)     super().update(update_tracks)","if old_album_title == track . metadata [ ""album"" ] :",if old_album_title != track . title :,35.9448917857391,35.9448917857391,0.0
"def on_input(self, target, message):     if message.strip() == """":         self.panel(""No commit message provided"")         return     if target:         command = [""git"", ""add""]         <MASK>             command.append(""--all"")         else:             command.extend((""--"", target))         self.run_command(command, functools.partial(self.add_done, message))     else:         self.add_done(message, """")","if target == ""*"" :","if target == ""all"" :",59.4603557501361,59.4603557501361,0.0
"def go_to_last_edit_location(self):     if self.last_edit_cursor_pos is not None:         filename, position = self.last_edit_cursor_pos         <MASK>             self.last_edit_cursor_pos = None             return         else:             self.load(filename)             editor = self.get_current_editor()             if position < editor.document().characterCount():                 editor.set_cursor_position(position)",if not osp . isfile ( filename ) :,if position is None :,5.70796903405875,5.70796903405875,0.0
"def returnByType(self, results):     new_results = {}     for r in results:         type_name = r.get(""type"", ""movie"") + ""s""         <MASK>             new_results[type_name] = []         new_results[type_name].append(r)     # Combine movies, needs a cleaner way..     if ""movies"" in new_results:         new_results[""movies""] = self.combineOnIMDB(new_results[""movies""])     return new_results",if type_name not in new_results :,if type_name not in new_results :,100.00000000000004,100.00000000000004,1.0
"def cache_sns_topics_across_accounts() -> bool:     function: str = f""{__name__}.{sys._getframe().f_code.co_name}""     # First, get list of accounts     accounts_d: list = async_to_sync(get_account_id_to_name_mapping)()     for account_id in accounts_d.keys():         if config.get(""environment"") == ""prod"":             cache_sns_topics_for_account.delay(account_id)         else:             <MASK>                 cache_sns_topics_for_account.delay(account_id)     stats.count(f""{function}.success"")     return True","if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :","if config . get ( ""environment"" ) == ""prod"" :",16.716822873683913,16.716822873683913,0.0
"def get(self, subject, topic):     """"""Handles GET requests.""""""     if subject in feconf.AVAILABLE_LANDING_PAGES:         <MASK>             self.render_template(""topic-landing-page.mainpage.html"")         else:             raise self.PageNotFoundException     else:         raise self.PageNotFoundException",if topic in feconf . AVAILABLE_LANDING_PAGES [ subject ] :,if topic in feconf . AVAILABLE_LANDING_PAGES :,69.63547789070398,69.63547789070398,0.0
"def callback(compiled):     <MASK>         logger.show_tabulated(             ""Compiled"", showpath(codepath), ""without writing to file.""         )     else:         with univ_open(destpath, ""w"") as opened:             writefile(opened, compiled)         logger.show_tabulated(""Compiled to"", showpath(destpath), ""."")     if self.show:         print(compiled)     if run:         if destpath is None:             self.execute(compiled, path=codepath, allow_show=False)         else:             self.execute_file(destpath)",if destpath is None :,if self . show_file :,7.809849842300637,7.809849842300637,0.0
"def _find_start_index(self, string, start, end):     while True:         index = string.find(""{"", start, end) - 1         if index < 0:             return -1         <MASK>             return index         start = index + 2","if self . _start_index_is_ok ( string , index ) :",if index > end :,1.2753613517831104,1.2753613517831104,0.0
"def _get_nlu_target_format(export_path: Text) -> Text:     guessed_format = loading.guess_format(export_path)     if guessed_format not in {MARKDOWN, RASA, RASA_YAML}:         if rasa.shared.data.is_likely_json_file(export_path):             guessed_format = RASA         elif rasa.shared.data.is_likely_markdown_file(export_path):             guessed_format = MARKDOWN         <MASK>             guessed_format = RASA_YAML     return guessed_format",elif rasa . shared . data . is_likely_yaml_file ( export_path ) :,if rasa . shared . data . is_likely_yaml_file ( export_path ) :,94.57416090031757,94.57416090031757,0.0
"def moveToThreadNext(self):     """"""Move a position to threadNext position.""""""     p = self     if p.v:         if p.v.children:             p.moveToFirstChild()         el<MASK>             p.moveToNext()         else:             p.moveToParent()             while p:                 if p.hasNext():                     p.moveToNext()                     break  # found                 p.moveToParent()             # not found.     return p",if p . hasNext ( ) :,if p . hasNext ( ) :,100.00000000000004,100.00000000000004,1.0
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None):     for attr in attributes:         value = getattr(obj, attr, None)         <MASK>             continue         name = name_fmt % attr         if formatter is not None:             value = formatter(attr, value)         info_add(name, value)",if value is None :,if value is None :,100.00000000000004,100.00000000000004,1.0
"def getElement(self, aboutUri, namespace, name):     for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, ""Description""):         <MASK>             attr = desc.getAttributeNodeNS(namespace, name)             if attr != None:                 yield attr             for element in desc.getElementsByTagNameNS(namespace, name):                 yield element","if desc . getAttributeNS ( RDF_NAMESPACE , ""about"" ) == aboutUri :",if aboutUri == self . aboutUri :,5.611955287553538,5.611955287553538,0.0
def run(self):     while not self.completed:         if self.block:             time.sleep(self.period)         else:             self._completed.wait(self.period)         self.counter += 1         try:             self.callback(self.counter)         except Exception:             self.stop()         <MASK>             dt = time.time() - self._start_time             if dt > self.timeout:                 self.stop()         if self.counter == self.count:             self.stop(),if self . timeout is not None :,if self . timeout :,38.80684294761701,38.80684294761701,0.0
"def _parse_fixits(message, titer, line):     """"""Parses fixit messages.""""""     while (         OutputParser.message_line_re.match(line) is None         and OutputParser.note_line_re.match(line) is None     ):         message_text = line.strip()         <MASK>             message.fixits.append(                 Note(                     message.path,                     message.line,                     line.find(message_text) + 1,                     message_text,                 )             )         line = next(titer)     return line","if message_text != """" :",if message_text :,31.772355751081438,31.772355751081438,0.0
"def _connect_db(self, force_reconnect=False):     thread_id = thread.get_ident()     if force_reconnect and thread_id in ENGINES:         del ENGINES[thread_id]     conn = None     try:         engine = ENGINES[thread_id]         conn = engine.connect()         _test = conn.execute(""SELECT 1"")         _test.fetchall()     except (KeyError, MySQLdb.OperationalError):         <MASK>             conn.close()         engine = sqla.create_engine(self.db_url, pool_recycle=3600)         ENGINES[thread_id] = engine         conn = engine.connect()     return conn",if conn :,if _test :,18.99589214128981,18.99589214128981,0.0
"def read(self, n):     if self.current_frame:         data = self.current_frame.read(n)         <MASK>             self.current_frame = None             return self.file_read(n)         if len(data) < n:             raise UnpicklingError(""pickle exhausted before end of frame"")         return data     else:         return self.file_read(n)",if not data and n != 0 :,if not data :,18.306026428729766,18.306026428729766,0.0
"def __setLoadCmd(self):     base = self.__rawLoadCmd     for _ in range(self.__machHeader.ncmds):         command = LOAD_COMMAND.from_buffer_copy(base)         <MASK>             segment = SEGMENT_COMMAND.from_buffer_copy(base)             self.__setSections(segment, base[56:], 32)         elif command.cmd == MACHOFlags.LC_SEGMENT_64:             segment = SEGMENT_COMMAND64.from_buffer_copy(base)             self.__setSections(segment, base[72:], 64)         base = base[command.cmdsize :]",if command . cmd == MACHOFlags . LC_SEGMENT :,if command . cmd == MACHOFlags . LC_SEGMENT_32 :,77.4403141014203,77.4403141014203,0.0
"def emit_post_sync_signal(created_models, verbosity, interactive, db):     # Emit the post_sync signal for every application.     for app in models.get_apps():         app_name = app.__name__.split(""."")[-2]         <MASK>             print(""Running post-sync handlers for application %s"" % app_name)         models.signals.post_syncdb.send(             sender=app,             app=app,             created_models=created_models,             verbosity=verbosity,             interactive=interactive,             db=db,         )",if verbosity >= 2 :,if app_name in created_models :,5.669791110976001,5.669791110976001,0.0
"def git_pull(args):     if len(args) <= 1:         repo = _get_repo()         _confirm_dangerous()         url = args[0] if len(args) == 1 else repo.remotes.get(""origin"", """")         if url in repo.remotes:             origin = url             url = repo.remotes.get(origin)         <MASK>             repo.pull(origin_uri=url)         else:             print(""No pull URL."")     else:         print(command_help[""git pull""])",if url :,if url :,100.00000000000004,0.0,1.0
"def version(self):     try:         return self._version     except AttributeError:         for line in self._get_metadata(self.PKG_INFO):             <MASK>                 self._version = safe_version(line.split("":"", 1)[1].strip())                 return self._version         else:             tmpl = ""Missing 'Version:' header and/or %s file""             raise ValueError(tmpl % self.PKG_INFO, self)","if line . lower ( ) . startswith ( ""version:"" ) :",if line :,1.1538129489094668,0.0,0.0
"def increment(self, metric, labels, delta):     """"""Increment a value by |delta|.""""""     with self._lock:         key = self._get_key(metric.name, labels)         <MASK>             start_time = self._store[key].start_time             value = self._store[key].value + delta         else:             start_time = time.time()             value = metric.default_value + delta         self._store[key] = _StoreValue(metric, labels, start_time, value)",if key in self . _store :,if key in self . _store :,100.00000000000004,100.00000000000004,1.0
"def get_current_connections(session):     """"""Retrieves open connections using the the given session""""""     # Use Show process list to count the open sesions.     res = session.sql(""SHOW PROCESSLIST"").execute()     rows = res.fetch_all()     connections = {}     for row in rows:         <MASK>             connections[row.get_string(""User"")] = [row.get_string(""Host"")]         else:             connections[row.get_string(""User"")].append(row.get_string(""Host""))     return connections","if row . get_string ( ""User"" ) not in connections :","if row . get_string ( ""Host"" ) in connections :",61.42150942225169,61.42150942225169,0.0
"def asset(*paths):     for path in paths:         fspath = www_root + ""/assets/"" + path         etag = """"         try:             <MASK>                 etag = asset_etag(fspath)             else:                 os.stat(fspath)         except FileNotFoundError as e:             if path == paths[-1]:                 if not os.path.exists(fspath + "".spt""):                     tell_sentry(e, {})             else:                 continue         except Exception as e:             tell_sentry(e, {})         return asset_url + path + (etag and ""?etag="" + etag)",if env . cache_static :,if etag :,9.138402379955025,0.0,0.0
def thread_loop(self) -> None:     while not self.stop_event.is_set():         time.sleep(1)         new_trials = self.study.trials         with self.lock:             need_to_add_callback = self.new_trials is None             self.new_trials = new_trials             <MASK>                 self.doc.add_next_tick_callback(self.update_callback),if need_to_add_callback :,if need_to_add_callback :,100.00000000000004,100.00000000000004,1.0
"def _cache_db_tables_iterator(tables, cache_alias, db_alias):     no_tables = not tables     cache_aliases = settings.CACHES if cache_alias is None else (cache_alias,)     db_aliases = settings.DATABASES if db_alias is None else (db_alias,)     for db_alias in db_aliases:         if no_tables:             tables = connections[db_alias].introspection.table_names()         <MASK>             for cache_alias in cache_aliases:                 yield cache_alias, db_alias, tables",if tables :,if cache_alias in tables :,14.535768424205482,14.535768424205482,0.0
"def remove_subscriber(self, topic, subscriber):     if subscriber in self.subscribers[topic]:         if hasattr(subscriber, ""_pyroRelease""):             subscriber._pyroRelease()         <MASK>             try:                 proxy = self.proxy_cache[subscriber._pyroUri]                 proxy._pyroRelease()                 del self.proxy_cache[subscriber._pyroUri]             except KeyError:                 pass         self.subscribers[topic].discard(subscriber)","if hasattr ( subscriber , ""_pyroUri"" ) :","if hasattr ( subscriber , ""_pyroUri"" ) :",100.00000000000004,100.00000000000004,1.0
"def test_constructor(job_id):     with patch(""apscheduler.job.Job._modify"") as _modify:         scheduler_mock = MagicMock(BaseScheduler)         job = Job(scheduler_mock, id=job_id)         assert job._scheduler is scheduler_mock         assert job._jobstore_alias is None         modify_kwargs = _modify.call_args[1]         <MASK>             assert len(modify_kwargs[""id""]) == 32         else:             assert modify_kwargs[""id""] == job_id",if job_id is None :,"if ""id"" in modify_kwargs :",6.742555929751843,6.742555929751843,0.0
"def get_connection(self):     if self.config.proxy_host != """":         return httplib.HTTPConnection(self.config.proxy_host, self.config.proxy_port)     else:         <MASK>             return httplib.HTTPSConnection(self.config.simpledb_host)         else:             return httplib.HTTPConnection(self.config.simpledb_host)",if self . config . use_https :,"if self . config . simpledb_host != """" :",31.455601883230702,31.455601883230702,0.0
"def notify_login(self, ipaddress=""""):     if app.NOTIFY_ON_LOGIN:         update_text = common.notifyStrings[common.NOTIFY_LOGIN_TEXT]         title = common.notifyStrings[common.NOTIFY_LOGIN]         <MASK>             self._notify_pht(title, update_text.format(ipaddress))",if update_text and title and ipaddress :,if self . _notify_pht :,6.413885305524152,6.413885305524152,0.0
"def _getItemHeight(self, item, ctrl=None):     """"""Returns the full height of the item to be inserted in the form""""""     if type(ctrl) == psychopy.visual.TextBox2:         return ctrl.size[1]     if type(ctrl) == psychopy.visual.Slider:         # Set radio button layout         if item[""layout""] == ""horiz"":             return 0.03 + ctrl.labelHeight * 3         <MASK>             # for vertical take into account the nOptions             return ctrl.labelHeight * len(item[""options""])","elif item [ ""layout"" ] == ""vert"" :",if type ( ctrl ) == psychopy . visual . Button :,6.837203339116283,6.837203339116283,0.0
"def _get_errors_lines(self):     """"""Return the number of lines that contains errors to highlight.""""""     errors_lines = []     block = self.document().begin()     while block.isValid():         user_data = get_user_data(block)         <MASK>             errors_lines.append(block.blockNumber())         block = block.next()     return errors_lines",if user_data . error :,"if user_data . error_code == ""error"" :",36.362270465000705,36.362270465000705,0.0
"def set_pbar_fraction(self, frac, progress, stage=None):     gtk.gdk.threads_enter()     try:         self.is_pulsing = False         self.set_stage_text(stage or _(""Processing...""))         self.pbar.set_text(progress)         if frac > 1:             frac = 1.0         <MASK>             frac = 0         self.pbar.set_fraction(frac)     finally:         gtk.gdk.threads_leave()",if frac < 0 :,if frac < 0 :,100.00000000000004,100.00000000000004,1.0
"def list_files(basedir):     """"""List files in the directory rooted at |basedir|.""""""     if not os.path.isdir(basedir):         raise NoSuchDirectory(basedir)     directories = [""""]     while directories:         d = directories.pop()         for basename in os.listdir(os.path.join(basedir, d)):             filename = os.path.join(d, basename)             if os.path.isdir(os.path.join(basedir, filename)):                 directories.append(filename)             <MASK>                 yield filename","elif os . path . exists ( os . path . join ( basedir , filename ) ) :",if os . path . isfile ( filename ) :,14.431113051977096,14.431113051977096,0.0
"def assistive(self):     """"""Detects if item can be used as assistance""""""     # Make sure we cache results     if self.__assistive is None:         assistive = False         # Go through all effects and find first assistive         for effect in self.effects.values():             <MASK>                 # If we find one, stop and mark item as assistive                 assistive = True                 break         self.__assistive = assistive     return self.__assistive",if effect . isAssistance is True :,if effect . item == self :,22.089591134157878,22.089591134157878,0.0
"def closest_unseen(self, row1, col1, filter=None):     # find the closest unseen from this row/col     min_dist = maxint     closest_unseen = None     for row in range(self.height):         for col in range(self.width):             if filter is None or (row, col) not in filter:                 if self.map[row][col] == UNSEEN:                     dist = self.distance(row1, col1, row, col)                     <MASK>                         min_dist = dist                         closest_unseen = (row, col)     return closest_unseen",if dist < min_dist :,if dist < min_dist :,100.00000000000004,100.00000000000004,1.0
"def _maybe_has_default_route(self):     for route in self.iter_routes():         <MASK>             return True     for iface in self.iter_interfaces():         for subnet in iface.get(""subnets"", []):             for route in subnet.get(""routes"", []):                 if self._is_default_route(route):                     return True     return False",if self . _is_default_route ( route ) :,if self . _is_default_route ( route ) :,100.00000000000004,100.00000000000004,1.0
"def data(self, data):     if data is None:         raise Exception(""Data cannot be None"")     val = []     for d in data:         if isinstance(d, str):             val.append(bytes(d, ""utf-8""))         <MASK>             val.append(d)         else:             raise Exception(                 ""Invalid type, data can only be an str or a bytes not {}: {}"".format(                     type(data), d                 )             )     self.__data = val","elif isinstance ( d , bytes ) :","if isinstance ( d , bytes ) :",84.08964152537145,84.08964152537145,0.0
"def get_one_segment_function(data, context, echoerr):     ext = data[""ext""]     function_name = context[-2][1].get(""function"")     if function_name:         module, function_name = get_function_strings(function_name, context, ext)         func = import_segment(function_name, data, context, echoerr, module=module)         <MASK>             yield func",if func :,if func :,100.00000000000004,0.0,1.0
"def generic_visit(self, node, parents=None):     parents = (parents or []) + [node]     for field, value in iter_fields(node):         if isinstance(value, list):             for item in value:                 <MASK>                     self.visit(item, parents)         elif isinstance(value, AST):             self.visit(value, parents)","if isinstance ( item , AST ) :","if isinstance ( item , AST ) :",100.00000000000004,100.00000000000004,1.0
"def find_scintilla_constants(f):     lexers = []     states = []     for name in f.order:         v = f.features[name]         <MASK>             if v[""FeatureType""] == ""val"":                 if name.startswith(""SCE_""):                     states.append((name, v[""Value""]))                 elif name.startswith(""SCLEX_""):                     lexers.append((name, v[""Value""]))     return (lexers, states)","if v [ ""Category"" ] != ""Deprecated"" :",if v :,2.247320757600649,0.0,0.0
"def things(self, query):     limit = query.pop(""limit"", 100)     offset = query.pop(""offset"", 0)     keys = set(self.docs)     for k, v in query.items():         <MASK>             # query keys need to be flattened properly,             # this corrects any nested keys that have been included             # in values.             flat = common.flatten_dict(v)[0]             k += ""."" + web.rstrips(flat[0], "".key"")             v = flat[1]         keys = set(k for k in self.filter_index(self.index, k, v) if k in keys)     keys = sorted(keys)     return keys[offset : offset + limit]","if isinstance ( v , dict ) :",if k in keys :,6.9717291216921975,6.9717291216921975,0.0
"def del_(self, key):     initial_hash = hash_ = self.hash(key)     while True:         if self._keys[hash_] is self._empty:             # That key was never assigned             return None         <MASK>             # key found, assign with deleted sentinel             self._keys[hash_] = self._deleted             self._values[hash_] = self._deleted             self._len -= 1             return         hash_ = self._rehash(hash_)         if initial_hash == hash_:             # table is full and wrapped around             return None",elif self . _keys [ hash_ ] == key :,if hash_ == self . _empty :,17.584661674110286,17.584661674110286,0.0
"def test_204_invalid_content_length(self):     # 204 status with non-zero content length is malformed     with ExpectLog(gen_log, "".*Response with code 204 should not have body""):         response = self.fetch(""/?error=1"")         <MASK>             self.skipTest(""requires HTTP/1.x"")         if self.http_client.configured_class != SimpleAsyncHTTPClient:             self.skipTest(""curl client accepts invalid headers"")         self.assertEqual(response.code, 599)",if not self . http1 :,if response . code != 204 :,7.267884212102741,7.267884212102741,0.0
"def __str__(self) -> str:     text = ""\n""     for k, r in self.result.items():         text += ""{}\n"".format(""#"" * 40)         <MASK>             text += ""# {} (failed)\n"".format(k)         else:             text += ""# {} (succeeded)\n"".format(k)         text += ""{}\n"".format(""#"" * 40)         for sub_r in r:             text += ""**** {}\n"".format(sub_r.name)             text += ""{}\n"".format(sub_r)     return text",if r . failed :,if r . failed :,100.00000000000004,100.00000000000004,1.0
"def DeleteTask():     oid = request.form.get(""oid"", """")     if oid:         result = Mongo.coll[""Task""].delete_one({""_id"": ObjectId(oid)})         <MASK>             result = Mongo.coll[""Result""].delete_many({""task_id"": ObjectId(oid)})             if result:                 return ""success""     return ""fail""",if result . deleted_count > 0 :,if result :,8.525588607164655,0.0,0.0
"def _replace_vars(self, line, extracted, env_variables):     for e in extracted:         <MASK>             value = env_variables.get(e)             if isinstance(value, dict) or isinstance(value, list):                 value = pprint.pformat(value)             decorated = self._decorate_var(e)             line = line.replace(decorated, str(value))     return line",if e in env_variables :,if e in env_variables :,100.00000000000004,100.00000000000004,1.0
"def should_include(service):     for f in filt:         if f == ""status"":             state = filt[f]             containers = project.containers([service.name], stopped=True)             if not has_container_with_state(containers, state):                 return False         elif f == ""source"":             source = filt[f]             if source == ""image"" or source == ""build"":                 <MASK>                     return False             else:                 raise UserError(""Invalid value for source filter: %s"" % source)         else:             raise UserError(""Invalid filter: %s"" % f)     return True",if source not in service . options :,"if not has_container_with_state ( containers , state ) :",3.4585921141027356,3.4585921141027356,0.0
def state_callback_loop():     if usercallback:         when = 1         while (             when             and not self.future_removed.done()             and not self.session.shutdownstarttime         ):             result = usercallback(self.get_state())             when = (await result) if iscoroutine(result) else result             <MASK>                 await sleep(when),if when > 0.0 and not self . session . shutdownstarttime :,if when :,3.136388772461349,0.0,0.0
"def __get_new_timeout(self, timeout):     """"""When using --timeout_multiplier=#.#""""""     self.__check_scope()     try:         timeout_multiplier = float(self.timeout_multiplier)         <MASK>             timeout_multiplier = 0.5         timeout = int(math.ceil(timeout_multiplier * timeout))         return timeout     except Exception:         # Wrong data type for timeout_multiplier (expecting int or float)         return timeout",if timeout_multiplier <= 0.5 :,if timeout_multiplier < 0 :,55.780028607687655,55.780028607687655,0.0
"def readexactly(self, n):     buf = b""""     while n:         yield IORead(self.s)         res = self.s.read(n)         assert res is not None         <MASK>             yield IOReadDone(self.s)             break         buf += res         n -= len(res)     return buf",if not res :,"if res == b""\x00"" :",5.522397783539471,5.522397783539471,0.0
"def contract_rendering_pane(event):     """"""Expand the rendering pane.""""""     c = event.get(""c"")     if c:         vr = c.frame.top.findChild(QtWidgets.QWidget, ""viewrendered_pane"")         <MASK>             vr.contract()         else:             # Just open the pane.             viewrendered(event)",if vr :,if vr :,100.00000000000004,0.0,1.0
"def translate_headers(self, environ):     """"""Translate CGI-environ header names to HTTP header names.""""""     for cgiName in environ:         # We assume all incoming header keys are uppercase already.         <MASK>             yield self.headerNames[cgiName], environ[cgiName]         elif cgiName[:5] == ""HTTP_"":             # Hackish attempt at recovering original header names.             translatedHeader = cgiName[5:].replace(""_"", ""-"")             yield translatedHeader, environ[cgiName]",if cgiName in self . headerNames :,"if cgiName [ : 5 ] == ""HTTP_"" :",6.837203339116283,6.837203339116283,0.0
"def get_value_from_string(self, string_value):     """"""Return internal representation starting from CFN/user-input value.""""""     param_value = self.get_default_value()     try:         <MASK>             string_value = str(string_value).strip()             if string_value != ""NONE"":                 param_value = int(string_value)     except ValueError:         self.pcluster_config.warn(             ""Unable to convert the value '{0}' to an Integer. ""             ""Using default value for parameter '{1}'"".format(string_value, self.key)         )     return param_value",if string_value is not None :,if string_value :,38.80684294761701,38.80684294761701,0.0
"def monitor_filter(self):     """"""Return filtered service objects list""""""     services = self.client.services.list(filters={""label"": ""com.ouroboros.enable""})     monitored_services = []     for service in services:         ouro_label = service.attrs[""Spec""][""Labels""].get(""com.ouroboros.enable"")         <MASK>             monitored_services.append(service)     self.data_manager.monitored_containers[self.socket] = len(monitored_services)     self.data_manager.set(self.socket)     return monitored_services","if not self . config . label_enable or ouro_label . lower ( ) in [ ""true"" , ""yes"" ] :",if ouro_label :,0.4541429459403983,0.4541429459403983,0.0
"def nextEditable(self):     """"""Moves focus of the cursor to the next editable window""""""     if self.currentEditable is None:         if len(self._editableChildren):             self._currentEditableRef = self._editableChildren[0]     else:         for ref in weakref.getweakrefs(self.currentEditable):             if ref in self._editableChildren:                 cei = self._editableChildren.index(ref)                 nei = cei + 1                 <MASK>                     nei = 0                 self._currentEditableRef = self._editableChildren[nei]     return self.currentEditable",if nei >= len ( self . _editableChildren ) :,if nei > cei :,10.536767850900915,10.536767850900915,0.0
"def linkify_cm_by_tp(self, timeperiods):     for rm in self:         mtp_name = rm.modulation_period.strip()         # The new member list, in id         mtp = timeperiods.find_by_name(mtp_name)         <MASK>             err = (                 ""Error: the business impact modulation '%s' got an unknown ""                 ""modulation_period '%s'"" % (rm.get_name(), mtp_name)             )             rm.configuration_errors.append(err)         rm.modulation_period = mtp","if mtp_name != """" and mtp is None :",if mtp is None :,15.340817918113808,15.340817918113808,0.0
def close_open_fds(keep=None):  # noqa     keep = [maybe_fileno(f) for f in (keep or []) if maybe_fileno(f) is not None]     for fd in reversed(range(get_fdmax(default=2048))):         <MASK>             try:                 os.close(fd)             except OSError as exc:                 if exc.errno != errno.EBADF:                     raise,if fd not in keep :,if fd not in keep :,100.00000000000004,100.00000000000004,1.0
"def _append_child_from_unparsed_xml(father_node, unparsed_xml):     """"""Append child xml nodes to a node.""""""     dom_tree = parseString(unparsed_xml)     if dom_tree.hasChildNodes():         first_child = dom_tree.childNodes[0]         <MASK>             child_nodes = first_child.childNodes             for _ in range(len(child_nodes)):                 childNode = child_nodes.item(0)                 father_node.appendChild(childNode)             return     raise DistutilsInternalError(         ""Could not Append append elements to "" ""the Windows msi descriptor.""     )",if first_child . hasChildNodes ( ) :,if first_child :,31.772355751081438,31.772355751081438,0.0
"def process_request(self, request):     for old, new in self.names_name:         request.uri = request.uri.replace(old, new)         <MASK>             body = six.ensure_str(request.body)             if old in body:                 request.body = body.replace(old, new)     return request",if is_text_payload ( request ) and request . body :,if request . body :,11.688396478408103,11.688396478408103,0.0
"def __init__(self, **options):     self.func_name_highlighting = get_bool_opt(options, ""func_name_highlighting"", True)     self.disabled_modules = get_list_opt(options, ""disabled_modules"", [])     self._functions = set()     if self.func_name_highlighting:         from pygments.lexers._luabuiltins import MODULES         for mod, func in MODULES.iteritems():             <MASK>                 self._functions.update(func)     RegexLexer.__init__(self, **options)",if mod not in self . disabled_modules :,if mod in self . disabled_modules :,71.89393375176813,71.89393375176813,0.0
"def GetBestSizeForParentSize(self, parentSize):     """"""Finds the best width and height given the parent's width and height.""""""     if len(self.GetChildren()) == 1:         win = self.GetChildren()[0]         <MASK>             temp_dc = wx.ClientDC(self)             childSize = win.GetBestSizeForParentSize(parentSize)             clientParentSize = self._art.GetPanelClientSize(                 temp_dc, self, wx.Size(*parentSize), None             )             overallSize = self._art.GetPanelSize(                 temp_dc, self, wx.Size(*clientParentSize), None             )             return overallSize     return self.GetSize()","if isinstance ( win , RibbonControl ) :",if win . GetSize ( ) == childSize :,6.27465531099474,6.27465531099474,0.0
"def pid_from_name(name):     processes = []     for pid in os.listdir(""/proc""):         try:             pid = int(pid)             pname, cmdline = SunProcess._name_args(pid)             <MASK>                 return pid             if name in cmdline.split("" "", 1)[0]:                 return pid         except:             pass     raise ProcessException(""No process with such name: %s"" % name)",if name in pname :,if pname in processes :,15.106876986783844,15.106876986783844,0.0
"def __get_file_by_num(self, num, file_list, idx=0):     for element in file_list:         if idx == num:             return element         <MASK>             i = self.__get_file_by_num(num, element[3], idx + 1)             if not isinstance(i, int):                 return i             idx = i         else:             idx += 1     return idx",if element [ 3 ] and element [ 4 ] :,if element [ 2 ] == num :,16.14682615668325,16.14682615668325,0.0
"def scan_block_scalar_indentation(self):     # See the specification for details.     chunks = []     max_indent = 0     end_mark = self.get_mark()     while self.peek() in "" \r\n\x85\u2028\u2029"":         if self.peek() != "" "":             chunks.append(self.scan_line_break())             end_mark = self.get_mark()         else:             self.forward()             <MASK>                 max_indent = self.column     return chunks, max_indent, end_mark",if self . column > max_indent :,if max_indent > self . column_size :,25.965358893403383,25.965358893403383,0.0
"def ant_map(m):     tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0]))     players = {}     for row in m:         tmp += ""m ""         for col in row:             if col == LAND:                 tmp += "".""             elif col == BARRIER:                 tmp += ""%""             <MASK>                 tmp += ""*""             elif col == UNSEEN:                 tmp += ""?""             else:                 players[col] = True                 tmp += chr(col + 97)         tmp += ""\n""     tmp = (""players %s\n"" % len(players)) + tmp     return tmp",elif col == FOOD :,if col == SEEN :,32.46679154750991,32.46679154750991,0.0
"def prepare_data(entry):     branch_wise_entries = {}     gross_pay = 0     for d in entry:         gross_pay += d.gross_pay         <MASK>             branch_wise_entries[d.branch][d.mode_of_payment] = d.net_pay         else:             branch_wise_entries.setdefault(d.branch, {}).setdefault(                 d.mode_of_payment, d.net_pay             )     return branch_wise_entries, gross_pay",if branch_wise_entries . get ( d . branch ) :,if d . mode_of_payment :,7.200209897263065,7.200209897263065,0.0
"def __init__(self, uuid=None, cluster_state=None, children=None, **kwargs):     self.uuid = uuid     self.cluster_state = cluster_state     if self.cluster_state is not None:         self.children = WeakSet(             self.cluster_state.tasks.get(task_id)             for task_id in children or ()             <MASK>         )     else:         self.children = WeakSet()     self._serializer_handlers = {         ""children"": self._serializable_children,         ""root"": self._serializable_root,         ""parent"": self._serializable_parent,     }     if kwargs:         self.__dict__.update(kwargs)",if task_id in self . cluster_state . tasks,if task_id in self . cluster_state . tasks :,91.21679090703874,91.21679090703874,0.0
"def listdir(self, d):     try:         return [             p             for p in os.listdir(d)             <MASK>         ]     except OSError:         return []","if os . path . basename ( p ) != ""CVS"" and os . path . isdir ( os . path . join ( d , p ) )",if p != d :,0.26788699106838104,0.26788699106838104,0.0
"def send_packed_command(self, command, check_health=True):     if not self._sock:         self.connect()     try:         <MASK>             command = [command]         for item in command:             self._sock.sendall(item)     except socket.error as e:         self.disconnect()         if len(e.args) == 1:             _errno, errmsg = ""UNKNOWN"", e.args[0]         else:             _errno, errmsg = e.args         raise ConnectionError(             ""Error %s while writing to socket. %s."" % (_errno, errmsg)         )     except Exception:         self.disconnect()         raise","if isinstance ( command , str ) :",if check_health :,6.9717291216921975,6.9717291216921975,0.0
"def run(self):     """"""Start the scanner""""""     logging.info(""Dirscanner starting up"")     self.shutdown = False     while not self.shutdown:         # Wait to be woken up or triggered         with self.loop_condition:             self.loop_condition.wait(self.dirscan_speed)         <MASK>             self.scan()",if self . dirscan_speed and not self . shutdown :,if self . loop_condition . is_set ( ) :,13.674406678232565,13.674406678232565,0.0
"def __aexit__(     self, exc_type: type, exc_value: BaseException, tb: TracebackType ) -> None:     if exc_type is not None:         await self.close()     await self._task     while not self._receive_queue.empty():         data = await self._receive_queue.get()         if isinstance(data, bytes):             self.response_data.extend(data)         <MASK>             raise data","elif not isinstance ( data , HTTPDisconnect ) :",if exc_type is not None :,5.795599612995366,5.795599612995366,0.0
"def f(msg):     text = extractor(msg)     for px in prefix:         <MASK>             chunks = text[len(px) :].split(separator)             return chunks[0], (chunks[1:],) if pass_args else ()     return ((None,),)  # to distinguish with `None`",if text . startswith ( px ) :,if px in text :,8.290829875388036,8.290829875388036,0.0
"def _flatten(*args):     ahs = set()     if len(args) > 0:         for item in args:             if type(item) is ActionHandle:                 ahs.add(item)             <MASK>                 for ah in item:                     if type(ah) is not ActionHandle:  # pragma:nocover                         raise ActionManagerError(""Bad argument type %s"" % str(ah))                     ahs.add(ah)             else:  # pragma:nocover                 raise ActionManagerError(""Bad argument type %s"" % str(item))     return ahs","elif type ( item ) in ( list , tuple , dict , set ) :",if len ( item ) > 0 :,8.126306442139647,8.126306442139647,0.0
"def find_class(self, module, name):     # Subclasses may override this.     sys.audit(""pickle.find_class"", module, name)     if self.proto < 3 and self.fix_imports:         if (module, name) in _compat_pickle.NAME_MAPPING:             module, name = _compat_pickle.NAME_MAPPING[(module, name)]         <MASK>             module = _compat_pickle.IMPORT_MAPPING[module]     __import__(module, level=0)     if self.proto >= 4:         return _getattribute(sys.modules[module], name)[0]     else:         return getattr(sys.modules[module], name)",elif module in _compat_pickle . IMPORT_MAPPING :,if self . proto >= 3 :,3.983253478176822,3.983253478176822,0.0
"def _send_until_done(self, data):     while True:         try:             return self.connection.send(data)         except OpenSSL.SSL.WantWriteError:             wr = util.wait_for_write(self.socket, self.socket.gettimeout())             <MASK>                 raise timeout()             continue         except OpenSSL.SSL.SysCallError as e:             raise SocketError(str(e))",if not wr :,if wr is None :,14.058533129758727,14.058533129758727,0.0
"def __new__(cls, *args, **kwargs):     """"""Hack to ensure method defined as async are implemented as such.""""""     coroutines = inspect.getmembers(BaseManager, predicate=inspect.iscoroutinefunction)     for coroutine in coroutines:         implemented_method = getattr(cls, coroutine[0])         <MASK>             raise RuntimeError(""The method %s must be a coroutine"" % implemented_method)     return super().__new__(cls, *args, **kwargs)",if not inspect . iscoroutinefunction ( implemented_method ) :,if not implemented_method :,17.28116170001394,17.28116170001394,0.0
"def add_directive(self, name, obj, content=None, arguments=None, **options):     if isinstance(obj, clstypes) and issubclass(obj, Directive):         <MASK>             raise ExtensionError(                 ""when adding directive classes, no "" ""additional arguments may be given""             )         directives.register_directive(name, directive_dwim(obj))     else:         obj.content = content         obj.arguments = arguments         obj.options = options         directives.register_directive(name, obj)",if content or arguments or options :,if not arguments :,9.930283522141846,9.930283522141846,0.0
"def create(self, w):     if w.use_eventloop:         # does not use dedicated timer thread.         w.timer = _Timer(max_interval=10.0)     else:         <MASK>             # Default Timer is set by the pool, as for example, the             # eventlet pool needs a custom timer implementation.             w.timer_cls = w.pool_cls.Timer         w.timer = self.instantiate(             w.timer_cls,             max_interval=w.timer_precision,             on_error=self.on_timer_error,             on_tick=self.on_timer_tick,         )",if not w . timer_cls :,if w . pool_cls :,27.890014303843827,27.890014303843827,0.0
"def _config(_molecule_file, request):     with open(_molecule_file) as f:         d = util.safe_load(f)     if hasattr(request, ""param""):         <MASK>             d2 = util.safe_load(request.getfixturevalue(request.param))         else:             d2 = request.getfixturevalue(request.param)         # print(100, d)         # print(200, d2)         d = util.merge_dicts(d, d2)         # print(300, d)     return d","if isinstance ( request . getfixturevalue ( request . param ) , str ) :",if request . param :,6.114461654585455,6.114461654585455,0.0
"def _instrument_model(self, model):     for key, value in list(         model.__dict__.items()     ):  # avoid ""dictionary keys changed during iteration""         <MASK>             new_layer = self._instrument(value)             if new_layer is not value:                 setattr(model, key, new_layer)         elif isinstance(value, list):             for i, item in enumerate(value):                 if isinstance(item, tf.keras.layers.Layer):                     value[i] = self._instrument(item)     return model","if isinstance ( value , tf . keras . layers . Layer ) :","if isinstance ( value , dict ) :",28.08708327044616,28.08708327044616,0.0
"def is_accepted_drag_event(self, event):     if event.source() == self.table:         return True     mime = event.mimeData()     if mime.hasUrls():         for url in mime.urls():             # Only support local files.             <MASK>                 break             # And only allow supported extensions.             filename = url.toLocalFile()             extension = os.path.splitext(filename)[1].lower()[1:]             if extension not in _dictionary_formats():                 break         else:             return True     return False",if not url . isLocalFile ( ) :,if url . isLocalFile ( ) :,72.89545183625967,72.89545183625967,0.0
"def explain(self, other, depth=0):     exp = super(UnionType, self).explain(other, depth)     for ndx, subtype in enumerate(self.params[""allowed_types""]):         <MASK>             exp += ""\n{}and"".format("""".join([""\t""] * depth))         exp += ""\n"" + subtype.explain(other, depth=depth + 1)     return exp",if ndx > 0 :,"if ndx == self . params [ ""allowed_types"" ] :",5.816635421147513,5.816635421147513,0.0
"def test_k_is_stochastic_parameter(self):     # k as stochastic parameter     aug = iaa.MedianBlur(k=iap.Choice([3, 5]))     seen = [False, False]     for i in sm.xrange(100):         observed = aug.augment_image(self.base_img)         if np.array_equal(observed, self.blur3x3):             seen[0] += True         <MASK>             seen[1] += True         else:             raise Exception(""Unexpected result in MedianBlur@2"")         if all(seen):             break     assert np.all(seen)","elif np . array_equal ( observed , self . blur5x5 ) :",if i == sm . xrange ( 1 ) :,6.786053138365654,6.786053138365654,0.0
"def test_get_message(self):     async with self.chat_client:         await self._create_thread()         async with self.chat_thread_client:             message_id = await self._send_message()             message = await self.chat_thread_client.get_message(message_id)             assert message.id == message_id             assert message.type == ChatMessageType.TEXT             assert message.content.message == ""hello world""         # delete chat threads         <MASK>             await self.chat_client.delete_chat_thread(self.thread_id)",if not self . is_playback ( ) :,if self . thread_id :,10.759051250985632,10.759051250985632,0.0
"def do_write_property(self, device, callback=None):     try:         iocb = (             device             <MASK>             else self.form_iocb(device, request_type=""writeProperty"")         )         deferred(self.request_io, iocb)         self.requests_in_progress.update({iocb: {""callback"": callback}})         iocb.add_callback(self.__general_cb)     except Exception as error:         log.exception(""exception: %r"", error)","if isinstance ( device , IOCB )",if callback is not None :,6.870636427700047,6.870636427700047,0.0
"def fit(self, dataset, force_retrain):     if force_retrain:         self.sub_unit_1[""fitted""] = True         self.sub_unit_1[""calls""] += 1         self.sub_unit_2[""fitted""] = True         self.sub_unit_2[""calls""] += 1     else:         if not self.sub_unit_1[""fitted""]:             self.sub_unit_1[""fitted""] = True             self.sub_unit_1[""calls""] += 1         <MASK>             self.sub_unit_2[""fitted""] = True             self.sub_unit_2[""calls""] += 1     return self","if not self . sub_unit_2 [ ""fitted"" ] :","if not self . sub_unit_2 [ ""fitted"" ] :",100.00000000000004,100.00000000000004,1.0
"def _insert_with_loop(self):     id_list = []     last_id = None     return_id_list = self._return_id_list     for row in self._rows:         last_id = InsertQuery(self.model_class, row).upsert(self._upsert).execute()         <MASK>             id_list.append(last_id)     if return_id_list:         return id_list     else:         return last_id",if return_id_list :,if last_id :,17.030578356760866,17.030578356760866,0.0
"def merge_block(self):     """"""merges a block in the map""""""     for i in range(self.block.x):         for j in range(self.block.x):             c = self.block.get(i, j)             <MASK>                 self.map[(i + self.block.pos.x, j + self.block.pos.y)] = c",if c :,if c is None :,23.643540225079384,23.643540225079384,0.0
"def configure_plex(config):     core.PLEX_SSL = int(config[""Plex""][""plex_ssl""])     core.PLEX_HOST = config[""Plex""][""plex_host""]     core.PLEX_PORT = config[""Plex""][""plex_port""]     core.PLEX_TOKEN = config[""Plex""][""plex_token""]     plex_section = config[""Plex""][""plex_sections""] or []     if plex_section:         <MASK>             plex_section = "","".join(plex_section)  # fix in case this imported as list.         plex_section = [tuple(item.split("","")) for item in plex_section.split(""|"")]     core.PLEX_SECTION = plex_section","if isinstance ( plex_section , list ) :","if ""|"" in plex_section :",18.04438612975343,18.04438612975343,0.0
"def select(self):     e = xlib.XEvent()     while xlib.XPending(self._display):         xlib.XNextEvent(self._display, e)         # Key events are filtered by the xlib window event         # handler so they get a shot at the prefiltered event.         <MASK>             if xlib.XFilterEvent(e, e.xany.window):                 continue         try:             dispatch = self._window_map[e.xany.window]         except KeyError:             continue         dispatch(e)","if e . xany . type not in ( xlib . KeyPress , xlib . KeyRelease ) :",if e . xany :,5.251935081834493,5.251935081834493,0.0
"def format_message(self):     bits = [self.message]     if self.possibilities:         <MASK>             bits.append(""Did you mean %s?"" % self.possibilities[0])         else:             possibilities = sorted(self.possibilities)             bits.append(""(Possible options: %s)"" % "", "".join(possibilities))     return ""  "".join(bits)",if len ( self . possibilities ) == 1 :,"if self . possibilities [ 0 ] == ""mean"" :",15.727800941615351,15.727800941615351,0.0
"def _collect_logs(model):     page_token = None     all_logs = []     while True:         paginated_logs = model.lookup_logs(now, later, page_token=page_token)         page_token = paginated_logs.next_page_token         all_logs.extend(paginated_logs.logs)         <MASK>             break     return all_logs",if page_token is None :,if not paginated_logs . next_page_token :,14.323145079400492,14.323145079400492,0.0
"def run(self):     while True:         context_id_list_tuple = self._inflated_addresses.get(block=True)         <MASK>             break         c_id, inflated_address_list = context_id_list_tuple         inflated_value_map = dict(inflated_address_list)         if c_id in self._contexts:             self._contexts[c_id].set_from_tree(inflated_value_map)",if context_id_list_tuple is _SHUTDOWN_SENTINEL :,if context_id_list_tuple is None :,61.44118374261937,61.44118374261937,0.0
"def _setup_prefix(self):     # we assume here that our metadata may be nested inside a ""basket""     # of multiple eggs; that's why we use module_path instead of .archive     path = self.module_path     old = None     while path != old:         <MASK>             self.egg_name = os.path.basename(path)             self.egg_info = os.path.join(path, ""EGG-INFO"")             self.egg_root = path             break         old = path         path, base = os.path.split(path)","if path . lower ( ) . endswith ( "".egg"" ) :",if os . path . isdir ( path ) :,8.27951003977077,8.27951003977077,0.0
"def get_filename(self, prompt):     okay = False     val = """"     while not okay:         val = raw_input(""%s: %s"" % (prompt, val))         val = os.path.expanduser(val)         if os.path.isfile(val):             okay = True         <MASK>             path = val             val = self.choose_from_list(os.listdir(path))             if val:                 val = os.path.join(path, val)                 okay = True             else:                 val = """"         else:             print(""Invalid value: %s"" % val)             val = """"     return val",elif os . path . isdir ( val ) :,if os . path . isdir ( val ) :,88.01117367933934,88.01117367933934,0.0
"def versions(self, sitename, data):     # handle the query of type {""query"": '{""key"": ""/books/ia:foo00bar"", ...}}     if ""query"" in data:         q = json.loads(data[""query""])         itemid = self._get_itemid(q.get(""key""))         <MASK>             key = q[""key""]             return json.dumps([self.dummy_edit(key)])     # if not just go the default way     return ConnectionMiddleware.versions(self, sitename, data)",if itemid :,if itemid :,100.00000000000004,0.0,1.0
"def read_stanza(self):     while True:         try:             stanza_end = self._buffer.index(b""\n"")             stanza = self.decoder.decode(self._buffer[:stanza_end])             self._buffer = self._buffer[stanza_end + 1 :]             colon = stanza.index("":"")             return stanza[:colon], stanza[colon + 1 :]         except ValueError:             bytes = self.read_bytes()             <MASK>                 return None             else:                 self._buffer += bytes",if not bytes :,if not bytes :,100.00000000000004,100.00000000000004,1.0
def decodeattrs(attrs):     names = []     for bit in range(16):         mask = 1 << bit         <MASK>             if attrnames.has_key(mask):                 names.append(attrnames[mask])             else:                 names.append(hex(mask))     return names,if attrs & mask :,if mask in attrs :,15.106876986783844,15.106876986783844,0.0
"def _set_http_cookie():     if conf.cookie:         <MASK>             conf.http_headers[HTTP_HEADER.COOKIE] = ""; "".join(                 map(lambda x: ""="".join(x), conf.cookie.items())             )         else:             conf.http_headers[HTTP_HEADER.COOKIE] = conf.cookie","if isinstance ( conf . cookie , dict ) :",if conf . cookie :,16.62083000646927,16.62083000646927,0.0
"def __ne__(self, other):     if isinstance(other, WeakMethod):         <MASK>             return self is not other         return weakref.ref.__ne__(self, other) or self._func_ref != other._func_ref     return True",if not self . _alive or not other . _alive :,if self . _func_ref is other . _func_ref :,17.678748653651848,17.678748653651848,0.0
"def update_unread(self, order_id, reset=False):     conn = Database.connect_database(self.PATH)     with conn:         cursor = conn.cursor()         <MASK>             cursor.execute(                 """"""UPDATE sales SET unread = unread + 1 WHERE id=?;"""""", (order_id,)             )         else:             cursor.execute(""""""UPDATE sales SET unread=0 WHERE id=?;"""""", (order_id,))         conn.commit()     conn.close()",if reset is False :,if reset :,32.34325178227722,0.0,0.0
"def _get_field_value(self, test, key, match):     if test.ver == ofproto_v1_0.OFP_VERSION:         members = inspect.getmembers(match)         for member in members:             if member[0] == key:                 field_value = member[1]             elif member[0] == ""wildcards"":                 wildcards = member[1]         if key == ""nw_src"":             field_value = test.nw_src_to_str(wildcards, field_value)         <MASK>             field_value = test.nw_dst_to_str(wildcards, field_value)     else:         field_value = match[key]     return field_value","elif key == ""nw_dst"" :","if key == ""nw_dst"" :",88.01117367933934,88.01117367933934,0.0
"def nested_filter(self, items, mask):     keep_current = self.current_mask(mask)     keep_nested_lookup = self.nested_masks(mask)     for k, v in items:         keep_nested = keep_nested_lookup.get(k)         <MASK>             if keep_nested is not None:                 if isinstance(v, dict):                     yield k, dict(self.nested_filter(v.items(), keep_nested))             else:                 yield k, v",if k in keep_current :,if keep_current is not None :,23.356898886410015,23.356898886410015,0.0
"def goToPrevMarkedHeadline(self, event=None):     """"""Select the next marked node.""""""     c = self     p = c.p     if not p:         return     p.moveToThreadBack()     wrapped = False     while 1:         <MASK>             break         elif p:             p.moveToThreadBack()         elif wrapped:             break         else:             wrapped = True             p = c.rootPosition()     if not p:         g.blue(""done"")     c.treeSelectHelper(p)  # Sets focus.",if p and p . isMarked ( ) :,if event is None :,5.70796903405875,5.70796903405875,0.0
"def sample(self, **config):     """"""Sample a configuration from this search space.""""""     ret = {}     ret.update(self.data)     kwspaces = self.kwspaces     kwspaces.update(config)     striped_keys = [k.split(SPLITTER)[0] for k in config.keys()]     for k, v in kwspaces.items():         <MASK>             if isinstance(v, NestedSpace):                 sub_config = _strip_config_space(config, prefix=k)                 ret[k] = v.sample(**sub_config)             else:                 ret[k] = v     return ret",if k in striped_keys :,if k not in striped_keys :,59.4603557501361,59.4603557501361,0.0
"def update_gradients_full(self, dL_dK, X, X2=None):     if self.ARD:         phi1 = self.phi(X)         <MASK>             self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK, phi1, phi1)         else:             phi2 = self.phi(X2)             self.variance.gradient = np.einsum(""ij,iq,jq->q"", dL_dK, phi1, phi2)     else:         self.variance.gradient = np.einsum(""ij,ij"", dL_dK, self._K(X, X2)) * self.beta",if X2 is None or X is X2 :,if self . AR :,5.70796903405875,5.70796903405875,0.0
"def post(self):     host_json = json.loads(request.data)     host_os = host_json.get(""os"")     if host_os:         result = get_monkey_executable(host_os.get(""type""), host_os.get(""machine""))         if result:             # change resulting from new base path             executable_filename = result[""filename""]             real_path = MonkeyDownload.get_executable_full_path(executable_filename)             <MASK>                 result[""size""] = os.path.getsize(real_path)                 return result     return {}",if os . path . isfile ( real_path ) :,if real_path :,11.141275535087015,11.141275535087015,0.0
"def _encode_data(     self,     data,     content_type, ):     if content_type is MULTIPART_CONTENT:         return encode_multipart(BOUNDARY, data)     else:         # Encode the content so that the byte representation is correct.         match = CONTENT_TYPE_RE.match(content_type)         <MASK>             charset = match.group(1)         else:             charset = settings.DEFAULT_CHARSET         return force_bytes(data, encoding=charset)",if match :,if match :,100.00000000000004,0.0,1.0
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]:     tokens = list(tokens)     i = 0     while ""e"" in tokens[i + 1 :]:         i = tokens.index(""e"", i + 1)         s = i - 1         e = i + 1         if not re.match(""[0-9]"", str(tokens[s])):             continue         if re.match(""[+-]"", str(tokens[e])):             e += 1         <MASK>             e += 1             tokens[s:e] = ["""".join(tokens[s:e])]             i -= 1     return tokens","if re . match ( ""[0-9]"" , str ( tokens [ e ] ) ) :","if ""e"" in tokens [ s : e ] :",4.950411723397576,4.950411723397576,0.0
"def convert_with_key(self, key, value, replace=True):     result = self.configurator.convert(value)     # If the converted value is different, save for next time     if value is not result:         <MASK>             self[key] = result         if type(result) in (ConvertingDict, ConvertingList, ConvertingTuple):             result.parent = self             result.key = key     return result",if replace :,if replace :,100.00000000000004,0.0,1.0
"def OnListEndLabelEdit(self, std, extra):     item = extra[0]     text = item[4]     if text is None:         return     item_id = self.GetItem(item[0])[6]     from bdb import Breakpoint     for bplist in Breakpoint.bplist.itervalues():         for bp in bplist:             <MASK>                 if text.strip().lower() == ""none"":                     text = None                 bp.cond = text                 break     self.RespondDebuggerData()",if id ( bp ) == item_id :,if item_id == item_id :,50.26587270045526,50.26587270045526,0.0
"def add(self, url: str, future_nzo: NzbObject, when: Optional[int] = None):     """"""Add an URL to the URLGrabber queue, 'when' is seconds from now""""""     if future_nzo and when:         # Always increase counter         future_nzo.url_tries += 1         # Too many tries? Cancel         <MASK>             self.fail_to_history(future_nzo, url, T(""Maximum retries""))             return         future_nzo.url_wait = time.time() + when     self.queue.put((url, future_nzo))",if future_nzo . url_tries > cfg . max_url_retries ( ) :,if future_nzo . url_tries > self . max_retries :,52.78890511109627,52.78890511109627,0.0
def _is_datetime_string(series):     if series.dtype == object:         not_numeric = False         try:             pd.to_numeric(series)         except Exception as e:             not_numeric = True         datetime_col = None         <MASK>             try:                 datetime_col = pd.to_datetime(series)             except Exception as e:                 return False         if datetime_col is not None:             return True     return False,if not_numeric :,if not_numeric :,100.00000000000004,100.00000000000004,1.0
"def _getEventAndObservers(self, event):     if isinstance(event, xpath.XPathQuery):         # Treat as xpath         observers = self._xpathObservers     else:         <MASK>             # Treat as event             observers = self._eventObservers         else:             # Treat as xpath             event = xpath.internQuery(event)             observers = self._xpathObservers     return event, observers",if self . prefix == event [ : len ( self . prefix ) ] :,"if isinstance ( event , xpath . Event ) :",3.2612121198882003,3.2612121198882003,0.0
"def test_wildcard_import():     bonobo = __import__(""bonobo"")     assert bonobo.__version__     for name in dir(bonobo):         # ignore attributes starting by underscores         if name.startswith(""_""):             continue         attr = getattr(bonobo, name)         <MASK>             continue         assert name in bonobo.__all__",if inspect . ismodule ( attr ) :,if not attr :,7.733712583165139,7.733712583165139,0.0
"def relint_views(wid=None):     windows = [sublime.Window(wid)] if wid else sublime.windows()     for window in windows:         for view in window.views():             <MASK>                 hit(view, ""relint_views"")",if view . buffer_id ( ) in persist . assigned_linters and view . is_primary ( ) :,if view . is_active ( ) :,10.595236773288903,10.595236773288903,0.0
def _check_for_unknown_gender(self):     if self.obj.get_gender() == Person.UNKNOWN:         d = GenderDialog(parent=self.window)         gender = d.run()         d.destroy()         <MASK>             self.obj.set_gender(gender),if gender >= 0 :,if gender :,23.174952587773145,0.0,0.0
"def add_to_path(self, fnames):     """"""Add fnames to path""""""     indexes = []     for path in fnames:         project = self.get_source_project(path)         <MASK>             self.parent_widget.emit(SIGNAL(""pythonpath_changed()""))             indexes.append(self.get_index(path))     if indexes:         self.reset_icon_provider()         for index in indexes:             self.update(index)",if project . add_to_pythonpath ( path ) :,if project . is_open ( ) :,17.39350277271197,17.39350277271197,0.0
"def validate(self, value):     if value.grid_id is not None:         if not isinstance(value, self.proxy_class):             self.error(""FileField only accepts GridFSProxy values"")         <MASK>             self.error(""Invalid GridFSProxy value"")","if not isinstance ( value . grid_id , ObjectId ) :","if not isinstance ( value , GridFSProxy ) :",34.03200951522627,34.03200951522627,0.0
"def shortcut(self, input, ch_out, stride, name, if_first=False):     ch_in = input.shape[1]     if ch_in != ch_out or stride != 1:         <MASK>             return self.conv_bn_layer(input, ch_out, 1, stride, name=name)         else:             return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name)     else:         return input",if if_first :,if if_first :,100.00000000000004,100.00000000000004,1.0
"def convert_path(ctx, tpath):     for points, code in tpath.iter_segments():         if code == Path.MOVETO:             ctx.move_to(*points)         elif code == Path.LINETO:             ctx.line_to(*points)         elif code == Path.CURVE3:             ctx.curve_to(                 points[0], points[1], points[0], points[1], points[2], points[3]             )         <MASK>             ctx.curve_to(*points)         elif code == Path.CLOSEPOLY:             ctx.close_path()",elif code == Path . CURVE4 :,if code == Path . CURVE2 :,54.10822690539397,54.10822690539397,0.0
"def _get_build_status(self, job_name, build_number):     try:         build_info = self.server.get_build_info(job_name, build_number)         <MASK>             return ""building""         else:             return ""built""     except jenkins.NotFoundException:         return ""not found""","if build_info [ ""building"" ] :",if build_info :,26.013004751144457,26.013004751144457,0.0
"def _parse_param_value(name, datatype, default):     if datatype == ""bool"":         if default.lower() == ""true"":             return True         elif default.lower() == ""false"":             return False         else:             _s = ""{}: Invalid default value '{}' for bool parameter {}""             raise SyntaxError(_s.format(self.name, default, p))     elif datatype == ""int"":         if type(default) == int:             return default         else:             return int(default, 0)     elif datatype == ""real"":         <MASK>             return default         else:             return float(default)     else:         return str(default)",if type ( default ) == float :,if type ( default ) == float :,100.00000000000004,100.00000000000004,1.0
"def get_fills(self, exchange_order_id):     async with aiohttp.ClientSession() as client:         response: aiohttp.ClientResponse = await client.get(             f""{BASE_URL}{FILLS_ROUTE}"",             params={""orderId"": exchange_order_id, ""limit"": 100},         )         <MASK>             try:                 msg = await response.json()             except ValueError:                 msg = await response.text()             raise DydxAsyncAPIError(response.status, msg)         return await response.json()",if response . status >= 300 :,if response . status != 200 :,38.260294162784454,38.260294162784454,0.0
"def semanticTags(self, semanticTags):     if semanticTags is None:         self.__semanticTags = OrderedDict()     # check     for key, value in list(semanticTags.items()):         if not isinstance(key, int):             raise TypeError(""At least one key is not a valid int position"")         if not isinstance(value, list):             raise TypeError(                 ""At least one value of the provided dict is not a list of string""             )         for x in value:             <MASK>                 raise TypeError(                     ""At least one value of the provided dict is not a list of string""                 )     self.__semanticTags = semanticTags","if not isinstance ( x , str ) :","if not isinstance ( x , str ) :",100.00000000000004,100.00000000000004,1.0
"def start_cutting_tool(self, event, axis, direction):     toggle = event.EventObject     self.cutting = toggle.Value     if toggle.Value:         # Disable the other toggles         for child in self.cutsizer.Children:             child = child.Window             <MASK>                 child.Value = False         self.cutting_axis = axis         self.cutting_direction = direction     else:         self.cutting_axis = None         self.cutting_direction = None     self.cutting_dist = None",if child != toggle :,if child . Value :,19.3576934939088,19.3576934939088,0.0
"def decoration_helper(self, patched, args, keywargs):     extra_args = []     with contextlib.ExitStack() as exit_stack:         for patching in patched.patchings:             arg = exit_stack.enter_context(patching)             if patching.attribute_name is not None:                 keywargs.update(arg)             <MASK>                 extra_args.append(arg)         args += tuple(extra_args)         yield (args, keywargs)",elif patching . new is DEFAULT :,if arg is not None :,8.170609724417774,8.170609724417774,0.0
def decodeattrs(attrs):     names = []     for bit in range(16):         mask = 1 << bit         if attrs & mask:             <MASK>                 names.append(attrnames[mask])             else:                 names.append(hex(mask))     return names,if attrnames . has_key ( mask ) :,if attrs & mask :,5.171845311465849,5.171845311465849,0.0
"def pytest_collection_modifyitems(items):     for item in items:         if item.nodeid.startswith(""tests/params""):             if ""stage"" not in item.keywords:                 item.add_marker(pytest.mark.stage(""unit""))             <MASK>                 item.add_marker(pytest.mark.init(rng_seed=123))","if ""init"" not in item . keywords :","if ""init"" not in item . keywords :",100.00000000000004,100.00000000000004,1.0
"def handle_socket(self, request):     conn = request.connection     while True:         chunk = conn.recv(4)         <MASK>             break         slen = struct.unpack("">L"", chunk)[0]         chunk = conn.recv(slen)         while len(chunk) < slen:             chunk = chunk + conn.recv(slen - len(chunk))         obj = pickle.loads(chunk)         record = logging.makeLogRecord(obj)         self.log_output += record.msg + ""\n""         self.handled.release()",if len ( chunk ) < 4 :,if not chunk :,7.733712583165139,7.733712583165139,0.0
"def on_source_foreach(self, model, path, iter, id):     m_id = model.get_value(iter, self.COLUMN_ID)     if m_id == id:         if self._foreach_mode == ""get"":             self._foreach_take = model.get_value(iter, self.COLUMN_ENABLED)         <MASK>             self._foreach_take = iter","elif self . _foreach_mode == ""set"" :","if self . _foreach_mode == ""set"" :",91.21679090703874,91.21679090703874,0.0
"def parts():     for l in lists.leaves:         head_name = l.get_head_name()         if head_name == ""System`List"":             yield l.leaves         <MASK>             raise MessageException(""Catenate"", ""invrp"", l)","elif head_name != ""System`Missing"" :","if head_name == ""System`List"" :",34.172334076593074,34.172334076593074,0.0
"def __fill_counter_values(self, command: str):     result = []     regex = r""(item[0-9]+\.counter_value)""     for token in re.split(regex, command):         <MASK>             try:                 result.append(str(self.simulator_config.item_dict[token].value))             except (KeyError, ValueError, AttributeError):                 logger.error(""Could not get counter value for "" + token)         else:             result.append(token)     return """".join(result)","if re . match ( regex , token ) is not None :",if token in self . simulator_config . item_dict :,4.368583925857938,4.368583925857938,0.0
"def IMPORTFROM(self, node):     <MASK>         if not self.futuresAllowed:             self.report(messages.LateFutureImport, node, [n.name for n in node.names])     else:         self.futuresAllowed = False     for alias in node.names:         if alias.name == ""*"":             self.scope.importStarred = True             self.report(messages.ImportStarUsed, node, node.module)             continue         name = alias.asname or alias.name         importation = Importation(name, node)         if node.module == ""__future__"":             importation.used = (self.scope, node)         self.addBinding(node, importation)","if node . module == ""__future__"" :","if node . module == ""__future__"" :",100.00000000000004,100.00000000000004,1.0
"def _split_batch_list(args, batch_list):     new_list = []     for batch in batch_list.batches:         new_list.append(batch)         <MASK>             yield batch_pb2.BatchList(batches=new_list)             new_list = []     if new_list:         yield batch_pb2.BatchList(batches=new_list)",if len ( new_list ) == args . batch_size_limit :,if new_list :,4.098646217784873,4.098646217784873,0.0
"def get_branch_or_use_upstream(branch_name, arg, repo):     if not branch_name:  # use upstream branch         current_b = repo.current_branch         upstream_b = current_b.upstream         <MASK>             raise ValueError(                 ""No {0} branch specified and the current branch has no upstream ""                 ""branch set"".format(arg)             )         ret = current_b.upstream     else:         ret = get_branch(branch_name, repo)     return ret",if not upstream_b :,if not upstream_b :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, **settings):     default_settings = self.get_default_settings()     for name, value in default_settings.items():         <MASK>             setattr(self, name, value)     for name, value in settings.items():         if name not in default_settings:             raise ImproperlyConfigured(                 ""Invalid setting '{}' for {}"".format(                     name,                     self.__class__.__name__,                 )             )         setattr(self, name, value)","if not hasattr ( self , name ) :",if name in settings :,6.316906128202129,6.316906128202129,0.0
"def _declare(self, name, obj, included=False, quals=0):     if name in self._declarations:         prevobj, prevquals = self._declarations[name]         if prevobj is obj and prevquals == quals:             return         <MASK>             raise api.FFIError(                 ""multiple declarations of %s (for interactive usage, ""                 ""try cdef(xx, override=True))"" % (name,)             )     assert ""__dotdotdot__"" not in name.split()     self._declarations[name] = (obj, quals)     if included:         self._included_declarations.add(obj)",if not self . _override :,if len ( self . _included_declarations ) > 1 :,13.065113298388567,13.065113298388567,0.0
"def include_file(name, fdir=tmp_dir, b64=False):     try:         if fdir is None:             fdir = """"         <MASK>             with io.open(os.path.join(fdir, name), ""rb"") as f:                 return base64.b64encode(f.read()).decode(""utf-8"")         else:             with io.open(os.path.join(fdir, name), ""r"", encoding=""utf-8"") as f:                 return f.read()     except (OSError, IOError) as e:         logger.error(""Could not include file '{}': {}"".format(name, e))",if b64 :,if b64 :,100.00000000000004,0.0,1.0
"def to_raw_json(self):     parts = {}     for p in self.parts:         <MASK>             parts[p[0]] = []         parts[p[0]].append({""value"": p[2], ""parameters"": p[1]})     children = [x.to_raw_json() for x in self.children]     return {         ""type"": self.__class__.__name__,         ""children"": children,         ""parts"": parts,     }",if p [ 0 ] not in parts :,"if not isinstance ( p , dict ) :",6.742555929751843,6.742555929751843,0.0
"def process_output(     output: str, filename: str, start_line: int ) -> Tuple[Optional[str], bool]:     error_found = False     for line in output.splitlines():         t = get_revealed_type(line, filename, start_line)         <MASK>             return t, error_found         elif ""error:"" in line:             error_found = True     return None, True  # finding no reveal_type is an error",if t :,if t :,100.00000000000004,0.0,1.0
"def __init__(     self, resize_keyboard=None, one_time_keyboard=None, selective=None, row_width=3 ):     if row_width > self.max_row_keys:         # Todo: Will be replaced with Exception in future releases         <MASK>             logger.error(                 ""Telegram does not support reply keyboard row width over %d.""                 % self.max_row_keys             )         row_width = self.max_row_keys     self.resize_keyboard = resize_keyboard     self.one_time_keyboard = one_time_keyboard     self.selective = selective     self.row_width = row_width     self.keyboard = []",if not DISABLE_KEYLEN_ERROR :,if resize_keyboard :,7.715486568024961,7.715486568024961,0.0
"def realizeElementExpressions(innerElement):     elementHasBeenRealized = False     for exp in innerElement.expressions:         if not hasattr(exp, ""realize""):             continue         # else:         before, during, after = exp.realize(innerElement)         elementHasBeenRealized = True         for n in before:             newStream.append(n)         <MASK>             newStream.append(during)         for n in after:             newStream.append(n)     if elementHasBeenRealized is False:         newStream.append(innerElement)",if during is not None :,if elementHasBeenRealized is True :,11.51015341649912,11.51015341649912,0.0
"def lex_number(self, pos):     # numeric literal     start = pos     found_dot = False     while pos < len(self.string) and (         self.string[pos].isdigit() or self.string[pos] == "".""     ):         <MASK>             if found_dot is True:                 raise ValueError(""Invalid number. Found multiple '.'"")             found_dot = True         # technically we allow more than one ""."" and let float()'s parsing         # complain later         pos += 1     val = self.string[start:pos]     return Token(TokenType.LNUM, val, len(val))","if self . string [ pos ] == ""."" :","if self . string [ pos ] == ""."" :",100.00000000000004,100.00000000000004,1.0
"def rename(src, dst):     # Try atomic or pseudo-atomic rename     if _rename(src, dst):         return     # Fall back to ""move away and replace""     try:         os.rename(src, dst)     except OSError as e:         <MASK>             raise         old = ""%s-%08x"" % (dst, random.randint(0, sys.maxsize))         os.rename(dst, old)         os.rename(src, dst)         try:             os.unlink(old)         except Exception:             pass",if e . errno != errno . EEXIST :,if e . errno == errno . EEXIST :,65.80370064762461,65.80370064762461,0.0
"def _the_callback(widget, event_id):     point = widget.GetCenter()     index = widget.WIDGET_INDEX     if hasattr(callback, ""__call__""):         if num > 1:             args = [point, index]         else:             args = [point]         <MASK>             args.append(widget)         try_callback(callback, *args)     return",if pass_widget :,"if event_id == ""click"" :",5.522397783539471,5.522397783539471,0.0
"def run(self):     for _ in range(self.n):         error = True         try:             self.collection.insert_one({""test"": ""insert""})             error = False         except:             <MASK>                 raise         if self.expect_exception:             assert error",if not self . expect_exception :,if self . expect_exception :,72.89545183625967,72.89545183625967,0.0
"def handle(self, *args: Any, **options: Any) -> None:     realm = self.get_realm(options)     if options[""all""]:         <MASK>             raise CommandError(                 ""You must specify a realm if you choose the --all option.""             )         self.fix_all_users(realm)         return     self.fix_emails(realm, options[""emails""])",if realm is None :,if not realm :,16.37226966703825,16.37226966703825,0.0
"def recv_tdi(self, nbits, pos):     bits = 0     for n in range(nbits * 2):         yield from self._wait_for_tck()         <MASK>             bits = (bits << 1) | (yield self.tdi.o)     return bits",if ( yield self . tck . o ) == pos :,if pos == 0 :,6.356491690403502,6.356491690403502,0.0
"def _split_head(self):     if not hasattr(self, ""_severed_head""):         <MASK>             tree = self._tree.copy()             head = tree.get_heading_text()             tree.remove_heading()             self._severed_head = (head, tree)         else:             self._severed_head = (None, None)     return self._severed_head",if self . _tree :,if self . _tree :,100.00000000000004,100.00000000000004,1.0
"def buildSearchTrie(self, choices):     searchtrie = trie.Trie()     for choice in choices:         for token in self.tokenizeChoice(choice):             <MASK>                 searchtrie[token] = []             searchtrie[token].append(choice)     return searchtrie",if not searchtrie . has_key ( token ) :,if token not in searchtrie :,5.274846355723257,5.274846355723257,0.0
"def format_sql(sql, params):     rv = []     if isinstance(params, dict):         # convert sql with named parameters to sql with unnamed parameters         conv = _FormatConverter(params)         if params:             sql = sql_to_string(sql)             sql = sql % conv             params = conv.params         else:             params = ()     for param in params or ():         <MASK>             rv.append(""NULL"")         param = safe_repr(param)         rv.append(param)     return sql, rv",if param is None :,if param is None :,100.00000000000004,100.00000000000004,1.0
def on_completed2():     doner[0] = True     if not qr:         if len(ql) > 0:             observer.on_next(False)             observer.on_completed()         <MASK>             observer.on_next(True)             observer.on_completed(),elif donel [ 0 ] :,if len ( qr ) > 0 :,6.567274736060395,6.567274736060395,0.0
"def notify_digest(self, frequency, changes):     notifications = defaultdict(list)     users = {}     for change in changes:         for user in self.get_users(frequency, change):             <MASK>                 notifications[user.pk].append(change)                 users[user.pk] = user     for user in users.values():         self.send_digest(             user.profile.language,             user.email,             notifications[user.pk],             subscription=user.current_subscription,         )",if change . project is None or user . can_access_project ( change . project ) :,if user . profile . language == self . profile . language :,4.869426103311578,4.869426103311578,0.0
"def _any_listener_using(self, target_group_arn):     for load_balancer in self.load_balancers.values():         for listener in load_balancer.listeners.values():             for rule in listener.rules:                 for action in rule.actions:                     <MASK>                         return True     return False","if action . data . get ( ""target_group_arn"" ) == target_group_arn :",if action . target_group_arn == target_group_arn :,49.05867907682778,49.05867907682778,0.0
"def train_dict(self, triples):     """"""Train a dict lemmatizer given training (word, pos, lemma) triples.""""""     # accumulate counter     ctr = Counter()     ctr.update([(p[0], p[1], p[2]) for p in triples])     # find the most frequent mappings     for p, _ in ctr.most_common():         w, pos, l = p         if (w, pos) not in self.composite_dict:             self.composite_dict[(w, pos)] = l         <MASK>             self.word_dict[w] = l     return",if w not in self . word_dict :,if l is not None :,5.484411595600381,5.484411595600381,0.0
"def parse_git_config(path):     """"""Parse git config file.""""""     config = dict()     section = None     with open(os.path.join(path, ""config""), ""r"") as f:         for line in f:             line = line.strip()             <MASK>                 section = line[1:-1].strip()                 config[section] = dict()             elif section:                 key, value = line.replace("" "", """").split(""="")                 config[section][key] = value     return config","if line . startswith ( ""["" ) :",if section :,3.361830360737634,0.0,0.0
"def send_signal(self, pid, signum):     if pid in self.processes:         process = self.processes[pid]         hook_result = self.call_hook(""before_signal"", pid=pid, signum=signum)         <MASK>             logger.debug(                 ""before_signal hook didn't return True ""                 ""=> signal %i is not sent to %i"" % (signum, pid)             )         else:             process.send_signal(signum)         self.call_hook(""after_signal"", pid=pid, signum=signum)     else:         logger.debug(""process %s does not exist"" % pid)",if signum != signal . SIGKILL and not hook_result :,if hook_result :,14.276239697197271,14.276239697197271,0.0
"def validate_pos_return(self):     if self.is_pos and self.is_return:         total_amount_in_payments = 0         for payment in self.payments:             total_amount_in_payments += payment.amount         invoice_total = self.rounded_total or self.grand_total         <MASK>             frappe.throw(                 _(""Total payments amount can't be greater than {}"").format(                     -invoice_total                 )             )",if total_amount_in_payments < invoice_total :,if total_amount_in_payments > invoice_total :,76.11606003349888,76.11606003349888,0.0
"def delete(key, inner_key=None):     if inner_key is not None:         try:             del cache[key][inner_key]             del use_count[key][inner_key]             <MASK>                 del cache[key]                 del use_count[key]             wrapper.cache_size -= 1         except KeyError:             return False         else:             return True     else:         try:             wrapper.cache_size -= len(cache[key])             del cache[key]             del use_count[key]         except KeyError:             return False         else:             return True",if not cache [ key ] :,if inner_key in cache :,9.287528999566801,9.287528999566801,0.0
"def insertionsort(array):     size = array.getsize()     array.reset(""Insertion sort"")     for i in range(1, size):         j = i - 1         while j >= 0:             <MASK>                 break             array.swap(j, j + 1)             j = j - 1     array.message(""Sorted"")","if array . compare ( j , j + 1 ) <= 0 :",if i == size :,2.383515454163372,2.383515454163372,0.0
"def publish_state(cls, payload, state):     try:         if isinstance(payload, LiveActionDB):             <MASK>                 cls.process(payload)             else:                 worker.get_worker().process(payload)     except Exception:         traceback.print_exc()         print(payload)",if state == action_constants . LIVEACTION_STATUS_REQUESTED :,"if state == ""publish"" :",17.267606045625936,17.267606045625936,0.0
"def change_opacity_function(self, new_f):     self.opacity_function = new_f     dr = self.radius / self.num_levels     sectors = []     for submob in self.submobjects:         if type(submob) == AnnularSector:             sectors.append(submob)     for (r, submob) in zip(np.arange(0, self.radius, dr), sectors):         <MASK>             # it's the shadow, don't dim it             continue         alpha = self.opacity_function(r)         submob.set_fill(opacity=alpha)",if type ( submob ) != AnnularSector :,if r == self . shadow :,6.413885305524152,6.413885305524152,0.0
"def is_suppressed_warning(     type: str, subtype: str, suppress_warnings: List[str] ) -> bool:     """"""Check the warning is suppressed or not.""""""     if type is None:         return False     for warning_type in suppress_warnings:         <MASK>             target, subtarget = warning_type.split(""."", 1)         else:             target, subtarget = warning_type, None         if target == type:             if (                 subtype is None                 or subtarget is None                 or subtarget == subtype                 or subtarget == ""*""             ):                 return True     return False","if ""."" in warning_type :","if ""."" in warning_type :",100.00000000000004,100.00000000000004,1.0
"def set_many(self, mapping, timeout=None):     timeout = self._normalize_timeout(timeout)     # Use transaction=False to batch without calling redis MULTI     # which is not supported by twemproxy     pipe = self._client.pipeline(transaction=False)     for key, value in _items(mapping):         dump = self.dump_object(value)         <MASK>             pipe.set(name=self.key_prefix + key, value=dump)         else:             pipe.setex(name=self.key_prefix + key, value=dump, time=timeout)     return pipe.execute()",if timeout == - 1 :,if timeout is None :,15.848738972120703,15.848738972120703,0.0
"def maybe_relative_path(path):     if not os.path.isabs(path):         return path  # already relative     dir = path     names = []     while True:         prevdir = dir         dir, name = os.path.split(prevdir)         if dir == prevdir or not dir:             return path  # failed to make it relative         names.append(name)         try:             <MASK>                 names.reverse()                 return os.path.join(*names)         except OSError:             pass","if samefile ( dir , os . curdir ) :",if name in names :,4.673289785800722,4.673289785800722,0.0
"def word_range(word):     for ind in range(len(word)):         temp = word[ind]         for c in [chr(x) for x in range(ord(""a""), ord(""z"") + 1)]:             <MASK>                 yield word[:ind] + c + word[ind + 1 :]",if c != temp :,if temp == temp :,32.46679154750991,32.46679154750991,0.0
"def validate(self):     self.update_soil_edit(""sand_composition"")     for soil_type in self.soil_types:         <MASK>             frappe.throw(_(""{0} should be a value between 0 and 100"").format(soil_type))     if sum(self.get(soil_type) for soil_type in self.soil_types) != 100:         frappe.throw(_(""Soil compositions do not add up to 100""))",if self . get ( soil_type ) > 100 or self . get ( soil_type ) < 0 :,if len ( self . get ( soil_type ) ) != 0 :,33.37829550552269,33.37829550552269,0.0
"def on_click(self, event):     run = self._is_running()     if event[""button""] == self.button_activate:         self.py3.command_run([""xscreensaver-command"", ""-activate""])     if event[""button""] == self.button_toggle:         <MASK>             self.py3.command_run([""xscreensaver-command"", ""-exit""])         else:             # Because we want xscreensaver to continue running after             # exit, we instead use preexec_fn=setpgrp here.             Popen(                 [""xscreensaver"", ""-no-splash"", ""-no-capture-stderr""],                 stdout=PIPE,                 stderr=PIPE,                 preexec_fn=setpgrp,             )",if run :,if run :,100.00000000000004,0.0,1.0
"def maybe_relative_path(path):     if not os.path.isabs(path):         return path  # already relative     dir = path     names = []     while True:         prevdir = dir         dir, name = os.path.split(prevdir)         <MASK>             return path  # failed to make it relative         names.append(name)         try:             if samefile(dir, os.curdir):                 names.reverse()                 return os.path.join(*names)         except OSError:             pass",if dir == prevdir or not dir :,if not os . path . isdir ( dir ) :,5.300156689756295,5.300156689756295,0.0
"def _format_micros(self, datestring):     parts = datestring[:-1].split(""."")     if len(parts) == 1:         <MASK>             return datestring[:-1] + "".000000Z""         else:             return datestring + "".000000Z""     else:         micros = parts[-1][:6] if len(parts[-1]) > 6 else parts[-1]         return ""."".join(parts[:-1] + [""{:06d}"".format(int(micros))]) + ""Z""","if datestring . endswith ( ""Z"" ) :","if parts [ 0 ] == ""Z"" :",15.851165692617148,15.851165692617148,0.0
"def preprocess_raw_enwik9(input_filename, output_filename):     with open(input_filename, ""r"") as f1:         with open(output_filename, ""w"") as f2:             while True:                 line = f1.readline()                 if not line:                     break                 line = list(enwik9_norm_transform([line]))[0]                 <MASK>                     if line[0] == "" "":                         line = line[1:]                     f2.writelines(line + ""\n"")","if line != "" "" and line != """" :",if line :,2.247320757600649,0.0,0.0
"def set(self, item, data):     if not type(item) is slice:         item = slice(item, item + len(data), None)     virt_item = self.item2virtitem(item)     if not virt_item:         return     off = 0     for s, n_item in virt_item:         <MASK>             i = slice(off, n_item.stop + off - n_item.start, n_item.step)             data_slice = data.__getitem__(i)             s.content.__setitem__(n_item, data_slice)             off = i.stop         else:             raise ValueError(""TODO XXX"")     return","if isinstance ( s , ProgBits ) :",if n_item . stop < off :,5.669791110976001,5.669791110976001,0.0
"def walk(msg, callback, data):     partnum = 0     for part in msg.walk():         # multipart/* are just containers         if part.get_content_maintype() == ""multipart"":             continue         ctype = part.get_content_type()         if ctype is None:             ctype = OCTET_TYPE         filename = part.get_filename()         <MASK>             filename = PART_FN_TPL % (partnum)         headers = dict(part)         LOG.debug(headers)         headers[""Content-Type""] = ctype         payload = util.fully_decoded_payload(part)         callback(data, filename, payload, headers)         partnum = partnum + 1",if not filename :,if filename is None :,14.058533129758727,14.058533129758727,0.0
"def _run_wes(args):     """"""Run CWL using a Workflow Execution Service (WES) endpoint""""""     main_file, json_file, project_name = _get_main_and_json(args.directory)     main_file = _pack_cwl(main_file)     if args.host and ""stratus"" in args.host:         _run_wes_stratus(args, main_file, json_file)     else:         opts = [""--no-wait""]         <MASK>             opts += [""--host"", args.host]         if args.auth:             opts += [""--auth"", args.auth]         cmd = [""wes-client""] + opts + [main_file, json_file]         _run_tool(cmd)",if args . host :,if args . host :,100.00000000000004,100.00000000000004,1.0
"def insertTestData(self, rows):     for row in rows:         if isinstance(row, Worker):             self.workers[row.id] = dict(                 id=row.id, name=row.name, paused=0, graceful=0, info=row.info             )         <MASK>             row.id = row.buildermasterid * 10000 + row.workerid             self.configured[row.id] = dict(                 buildermasterid=row.buildermasterid, workerid=row.workerid             )         elif isinstance(row, ConnectedWorker):             self.connected[row.id] = dict(masterid=row.masterid, workerid=row.workerid)","elif isinstance ( row , ConfiguredWorker ) :","if isinstance ( row , ConfigurationWorker ) :",41.11336169005198,41.11336169005198,0.0
"def local_shape_to_shape_i(node):     if node.op == T.shape:         # This optimization needs ShapeOpt and fgraph.shape_feature         <MASK>             return         shape_feature = node.fgraph.shape_feature         ret = shape_feature.make_vector_shape(node.inputs[0])         # We need to copy over stack trace from input to output         copy_stack_trace(node.outputs[0], ret)         return [ret]","if not hasattr ( node . fgraph , ""shape_feature"" ) :",if node . op == T . shape :,6.4005180884547785,6.4005180884547785,0.0
"def get_config():     """"""Get INI parser with version.ini data.""""""     # TODO(hanuszczak): See comment in `setup.py` for `grr-response-proto`.     ini_path = os.path.join(THIS_DIRECTORY, ""version.ini"")     <MASK>         ini_path = os.path.join(THIS_DIRECTORY, ""../../version.ini"")         if not os.path.exists(ini_path):             raise RuntimeError(""Couldn't find version.ini"")     config = configparser.ConfigParser()     config.read(ini_path)     return config",if not os . path . exists ( ini_path ) :,if not os . path . exists ( ini_path ) :,100.00000000000004,100.00000000000004,1.0
"def init_weights(self, pretrained=None):     if isinstance(pretrained, str):         logger = logging.getLogger()         load_checkpoint(self, pretrained, strict=False, logger=logger)     elif pretrained is None:         for m in self.modules():             <MASK>                 kaiming_init(m)             elif isinstance(m, (_BatchNorm, nn.GroupNorm)):                 constant_init(m, 1)     else:         raise TypeError(""pretrained must be a str or None"")","if isinstance ( m , nn . Conv2d ) :","if isinstance ( m , nn . KaimingNorm ) :",70.71067811865478,70.71067811865478,0.0
"def isValidDateString(config_param_name, value, valid_value):     try:         <MASK>             return value         day, month, year = value.split(""-"")         if int(day) < 1 or int(day) > 31:             raise DateStringValueError(config_param_name, value)         if int(month) < 1 or int(month) > 12:             raise DateStringValueError(config_param_name, value)         if int(year) < 1900 or int(year) > 2013:             raise DateStringValueError(config_param_name, value)         return value     except Exception:         raise DateStringValueError(config_param_name, value)","if value == ""DD-MM-YYYY"" :",if not valid_value :,7.654112967106117,7.654112967106117,0.0
"def from_obj(cls, py_obj):     if not isinstance(py_obj, Image):         raise TypeError(""py_obj must be a wandb.Image"")     else:         <MASK>             box_keys = list(py_obj._boxes.keys())         else:             box_keys = []         if hasattr(py_obj, ""masks"") and py_obj.masks:             mask_keys = list(py_obj.masks.keys())         else:             mask_keys = []         return cls(box_keys, mask_keys)","if hasattr ( py_obj , ""_boxes"" ) and py_obj . _boxes :","if hasattr ( py_obj , ""boxes"" ) and py_obj . _boxes :",86.06031405392808,86.06031405392808,0.0
"def _path_type(st, lst):     parts = []     if st:         if stat.S_ISREG(st.st_mode):             parts.append(""file"")         <MASK>             parts.append(""dir"")         else:             parts.append(""other"")     if lst:         if stat.S_ISLNK(lst.st_mode):             parts.append(""link"")     return "" "".join(parts)",elif stat . S_ISDIR ( st . st_mode ) :,if stat . S_ISLNK ( st . st_mode ) :,69.97522298221911,69.97522298221911,0.0
"def is_destructive(queries):     """"""Returns if any of the queries in *queries* is destructive.""""""     keywords = (""drop"", ""shutdown"", ""delete"", ""truncate"", ""alter"")     for query in sqlparse.split(queries):         if query:             <MASK>                 return True             elif query_starts_with(                 query, [""update""]             ) is True and not query_has_where_clause(query):                 return True     return False","if query_starts_with ( query , keywords ) is True :",if query in keywords :,4.199688916946863,4.199688916946863,0.0
"def _store_gsuite_membership_post(self):     """"""Flush storing gsuite memberships.""""""     if not self.member_cache:         return     self.session.flush()     # session.execute automatically flushes     if self.membership_items:         <MASK>             # SQLite doesn't support bulk insert             for item in self.membership_items:                 stmt = self.dao.TBL_MEMBERSHIP.insert(item)                 self.session.execute(stmt)         else:             stmt = self.dao.TBL_MEMBERSHIP.insert(self.membership_items)             self.session.execute(stmt)","if get_sql_dialect ( self . session ) == ""sqlite"" :",if self . db . db_id == self . db_id :,7.727578693391118,7.727578693391118,0.0
"def forward(self, inputs: paddle.Tensor):     outputs = []     blocks = self.block(inputs)     route = None     for i, block in enumerate(blocks):         <MASK>             block = paddle.concat([route, block], axis=1)         route, tip = self.yolo_blocks[i](block)         block_out = self.block_outputs[i](tip)         outputs.append(block_out)         if i < 2:             route = self.route_blocks_2[i](route)             route = self.upsample(route)     return outputs",if i > 0 :,if i < 2 :,23.643540225079384,23.643540225079384,0.0
"def deep_dict(self, root=None):     if root is None:         root = self     result = {}     for key, value in root.items():         <MASK>             result[key] = self.deep_dict(root=self.__class__._get_next(key, root))         else:             result[key] = value     return result","if isinstance ( value , dict ) :","if isinstance ( value , dict ) :",100.00000000000004,100.00000000000004,1.0
"def _parse_param_list(self, content):     r = Reader(content)     params = []     while not r.eof():         header = r.read().strip()         <MASK>             arg_name, arg_type = header.split("" : "")[:2]         else:             arg_name, arg_type = header, """"         desc = r.read_to_next_unindented_line()         desc = dedent_lines(desc)         params.append((arg_name, arg_type, desc))     return params","if "" : "" in header :",if header :,16.605579150202516,0.0,0.0
"def _ungroup(sequence, groups=None):     for v in sequence:         <MASK>             if groups is not None:                 groups.append(list(_ungroup(v, groups=None)))             for v in _ungroup(v, groups):                 yield v         else:             yield v","if isinstance ( v , ( list , tuple ) ) :","if isinstance ( v , list ) :",37.28878639930421,37.28878639930421,0.0
"def _add_resource_group(obj):     if isinstance(obj, list):         for array_item in obj:             _add_resource_group(array_item)     elif isinstance(obj, dict):         try:             if ""resourcegroup"" not in [x.lower() for x in obj.keys()]:                 if obj[""id""]:                     obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""]         except (KeyError, IndexError, TypeError):             pass         for item_key in obj:             <MASK>                 _add_resource_group(obj[item_key])","if item_key != ""sourceVault"" :",if item_key in obj :,28.319415510892387,28.319415510892387,0.0
"def haslayer(self, cls):     """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax.""""""     if self.__class__ == cls or self.__class__.__name__ == cls:         return 1     for f in self.packetfields:         fvalue_gen = self.getfieldval(f.name)         if fvalue_gen is None:             continue         if not f.islist:             fvalue_gen = SetGen(fvalue_gen, _iterpacket=0)         for fvalue in fvalue_gen:             <MASK>                 ret = fvalue.haslayer(cls)                 if ret:                     return ret     return self.payload.haslayer(cls)","if isinstance ( fvalue , Packet ) :",if not fvalue . islist :,7.654112967106117,7.654112967106117,0.0
"def _post_attachment(self, message, channel, color, sub_fields=None):     if channel is None:         message_channels = self.channels     else:         message_channels = [channel]     for message_channel in message_channels:         attachment = {             ""fallback"": message,             ""text"": message,             ""color"": color,         }         <MASK>             attachment[""fields""] = sub_fields         self.slack_client.api_call(             ""chat.postMessage"",             channel=message_channel,             attachments=[attachment],             as_user=True,         )",if sub_fields is not None :,if sub_fields is not None :,100.00000000000004,100.00000000000004,1.0
"def create(cls, repository, args):     key = cls()     passphrase = os.environ.get(""ATTIC_PASSPHRASE"")     if passphrase is not None:         passphrase2 = passphrase     else:         passphrase, passphrase2 = 1, 2     while passphrase != passphrase2:         passphrase = getpass(""Enter passphrase: "")         <MASK>             print(""Passphrase must not be blank"")             continue         passphrase2 = getpass(""Enter same passphrase again: "")         if passphrase != passphrase2:             print(""Passphrases do not match"")     key.init(repository, passphrase)     if passphrase:         print(""Remember your passphrase. Your data will be inaccessible without it."")     return key",if not passphrase :,if passphrase == passphrase :,17.965205598154213,17.965205598154213,0.0
"def _generate_create_date(self):     if self.timezone is not None:         # First, assume correct capitalization         tzinfo = tz.gettz(self.timezone)         <MASK>             # Fall back to uppercase             tzinfo = tz.gettz(self.timezone.upper())         if tzinfo is None:             raise util.CommandError(""Can't locate timezone: %s"" % self.timezone)         create_date = (             datetime.datetime.utcnow().replace(tzinfo=tz.tzutc()).astimezone(tzinfo)         )     else:         create_date = datetime.datetime.now()     return create_date",if tzinfo is None :,if tzinfo is None :,100.00000000000004,100.00000000000004,1.0
"def _read_header_lines(fp):     """"""Read lines with headers until the start of body""""""     lines = deque()     for line in fp:         if is_empty(line):             break         # tricky case if it's not a header and not an empty line         # usually means that user forgot to separate the body and newlines         # so ""unread"" this line here, what means to treat it like a body         <MASK>             fp.seek(fp.tell() - len(line))             break         lines.append(line)     return lines",if not _RE_HEADER . match ( line ) :,if not is_header ( line ) :,26.432408210372945,26.432408210372945,0.0
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp):     uris = data.get_uris()     files = []     for uri in uris:         try:             uri_tuple = GLib.filename_from_uri(uri)         except:             continue         uri, unused = uri_tuple         <MASK>             if utils.is_media_file(uri) == True:                 files.append(uri)     if len(files) == 0:         return     open_dropped_files(files)",if os . path . exists ( uri ) == True :,if not uri_tuple :,3.3264637832151163,3.3264637832151163,0.0
"def remove_importlib(frame, options):     if frame is None:         return None     for child in frame.children:         remove_importlib(child, options=options)         <MASK>             # remove this node, moving the self_time and children up to the parent             frame.self_time += child.self_time             frame.add_children(child.children, after=child)             child.remove_from_parent()     return frame","if ""<frozen importlib._bootstrap"" in child . file_path :",if child . self_time is not None :,5.791428261249877,5.791428261249877,0.0
"def __call__(self, graph):     for layer_name, data in self.params:         <MASK>             node = graph.get_node(layer_name)             node.data = self.adjust_parameters(node, data)         else:             print_stderr(""Ignoring parameters for non-existent layer: %s"" % layer_name)     return graph",if layer_name in graph :,if layer_name in graph . layers :,61.04735835807847,61.04735835807847,0.0
"def test_with_three_points(self):     cba = ia.Polygon([(1, 2), (3, 4), (5, 5)])     for i, xy in enumerate(cba):         assert i in [0, 1, 2]         if i == 0:             assert np.allclose(xy, (1, 2))         <MASK>             assert np.allclose(xy, (3, 4))         elif i == 2:             assert np.allclose(xy, (5, 5))     assert i == 2",elif i == 1 :,if i == 1 :,75.98356856515926,75.98356856515926,0.0
"def _serve(self):     self._conn = self.manager.request(REQUEST_DNS_LISTENER, self.domain)     conn = MsgPackMessages(self._conn)     while self.active:         request = conn.recv()         if not request:             logger.warning(""DNS: Recieved empty request. Shutdown"")             self.stop()             break         now = time.time()         response = self.handler.process(request)         if not response:             response = []         used = time.time() - now         <MASK>             logger.warning(""DNS: Slow processing speed (%s)s"", used)         conn.send(response)",if used > 1 :,if used > self . speed :,26.269098944241588,26.269098944241588,0.0
"def read(cls, fp, **kwargs):     major_version, minor_version, count = read_fmt(""2HI"", fp)     items = []     for _ in range(count):         length = read_fmt(""I"", fp)[0] - 4         <MASK>             with io.BytesIO(fp.read(length)) as f:                 items.append(Annotation.read(f))     return cls(major_version=major_version, minor_version=minor_version, items=items)",if length > 0 :,if length > 0 :,100.00000000000004,100.00000000000004,1.0
"def save_uploaded_files():     files = []     unzip = bool(request.form.get(""unzip"") in [""true"", ""on""])     for uploaded_file in request.files.getlist(""files""):         <MASK>             with zipfile.ZipFile(uploaded_file, ""r"") as zf:                 for info in zf.infolist():                     name = info.filename                     size = info.file_size                     data = zf.read(name)                     if size > 0:                         files.append(save_file(data, filename=name.split(""/"")[-1]))         else:             files.append(save_file(uploaded_file))     return files",if unzip and zipfile . is_zipfile ( uploaded_file ) :,if unzip :,1.6102756877232731,0.0,0.0
"def analyze_string_content(self, string, line_num, filename):     output = {}     if self.keyword_exclude and self.keyword_exclude.search(string):         return output     for identifier in self.secret_generator(         string,         filetype=determine_file_type(filename),     ):         <MASK>             continue         secret = PotentialSecret(             self.secret_type,             filename,             identifier,             line_num,         )         output[secret] = secret     return output",if self . is_secret_false_positive ( identifier ) :,if identifier is None :,2.497149970415641,2.497149970415641,0.0
"def _validate_and_set_default_hyperparameters(self):     """"""Placeholder docstring""""""     # Check if all the required hyperparameters are set. If there is a default value     # for one, set it.     for name, definition in self.hyperparameter_definitions.items():         if name not in self.hyperparam_dict:             spec = definition[""spec""]             if ""DefaultValue"" in spec:                 self.hyperparam_dict[name] = spec[""DefaultValue""]             <MASK>                 raise ValueError(""Required hyperparameter: %s is not set"" % name)","elif ""IsRequired"" in spec and spec [ ""IsRequired"" ] :","if ""spec"" in spec :",10.694820729788422,10.694820729788422,0.0
"def get_code(self, fullname=None):     fullname = self._fix_name(fullname)     if self.code is None:         mod_type = self.etc[2]         if mod_type == imp.PY_SOURCE:             source = self.get_source(fullname)             self.code = compile(source, self.filename, ""exec"")         elif mod_type == imp.PY_COMPILED:             self._reopen()             try:                 self.code = read_code(self.file)             finally:                 self.file.close()         <MASK>             self.code = self._get_delegate().get_code()     return self.code",elif mod_type == imp . PKG_DIRECTORY :,if mod_type == imp . PY_DELEGATE :,54.91004867761124,54.91004867761124,0.0
"def eigh_abstract_eval(operand, lower):     if isinstance(operand, ShapedArray):         <MASK>             raise ValueError(                 ""Argument to symmetric eigendecomposition must have shape [..., n, n],""                 ""got shape {}"".format(operand.shape)             )         batch_dims = operand.shape[:-2]         n = operand.shape[-1]         v = ShapedArray(batch_dims + (n, n), operand.dtype)         w = ShapedArray(batch_dims + (n,), lax.lax._complex_basetype(operand.dtype))     else:         v, w = operand, operand     return v, w",if operand . ndim < 2 or operand . shape [ - 2 ] != operand . shape [ - 1 ] :,if lower :,0.031613182084712046,0.0,0.0
"def conninfo_parse(dsn):     ret = {}     length = len(dsn)     i = 0     while i < length:         if dsn[i].isspace():             i += 1             continue         param_match = PARAMETER_RE.match(dsn[i:])         <MASK>             return         param = param_match.group(1)         i += param_match.end()         if i >= length:             return         value, end = read_param_value(dsn[i:])         if value is None:             return         i += end         ret[param] = value     return ret",if not param_match :,if param_match is None :,27.77619034011791,27.77619034011791,0.0
"def load_weights_from_unsupervised(self, unsupervised_model):     update_state_dict = copy.deepcopy(self.network.state_dict())     for param, weights in unsupervised_model.network.state_dict().items():         if param.startswith(""encoder""):             # Convert encoder's layers name to match             new_param = ""tabnet."" + param         else:             new_param = param         <MASK>             # update only common layers             update_state_dict[new_param] = weights     self.network.load_state_dict(update_state_dict)",if self . network . state_dict ( ) . get ( new_param ) is not None :,"if weights . shape == ""layer"" :",1.8382465348186563,1.8382465348186563,0.0
"def viewer_setup(self):     for key, value in DEFAULT_CAMERA_CONFIG.items():         <MASK>             getattr(self.viewer.cam, key)[:] = value         else:             setattr(self.viewer.cam, key, value)","if isinstance ( value , np . ndarray ) :",if value is not None :,5.484411595600381,5.484411595600381,0.0
"def colormap_changed(change):     if change[""new""]:         cmap_colors = [             color[1:] for color in cmap.step.__dict__[""_schemes""][colormap.value]         ]         palette.value = "", "".join(cmap_colors)         colorbar = getattr(cmap.step, colormap.value)         colorbar_output = self.colorbar_widget         with colorbar_output:             colorbar_output.clear_output()             display(colorbar)         <MASK>             labels = [f""Class {i+1}"" for i in range(len(palette.value.split("","")))]             legend_labels.value = "", "".join(labels)","if len ( palette . value ) > 0 and "","" in palette . value :","if change [ ""old"" ] :",2.2375594425769316,2.2375594425769316,0.0
"def invalidate(self, layers=None):     if layers is None:         layers = Layer.AllLayers     if layers:         layers = set(layers)         self.invalidLayers.update(layers)         blockRenderers = [             br             for br in self.blockRenderers             if br.layer is Layer.Blocks or br.layer not in layers         ]         <MASK>             self.forgetDisplayLists()         self.blockRenderers = blockRenderers         if self.renderer.showRedraw and Layer.Blocks in layers:             self.needsRedisplay = True",if len ( blockRenderers ) < len ( self . blockRenderers ) :,if self . renderer . showRedraw :,6.628576403773604,6.628576403773604,0.0
"def fromstring(cls, input):     productions = []     for linenum, line in enumerate(input.split(""\n"")):         line = line.strip()         <MASK>             continue         try:             productions += _read_dependency_production(line)         except ValueError:             raise ValueError(""Unable to parse line %s: %s"" % (linenum, line))     if len(productions) == 0:         raise ValueError(""No productions found!"")     return DependencyGrammar(productions)","if line . startswith ( ""#"" ) or line == """" :",if not line :,1.0466441829132096,1.0466441829132096,0.0
"def repl(m, base_path, rel_path=None):     if m.group(""comments""):         tag = m.group(""comments"")     else:         tag = m.group(""open"")         <MASK>             tag += RE_TAG_LINK_ATTR.sub(                 lambda m2: repl_absolute(m2, base_path), m.group(""attr"")             )         else:             tag += RE_TAG_LINK_ATTR.sub(                 lambda m2: repl_relative(m2, base_path, rel_path), m.group(""attr"")             )         tag += m.group(""close"")     return tag",if rel_path is None :,if rel_path is None :,100.00000000000004,100.00000000000004,1.0
"def encode(path):     if isinstance(path, str_cls):         try:             path = path.encode(fs_encoding, ""strict"")         except UnicodeEncodeError:             <MASK>                 raise             path = path.encode(fs_fallback_encoding, ""strict"")     return path",if not platform . is_linux ( ) :,if path . endswith ( fs_fallback_encoding ) :,8.91376552139813,8.91376552139813,0.0
"def __iter__(self):     base_iterator = super(ProcessIterable, self).__iter__()     if getattr(self.queryset, ""_coerced"", False):         for process in base_iterator:             <MASK>                 process = coerce_to_related_instance(                     process, process.flow_class.process_class                 )             yield process     else:         for process in base_iterator:             yield process","if isinstance ( process , self . queryset . model ) :",if process . flow_class . process_class_id is not None :,3.6570159134143823,3.6570159134143823,0.0
"def footnotes_under(n: Element) -> Iterator[nodes.footnote]:     if isinstance(n, nodes.footnote):         yield n     else:         for c in n.children:             <MASK>                 continue             elif isinstance(c, nodes.Element):                 yield from footnotes_under(c)","if isinstance ( c , addnodes . start_of_file ) :","if c . tag == ""footnote"" :",3.9778149665594618,3.9778149665594618,0.0
"def _process_submissions(self) -> None:     """"""Process all submissions which have not been processed yet.""""""     while self._to_be_processed:         job = self._to_be_processed[0]         job.process()  # trigger computation         <MASK>             heapq.heappush(                 self._steady_priority_queue,                 OrderedJobs(job.release_time, self._order, job),             )         self._to_be_processed.popleft()  # remove right after it is added to the heap queue         self._order += 1",if not self . batch_mode :,if job . release_time :,8.051153633013374,8.051153633013374,0.0
"def valid_localparts(strip_delimiters=False):     for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""):         # strip line, skip over empty lines         line = line.strip()         if line == """":             continue         # skip over comments or empty lines         match = COMMENT.match(line)         <MASK>             continue         # skip over localparts with delimiters         if strip_delimiters:             if "","" in line or "";"" in line:                 continue         yield line",if match :,if not match :,35.35533905932737,35.35533905932737,0.0
"def _get_payload_hash(self, method, data=None):     if method in (""POST"", ""PUT""):         if data:             <MASK>                 # File upload; don't try to read the entire payload                 return UNSIGNED_PAYLOAD             return _hash(data)         else:             return UNSIGNED_PAYLOAD     else:         return _hash("""")","if hasattr ( data , ""next"" ) or hasattr ( data , ""__next__"" ) :","if data . endswith ( "".json"" ) :",5.7918514721882834,5.7918514721882834,0.0
"def get_download_info(self):     try:         download_info = self.api.get_download_info(self.game)         result = True     except NoDownloadLinkFound as e:         print(e)         <MASK>             Config.unset(""current_download"")         GLib.idle_add(             self.parent.parent.show_error,             _(""Download error""),             _(                 ""There was an error when trying to fetch the download link!\n{}"".format(                     e                 )             ),         )         download_info = False         result = False     return result, download_info","if Config . get ( ""current_download"" ) == self . game . id :",if e . status == 404 :,3.5114603692955577,3.5114603692955577,0.0
"def find_id(self, doc_id):     self._lock.acquire()     try:         doc = self._docs.get(doc_id)         <MASK>             doc = copy.deepcopy(doc)             doc[""id""] = doc_id             return doc     finally:         self._lock.release()",if doc :,if doc is not None :,17.965205598154213,17.965205598154213,0.0
"def assign_art(self, session, task):     """"""Place the discovered art in the filesystem.""""""     if task in self.art_candidates:         candidate = self.art_candidates.pop(task)         self._set_art(task.album, candidate, not self.src_removed)         <MASK>             task.prune(candidate.path)",if self . src_removed :,if candidate . path :,9.423716574733431,9.423716574733431,0.0
"def _replace_named(self, named, replace_scalar):     for item in named:         for name, value in self._get_replaced_named(item, replace_scalar):             <MASK>                 raise DataError(""Argument names must be strings."")             yield name, value",if not is_string ( name ) :,"if not isinstance ( name , ( str , unicode ) ) :",10.700801516876487,10.700801516876487,0.0
"def qtTypeIdent(conn, *args):     # We're not using the conn object at the moment, but - we will     # modify the     # logic to use the server version specific keywords later.     res = None     value = None     for val in args:         # DataType doesn't have len function then convert it to string         if not hasattr(val, ""__len__""):             val = str(val)         if len(val) == 0:             continue         value = val         <MASK>             value = value.replace('""', '""""')             value = '""' + value + '""'         res = ((res and res + ""."") or """") + value     return res","if Driver . needsQuoting ( val , True ) :","if val . startswith ( ""'"" ) :",11.044795567078939,11.044795567078939,0.0
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops):     for n in tileable_graph:         <MASK>             continue         tiled_n = get_tiled(n)         if has_unknown_shape(tiled_n):             if any(c.key not in chunk_result for c in tiled_n.chunks):                 # some of the chunks has been fused                 continue             new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result)             for node in (n, tiled_n):                 node._update_shape(tuple(sum(nsplit) for nsplit in new_nsplits))             tiled_n._nsplits = new_nsplits",if n . op in failed_ops :,if n . key not in failed_ops :,52.53819788848316,52.53819788848316,0.0
"def _read_filter(self, data):     if data:         <MASK>             self.inner_sha.update(data)         if self.expected_inner_md5sum:             self.inner_md5.update(data)     return data",if self . expected_inner_sha256 :,if self . expected_inner_shasum :,75.06238537503395,75.06238537503395,0.0
"def find_previous_editable(self, *args):     if self.editw == 0:         if self._active_page > 0:             self.switch_page(self._active_page - 1)     if not self.editw == 0:         # remember that xrange does not return the 'last' value,         # so go to -1, not 0! (fence post error in reverse)         for n in range(self.editw - 1, -1, -1):             <MASK>                 self.editw = n                 break",if self . _widgets__ [ n ] . editable and not self . _widgets__ [ n ] . hidden :,if n > self . editw :,1.094210492097772,1.094210492097772,0.0
"def _get_event_for_message(self, message_id):     with self.event_lock:         <MASK>             raise RuntimeError(                 ""Event for message[{}] should have been created before accessing"".format(                     message_id                 )             )         return self._events[message_id]",if message_id not in self . _events :,if not self . _events [ message_id ] :,33.521134190816646,33.521134190816646,0.0
"def _get_deepest(self, t):     if isinstance(t, list):         if len(t) == 1:             return t[0]         else:             for part in t:                 res = self._get_deepest(part)                 <MASK>                     return res             return None     return None",if res :,if res :,100.00000000000004,0.0,1.0
"def _get_notify(self, action_node):     if action_node.name not in self._skip_notify_tasks:         <MASK>             task_notify = NotificationsHelper.to_model(action_node.notify)             return task_notify         elif self._chain_notify:             return self._chain_notify     return None",if action_node . notify :,if action_node . notify :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, centered=None, shape_params=()):     assert centered is None or isinstance(centered, (float, torch.Tensor))     assert isinstance(shape_params, (tuple, list))     assert all(isinstance(name, str) for name in shape_params)     if is_validation_enabled():         if isinstance(centered, float):             assert 0 <= centered and centered <= 1         <MASK>             assert (0 <= centered).all()             assert (centered <= 1).all()         else:             assert centered is None     self.centered = centered     self.shape_params = shape_params","elif isinstance ( centered , torch . Tensor ) :","if isinstance ( centered , torch . Tensor ) :",88.01117367933934,88.01117367933934,0.0
"def collect(self):     for nickname in self.squid_hosts.keys():         squid_host = self.squid_hosts[nickname]         fulldata = self._getData(squid_host[""host""], squid_host[""port""])         if fulldata is not None:             fulldata = fulldata.splitlines()             for data in fulldata:                 matches = self.stat_pattern.match(data)                 <MASK>                     self.publish_counter(                         ""%s.%s"" % (nickname, matches.group(1)), float(matches.group(2))                     )",if matches :,if matches :,100.00000000000004,0.0,1.0
"def test_len(self):     eq = self.assertEqual     eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol="""")))     for size in range(15):         if size == 0:             bsize = 0         elif size <= 3:             bsize = 4         elif size <= 6:             bsize = 8         <MASK>             bsize = 12         elif size <= 12:             bsize = 16         else:             bsize = 20         eq(base64MIME.base64_len(""x"" * size), bsize)",elif size <= 9 :,if size <= 8 :,32.46679154750991,32.46679154750991,0.0
"def wait_for_initial_conf(self, timeout=1.0):     logger.info(""Waiting for initial configuration"")     cur_timeout = timeout     # Arbiter do not already set our have_conf param     while not self.new_conf and not self.interrupted:         elapsed, _, _ = self.handleRequests(cur_timeout)         if elapsed:             cur_timeout -= elapsed             <MASK>                 continue             cur_timeout = timeout         sys.stdout.write(""."")         sys.stdout.flush()",if cur_timeout > 0 :,if cur_timeout <= 0 :,41.11336169005198,41.11336169005198,0.0
"def __init__(self, querylist=None):     self.query_id = -1     if querylist is None:         self.querylist = []     else:         self.querylist = querylist         for query in self.querylist:             if self.query_id == -1:                 self.query_id = query.query_id             else:                 <MASK>                     raise ValueError(""query in list must be same query_id"")",if self . query_id != query . query_id :,if query . query_id != self . query_id :,79.71755824799465,79.71755824799465,0.0
"def candidates() -> Generator[""Symbol"", None, None]:     s = self     if Symbol.debug_lookup:         Symbol.debug_print(""searching in self:"")         print(s.to_string(Symbol.debug_indent + 1), end="""")     while True:         if matchSelf:             yield s         <MASK>             yield from s.children_recurse_anon         else:             yield from s._children         if s.siblingAbove is None:             break         s = s.siblingAbove         if Symbol.debug_lookup:             Symbol.debug_print(""searching in sibling:"")             print(s.to_string(Symbol.debug_indent + 1), end="""")",if recurseInAnon :,if s . children_recurse_anon is not None :,4.02724819242185,4.02724819242185,0.0
"def get_default_params(problem_type: str, penalty: str):     # TODO: get seed from seeds provider     if problem_type == REGRESSION:         default_params = {""C"": None, ""random_state"": 0, ""fit_intercept"": True}         <MASK>             default_params[""solver""] = ""auto""     else:         default_params = {             ""C"": None,             ""random_state"": 0,             ""solver"": _get_solver(problem_type),             ""n_jobs"": -1,             ""fit_intercept"": True,         }     model_params = list(default_params.keys())     return model_params, default_params",if penalty == L2 :,"if penalty == ""auto"" :",36.55552228545123,36.55552228545123,0.0
"def _UploadDirectory(local_dir: str, gcs_bucket: storage.Bucket, gcs_dir: str):     """"""Upload the contents of a local directory to a GCS Bucket.""""""     for file_name in os.listdir(local_dir):         path = os.path.join(local_dir, file_name)         <MASK>             logging.info(""Skipping %s as it's not a file."", path)             continue         logging.info(""Uploading: %s"", path)         gcs_blob = gcs_bucket.blob(f""{gcs_dir}/{file_name}"")         gcs_blob.upload_from_filename(path)",if not os . path . isfile ( path ) :,if not os . path . isfile ( path ) :,100.00000000000004,100.00000000000004,1.0
"def decode_query_ids(self, trans, conditional):     if conditional.operator == ""and"":         self.decode_query_ids(trans, conditional.left)         self.decode_query_ids(trans, conditional.right)     else:         left_base = conditional.left.split(""."")[0]         if left_base in self.FIELDS:             field = self.FIELDS[left_base]             <MASK>                 conditional.right = trans.security.decode_id(conditional.right)",if field . id_decode :,if field . id == trans . security . decode_id ( conditional . left ) :,14.576846149722611,14.576846149722611,0.0
"def data_dir(self) -> Path:     try:         from appdirs import user_data_dir     except ImportError:         # linux         path = Path.home() / "".local"" / ""share""         <MASK>             return path / ""dephell""         # mac os         path = Path.home() / ""Library"" / ""Application Support""         if path.exists():             return path / ""dephell""         self.pip_main([""install"", ""appdirs""])         from appdirs import user_data_dir     return Path(user_data_dir(""dephell""))",if path . exists ( ) :,if path . exists ( ) :,100.00000000000004,100.00000000000004,1.0
"def setGameCard(self, isGameCard=False):     if isGameCard:         targetValue = 1     else:         targetValue = 0     for nca in self:         if isinstance(nca, Nca):             <MASK>                 continue             Print.info(""writing isGameCard for %s, %d"" % (str(nca._path), targetValue))             nca.header.setIsGameCard(targetValue)",if nca . header . getIsGameCard ( ) == targetValue :,if nca . header . isIsGameCard ( targetValue ) :,37.2517695219694,37.2517695219694,0.0
"def check_apns_certificate(ss):     mode = ""start""     for s in ss.split(""\n""):         if mode == ""start"":             if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s:                 mode = ""key""         <MASK>             if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s:                 mode = ""end""                 break             elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s:                 raise ImproperlyConfigured(                     ""Encrypted APNS private keys are not supported""                 )     if mode != ""end"":         raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")","elif mode == ""key"" :","if s . startswith ( ""Proc-Type"" ) :",5.522397783539471,5.522397783539471,0.0
"def register_aggregate_groups(conn, *groups):     seen = set()     for group in groups:         klasses = AGGREGATE_COLLECTION[group]         for klass in klasses:             name = getattr(klass, ""name"", klass.__name__)             <MASK>                 seen.add(name)                 conn.create_aggregate(name, -1, klass)",if name not in seen :,if name not in seen :,100.00000000000004,100.00000000000004,1.0
"def _impl(inputs, input_types):     data = inputs[0]     axis = None     keepdims = False     if len(inputs) > 2:  # default, torch have only data, axis=None, keepdims=False         if isinstance(inputs[1], int):             axis = int(inputs[1])         <MASK>             axis = inputs[1]         else:             axis = list(_infer_shape(inputs[1]))         keepdims = bool(inputs[2])     return get_relay_op(name)(data, axis=axis, keepdims=keepdims)",elif _is_int_seq ( inputs [ 1 ] ) :,"if isinstance ( inputs [ 2 ] , list ) :",13.977689291213357,13.977689291213357,0.0
"def walks_generator():     if filelist is not None:         bucket = []         for filename in filelist:             with io.open(filename) as inf:                 for line in inf:                     walk = [int(x) for x in line.strip(""\n"").split("" "")]                     bucket.append(walk)                     if len(bucket) == batch_size:                         yield bucket                         bucket = []         <MASK>             yield bucket     else:         for _ in range(epoch):             for nodes in graph.node_batch_iter(batch_size):                 walks = graph.random_walk(nodes, walk_len)                 yield walks",if len ( bucket ) :,if len ( bucket ) == walk_len :,36.72056269893591,36.72056269893591,0.0
"def _calculate_runtimes(states):     results = {""runtime"": 0.00, ""num_failed_states"": 0, ""num_passed_states"": 0}     for state, resultset in states.items():         <MASK>             # Count the pass vs failures             if resultset[""result""]:                 results[""num_passed_states""] += 1             else:                 results[""num_failed_states""] += 1             # Count durations             results[""runtime""] += resultset[""duration""]     log.debug(""Parsed state metrics: {}"".format(results))     return results","if isinstance ( resultset , dict ) and ""duration"" in resultset :","if state == ""pass"" :",3.689111847432509,3.689111847432509,0.0
"def _replicator_primary_device() -> snt_replicator.Replicator:     # NOTE: The explicit device list is required since currently Replicator     # only considers CPU and GPU devices. This means on TPU by default we only     # mirror on the local CPU.     for device_type in (""TPU"", ""GPU"", ""CPU""):         devices = tf.config.experimental.list_logical_devices(device_type=device_type)         <MASK>             devices = [d.name for d in devices]             logging.info(""Replicating over %s"", devices)             return snt_replicator.Replicator(devices=devices)     assert False, ""No TPU/GPU or CPU found""",if devices :,if devices :,100.00000000000004,0.0,1.0
"def get_tag_values(self, event):     http = event.interfaces.get(""sentry.interfaces.Http"")     if not http:         return []     if not http.headers:         return []     headers = http.headers     # XXX: transitional support for workers     if isinstance(headers, dict):         headers = headers.items()     output = []     for key, value in headers:         <MASK>             continue         ua = Parse(value)         if not ua:             continue         result = self.get_tag_from_ua(ua)         if result:             output.append(result)     return output","if key != ""User-Agent"" :",if not value :,6.988198185490689,6.988198185490689,0.0
"def general(metadata, value):     if metadata.get(""commands"") and value:         <MASK>             v = quote(value)         else:             v = value         return u""{0} {1}"".format(metadata[""commands""][0], v)     else:         if not value:             return None         elif not metadata.get(""nargs""):             return quote(value)         else:             return value","if not metadata . get ( ""nargs"" ) :","if metadata . get ( ""nargs"" ) :",81.76129038784515,81.76129038784515,0.0
"def _actions_read(self, c):     self.action_input.handle_read(c)     if c in [curses.KEY_ENTER, util.KEY_ENTER2]:         # take action         if self.action_input.selected_index == 0:  # Cancel             self.back_to_parent()         elif self.action_input.selected_index == 1:  # Apply             self._apply_prefs()             client.core.get_config().addCallback(self._update_preferences)         <MASK>  # OK             self._apply_prefs()             self.back_to_parent()",elif self . action_input . selected_index == 2 :,"if c in [ util . KEY_ENTER2 , util . KEY_ENTER2 ] :",3.4197980307804725,3.4197980307804725,0.0
def logic():     if reset == 1:         lfsr.next = 1     else:         <MASK>             # lfsr.next[24:1] = lfsr[23:0]             lfsr.next = lfsr << 1             lfsr.next[0] = lfsr[23] ^ lfsr[22] ^ lfsr[21] ^ lfsr[16],if enable :,if reset == 2 :,9.652434877402245,9.652434877402245,0.0
"def action_delete(self, request, attachments):     deleted_attachments = []     desynced_posts = []     for attachment in attachments:         <MASK>             deleted_attachments.append(attachment.pk)             desynced_posts.append(attachment.post_id)     if desynced_posts:         with transaction.atomic():             for post in Post.objects.filter(id__in=desynced_posts):                 self.delete_from_cache(post, deleted_attachments)     for attachment in attachments:         attachment.delete()     message = _(""Selected attachments have been deleted."")     messages.success(request, message)",if attachment . post :,if attachment . post_id :,43.47208719449914,43.47208719449914,0.0
"def __getitem__(self, index):     if self._check():         if isinstance(index, int):             if index < 0 or index >= len(self.features):                 raise IndexError(index)             if self.features[index] is None:                 feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index)                 if feature:                     (feature,) = _unpack(""!H"", feature[:2])                     self.features[index] = FEATURE[feature]             return self.features[index]         <MASK>             indices = index.indices(len(self.features))             return [self.__getitem__(i) for i in range(*indices)]","elif isinstance ( index , slice ) :",if index . isdigit ( ) :,13.540372457315735,13.540372457315735,0.0
"def _skip_start(self):     start, stop = self.start, self.stop     for chunk in self.app_iter:         self._pos += len(chunk)         <MASK>             continue         elif self._pos == start:             return b""""         else:             chunk = chunk[start - self._pos :]             if stop is not None and self._pos > stop:                 chunk = chunk[: stop - self._pos]                 assert len(chunk) == stop - start             return chunk     else:         raise StopIteration()",if self . _pos < start :,if start is None :,7.715486568024961,7.715486568024961,0.0
"def get_files(d):     f = []     for root, dirs, files in os.walk(d):         for name in files:             <MASK>                 continue             if ""qemux86copy-"" in root or ""qemux86-"" in root:                 continue             if ""do_build"" not in name and ""do_populate_sdk"" not in name:                 f.append(os.path.join(root, name))     return f","if ""meta-environment"" in root or ""cross-canadian"" in root :","if ""qemux86"" in root :",20.74856543476741,20.74856543476741,0.0
"def _load_windows_store_certs(self, storename, purpose):     certs = bytearray()     try:         for cert, encoding, trust in enum_certificates(storename):             # CA certs are never PKCS#7 encoded             <MASK>                 if trust is True or purpose.oid in trust:                     certs.extend(cert)     except PermissionError:         warnings.warn(""unable to enumerate Windows certificate store"")     if certs:         self.load_verify_locations(cadata=certs)     return certs","if encoding == ""x509_asn"" :","if encoding == ""PKCS#7"" :",45.180100180492246,45.180100180492246,0.0
"def test_tokenizer_identifier_with_correct_config(self):     for tokenizer_class in [BertTokenizer, BertTokenizerFast, AutoTokenizer]:         tokenizer = tokenizer_class.from_pretrained(""wietsedv/bert-base-dutch-cased"")         self.assertIsInstance(tokenizer, (BertTokenizer, BertTokenizerFast))         <MASK>             self.assertEqual(tokenizer.basic_tokenizer.do_lower_case, False)         else:             self.assertEqual(tokenizer.do_lower_case, False)         self.assertEqual(tokenizer.model_max_length, 512)","if isinstance ( tokenizer , BertTokenizer ) :",if tokenizer . basic_tokenizer :,7.492442692259767,7.492442692259767,0.0
"def run(self):     global WAITING_BEFORE_START     time.sleep(WAITING_BEFORE_START)     while self.keep_alive:         path_id, module, resolve = self.queue_receive.get()         if path_id is None:             continue         self.lock.acquire()         self.modules[path_id] = module         self.lock.release()         <MASK>             resolution = self._resolve_with_other_modules(resolve)             self._relations[path_id] = []             for package in resolution:                 self._relations[path_id].append(resolution[package])             self.queue_send.put((path_id, module, False, resolution))",if resolve :,if resolve is not None :,17.965205598154213,17.965205598154213,0.0
"def __new__(mcs, name, bases, attrs):     include_profile = include_trace = include_garbage = True     bases = list(bases)     if name == ""SaltLoggingClass"":         for base in bases:             if hasattr(base, ""trace""):                 include_trace = False             <MASK>                 include_garbage = False     if include_profile:         bases.append(LoggingProfileMixin)     if include_trace:         bases.append(LoggingTraceMixin)     if include_garbage:         bases.append(LoggingGarbageMixin)     return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)","if hasattr ( base , ""garbage"" ) :","if hasattr ( base , ""garbage"" ) :",100.00000000000004,100.00000000000004,1.0
"def __str__(self, prefix="""", printElemNumber=0):     res = """"     if self.has_owner_:         res += prefix + (""owner: %s\n"" % self.DebugFormatString(self.owner_))     cnt = 0     for e in self.entries_:         elm = """"         <MASK>             elm = ""(%d)"" % cnt         res += prefix + (""entries%s <\n"" % elm)         res += e.__str__(prefix + ""  "", printElemNumber)         res += prefix + "">\n""         cnt += 1     return res",if printElemNumber :,if e . has_owner_ :,6.567274736060395,6.567274736060395,0.0
"def parse_tag(self):     buf = []     escaped = False     for c in self.get_next_chars():         if escaped:             buf.append(c)         elif c == ""\\"":             escaped = True         <MASK>             return """".join(buf)         else:             buf.append(c)     raise Exception(""Unclosed tag "" + """".join(buf))","elif c == "">"" :","if c == ""\\"" :",35.49481056010054,35.49481056010054,0.0
"def get_batches(train_nodes, train_labels, batch_size=64, shuffle=True):     if shuffle:         random.shuffle(train_nodes)     total = train_nodes.shape[0]     for i in range(0, total, batch_size):         <MASK>             cur_nodes = train_nodes[i : i + batch_size]             cur_labels = train_labels[cur_nodes]             yield cur_nodes, cur_labels",if i + batch_size <= total :,if i + batch_size < total :,71.89393375176813,71.89393375176813,0.0
"def _get_all_info_lines(data):     infos = []     for row in data:         splitrow = row.split()         if len(splitrow) > 0:             <MASK>                 infos.append("" "".join(splitrow[1:]))     return infos","if splitrow [ 0 ] == ""INFO:"" :","if splitrow [ 0 ] == ""info"" :",67.74702029865007,67.74702029865007,0.0
"def _validate_client_public_key(self, username, key_data):     """"""Validate a client public key for the specified user""""""     try:         key = decode_ssh_public_key(key_data)     except KeyImportError:         return None     options = None     if self._client_keys:         options = self._client_keys.validate(key, self._peer_addr)     if options is None:         result = self._owner.validate_public_key(username, key)         if asyncio.iscoroutine(result):             result = yield from result         <MASK>             return None         options = {}     self._key_options = options     return key",if not result :,if not result :,100.00000000000004,100.00000000000004,1.0
"def attach_related_versions(addons, addon_dict=None):     if addon_dict is None:         addon_dict = {addon.id: addon for addon in addons}     all_ids = set(filter(None, (addon._current_version_id for addon in addons)))     versions = list(Version.objects.filter(id__in=all_ids).order_by())     for version in versions:         try:             addon = addon_dict[version.addon_id]         except KeyError:             log.info(""Version %s has an invalid add-on id."" % version.id)             continue         <MASK>             addon._current_version = version         version.addon = addon",if addon . _current_version_id == version . id :,if addon . id == version . id :,47.65082587109519,47.65082587109519,0.0
"def move_view(obj, evt):     position = obj.GetCurrentCursorPosition()     for other_axis, axis_number in self._axis_names.iteritems():         <MASK>             continue         ipw3d = getattr(self, ""ipw_3d_%s"" % other_axis)         ipw3d.ipw.slice_position = position[axis_number]",if other_axis == axis_name :,if other_axis not in position :,28.46946938149361,28.46946938149361,0.0
"def func_wrapper(*args, **kwargs):     warnings.simplefilter(""always"", DeprecationWarning)  # turn off filter     for old, new in arg_mapping.items():         <MASK>             warnings.warn(                 f""Keyword argument '{old}' has been ""                 f""deprecated in favour of '{new}'. ""                 f""'{old}' will be removed in a future version."",                 category=DeprecationWarning,                 stacklevel=2,             )             val = kwargs.pop(old)             kwargs[new] = val     # reset filter     warnings.simplefilter(""default"", DeprecationWarning)     return func(*args, **kwargs)",if old in kwargs :,if old in kwargs :,100.00000000000004,100.00000000000004,1.0
"def inner_connection_checker(self, *args, **kwargs):     LOG.debug(""in _connection_checker"")     for attempts in range(5):         try:             return func(self, *args, **kwargs)         except exception.VolumeBackendAPIException as e:             pattern = re.compile(r"".*Session id expired$"")             matches = pattern.match(six.text_type(e))             if matches:                 <MASK>                     LOG.debug(""Session might have expired."" "" Trying to relogin"")                     self._login()                     continue             LOG.error(""Re-throwing Exception %s"", e)             raise",if attempts < 4 :,if not matches :,14.794015674776452,14.794015674776452,0.0
"def set(self, pcount):     """"""Set channel prefetch_count setting.""""""     if pcount != self.prev:         new_value = pcount         <MASK>             logger.warning(                 ""QoS: Disabled: prefetch_count exceeds %r"", PREFETCH_COUNT_MAX             )             new_value = 0         logger.debug(""basic.qos: prefetch_count->%s"", new_value)         self.callback(prefetch_count=new_value)         self.prev = pcount     return pcount",if pcount > PREFETCH_COUNT_MAX :,if new_value > PREFETCH_COUNT_MAX :,59.00468726392806,59.00468726392806,0.0
"def _build_gcs_object_key(self, key):     if self.platform_specific_separator:         <MASK>             gcs_object_key = os.path.join(                 self.prefix, self._convert_key_to_filepath(key)             )         else:             gcs_object_key = self._convert_key_to_filepath(key)     else:         if self.prefix:             gcs_object_key = ""/"".join((self.prefix, self._convert_key_to_filepath(key)))         else:             gcs_object_key = self._convert_key_to_filepath(key)     return gcs_object_key",if self . prefix :,if self . prefix :,100.00000000000004,100.00000000000004,1.0
"def number_operators(self, a, b, skip=[]):     dict = {""a"": a, ""b"": b}     for name, expr in self.binops.items():         <MASK>             name = ""__%s__"" % name             if hasattr(a, name):                 res = eval(expr, dict)                 self.binop_test(a, b, res, expr, name)     for name, expr in self.unops.items():         if name not in skip:             name = ""__%s__"" % name             if hasattr(a, name):                 res = eval(expr, dict)                 self.unop_test(a, res, expr, name)",if name not in skip :,if name not in skip :,100.00000000000004,100.00000000000004,1.0
def isCurveMonotonic(set_):     for i in range(len(set_) - 1):         # ==== added by zli =======         <MASK>             return False         # ==== added by zli =======         # ==== added by zli =======         # if set_[i][1] > set_[i + 1][1]:         if set_[i][1] >= set_[i + 1][1]:             # ==== added by zli =======             return False     return True,if set_ [ i ] [ 0 ] >= set_ [ i + 1 ] [ 0 ] :,if set_ [ i ] < set_ [ i ] :,31.1970851193133,31.1970851193133,0.0
"def show_topics():     """"""prints all available miscellaneous help topics.""""""     print(_stash.text_color(""Miscellaneous Topics:"", ""yellow""))     for pp in PAGEPATHS:         <MASK>             continue         content = os.listdir(pp)         for pn in content:             if ""."" in pn:                 name = pn[: pn.index(""."")]             else:                 name = pn             print(name)",if not os . path . isdir ( pp ) :,"if pp == """" :",4.880869806051147,4.880869806051147,0.0
"def test_send_error(self):     allow_transfer_encoding_codes = (205, 304)     for code in (101, 102, 204, 205, 304):         self.con.request(""SEND_ERROR"", ""/{}"".format(code))         res = self.con.getresponse()         self.assertEqual(code, res.status)         self.assertEqual(None, res.getheader(""Content-Length""))         self.assertEqual(None, res.getheader(""Content-Type""))         <MASK>             self.assertEqual(None, res.getheader(""Transfer-Encoding""))         data = res.read()         self.assertEqual(b"""", data)",if code not in allow_transfer_encoding_codes :,if not code in allow_transfer_encoding_codes :,76.32661822742675,76.32661822742675,0.0
"def _length_hint(obj):     """"""Returns the length hint of an object.""""""     try:         return len(obj)     except (AttributeError, TypeError):         try:             get_hint = type(obj).__length_hint__         except AttributeError:             return None         try:             hint = get_hint(obj)         except TypeError:             return None         <MASK>             return None         return hint","if hint is NotImplemented or not isinstance ( hint , int_types ) or hint < 0 :",if hint is None :,2.5983349617896914,2.5983349617896914,0.0
"def _rmtree(self, path):     # Essentially a stripped down version of shutil.rmtree.  We can't     # use globals because they may be None'ed out at shutdown.     for name in self._listdir(path):         fullname = self._path_join(path, name)         try:             isdir = self._isdir(fullname)         except self._os_error:             isdir = False         <MASK>             self._rmtree(fullname)         else:             try:                 self._remove(fullname)             except self._os_error:                 pass     try:         self._rmdir(path)     except self._os_error:         pass",if isdir :,if isdir :,100.00000000000004,0.0,1.0
"def get_sources(self, sources=None):     """"""Returns all sources from this provider.""""""     self._load()     if sources is None:         sources = list(self.data.keys())     elif not isinstance(sources, (list, tuple)):         sources = [sources]     for source in sources:         <MASK>             raise KeyError(                 ""Invalid data key: {}. Valid keys are: {}"".format(                     source, "", "".join(str(k) for k in self.data)                 )             )     return {k: self.data[k] for k in sources}",if source not in self . data :,"if not isinstance ( source , str ) :",6.742555929751843,6.742555929751843,0.0
"def do_shorts(     opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str] ) -> Tuple[List[Tuple[str, str]], List[str]]:     while optstring != """":         opt, optstring = optstring[0], optstring[1:]         if short_has_arg(opt, shortopts):             <MASK>                 if not args:                     raise GetoptError(""option -%s requires argument"" % opt, opt)                 optstring, args = args[0], args[1:]             optarg, optstring = optstring, """"         else:             optarg = """"         opts.append((""-"" + opt, optarg))     return opts, args","if optstring == """" :",if optarg not in opts :,8.170609724417774,8.170609724417774,0.0
"def _sanitize_dict(self, config_dict, allow_val_change=None, ignore_keys: set = None):     sanitized = {}     for k, v in six.iteritems(config_dict):         <MASK>             continue         k, v = self._sanitize(k, v, allow_val_change)         sanitized[k] = v     return sanitized",if ignore_keys and k in ignore_keys :,if k in ignore_keys :,47.486944442513455,47.486944442513455,0.0
def x(data):     count = 0     while count < 10:         data.start_example(SOME_LABEL)         b = data.draw_bits(1)         <MASK>             count += 1         data.stop_example(discard=not b)     data.mark_interesting(),if b :,if b :,100.00000000000004,0.0,1.0
"def prompt_for_resume(config):     logger = logging.getLogger(""changeme"")     logger.error(         ""A previous scan was interrupted. Type R to resume or F to start a fresh scan""     )     answer = """"     while not (answer == ""R"" or answer == ""F""):         prompt = ""(R/F)> ""         answer = """"         try:             answer = raw_input(prompt)         except NameError:             answer = input(prompt)         if answer.upper() == ""F"":             logger.debug(""Forcing a fresh scan"")         <MASK>             logger.debug(""Resuming previous scan"")             config.resume = True     return config.resume","elif answer . upper ( ) == ""R"" :","if answer . upper ( ) == ""R"" :",90.36020036098445,90.36020036098445,0.0
"def _evaluate_local_single(self, iterator):     for batch in iterator:         in_arrays = convert._call_converter(self.converter, batch, self.device)         with function.no_backprop_mode():             <MASK>                 results = self.calc_local(*in_arrays)             elif isinstance(in_arrays, dict):                 results = self.calc_local(**in_arrays)             else:                 results = self.calc_local(in_arrays)         if self._progress_hook:             self._progress_hook(batch)         yield results","if isinstance ( in_arrays , tuple ) :","if isinstance ( in_arrays , list ) :",70.71067811865478,70.71067811865478,0.0
"def _send_until_done(self, data):     while True:         try:             return self.connection.send(data)         except OpenSSL.SSL.WantWriteError:             <MASK>                 raise timeout()             continue         except OpenSSL.SSL.SysCallError as e:             raise SocketError(str(e))","if not util . wait_for_write ( self . socket , self . socket . gettimeout ( ) ) :",if self . timeout :,0.6942039088478356,0.6942039088478356,0.0
"def _read_jtl_chunk(self, jtl):     data = jtl.read(1024 * 1024 * 10)     if data:         parts = data.rsplit(""\n"", 1)         <MASK>             ready_chunk = self.buffer + parts[0] + ""\n""             self.buffer = parts[1]             df = string_to_df(ready_chunk)             self.stat_queue.put(df)             return df         else:             self.buffer += parts[0]     else:         if self.jmeter_finished:             self.agg_finished = True         jtl.readline()     return None",if len ( parts ) > 1 :,if len ( parts ) == 2 :,46.713797772819994,46.713797772819994,0.0
"def __new__(mcl, classname, bases, dictionary):     slots = list(dictionary.get(""__slots__"", []))     for getter_name in [key for key in dictionary if key.startswith(""get_"")]:         name = getter_name         slots.append(""__"" + name)         getter = dictionary.pop(getter_name)         setter = dictionary.get(setter_name, None)         <MASK>             del dictionary[setter_name]         dictionary[name] = property(getter.setter)         dictionary[""__slots__""] = tuple(slots)         return super().__new__(mcl, classname, bases, dictionary)","if setter is not None and isinstance ( setter , collections . Callable ) :",if setter :,1.1538129489094668,0.0,0.0
"def tex_coords(self):     """"""Array of texture coordinate data.""""""     if ""multi_tex_coords"" not in self.domain.attribute_names:         <MASK>             domain = self.domain             attribute = domain.attribute_names[""tex_coords""]             self._tex_coords_cache = attribute.get_region(                 attribute.buffer, self.start, self.count             )             self._tex_coords_cache_version = domain._version         region = self._tex_coords_cache         region.invalidate()         return region.array     else:         return None",if self . _tex_coords_cache_version != self . domain . _version :,"if ""tex_coords"" in domain . attribute_names :",9.491068672993098,9.491068672993098,0.0
"def index(self, sub, start=0):     """"""Returns the index of the closing bracket""""""     br = ""([{<""["")]}>"".index(sub)]     count = 0     for i in range(start, len(self.string)):         char = self.string[i]         <MASK>             count += 1         elif char == sub:             if count > 0:                 count -= 1             else:                 return i     err = ""Closing bracket {!r} missing in string {!r}"".format(         sub, """".join(self.original)     )     raise ParseError(err)",if char == br :,if char in br :,24.736929544091932,24.736929544091932,0.0
"def test_createFile(self):     text = ""This is a test!""     path = tempfile.mktemp()     try:         koDoc = self._koDocFromPath(path, load=False)         koDoc.buffer = text         koDoc.save(0)         del koDoc         koDoc2 = self._koDocFromPath(path)         assert koDoc2.buffer == text     finally:         <MASK>             os.unlink(path)  # clean up",if os . path . exists ( path ) :,if koDoc2 . buffer :,5.171845311465849,5.171845311465849,0.0
"def __editScopeHasEdit(self, attributeHistory):     with attributeHistory.context:         tweak = GafferScene.EditScopeAlgo.acquireParameterEdit(             attributeHistory.scene.node(),             attributeHistory.context[""scene:path""],             attributeHistory.attributeName,             IECoreScene.ShaderNetwork.Parameter("""", self.__parameter),             createIfNecessary=False,         )         <MASK>             return False         return tweak[""enabled""].getValue()",if tweak is None :,if not tweak :,16.37226966703825,16.37226966703825,0.0
"def mail_migrator(app, schema_editor):     Event_SettingsStore = app.get_model(""pretixbase"", ""Event_SettingsStore"")     for ss in Event_SettingsStore.objects.filter(         key__in=[             ""mail_text_order_approved"",             ""mail_text_order_placed"",             ""mail_text_order_placed_require_approval"",         ]     ):         chgd = ss.value.replace(""{date}"", ""{expire_date}"")         <MASK>             ss.value = chgd             ss.save()             cache.delete(""hierarkey_{}_{}"".format(""event"", ss.object_id))",if chgd != ss . value :,if chgd :,11.898417391331403,0.0,0.0
"def __get_limits(self):     dimension = len(self.__tree.get_root().data)     nodes = self.__get_all_nodes()     max, min = [float(""-inf"")] * dimension, [float(""+inf"")] * dimension     for node in nodes:         for d in range(dimension):             if max[d] < node.data[d]:                 max[d] = node.data[d]             <MASK>                 min[d] = node.data[d]     return min, max",if min [ d ] > node . data [ d ] :,if min [ d ] > node . data [ d ] :,100.00000000000004,100.00000000000004,1.0
"def get_complete_position(self, context: UserContext) -> int:     # Check member prefix pattern.     for prefix_pattern in convert2list(         self.get_filetype_var(context[""filetype""], ""prefix_patterns"")     ):         m = re.search(self._object_pattern + prefix_pattern + r""\w*$"", context[""input""])         <MASK>             continue         self._prefix = re.sub(r""\w*$"", """", m.group(0))         m = re.search(r""\w*$"", context[""input""])         if m:             return m.start()     return -1","if m is None or prefix_pattern == """" :",if not m :,2.215745752614824,2.215745752614824,0.0
"def _stderr_supports_color():     try:         if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty():             if curses:                 curses.setupterm()                 if curses.tigetnum(""colors"") > 0:                     return True             <MASK>                 if sys.stderr is getattr(                     colorama.initialise, ""wrapped_stderr"", object()                 ):                     return True     except Exception:         # Very broad exception handling because it's always better to         # fall back to non-colored logs than to break at startup.         pass     return False",elif colorama :,if colorama . is_color_enabled ( ) :,4.456882760699063,4.456882760699063,0.0
"def setLabelColumnWidth(self, panel, width):     for child in panel.GetChildren():         <MASK>             size = child.GetSize()             size[0] = width             child.SetBestSize(size)","if isinstance ( child , wx . lib . stattext . GenStaticText ) :",if child . GetSize ( ) == width :,4.402175903404695,4.402175903404695,0.0
"def update(self, other):     if other.M is None:         <MASK>             self.items.update(other.items)         else:             for i in other.items:                 self.add(i)         return     if self.M is None:         self.convert()     self.M = array.array(""B"", list(map(max, list(zip(self.M, other.M)))))",if self . M is None :,if self . items is not None :,27.054113452696992,27.054113452696992,0.0
"def on_end_epoch(self, state):     if self.write_epoch_metrics:         <MASK>             self.writer.add_text(                 ""epoch"",                 ""<h4>Epoch {}</h4>"".format(state[torchbearer.EPOCH])                 + self.table_formatter(str(state[torchbearer.METRICS])),                 1,             )         else:             self.writer.add_text(                 ""epoch"",                 self.table_formatter(str(state[torchbearer.METRICS])),                 state[torchbearer.EPOCH],             )",if self . visdom :,if self . write_epoch_metrics :,19.070828081828378,19.070828081828378,0.0
"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool:     """"""Check if the conversation is in need for a user message.""""""     tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED)     for i, e in enumerate(reversed(tracker.get(""events"", []))):         <MASK>             return False         elif e.get(""event"") == ActionExecuted.type_name:             return e.get(""name"") == ACTION_LISTEN_NAME     return False","if e . get ( ""event"" ) == UserUttered . type_name :",if i == 0 :,3.086457674499703,3.086457674499703,0.0
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None):     assert nw_id != self.nw_id_unknown     ret = []     for port in self.get_ports(dpid):         nw_id_ = port.network_id         <MASK>             continue         if nw_id_ == nw_id:             ret.append(port.port_no)         elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external:             ret.append(port.port_no)     return ret",if port . port_no == in_port :,if not in_port :,19.765609300943975,19.765609300943975,0.0
"def next_month(billing_cycle_anchor: datetime, dt: datetime) -> datetime:     estimated_months = round((dt - billing_cycle_anchor).days * 12.0 / 365)     for months in range(max(estimated_months - 1, 0), estimated_months + 2):         proposed_next_month = add_months(billing_cycle_anchor, months)         <MASK>             return proposed_next_month     raise AssertionError(         ""Something wrong in next_month calculation with ""         f""billing_cycle_anchor: {billing_cycle_anchor}, dt: {dt}""     )",if 20 < ( proposed_next_month - dt ) . days < 40 :,if proposed_next_month :,16.02643071979889,16.02643071979889,0.0
"def wait_complete(self):     """"""Wait for futures complete done.""""""     for future in concurrent.futures.as_completed(self._futures.keys()):         try:             error = future.exception()         except concurrent.futures.CancelledError:             break         name = self._futures[future]         <MASK>             err_msg = 'Extracting ""{0}"", got: {1}'.format(name, error)             logger.error(err_msg)",if error is not None :,if error :,23.174952587773145,0.0,0.0
"def _accept_with(cls, orm, target):     if target is orm.mapper:         return mapperlib.Mapper     elif isinstance(target, type):         if issubclass(target, mapperlib.Mapper):             return target         else:             mapper = _mapper_or_none(target)             <MASK>                 return mapper             else:                 return _MapperEventsHold(target)     else:         return target",if mapper is not None :,if mapper :,23.174952587773145,0.0,0.0
"def gvariant_args(args: List[Any]) -> str:     """"""Convert args into gvariant.""""""     gvariant = """"     for arg in args:         <MASK>             gvariant += "" {}"".format(str(arg).lower())         elif isinstance(arg, (int, float)):             gvariant += f"" {arg}""         elif isinstance(arg, str):             gvariant += f' ""{arg}""'         else:             gvariant += f"" {arg!s}""     return gvariant.lstrip()","if isinstance ( arg , bool ) :","if isinstance ( arg , ( str , unicode ) ) :",36.462858619364674,36.462858619364674,0.0
"def _list_cases(suite):     for test in suite:         if isinstance(test, unittest.TestSuite):             _list_cases(test)         <MASK>             if support.match_test(test):                 print(test.id())","elif isinstance ( test , unittest . TestCase ) :",if support . match_test ( test ) :,12.549310621989482,12.549310621989482,0.0
def get_and_set_all_disambiguation(self):     all_disambiguations = []     for page in self.pages:         <MASK>             all_disambiguations.extend(page.relations.disambiguation_links_norm)         if page.relations.disambiguation_links is not None:             all_disambiguations.extend(page.relations.disambiguation_links)     return set(all_disambiguations),if page . relations . disambiguation_links_norm is not None :,if page . relations . disambiguation_links_norm is not None :,100.00000000000004,100.00000000000004,1.0
"def test_decode_invalid(self):     testcases = [         (b""xn--w&"", ""strict"", UnicodeError()),         (b""xn--w&"", ""ignore"", ""xn-""),     ]     for puny, errors, expected in testcases:         with self.subTest(puny=puny, errors=errors):             <MASK>                 self.assertRaises(UnicodeError, puny.decode, ""punycode"", errors)             else:                 self.assertEqual(puny.decode(""punycode"", errors), expected)","if isinstance ( expected , Exception ) :",if puny . decode is not None :,6.567274736060395,6.567274736060395,0.0
"def find_globs(walker, patterns, matches):     for root, dirs, files in walker:         for d in dirs:             d = join(root, d)             for pattern in patterns:                 for p in Path(d).glob(pattern):                     matches.add(str(p))         sub_files = set()         for p in matches:             <MASK>                 for f in files:                     sub_files.add(join(root, f))         matches.update(sub_files)",if root . startswith ( p ) :,if p . startswith ( root ) :,29.071536848410968,29.071536848410968,0.0
"def parse_stack_trace(self, it, line):     """"""Iterate over lines and parse stack traces.""""""     events = []     stack_traces = []     while self.stack_trace_re.match(line):         event = self.parse_stack_trace_line(line)         <MASK>             events.append(event)         stack_traces.append(line)         line = get_next(it)     events.reverse()     return stack_traces, events, line",if event :,if event :,100.00000000000004,0.0,1.0
"def process(self):     """"""Do processing necessary, storing result in feature.""""""     summation = 0  # count of all     histo = self.data[""flat.notes.quarterLengthHistogram""]     if not histo:         raise NativeFeatureException(""input lacks notes"")     maxKey = 0  # max found for any one key     for key in histo:         # all defined keys should be greater than zero, but just in case         if histo[key] > 0:             summation += histo[key]             <MASK>                 maxKey = histo[key]     self.feature.vector[0] = maxKey / summation",if histo [ key ] >= maxKey :,if histo [ key ] > maxKey :,67.5291821812656,67.5291821812656,0.0
"def load_resource(name):     """"""return file contents for files within the package root folder""""""     try:         <MASK>             return sublime.load_resource(""Packages/Markdown Preview/{0}"".format(name))         else:             filename = os.path.join(                 sublime.packages_path(), INSTALLED_DIRECTORY, os.path.normpath(name)             )             return load_utf8(filename)     except:         print(""Error while load_resource('%s')"" % name)         traceback.print_exc()         return """"",if is_ST3 ( ) :,if sublime . is_markdown ( ) :,23.356898886410015,23.356898886410015,0.0
"def get_password(self, service, repo_url):     if self.is_unlocked:         asyncio.set_event_loop(asyncio.new_event_loop())         collection = secretstorage.get_default_collection(self.connection)         attributes = {""application"": ""Vorta"", ""service"": service, ""repo_url"": repo_url}         items = list(collection.search_items(attributes))         logger.debug(""Found %i passwords matching repo URL."", len(items))         <MASK>             return items[0].get_secret().decode(""utf-8"")     return None",if len ( items ) > 0 :,if items :,7.49553326588684,0.0,0.0
"def get_files(d):     res = []     for p in glob.glob(os.path.join(d, ""*"")):         if not p:             continue         (pth, fname) = os.path.split(p)         if fname == ""output"":             continue         if fname == ""PureMVC_Python_1_0"":             continue         if fname[-4:] == "".pyc"":  # ehmm.. no.             continue         <MASK>             get_dir(p)         else:             res.append(p)     return res",if os . path . isdir ( p ) :,"if fname [ 0 ] == ""dist"" :",4.456882760699063,4.456882760699063,0.0
"def test_nic_names(self):     p = subprocess.Popen([""ipconfig"", ""/all""], stdout=subprocess.PIPE)     out = p.communicate()[0]     if PY3:         out = str(out, sys.stdout.encoding)     nics = psutil.net_io_counters(pernic=True).keys()     for nic in nics:         <MASK>             continue         if nic not in out:             self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)","if ""pseudo-interface"" in nic . replace ( "" "" , ""-"" ) . lower ( ) :",if nic not in out :,0.9422327045951052,0.9422327045951052,0.0
"def vexop_to_simop(op, extended=True, fp=True):     res = operations.get(op)     if res is None and extended:         attrs = op_attrs(op)         <MASK>             raise UnsupportedIROpError(""Operation not implemented"")         res = SimIROp(op, **attrs)     if res is None:         raise UnsupportedIROpError(""Operation not implemented"")     if res._float and not fp:         raise UnsupportedIROpError(""Floating point support disabled"")     return res",if attrs is None :,if attrs is None :,100.00000000000004,100.00000000000004,1.0
"def rule_builder_add_value(self, value, screenshot_name=None):     rule_builder = self.components.rule_builder     rule_builder.menu_button_column.wait_for_and_click()     with self.rule_builder_rule_editor(""add-column-value"") as editor_element:         filter_input = editor_element.find_element_by_css_selector(""input[type='text']"")         filter_input.clear()         filter_input.send_keys(value)         <MASK>             self.screenshot(screenshot_name)",if screenshot_name :,if screenshot_name :,100.00000000000004,100.00000000000004,1.0
"def make_open_socket(self):     s = socket.socket()     try:         s.bind(DEFAULT_BIND_ADDR_TUPLE)         <MASK>             # Windows and linux (with psutil) doesn't show as open until             # we call listen (linux with lsof accepts either)             s.listen(1)         self.assert_open(s, s.fileno())     except:         s.close()         s = None         raise     return s",if WIN or greentest . LINUX :,if self . is_windows :,8.643019616048525,8.643019616048525,0.0
"def handle_ray_task_error(e):     for s in e.traceback_str.split(""\n"")[::-1]:         <MASK>             try:                 raise getattr(builtins, s.split("":"")[0])("""".join(s.split("":"")[1:]))             except AttributeError as att_err:                 if ""module"" in str(att_err) and builtins.__name__ in str(att_err):                     pass                 else:                     raise att_err     raise e","if ""Error"" in s or ""Exception"" in s :","if s . startswith ( ""module"" ) :",4.648378982882215,4.648378982882215,0.0
"def compare_multiple_events(i, expected_results, actual_results):     events_in_a_row = []     j = i     while j < len(expected_results) and isinstance(         actual_results[j], actual_results[i].__class__     ):         events_in_a_row.append(actual_results[j])         j += 1     message = """"     for event in events_in_a_row:         for k in range(i, j):             passed, message = compare_events(expected_results[k], event)             <MASK>                 expected_results[k] = None                 break         else:             return i, False, message     return j, True, """"",if passed :,if not passed :,35.35533905932737,35.35533905932737,0.0
"def ListSubscriptions(self, params):     queryreturn = sqlQuery(""""""SELECT label, address, enabled FROM subscriptions"""""")     data = '{""subscriptions"":['     for row in queryreturn:         label, address, enabled = row         label = shared.fixPotentiallyInvalidUTF8Data(label)         <MASK>             data += "",""         data += json.dumps(             {                 ""label"": label.encode(""base64""),                 ""address"": address,                 ""enabled"": enabled == 1,             },             indent=4,             separators=("","", "": ""),         )     data += ""]}""     return data",if len ( data ) > 20 :,if label :,6.5479514338598115,0.0,0.0
"def compile(self, args):     compiled_args = {}     for key, value in six.iteritems(args):         <MASK>             compiled_args[key] = str(value)         else:             compiled_args[key] = sjson_dumps(value)     return self._minified_code % compiled_args",if key in self . clean_args :,"if isinstance ( value , str ) :",5.795599612995366,5.795599612995366,0.0
"def insert(self, pack_id, data):     if (pack_id not in self.queue) and pack_id > self.begin_id:         self.queue[pack_id] = PacketInfo(data)         if self.end_id == pack_id:             self.end_id = pack_id + 1         <MASK>             eid = self.end_id             while eid < pack_id:                 self.miss_queue.add(eid)                 eid += 1             self.end_id = pack_id + 1         else:             self.miss_queue.remove(pack_id)",elif self . end_id < pack_id :,if self . end_id > pack_id :,58.77283725105324,58.77283725105324,0.0
"def _target_generator(self):     # since we do not have predictions yet, so we ignore sampling here     if self._internal_target_generator is None:         <MASK>             return None         from ....model_zoo.ssd.target import SSDTargetGenerator         self._internal_target_generator = SSDTargetGenerator(             iou_thresh=self._iou_thresh,             stds=self._box_norm,             negative_mining_ratio=-1,             **self._kwargs         )         return self._internal_target_generator     else:         return self._internal_target_generator",if self . _anchors_none :,if self . _is_sampling :,38.260294162784454,38.260294162784454,0.0
"def test_heapsort(self):     # Exercise everything with repeated heapsort checks     for trial in range(100):         size = random.randrange(50)         data = [random.randrange(25) for i in range(size)]         <MASK>  # Half of the time, use heapify             heap = data[:]             self.module.heapify(heap)         else:  # The rest of the time, use heappush             heap = []             for item in data:                 self.module.heappush(heap, item)         heap_sorted = [self.module.heappop(heap) for i in range(size)]         self.assertEqual(heap_sorted, sorted(data))",if trial & 1 :,if trial == 0 :,17.965205598154213,17.965205598154213,0.0
"def wait(self, timeout=None):     if self.returncode is None:         if timeout is None:             msecs = _subprocess.INFINITE         else:             msecs = max(0, int(timeout * 1000 + 0.5))         res = _subprocess.WaitForSingleObject(int(self._handle), msecs)         <MASK>             code = _subprocess.GetExitCodeProcess(self._handle)             if code == TERMINATE:                 code = -signal.SIGTERM             self.returncode = code     return self.returncode",if res == _subprocess . WAIT_OBJECT_0 :,if res is None :,4.773548444510098,4.773548444510098,0.0
"def _on_change(self):     changed = False     self.save()     for key, value in self.data.items():         if isinstance(value, bool):             if value:                 changed = True                 break         if isinstance(value, int):             if value != 1:                 changed = True                 break         <MASK>             continue         elif len(value) != 0:             changed = True             break     self._reset_button.disabled = not changed",elif value is None :,"if key != ""id"" :",5.522397783539471,5.522397783539471,0.0
"def isnotsurplus(self, item: T) -> bool:     if not self.matchers:         <MASK>             self.mismatch_description.append_text(                 ""not matched: ""             ).append_description_of(item)         return False     return True",if self . mismatch_description :,if item . is_text ( ) :,6.742555929751843,6.742555929751843,0.0
"def resolve_env_secrets(config, environ):     """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ""""""     if isinstance(config, dict):         <MASK>             return environ.get(list(config.values())[0])         elif list(config.keys()) == [""$file""]:             return open(list(config.values())[0]).read()         else:             return {                 key: resolve_env_secrets(value, environ)                 for key, value in config.items()             }     elif isinstance(config, list):         return [resolve_env_secrets(value, environ) for value in config]     else:         return config","if list ( config . keys ( ) ) == [ ""$env"" ] :","if list ( config . keys ( ) ) == [ ""$env"" ] :",100.00000000000004,100.00000000000004,1.0
"def __open__(filename, *args, **kwargs):     if os.path.isfile(filename):         return __realopen__(filename, *args, **kwargs)     if not os.path.isabs(filename):         datafilename = __papplet__.dataPath(filename)         <MASK>             return __realopen__(datafilename, *args, **kwargs)         sketchfilename = __papplet__.sketchPath(filename)     if os.path.isfile(sketchfilename):         return __realopen__(sketchfilename, *args, **kwargs)     # Fail naturally     return __realopen__(filename, *args, **kwargs)",if os . path . isfile ( datafilename ) :,if os . path . isfile ( datafilename ) :,100.00000000000004,100.00000000000004,1.0
def run(self):     while not self.completed:         <MASK>             time.sleep(self.period)         else:             self._completed.wait(self.period)         self.counter += 1         try:             self.callback(self.counter)         except Exception:             self.stop()         if self.timeout is not None:             dt = time.time() - self._start_time             if dt > self.timeout:                 self.stop()         if self.counter == self.count:             self.stop(),if self . block :,if self . timeout is None :,26.269098944241588,26.269098944241588,0.0
"def remove(self, path, config=None, error_on_path=False, defaults=None):     if not path:         <MASK>             raise NoSuchSettingsPath()         return     if config is not None or defaults is not None:         if config is None:             config = self._config         if defaults is None:             defaults = dict(self._map.parents)         chain = HierarchicalChainMap(config, defaults)     else:         chain = self._map     try:         chain.del_by_path(path)         self._mark_dirty()     except KeyError:         if error_on_path:             raise NoSuchSettingsPath()         pass",if error_on_path :,if error_on_path :,100.00000000000004,100.00000000000004,1.0
"def structured_dot_grad(sparse_A, dense_B, ga):     if sparse_A.type.format in (""csc"", ""csr""):         <MASK>             sdgcsx = sdg_csc             CSx = CSC         else:             sdgcsx = sdg_csr             CSx = CSR         g_A_data = sdgcsx(csm_indices(sparse_A), csm_indptr(sparse_A), dense_B, ga)         return CSx(             g_A_data, csm_indices(sparse_A), csm_indptr(sparse_A), csm_shape(sparse_A)         )     else:         raise NotImplementedError()","if sparse_A . type . format == ""csc"" :","if sparse_A . type . format == ""csc"" :",100.00000000000004,100.00000000000004,1.0
"def step_async(self, actions):     listify = True     try:         <MASK>             listify = False     except TypeError:         pass     if not listify:         self.actions = actions     else:         assert (             self.num_envs == 1         ), f""actions {actions} is either not a list or has a wrong size - cannot match to {self.num_envs} environments""         self.actions = [actions]",if len ( actions ) == self . num_envs :,if actions is not None :,3.3264637832151163,3.3264637832151163,0.0
"def tempFailureRetry(func, *args, **kwargs):     while True:         try:             return func(*args, **kwargs)         except (os.error, IOError) as ex:             <MASK>                 continue             else:                 raise",if ex . errno == errno . EINTR :,if ex . errno == errno . EAGAIN :,78.25422900366438,78.25422900366438,0.0
"def test_learning_always_changes_generation(chars, order):     learner = LStar(lambda s: len(s) == 1 and s[0] in chars)     for c in order:         prev = learner.generation         s = bytes([c])         <MASK>             learner.learn(s)             assert learner.generation > prev",if learner . dfa . matches ( s ) != learner . member ( s ) :,if c in chars :,0.9435209353331546,0.9435209353331546,0.0
"def test_costs_5D_noisy_names(signal_bkps_5D_noisy, cost_name):     signal, bkps = signal_bkps_5D_noisy     cost = cost_factory(cost_name)     cost.fit(signal)     cost.error(0, 100)     cost.error(100, signal.shape[0])     cost.error(10, 50)     cost.sum_of_costs(bkps)     with pytest.raises(NotEnoughPoints):         <MASK>             cost.min_size = 4             cost.error(1, 2)         else:             cost.error(1, 2)","if cost_name == ""cosine"" :","if not isinstance ( cost , np . ndarray ) :",4.9323515694897075,4.9323515694897075,0.0
"def remove_empty_dirs(dirname):     logger.debug(""remove_empty_dirs '%s'"" % (dirname))     try:         <MASK>             dirname = dirname.encode(""utf-8"")         os.removedirs(dirname)         logger.debug(""remove_empty_dirs '%s' done"" % (dirname))     except OSError as exc:  # Python >2.5         if exc.errno == errno.ENOTEMPTY:             logger.debug(""remove_empty_dirs '%s' not empty"" % (dirname))             pass         else:             raise     except Exception as e:         logger.exception(e)         logger.error(""remove_empty_dirs exception: "" + dirname)         raise e","if not isinstance ( dirname , str ) :",if os . path . isfile ( dirname ) :,12.549310621989482,12.549310621989482,0.0
"def get_unique_attribute(self, name: str):     feat = None     for f in self.features:         <MASK>             if feat is not None:                 raise RuntimeError(""The attribute was not unique."")             feat = f     if feat is None:         raise RuntimeError(""The attribute did not exist"")     return getattr(feat, name)","if self . _return_feature ( f ) and hasattr ( f , name ) :",if f . name == name :,2.365931054820936,2.365931054820936,0.0
"def get_allocated_address(     self, config: ActorPoolConfig, allocated: allocated_type ) -> str:     addresses = config.get_external_addresses(label=self.label)     for addr in addresses:         occupied = False         for strategy, _ in allocated.get(addr, dict()).values():             if strategy == self:                 occupied = True                 break         <MASK>             return addr     raise NoIdleSlot(         f""No idle slot for creating actor "" f""with label {self.label}, mark {self.mark}""     )",if not occupied :,if not occupied :,100.00000000000004,100.00000000000004,1.0
"def __deepcopy__(self, memo):     cls = self.__class__     result = cls.__new__(cls)     memo[id(self)] = result     for key, value in self.__dict__.items():         <MASK>             setattr(result, key, copy.copy(value))         else:             setattr(result, key, copy.deepcopy(value, memo))     return result",if key in cls . dynamic_methods :,"if isinstance ( value , dict ) :",5.795599612995366,5.795599612995366,0.0
def restore_forward(model):     for child in model.children():         # leaf node         <MASK>             child.forward = child.old_forward             child.old_forward = None         else:             restore_forward(child),"if is_leaf ( child ) and hasattr ( child , ""old_forward"" ) :",if child . old_forward is not None :,7.848774327547447,7.848774327547447,0.0
"def add(self, obj, allow_duplicates=False):     if allow_duplicates or obj not in self._constants:         self._constant_pool.append(obj)         self._constants[obj] = len(self)         <MASK>             self._constant_pool.append(None)","if obj . __class__ in ( Double , Long ) :",if obj in self . _constant_pool :,8.27951003977077,8.27951003977077,0.0
"def find_file_copyright_notices(fname):     ret = set()     f = open(fname)     lines = f.readlines()     for l in lines[:80]:  # hmmm, assume copyright to be in first 80 lines         idx = l.lower().find(""copyright"")         if idx < 0:             continue         copyright = l[idx + 9 :].strip()         if not copyright:             continue         copyright = sanitise(copyright)         # hmm, do a quick check to see if there's a year,         # if not, skip it         <MASK>             continue         ret.add(copyright)     return ret","if not copyright . find ( ""200"" ) >= 0 and not copyright . find ( ""199"" ) >= 0 :",if idx == - 1 :,0.5334071169690752,0.5334071169690752,0.0
"def callback(lexer, match, context):     text = match.group()     extra = """"     if start:         context.next_indent = len(text)         if context.next_indent < context.indent:             while context.next_indent < context.indent:                 context.indent = context.indent_stack.pop()             <MASK>                 extra = text[context.indent :]                 text = text[: context.indent]     else:         context.next_indent += len(text)     if text:         yield match.start(), TokenClass, text     if extra:         yield match.start() + len(text), TokenClass.Error, extra     context.pos = match.end()",if context . next_indent > context . indent :,if context . next_indent < context . indent_stack . size :,42.311785416105785,42.311785416105785,0.0
"def queries(self):     if DEV:         cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s)         if not cmd.check(f""docker check for {self.path.k8s}""):             if not cmd.stdout.strip():                 log_cmd = ShellCommand(                     ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT                 )                 <MASK>                     print(cmd.stdout)                 pytest.exit(f""container failed to start for {self.path.k8s}"")     return ()","if log_cmd . check ( f""docker logs for {self.path.k8s}"" ) :",if not log_cmd . check ( ) :,18.035689048530983,18.035689048530983,0.0
"def nodes(self):     if not self._nodes:         nodes = self.cluster_group.instances()         self._nodes = []         master = self.master_node         nodeid = 1         for node in nodes:             if node.state not in [""pending"", ""running""]:                 continue             <MASK>                 self._nodes.insert(0, master)                 continue             self._nodes.append(Node(node, self.key_location, ""node%.3d"" % nodeid))             nodeid += 1     else:         for node in self._nodes:             log.debug(""refreshing instance %s"" % node.id)             node.update()     return self._nodes",if node . id == master . id :,if node . id not in self . _nodes :,25.965358893403383,25.965358893403383,0.0
"def match(cls, agent_name, guid, uri, media=None):     # Retrieve `Agent` for provided `guid`     agent = Agents.get(agent_name)     if agent is None:         <MASK>             # First occurrence of unsupported agent             log.warn(""Unsupported metadata agent: %s"" % agent_name)             # Mark unsupported agent as ""seen""             unsupported_agents[agent_name] = True             return False         # Duplicate occurrence of unsupported agent         log.warn(             ""Unsupported metadata agent: %s"" % agent_name, extra={""duplicate"": True}         )         return False     # Fill `guid` with details from agent     return agent.fill(guid, uri, media)",if agent_name not in unsupported_agents :,if agent_name not in unsupported_agents :,100.00000000000004,100.00000000000004,1.0
"def __createRandom(plug):     node = plug.node()     parentNode = node.ancestor(Gaffer.Node)     with Gaffer.UndoScope(node.scriptNode()):         randomNode = Gaffer.Random()         parentNode.addChild(randomNode)         if isinstance(plug, (Gaffer.FloatPlug, Gaffer.IntPlug)):             plug.setInput(randomNode[""outFloat""])         <MASK>             plug.setInput(randomNode[""outColor""])     GafferUI.NodeEditor.acquire(randomNode)","elif isinstance ( plug , Gaffer . Color3fPlug ) :","if isinstance ( plug , Gaffer . ColorPlug ) :",58.14307369682194,58.14307369682194,0.0
"def post_arrow(self, arr: pa.Table, graph_type: str, opts: str = """"):     dataset_id = self.dataset_id     tok = self.token     sub_path = f""api/v2/upload/datasets/{dataset_id}/{graph_type}/arrow""     try:         resp = self.post_arrow_generic(sub_path, tok, arr, opts)         out = resp.json()         <MASK>             raise Exception(""No success indicator in server response"")         return out     except Exception as e:         logger.error(""Failed to post arrow to %s"", sub_path, exc_info=True)         raise e","if not ( ""success"" in out ) or not out [ ""success"" ] :",if not resp . ok :,2.4313261880301806,2.4313261880301806,0.0
"def dict_to_XML(tag, dictionary, **kwargs):     """"""Return XML element converting dicts recursively.""""""     elem = Element(tag, **kwargs)     for key, val in dictionary.items():         <MASK>             child = dict_to_XML(""layer"", val, name=key)         elif isinstance(val, MutableMapping):             child = dict_to_XML(key, val)         else:             if tag == ""config"":                 child = Element(""variable"", name=key)             else:                 child = Element(key)             child.text = str(val)         elem.append(child)     return elem","if tag == ""layers"" :","if tag == ""layer"" :",59.4603557501361,59.4603557501361,0.0
"def apply_incpaths_ml(self):     inc_lst = self.includes.split()     lst = self.incpaths_lst     for dir in inc_lst:         node = self.path.find_dir(dir)         <MASK>             error(""node not found: "" + str(dir))             continue         if not node in lst:             lst.append(node)         self.bld_incpaths_lst.append(node)",if not node :,if node is None :,14.058533129758727,14.058533129758727,0.0
"def _table_reprfunc(self, row, col, val):     if self._table.column_names[col].endswith(""Size""):         if isinstance(val, compat.string_types):             return ""  %s"" % val         elif val < 1024 ** 2:             return ""  %.1f KB"" % (val / 1024.0 ** 1)         <MASK>             return ""  %.1f MB"" % (val / 1024.0 ** 2)         else:             return ""  %.1f GB"" % (val / 1024.0 ** 3)     if col in (0, """"):         return str(val)     else:         return ""  %s"" % val",elif val < 1024 ** 3 :,if val < 1024 ** 3 :,84.08964152537145,84.08964152537145,0.0
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None):     """"""cache hidden states into memory.""""""     if mem_len is None or mem_len == 0:         return None     else:         <MASK>             curr_out = curr_out[:reuse_len]         if prev_mem is None:             new_mem = curr_out[-mem_len:]         else:             new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]     new_mem.stop_gradient = True     return new_mem",if reuse_len is not None and reuse_len > 0 :,if reuse_len is not None :,41.06951993704473,41.06951993704473,0.0
"def GROUP_CONCAT(builder, distinct, expr, sep=None):     assert distinct in (None, True, False)     result = distinct and ""GROUP_CONCAT(DISTINCT "" or ""GROUP_CONCAT("", builder(expr)     if sep is not None:         <MASK>             result = result, "" SEPARATOR "", builder(sep)         else:             result = result, "", "", builder(sep)     return result, "")""","if builder . provider . dialect == ""MySQL"" :",if sep in expr :,3.1325998243558226,3.1325998243558226,0.0
"def __init__(self, *args, **kwargs):     super().__init__(*args, **kwargs)     self.custom_fields = []     self.obj_type = ContentType.objects.get_for_model(self.model)     # Add all applicable CustomFields to the form     custom_fields = CustomField.objects.filter(content_types=self.obj_type)     for cf in custom_fields:         # Annotate non-required custom fields as nullable         <MASK>             self.nullable_fields.append(cf.name)         self.fields[cf.name] = cf.to_form_field(             set_initial=False, enforce_required=False         )         # Annotate this as a custom field         self.custom_fields.append(cf.name)",if not cf . required :,if cf . required :,57.89300674674101,57.89300674674101,0.0
"def is_child_of(self, item_hash, possible_child_hash):     if self.get_last(item_hash) != self.get_last(possible_child_hash):         return None     while True:         <MASK>             return True         if possible_child_hash not in self.items:             return False         possible_child_hash = self.items[possible_child_hash].previous_hash",if possible_child_hash == item_hash :,if item_hash == possible_child_hash :,66.10079036990797,66.10079036990797,0.0
"def validate(self):     self.assertEqual(len(self.inputs), len(self.outputs))     for batch_in, batch_out in zip(self.inputs, self.outputs):         self.assertEqual(len(batch_in), len(batch_out))         <MASK>             self.validate_unordered_batch(batch_in, batch_out)         else:             for in_data, out_data in zip(batch_in, batch_out):                 self.assertEqual(in_data.shape, out_data.shape)                 if not self.use_parallel_executor:                     self.assertTrue((in_data == out_data).all())",if self . use_parallel_executor and not self . use_double_buffer :,if self . use_parallel_executor :,32.70962178059004,32.70962178059004,0.0
"def add_cells(self, cells):     for cell in cells:         <MASK>             id = len(self.cell_id_map)             self.cell_id_map[cell] = id             self.id_cell_map[id] = cell",if cell not in self . cell_id_map :,if cell . id == self . id_cell_map [ id ] :,10.227581634700618,10.227581634700618,0.0
"def _verify_out(marker="">>""):     if shared:         self.assertIn(""libapp_lib.dylib"", self.client.out)     else:         <MASK>             self.assertIn(""libapp_lib.a"", self.client.out)         else:  # Incremental build not the same msg             self.assertIn(""Built target app_lib"", self.client.out)     out = str(self.client.out).splitlines()     for k, v in vals.items():         self.assertIn(""%s %s: %s"" % (marker, k, v), out)","if marker == "">>"" :",if shared :,4.691812222477093,0.0,0.0
"def Visit_expr(self, node):  # pylint: disable=invalid-name     # expr ::= xor_expr ('|' xor_expr)*     for child in node.children:         self.Visit(child)         <MASK>             _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)","if isinstance ( child , pytree . Leaf ) and child . value == ""|"" :","if isinstance ( child , ast . Expr ) :",17.96191510244705,17.96191510244705,0.0
"def fill_members(self):     if self._get_retrieve():         after = self.after.id if self.after else None         data = await self.get_members(self.guild.id, self.retrieve, after)         if not data:             # no data, terminate             return         <MASK>             self.limit = 0  # terminate loop         self.after = Object(id=int(data[-1][""user""][""id""]))         for element in reversed(data):             await self.members.put(self.create_member(element))",if len ( data ) < 1000 :,if self . limit > self . limit :,5.669791110976001,5.669791110976001,0.0
"def assert_warns(expected):     with warnings.catch_warnings(record=True) as w:         warnings.simplefilter(""always"")         yield     # Python 2 does not raise warnings multiple times from the same stack     # frame.     if sys.version_info >= (3, 0):         <MASK>             try:                 exc_name = expected.__name__             except AttributeError:                 exc_name = str(expected)             raise AssertionError(""%s not triggerred"" % exc_name)","if not any ( isinstance ( m . message , expected ) for m in w ) :",if expected is not None :,1.5534791020152603,1.5534791020152603,0.0
"def __init__(self, measures):     """"""Constructs a ContingencyMeasures given a NgramAssocMeasures class""""""     self.__class__.__name__ = ""Contingency"" + measures.__class__.__name__     for k in dir(measures):         <MASK>             continue         v = getattr(measures, k)         if not k.startswith(""_""):             v = self._make_contingency_fn(measures, v)         setattr(self, k, v)","if k . startswith ( ""__"" ) :","if k == ""Contingency"" :",9.545138913210204,9.545138913210204,0.0
"def _omit_keywords(self, context):     omitted_kws = 0     for event, elem in context:         # Teardowns aren't omitted to allow checking suite teardown status.         omit = elem.tag == ""kw"" and elem.get(""type"") != ""teardown""         start = event == ""start""         <MASK>             omitted_kws += 1         if not omitted_kws:             yield event, elem         elif not start:             elem.clear()         if omit and not start:             omitted_kws -= 1",if omit and start :,if omit and start :,100.00000000000004,100.00000000000004,1.0
"def read_block(buffer, i):     offset = i * BLOCK_LENGTH % config.CAPTURE_BUFFER     while True:         if buffer[offset] == BLOCK_MARKER.END:             return None         while buffer[offset] == BLOCK_MARKER.WRITE:             time.sleep(SHORT_SENSOR_SLEEP_TIME)         buffer[offset] = BLOCK_MARKER.READ         buffer.seek(offset + 1)         length = struct.unpack(""=H"", buffer.read(2))[0]         retval = buffer.read(length)         <MASK>             break     buffer[offset] = BLOCK_MARKER.NOP     return retval",if buffer [ offset ] == BLOCK_MARKER . READ :,if retval == 0 :,6.011598678897526,6.011598678897526,0.0
def _start(self):     try:         instance_info = self._get_instance_info()         <MASK>             self._multipass_cmd.start(instance_name=self.instance_name)     except errors.ProviderInfoError as instance_error:         # Until we have proper multipass error codes to know if this         # was a communication error we should keep this error tracking         # and generation here.         raise errors.ProviderInstanceNotFoundError(             instance_name=self.instance_name         ) from instance_error,if not instance_info . is_running ( ) :,if instance_info :,11.141275535087015,11.141275535087015,0.0
"def _river_driver(self):     if self._cached_river_driver:         return self._cached_river_driver     else:         <MASK>             self._cached_river_driver = MsSqlDriver(                 self.workflow, self.wokflow_object_class, self.field_name             )         else:             self._cached_river_driver = OrmDriver(                 self.workflow, self.wokflow_object_class, self.field_name             )         return self._cached_river_driver",if app_config . IS_MSSQL :,if self . is_sql :,6.979367151952678,6.979367151952678,0.0
"def __LazyMap__(self, attr):     try:         <MASK>             debug_attr_print(                 ""%s.__LazyMap__(%s) added something"" % (self._username_, attr)             )             return 1     except AttributeError:         return 0",if self . _LazyAddAttr_ ( attr ) :,if attr in self . _lazy_map :,19.304869754804493,19.304869754804493,0.0
"def prepare(self, data=None, user=None):     """"""Prepare activation for execution.""""""     super(ManagedStartViewActivation, self).prepare.original()     self.task.owner = user     management_form_class = self.get_management_form_class()     self.management_form = management_form_class(data=data, instance=self.task)     if data:         <MASK>             raise FlowRuntimeError(                 ""Activation metadata is broken {}"".format(self.management_form.errors)             )         self.task = self.management_form.save(commit=False)",if not self . management_form . is_valid ( ) :,if self . management_form . errors :,36.21513850221006,36.21513850221006,0.0
"def PreprocessConditionalStatement(self, IfList, ReplacedLine):     while self:         if self.__Token:             x = 1         elif not IfList:             if self <= 2:                 continue             RegionSizeGuid = 3             <MASK>                 RegionLayoutLine = 5                 continue             RegionLayoutLine = self.CurrentLineNumber     return 1",if not RegionSizeGuid :,ifself <= 3 :,10.682175159905848,10.682175159905848,0.0
"def _get_completion(self, document):     try:         completion_header = document.xpath(""//div[@id='complete_day']"")[0]         completion_message = completion_header.getchildren()[0]         <MASK>             return False         elif ""day_complete_message"" in completion_message.classes:             return True     except IndexError:         return False  # Who knows, probably not my diary.","if ""day_incomplete_message"" in completion_message . classes :","if ""day_complete_day"" in completion_message . classes :",66.06328636027618,66.06328636027618,0.0
"def run(self):     DISPATCH_SYNC = components.interfaces.nsIEventTarget.DISPATCH_SYNC     try:         <MASK>             return         for match in findlib2.find_all_matches(self.regex, self.text):             if self._stopped:                 return             self.target.dispatch(lambda: self.callback(match), DISPATCH_SYNC)             if self._stopped:                 return         self.target.dispatch(lambda: self.callback(None), DISPATCH_SYNC)     finally:         self.callback = None         self.target = None",if self . _stopped :,if self . callback is not None :,22.089591134157878,22.089591134157878,0.0
"def to_key(literal_or_identifier):     """"""returns string representation of this object""""""     if literal_or_identifier[""type""] == ""Identifier"":         return literal_or_identifier[""name""]     elif literal_or_identifier[""type""] == ""Literal"":         k = literal_or_identifier[""value""]         if isinstance(k, float):             return unicode(float_repr(k))         <MASK>             return compose_regex(k)         elif isinstance(k, bool):             return ""true"" if k else ""false""         elif k is None:             return ""null""         else:             return unicode(k)","elif ""regex"" in literal_or_identifier :","if isinstance ( k , str ) :",3.7954847898457067,3.7954847898457067,0.0
"def process_image_pre_creation(sender, instance: Image, **kwargs):     # FIXME(winkidney): May have issue on determining if it     #  is created or not     if instance.pk is not None:         return     for plugin in _plugin_instances:         process_fn = getattr(plugin, ""process_image_pre_creation"", None)         <MASK>             continue         try:             process_fn(                 django_settings=settings,                 image_instance=instance,             )         except Exception:             logging.exception(                 ""Error occurs while trying to access plugin's pin_pre_save ""                 ""for plugin %s"" % plugin             )",if process_fn is None :,if not process_fn :,29.05925408079185,29.05925408079185,0.0
"def check_screenshots(self):     # If we arrive here, there have not been any failures yet     if self.interactive:         self._commit_screenshots()     else:         <MASK>             self._validate_screenshots()             # Always commit the screenshots here. They can be used for the next test run.             # If reference screenshots were already present and there was a mismatch, it should             # have failed above.             self._commit_screenshots()         elif self.allow_missing_screenshots:             warnings.warn(""No committed reference screenshots available. Ignoring."")         else:             self.fail(                 ""No committed reference screenshots available. Run interactive first.""             )",if self . _has_reference_screenshots ( ) :,if self . allow_missing_screenshots :,17.39350277271197,17.39350277271197,0.0
"def on_task_abort(self, task, config):     if ""abort"" in config:         <MASK>             return         log.debug(""sending abort notification"")         self.send_notification(             config[""abort""][""title""],             config[""abort""][""message""],             config[""abort""][""via""],             template_renderer=task.render,         )",if task . silent_abort :,"if not config [ ""abort"" ] :",6.27465531099474,6.27465531099474,0.0
"def block_users(self, user_ids):     broken_items = []     self.logger.info(""Going to block %d users."" % len(user_ids))     for user_id in tqdm(user_ids):         <MASK>             self.error_delay()             broken_items = user_ids[user_ids.index(user_id) :]             break     self.logger.info(""DONE: Total blocked %d users."" % self.total[""blocks""])     return broken_items",if not self . block ( user_id ) :,if user_id not in self . total :,19.987488607301447,19.987488607301447,0.0
"def find_widget_by_id(self, id, parent=None):     """"""Recursively searches for widget with specified ID""""""     if parent == None:         if id in self:             return self[id]  # Do things fast if possible         parent = self[""editor""]     for c in parent.get_children():         if hasattr(c, ""get_id""):             if c.get_id() == id:                 return c         if isinstance(c, Gtk.Container):             r = self.find_widget_by_id(id, c)             <MASK>                 return r     return None",if not r is None :,if r is not None :,25.40663740773074,25.40663740773074,0.0
"def addClasses(self, name):     # Result: void - None     # In: name: string     for n in name.split():         try:             k, method = n.split(""."")         except ValueError:             k = n             method = None         self.classes[k] = 1         <MASK>             self.methods.setdefault(k, {})[method] = 1",if method is not None :,if method :,23.174952587773145,0.0,0.0
"def Read(self, lex_mode):     while True:         t = self._Read(lex_mode)         self.was_line_cont = t.id == Id.Ignored_LineCont         # TODO: Change to ALL IGNORED types, once you have SPACE_TOK.  This means         # we don't have to handle them in the VS_1/VS_2/etc. states.         <MASK>             break     # log('Read() Returning %s', t)     return t",if t . id != Id . Ignored_LineCont :,if t . id == Id . SPACE_TOK :,32.649710286280516,32.649710286280516,0.0
"def _dir_guildfile(dir, ctx):     from guild import guildfile     try:         return guildfile.for_dir(dir)     except guildfile.NoModels:         <MASK>             help_suffix = "" or '%s' for help"" % click_util.cmd_help(ctx)         else:             help_suffix = """"         cli.error(             ""%s does not contain a Guild file (guild.yml)\n""             ""Try specifying a project path or package name%s.""             % (cwd_desc(dir), help_suffix)         )     except guildfile.GuildfileError as e:         cli.error(str(e))",if ctx :,"if ctx . command == ""help"" :",9.287528999566801,9.287528999566801,0.0
"def check_response(self, response):     """"""Specialized version of check_response().""""""     for line in response:         # Skip blank lines:         if not line.strip():             continue         <MASK>             return         elif line.startswith(b""Benutzer/Passwort Fehler""):             raise BadLogin(line)         else:             raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))","if line . startswith ( b""OK"" ) :","if line . startswith ( b""Benutzer/Passwort Fehler"" ) :",53.16967153331756,53.16967153331756,0.0
"def ParseResponses(     self,     knowledge_base: rdf_client.KnowledgeBase,     responses: Iterable[rdfvalue.RDFValue], ) -> Iterator[rdf_client.User]:     for response in responses:         if not isinstance(response, rdf_client_fs.StatEntry):             raise TypeError(f""Unexpected response type: `{type(response)}`"")         # TODO: `st_mode` has to be an `int`, not `StatMode`.         <MASK>             homedir = response.pathspec.path             username = os.path.basename(homedir)             if username not in self._ignore_users:                 yield rdf_client.User(username=username, homedir=homedir)",if stat . S_ISDIR ( int ( response . st_mode ) ) :,if response . st_mode != rdf_client_fs . StatMode . ST_MODE :,20.76047003130265,20.76047003130265,0.0
"def __call__(self, x, uttid=None):     if self.utt2spk is not None:         spk = self.utt2spk[uttid]     else:         spk = uttid     if not self.reverse:         <MASK>             x = np.add(x, self.bias[spk])         if self.norm_vars:             x = np.multiply(x, self.scale[spk])     else:         if self.norm_vars:             x = np.divide(x, self.scale[spk])         if self.norm_means:             x = np.subtract(x, self.bias[spk])     return x",if self . norm_means :,if self . norm_means :,100.00000000000004,100.00000000000004,1.0
"def hasFixtures(self, ctx_callback=None):     context = self.context     if context is None:         return False     if self.implementsAnyFixture(context, ctx_callback=ctx_callback):         return True     # My context doesn't have any, but its ancestors might     factory = self.factory     if factory:         ancestors = factory.context.get(self, [])         for ancestor in ancestors:             <MASK>                 return True     return False","if self . implementsAnyFixture ( ancestor , ctx_callback = ctx_callback ) :",if ancestor . is_class :,2.71487017505439,2.71487017505439,0.0
def UpdateControlState(self):     active = self.demoModules.GetActiveID()     # Update the radio/restore buttons     for moduleID in self.radioButtons:         btn = self.radioButtons[moduleID]         if moduleID == active:             btn.SetValue(True)         else:             btn.SetValue(False)         if self.demoModules.Exists(moduleID):             btn.Enable(True)             <MASK>                 self.btnRestore.Enable(True)         else:             btn.Enable(False)             if moduleID == modModified:                 self.btnRestore.Enable(False),if moduleID == modModified :,if moduleID == active :,53.7284965911771,53.7284965911771,0.0
"def ignore_proxy_host(self):     """"""Check if self.host is in the $no_proxy ignore list.""""""     if urllib.proxy_bypass(self.host):         return True     no_proxy = os.environ.get(""no_proxy"")     if no_proxy:         entries = [parse_host_port(x) for x in no_proxy.split("","")]         for host, port in entries:             <MASK>                 return True     return False",if host . lower ( ) == self . host and port == self . port :,if host == port :,3.2210586071561242,3.2210586071561242,0.0
"def run(self, _):     view = self.view     if not view.settings().get(""terminus_view""):         return     terminal = Terminal.from_id(view.id())     if terminal:         terminal.close()         panel_name = terminal.panel_name         <MASK>             window = panel_window(view)             if window:                 window.destroy_output_panel(panel_name)         else:             view.close()",if panel_name :,if panel_name :,100.00000000000004,100.00000000000004,1.0
"def get_docname_for_node(self, node: Node) -> str:     while node:         <MASK>             return self.env.path2doc(node[""source""])         elif isinstance(node, addnodes.start_of_file):             return node[""docname""]         else:             node = node.parent     return None  # never reached here. only for type hinting","if isinstance ( node , nodes . document ) :","if isinstance ( node , addnodes . start_of_file ) :",31.61487584488944,31.61487584488944,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             self.add_version(d.getPrefixedString())             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt == 0 :,100.00000000000004,100.00000000000004,1.0
"def _maybe_female(self, path_elements, female, strict):     if female:         if self.has_gender_differences:             elements = path_elements + [""female""]             try:                 return self._get_file(elements, "".png"", strict=strict)             except ValueError:                 <MASK>                     raise         elif strict:             raise ValueError(""Pokemon %s has no gender differences"" % self.species_id)     return self._get_file(path_elements, "".png"", strict=strict)",if strict :,if self . has_gender_differences :,5.669791110976001,5.669791110976001,0.0
"def OnKeyUp(self, event):     if self._properties.modifiable:         if event.GetKeyCode() == wx.WXK_ESCAPE:             self._cancel_editing()         elif event.GetKeyCode() == wx.WXK_RETURN:             self._update_value()         <MASK>             self.SetValue("""")     if event.GetKeyCode() != wx.WXK_RETURN:         # Don't send skip event if enter key is pressed         # On some platforms this event is sent too late and causes crash         event.Skip()",elif event . GetKeyCode ( ) == wx . WXK_DELETE :,if event . GetKeyCode ( ) == wx . WXK_ENTER :,77.4403141014203,77.4403141014203,0.0
"def sync_up_to_new_location(self, worker_ip):     if worker_ip != self.worker_ip:         logger.debug(""Setting new worker IP to %s"", worker_ip)         self.set_worker_ip(worker_ip)         self.reset()         <MASK>             logger.warning(""Sync up to new location skipped. This should not occur."")     else:         logger.warning(""Sync attempted to same IP %s."", worker_ip)",if not self . sync_up ( ) :,if self . new_location :,10.759051250985632,10.759051250985632,0.0
"def _get_download_link(self, url, download_type=""torrent""):     links = {         ""torrent"": """",         ""magnet"": """",     }     try:         data = self.session.get(url).text         with bs4_parser(data) as html:             downloads = html.find(""div"", {""class"": ""download""})             <MASK>                 for download in downloads.findAll(""a""):                     link = download[""href""]                     if link.startswith(""magnet""):                         links[""magnet""] = link                     else:                         links[""torrent""] = urljoin(self.urls[""base_url""], link)     except Exception:         pass     return links[download_type]",if downloads :,if downloads :,100.00000000000004,0.0,1.0
"def force_ipv4(self, *args):     """"""only ipv4 localhost in /etc/hosts""""""     logg.debug(""checking /etc/hosts for '::1 localhost'"")     lines = []     for line in open(self.etc_hosts()):         if ""::1"" in line:             newline = re.sub(""\\slocalhost\\s"", "" "", line)             <MASK>                 logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip())                 line = newline         lines.append(line)     f = open(self.etc_hosts(), ""w"")     for line in lines:         f.write(line)     f.close()",if line != newline :,"if ""::1"" in line :",6.27465531099474,6.27465531099474,0.0
"def prepare(self):     # Maybe the brok is a old daemon one or was already prepared     # if so, the data is already ok     if hasattr(self, ""prepared"") and not self.prepared:         self.data = SafeUnpickler.loads(self.data)         <MASK>             self.data[""instance_id""] = self.instance_id     self.prepared = True","if hasattr ( self , ""instance_id"" ) :",if self . instance_id :,14.231728394642222,14.231728394642222,0.0
"def _test_compute_q0(self):     # Stub code to search a logq space and figure out logq0 by eyeballing     # results. This code does not run with the tests. Remove underscore to run.     sigma = 15     order = 250     logqs = np.arange(-290, -270, 1)     count = 0     for logq in logqs:         count += 1         sys.stdout.write(             ""\t%0.5g: %0.10g"" % (logq, pate.rdp_gaussian(logq, sigma, order))         )         sys.stdout.flush()         <MASK>             print("""")",if count % 5 == 0 :,if count > 0 :,16.58165975077607,16.58165975077607,0.0
"def valid_fieldnames(fieldnames):     """"""check if fieldnames are valid""""""     for fieldname in fieldnames:         <MASK>             return True         elif fieldname in fieldname_map and fieldname_map[fieldname] == ""source"":             return True     return False","if fieldname in canonical_field_names and fieldname == ""source"" :","if fieldname_map [ fieldname ] == ""source"" :",35.93205036210866,35.93205036210866,0.0
"def ns_provide(self, id_):     global controllers, layouts     if id_ == ""_leo_viewrendered"":         c = self.c         vr = controllers.get(c.hash()) or ViewRenderedController(c)         h = c.hash()         controllers[h] = vr         <MASK>             layouts[h] = c.db.get(""viewrendered_default_layouts"", (None, None))         # return ViewRenderedController(self.c)         return vr",if not layouts . get ( h ) :,"if id_ == ""_leo_layout"" :",4.02724819242185,4.02724819242185,0.0
"def remove(self, path, config=None, error_on_path=False, defaults=None):     if not path:         if error_on_path:             raise NoSuchSettingsPath()         return     if config is not None or defaults is not None:         if config is None:             config = self._config         <MASK>             defaults = dict(self._map.parents)         chain = HierarchicalChainMap(config, defaults)     else:         chain = self._map     try:         chain.del_by_path(path)         self._mark_dirty()     except KeyError:         if error_on_path:             raise NoSuchSettingsPath()         pass",if defaults is None :,if defaults is None :,100.00000000000004,100.00000000000004,1.0
"def _mongo_query_and(self, queries):     if len(queries) == 1:         return queries[0]     query = {}     for q in queries:         for k, v in q.items():             if k not in query:                 query[k] = {}             <MASK>                 # TODO check exists of k in query, may be it should be update                 query[k] = v             else:                 query[k].update(v)     return query","if isinstance ( v , list ) :",if v is None :,7.715486568024961,7.715486568024961,0.0
"def write(self, data):     self.size -= len(data)     passon = None     if self.size > 0:         self.data.append(data)     else:         <MASK>             data, passon = data[: self.size], data[self.size :]         else:             passon = b""""         if data:             self.data.append(data)     return passon",if self . size :,if data :,17.799177396293473,0.0,0.0
"def updateVar(name, data, mode=None):     if mode:         if mode == ""append"":             core.config.globalVariables[name].append(data)         <MASK>             core.config.globalVariables[name].add(data)     else:         core.config.globalVariables[name] = data","elif mode == ""add"" :","if mode == ""add"" :",84.08964152537145,84.08964152537145,0.0
"def vi_pos_back_short(line, index=0, count=1):     line = vi_list(line)     try:         for i in range(count):             index -= 1             while vi_is_space(line[index]):                 index -= 1             in_word = vi_is_word(line[index])             <MASK>                 while vi_is_word(line[index]):                     index -= 1             else:                 while not vi_is_word_or_space(line[index]):                     index -= 1         return index + 1     except IndexError:         return 0",if in_word :,if in_word :,100.00000000000004,100.00000000000004,1.0
"def _truncate_to_length(generator, len_map=None):     for example in generator:         example = list(example)         <MASK>             for key, max_len in len_map.items():                 example_len = example[key].shape                 if example_len > max_len:                     example[key] = np.resize(example[key], max_len)         yield tuple(example)",if len_map is not None :,if len_map is not None :,100.00000000000004,100.00000000000004,1.0
"def decorate(f):     # call-signature of f is exposed via __wrapped__.     # we want it to mimic Obj.__init__     f.__wrapped__ = Obj.__init__     f._uses_signature = Obj     # Supplement the docstring of f with information from Obj     if Obj.__doc__:         doclines = Obj.__doc__.splitlines()         <MASK>             doc = f.__doc__ + ""\n"".join(doclines[1:])         else:             doc = ""\n"".join(doclines)         try:             f.__doc__ = doc         except AttributeError:             # __doc__ is not modifiable for classes in Python < 3.3             pass     return f",if f . __doc__ :,if len ( doclines ) > 1 :,5.795599612995366,5.795599612995366,0.0
"def IncrementErrorCount(self, category):     """"""Bumps the module's error statistic.""""""     self.error_count += 1     if self.counting in (""toplevel"", ""detailed""):         if self.counting != ""detailed"":             category = category.split(""/"")[0]         <MASK>             self.errors_by_category[category] = 0         self.errors_by_category[category] += 1",if category not in self . errors_by_category :,if category not in self . errors_by_category :,100.00000000000004,100.00000000000004,1.0
"def _delete_fields(self, data):     data = self._del(         data, [""speaker_ids"", ""track_id"", ""microlocation_id"", ""session_type_id""]     )     # convert datetime fields     for _ in [""start_time_tz"", ""end_time_tz""]:         <MASK>             data[_] = SESSION_POST[_[0:-3]].from_str(data[_])             data[_[0:-3]] = data.pop(_)     return data",if _ in data :,if _ in SESSION_POST :,26.269098944241588,26.269098944241588,0.0
"def get_strings_of_set(word, char_set, threshold=20):     count = 0     letters = """"     strings = []     for char in word:         if char in char_set:             letters += char             count += 1         else:             <MASK>                 strings.append(letters)             letters = """"             count = 0     if count > threshold:         strings.append(letters)     return strings",if count > threshold :,if count > threshold :,100.00000000000004,100.00000000000004,1.0
"def _ArgumentListHasDictionaryEntry(self, token):     """"""Check if the function argument list has a dictionary as an arg.""""""     if _IsArgumentToFunction(token):         while token:             if token.value == ""{"":                 length = token.matching_bracket.total_length - token.total_length                 return length + self.stack[-2].indent > self.column_limit             <MASK>                 break             if token.OpensScope():                 token = token.matching_bracket             token = token.next_token     return False",if token . ClosesScope ( ) :,"if token . value == ""}"" :",16.784459625186194,16.784459625186194,0.0
"def check_apns_certificate(ss):     mode = ""start""     for s in ss.split(""\n""):         if mode == ""start"":             if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s:                 mode = ""key""         elif mode == ""key"":             if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s:                 mode = ""end""                 break             <MASK>                 raise ImproperlyConfigured(                     ""Encrypted APNS private keys are not supported""                 )     if mode != ""end"":         raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")","elif s . startswith ( ""Proc-Type"" ) and ""ENCRYPTED"" in s :","if mode != ""key"" :",2.673705182447105,2.673705182447105,0.0
"def main(self):     self.model.clear()     self.callman.unregister_all()     active_handle = self.get_active(""Person"")     if active_handle:         active = self.dbstate.db.get_person_from_handle(active_handle)         <MASK>             self.callman.register_obj(active)             self.display_citations(active)         else:             self.set_has_data(False)     else:         self.set_has_data(False)",if active :,if active :,100.00000000000004,0.0,1.0
"def _validate(self) -> None:     # Paren validation and such     super(Tuple, self)._validate()     if len(self.elements) == 0:         <MASK>  # assumes len(lpar) == len(rpar), via superclass             raise CSTValidationError(                 ""A zero-length tuple must be wrapped in parentheses.""             )",if len ( self . lpar ) == 0 :,if len ( self . elements ) == 0 :,70.16879391277372,70.16879391277372,0.0
"def _session_from_arg(self, session_obj, lock_type=None):     if not isinstance(session_obj, self.ISession):         vm = self._machine_from_arg(session_obj)         lock_type = lock_type or self.LockType.null         <MASK>             return vm.create_session(lock_type)         return None     return session_obj",if vm :,if lock_type :,12.703318703865365,12.703318703865365,0.0
"def _decorator(cls):     for name, meth in inspect.getmembers(cls, inspect.isroutine):         if name not in cls.__dict__:             continue         <MASK>             if not private and name.startswith(""_""):                 continue         if name in butnot:             continue         setattr(cls, name, decorator(meth))     return cls","if name != ""__init__"" :",if meth is not None :,3.550932348642477,3.550932348642477,0.0
"def pdb(message=""""):     """"""Fall into pdb.""""""     import pdb  # Required: we have just defined pdb as a function!     if app and not app.useIpython:         # from leo.core.leoQt import QtCore         # This is more portable.         try:             import PyQt5.QtCore as QtCore         except ImportError:             try:                 import PyQt4.QtCore as QtCore             except ImportError:                 QtCore = None         <MASK>             # pylint: disable=no-member             QtCore.pyqtRemoveInputHook()     if message:         print(message)     pdb.set_trace()",if QtCore :,if QtCore . pyqtRemoveInputHook :,23.643540225079384,23.643540225079384,0.0
"def get_s3_bucket_locations(buckets, self_log=False):     """"""return (bucket_name, prefix) for all s3 logging targets""""""     for b in buckets:         if b.get(""Logging""):             <MASK>                 if b[""Name""] != b[""Logging""][""TargetBucket""]:                     continue             yield (b[""Logging""][""TargetBucket""], b[""Logging""][""TargetPrefix""])         if not self_log and b[""Name""].startswith(""cf-templates-""):             yield (b[""Name""], """")",if self_log :,"if b [ ""Name"" ] != ""cf-templates-"" :",3.673526562988939,3.673526562988939,0.0
"def prepare_fields(self):     # See clean()     for k, v in self.fields.items():         v._required = v.required         v.required = False         v.widget.is_required = False         <MASK>             v._required = v.one_required             v.one_required = False             v.widget.enabled_locales = self.locales","if isinstance ( v , I18nFormField ) :","if k == ""one_required"" :",4.990049701936832,4.990049701936832,0.0
"def __pack__(self):     new_values = []     for i in xrange(len(self.__unpacked_data_elms__)):         for key in self.__keys__[i]:             new_val = getattr(self, key)             old_val = self.__unpacked_data_elms__[i]             # In the case of Unions, when the first changed value             # is picked the loop is exited             <MASK>                 break         new_values.append(new_val)     return struct.pack(self.__format__, *new_values)",if new_val != old_val :,if old_val != new_val :,51.33450480401705,51.33450480401705,0.0
"def run(self):     pwd_found = []     if constant.user_dpapi and constant.user_dpapi.unlocked:         main_vault_directory = os.path.join(             constant.profile[""APPDATA""], u"".."", u""Local"", u""Microsoft"", u""Vault""         )         <MASK>             for vault_directory in os.listdir(main_vault_directory):                 cred = constant.user_dpapi.decrypt_vault(                     os.path.join(main_vault_directory, vault_directory)                 )                 if cred:                     pwd_found.append(cred)     return pwd_found",if os . path . exists ( main_vault_directory ) :,if main_vault_directory :,24.601580968354618,24.601580968354618,0.0
"def on_revision_plugin_revision_pre_save(**kwargs):     instance = kwargs[""instance""]     if kwargs.get(""created"", False):         update_previous_revision = (             not instance.previous_revision             and instance.plugin             and instance.plugin.current_revision             and instance.plugin.current_revision != instance         )         <MASK>             instance.previous_revision = instance.plugin.current_revision     if not instance.revision_number:         try:             previous_revision = instance.plugin.revision_set.latest()             instance.revision_number = previous_revision.revision_number + 1         except RevisionPluginRevision.DoesNotExist:             instance.revision_number = 1",if update_previous_revision :,if update_previous_revision :,100.00000000000004,100.00000000000004,1.0
"def __setattr__(self, name, value):     super().__setattr__(name, value)     field = self._fields.get(name)     if field:         self.check_field_type(field, value)         <MASK>             raise TypeError(f""cannot set immutable {name} on {self!r}"")",if name in self . __ast_frozen_fields__ :,if not self . _is_immutable :,10.83584318341755,10.83584318341755,0.0
"def _check_for_req_data(data):     required_args = [""columns""]     for arg in required_args:         <MASK>             return True, make_json_response(                 status=400,                 success=0,                 errormsg=gettext(""Could not find required parameter ({})."").format(arg),             )     return False, """"","if arg not in data or ( isinstance ( data [ arg ] , list ) and len ( data [ arg ] ) < 1 ) :",if data [ arg ] == arg :,4.15741108465639,4.15741108465639,0.0
"def train_dict(self, triples):     """"""Train a dict lemmatizer given training (word, pos, lemma) triples.""""""     # accumulate counter     ctr = Counter()     ctr.update([(p[0], p[1], p[2]) for p in triples])     # find the most frequent mappings     for p, _ in ctr.most_common():         w, pos, l = p         <MASK>             self.composite_dict[(w, pos)] = l         if w not in self.word_dict:             self.word_dict[w] = l     return","if ( w , pos ) not in self . composite_dict :",if p in self . composite_dict :,43.067300377637174,43.067300377637174,0.0
"def render(type_, obj, context):     if type_ == ""foreign_key"":         return None     if type_ == ""column"":         if obj.name == ""y"":             return None         <MASK>             return False         else:             return ""col(%s)"" % obj.name     if type_ == ""type"" and isinstance(obj, MySpecialType):         context.imports.add(""from mypackage import MySpecialType"")         return ""MySpecialType()""     return ""render:%s"" % type_","elif obj . name == ""q"" :","if obj . name == ""col"" :",58.14307369682194,58.14307369682194,0.0
"def test_knows_when_stepping_back_possible(self):     iterator = bidirectional_iterator.BidirectionalIterator([0, 1, 2, 3])     commands = [0, 1, 0, 0, 1, 1, 0, 0, 0, 0]     command_count = 0     results = []     for _ in iterator:         <MASK>             iterator.step_back_on_next_iteration()         results.append(iterator.can_step_back())         command_count += 1     assert results == [False, True, False, True, True, True, False, True, True, True]",if commands [ command_count ] :,if command_count == 1 :,23.356898886410015,23.356898886410015,0.0
"def flask_debug_true(context):     if context.is_module_imported_like(""flask""):         if context.call_function_name_qual.endswith("".run""):             <MASK>                 return bandit.Issue(                     severity=bandit.HIGH,                     confidence=bandit.MEDIUM,                     text=""A Flask app appears to be run with debug=True, ""                     ""which exposes the Werkzeug debugger and allows ""                     ""the execution of arbitrary code."",                     lineno=context.get_lineno_for_call_arg(""debug""),                 )","if context . check_call_arg_value ( ""debug"" , ""True"" ) :","if context . call_function_name_qual . endswith ( ""debug"" ) :",24.54908011346863,24.54908011346863,0.0
"def __exit__(self, exc_type, exc_val, exc_tb):     if self._should_meta_profile:         end_time = timezone.now()         exception_raised = exc_type is not None         if exception_raised:             Logger.error(                 ""Exception when performing meta profiling, dumping trace below""             )             traceback.print_exception(exc_type, exc_val, exc_tb)         request = getattr(DataCollector().local, ""request"", None)         <MASK>             curr = request.meta_time or 0             request.meta_time = curr + _time_taken(self.start_time, end_time)",if request :,if request :,100.00000000000004,0.0,1.0
"def get_job_offer(ja_list):     ja_joff_map = {}     offers = frappe.get_all(         ""Job Offer"",         filters=[[""job_applicant"", ""IN"", ja_list]],         fields=[""name"", ""job_applicant"", ""status"", ""offer_date"", ""designation""],     )     for offer in offers:         <MASK>             ja_joff_map[offer.job_applicant] = [offer]         else:             ja_joff_map[offer.job_applicant].append(offer)     return ja_joff_map",if offer . job_applicant not in ja_joff_map . keys ( ) :,if offer . job_applicant not in ja_joff_map :,70.37688239435663,70.37688239435663,0.0
"def _get_deepest(self, t):     if isinstance(t, list):         <MASK>             return t[0]         else:             for part in t:                 res = self._get_deepest(part)                 if res:                     return res             return None     return None",if len ( t ) == 1 :,if len ( t ) == 1 :,100.00000000000004,100.00000000000004,1.0
"def test_main(self):     root = os.path.dirname(mutagen.__path__[0])     skip = [os.path.join(root, ""docs""), os.path.join(root, ""venv"")]     for dirpath, dirnames, filenames in os.walk(root):         <MASK>             continue         for filename in filenames:             if filename.endswith("".py""):                 path = os.path.join(dirpath, filename)                 self._check_encoding(path)",if any ( ( dirpath . startswith ( s + os . sep ) or s == dirpath ) for s in skip ) :,if dirnames not in skip :,0.6886817952024693,0.6886817952024693,0.0
"def xview(self, mode=None, value=None, units=None):     if type(value) == str:         value = float(value)     if mode is None:         return self.hsb.get()     elif mode == ""moveto"":         frameWidth = self.innerframe.winfo_reqwidth()         self._startX = value * float(frameWidth)     else:  # mode == 'scroll'         clipperWidth = self._clipper.winfo_width()         <MASK>             jump = int(clipperWidth * self._jfraction)         else:             jump = clipperWidth         self._startX = self._startX + value * jump     self.reposition()","if units == ""units"" :","if mode == ""jfraction"" :",27.054113452696992,27.054113452696992,0.0
"def test_training_script_with_max_history_set(tmpdir):     train_dialogue_model(         DEFAULT_DOMAIN_PATH,         DEFAULT_STORIES_FILE,         tmpdir.strpath,         interpreter=RegexInterpreter(),         policy_config=""data/test_config/max_hist_config.yml"",         kwargs={},     )     agent = Agent.load(tmpdir.strpath)     for policy in agent.policy_ensemble.policies:         <MASK>             if type(policy) == FormPolicy:                 assert policy.featurizer.max_history == 2             else:                 assert policy.featurizer.max_history == 5","if hasattr ( policy . featurizer , ""max_history"" ) :",if type ( policy ) == TextPolicy :,6.87938864869854,6.87938864869854,0.0
"def generate_auto_complete(self, base, iterable_var):     sugg = []     for entry in iterable_var:         compare_entry = entry         compare_base = base         <MASK>             compare_entry = compare_entry.lower()             compare_base = compare_base.lower()         if self.compare_entries(compare_entry, compare_base):             if entry not in sugg:                 sugg.append(entry)     return sugg",if self . settings . get ( IGNORE_CASE_SETTING ) :,if compare_base is not None :,3.4331054109918173,3.4331054109918173,0.0
"def marker_expr(remaining):     if remaining and remaining[0] == ""("":         result, remaining = marker(remaining[1:].lstrip())         <MASK>             raise SyntaxError(""unterminated parenthesis: %s"" % remaining)         remaining = remaining[1:].lstrip()     else:         lhs, remaining = marker_var(remaining)         while remaining:             m = MARKER_OP.match(remaining)             if not m:                 break             op = m.groups()[0]             remaining = remaining[m.end() :]             rhs, remaining = marker_var(remaining)             lhs = {""op"": op, ""lhs"": lhs, ""rhs"": rhs}         result = lhs     return result, remaining","if remaining [ 0 ] != "")"" :","if remaining [ 0 ] == "")"" :",70.16879391277372,70.16879391277372,0.0
"def __repr__(self):     """"""Dump the class data in the format of a .netrc file.""""""     rep = """"     for host in self.hosts.keys():         attrs = self.hosts[host]         rep = rep + ""machine "" + host + ""\n\tlogin "" + repr(attrs[0]) + ""\n""         <MASK>             rep = rep + ""account "" + repr(attrs[1])         rep = rep + ""\tpassword "" + repr(attrs[2]) + ""\n""     for macro in self.macros.keys():         rep = rep + ""macdef "" + macro + ""\n""         for line in self.macros[macro]:             rep = rep + line         rep = rep + ""\n""     return rep",if attrs [ 1 ] :,if attrs [ 1 ] :,100.00000000000004,100.00000000000004,1.0
"def _parse_policies(self, policies_yaml):     for item in policies_yaml:         id_ = required_key(item, ""id"")         controls_ids = required_key(item, ""controls"")         <MASK>             if controls_ids != ""all"":                 msg = ""Policy {id_} contains invalid controls list {controls}."".format(                     id_=id_, controls=str(controls_ids)                 )                 raise ValueError(msg)         self.policies[id_] = controls_ids","if not isinstance ( controls_ids , list ) :",if id_ not in self . policies :,5.3990167242108145,5.3990167242108145,0.0
"def __set__(self, obj, value):  # noqa     if (         value is not None         and self.field._currency_field.null         and not isinstance(value, MONEY_CLASSES + (Decimal,))     ):         # For nullable fields we need either both NULL amount and currency or both NOT NULL         raise ValueError(""Missing currency value"")     if isinstance(value, BaseExpression):         <MASK>             value = self.prepare_value(obj, value.value)         elif not isinstance(value, Func):             validate_money_expression(obj, value)             prepare_expression(value)     else:         value = self.prepare_value(obj, value)     obj.__dict__[self.field.name] = value","if isinstance ( value , Value ) :","if isinstance ( value , Money ) :",59.4603557501361,59.4603557501361,0.0
"def Children(self):     """"""Returns a list of all of this object's owned (strong) children.""""""     children = []     for property, attributes in self._schema.iteritems():         (is_list, property_type, is_strong) = attributes[0:3]         <MASK>             if not is_list:                 children.append(self._properties[property])             else:                 children.extend(self._properties[property])     return children",if is_strong and property in self . _properties :,if not is_strong :,12.629099764064142,12.629099764064142,0.0
"def next_item(self, direction):     """"""Selects next menu item, based on self._direction""""""     start, i = -1, 0     try:         start = self.items.index(self._selected)         i = start + direction     except:         pass     while True:         if i == start:             # Cannot find valid menu item             self.select(start)             break         if i >= len(self.items):             i = 0             continue         if i < 0:             i = len(self.items) - 1             continue         if self.select(i):             break         i += direction         <MASK>             start = 0",if start < 0 :,if i >= start :,10.682175159905853,10.682175159905853,0.0
"def setup_displace(self):     self.displace_mod = None     self.displace_strength = 0.020     for mod in self.obj.modifiers:         <MASK>             self.displace_mod = mod             self.displace_strength = mod.strength     if not self.displace_mod:         bpy.ops.object.modifier_add(type=""DISPLACE"")         self.displace_mod = self.obj.modifiers[-1]         self.displace_mod.show_expanded = False         self.displace_mod.strength = self.displace_strength         self.displace_mod.show_render = False         self.displace_mod.show_viewport = False","if mod . type == ""DISPLACE"" :","if mod . type == ""DISPLACE"" :",100.00000000000004,100.00000000000004,1.0
"def set_json_body(cls, request_builder):     old_body = request_builder.info.pop(""data"", {})     if isinstance(old_body, abc.Mapping):         body = request_builder.info.setdefault(""json"", {})         for path in old_body:             <MASK>                 cls._sequence_path_resolver(path, old_body[path], body)             else:                 body[path] = old_body[path]     else:         request_builder.info.setdefault(""json"", old_body)","if isinstance ( path , tuple ) :",if path in body :,7.715486568024961,7.715486568024961,0.0
"def build(opt):     dpath = os.path.join(opt[""datapath""], ""DBLL"")     version = None     if not build_data.built(dpath, version_string=version):         print(""[building data: "" + dpath + ""]"")         <MASK>             # An older version exists, so remove these outdated files.             build_data.remove_dir(dpath)         build_data.make_dir(dpath)         # Download the data.         for downloadable_file in RESOURCES:             downloadable_file.download_file(dpath)         # Mark the data as built.         build_data.mark_done(dpath, version_string=version)",if build_data . built ( dpath ) :,if version_string in VERSIONS :,5.630400552901077,5.630400552901077,0.0
"def test_prefix_lm(self):     num_tries = 100     original = ""This is a long test with lots of words to see if it works ok.""     dataset = tf.data.Dataset.from_tensor_slices({""text"": [original] * num_tries})     dataset = prep.prefix_lm(dataset)     for data in test_utils.dataset_as_text(dataset):         inputs = data[""inputs""].replace(""prefix: "", """")         targets = data[""targets""]         reconstructed = """".join(inputs)         <MASK>             reconstructed += "" ""         reconstructed += """".join(targets)         self.assertEqual(reconstructed, original)",if inputs :,if targets :,34.66806371753173,0.0,0.0
"def leading_whitespace(self, inputstring):     """"""Get leading whitespace.""""""     leading_ws = []     for i, c in enumerate(inputstring):         if c in legal_indent_chars:             leading_ws.append(c)         else:             break         <MASK>             self.indchar = c         elif c != self.indchar:             self.strict_err_or_warn(""found mixing of tabs and spaces"", inputstring, i)     return """".join(leading_ws)",if self . indchar is None :,if i == 0 :,8.170609724417774,8.170609724417774,0.0
"def __init__(self, text):     self.mappings = {}     self.attributes = collections.defaultdict(set)     for stanza in _ParseTextProperties(text):         processor_id, single_values, multiple_values = self._ParseStanza(stanza)         if processor_id is None:  # can be 0             continue         <MASK>             logging.warn(""Processor id %s seen twice in %s"", processor_id, text)             continue         self.mappings[processor_id] = single_values         for key, value in multiple_values.items():             self.attributes[key].add(value)",if processor_id in self . mappings :,if processor_id in self . mappings :,100.00000000000004,100.00000000000004,1.0
"def __iter__(self):     for chunk in self.source:         <MASK>             self.wait_counter = 0             yield chunk         elif self.wait_counter < self.wait_cntr_max:             self.wait_counter += 1         else:             logger.warning(                 ""Data poller has been receiving no data for {} seconds.\n""                 ""Closing data poller"".format(self.wait_cntr_max * self.poll_period)             )             break         time.sleep(self.poll_period)",if chunk is not None :,if self . wait_counter > self . wait_cntr_max :,3.1251907639724417,3.1251907639724417,0.0
"def download(self, prefetch=False):     while self.running:         try:             <MASK>                 (path, start, end) = self.prefetch_queue.get(                     True, 1                 )  # 1 second time-out             else:                 (path, start, end) = self.download_queue.get(                     True, 1                 )  # 1 second time-out             self.download_data(path, start, end)             if prefetch:                 self.prefetch_queue.task_done()             else:                 self.download_queue.task_done()         except Queue.Empty:             pass",if prefetch :,if prefetch :,100.00000000000004,0.0,1.0
"def process_messages(self, found_files, messages):     for message in messages:         <MASK>             message.to_absolute_path(self.config.workdir)         else:             message.to_relative_path(self.config.workdir)     if self.config.blending:         messages = blender.blend(messages)     filepaths = found_files.iter_module_paths(abspath=False)     return postfilter.filter_messages(filepaths, self.config.workdir, messages)",if self . config . absolute_paths :,if self . config . relative :,48.35447404743731,48.35447404743731,0.0
"def set_indentation_params(self, ispythonsource, guess=1):     if guess and ispythonsource:         i = self.guess_indent()         <MASK>             self.indentwidth = i         if self.indentwidth != self.tabwidth:             self.usetabs = 0     self.editwin.set_tabwidth(self.tabwidth)",if 2 <= i <= 8 :,if i :,5.370784274455332,0.0,0.0
"def to_tree(self, tagname=None, value=None, namespace=None):     namespace = getattr(self, ""namespace"", namespace)     if value is not None:         <MASK>             tagname = ""{%s}%s"" % (namespace, tagname)         el = Element(tagname)         el.text = safe_string(value)         return el",if namespace is not None :,if tagname is not None :,53.7284965911771,53.7284965911771,0.0
"def execute(self, argv: List) -> bool:     if not argv:         print(""ERROR: You must give at least one module to download."")         return False     for _arg in argv:         result = module_server.search_module(_arg)         CacheUpdater(""hub_download"", _arg).start()         <MASK>             url = result[0][""url""]             with log.ProgressBar(""Download {}"".format(url)) as bar:                 for file, ds, ts in utils.download_with_progress(url):                     bar.update(float(ds) / ts)         else:             print(""ERROR: Could not find a HubModule named {}"".format(_arg))     return True",if result :,if result :,100.00000000000004,0.0,1.0
"def visit_type_type(self, t: TypeType) -> ProperType:     if isinstance(self.s, TypeType):         typ = self.meet(t.item, self.s.item)         <MASK>             typ = TypeType.make_normalized(typ, line=t.line)         return typ     elif isinstance(self.s, Instance) and self.s.type.fullname == ""builtins.type"":         return t     elif isinstance(self.s, CallableType):         return self.meet(t, self.s)     else:         return self.default(self.s)","if not isinstance ( typ , NoneType ) :",if typ :,5.370784274455332,0.0,0.0
"def run(self, paths=[]):     items = []     for item in SideBarSelection(paths).getSelectedItems():         items.append(item.name())     if len(items) > 0:         sublime.set_clipboard(""\n"".join(items))         <MASK>             sublime.status_message(""Items copied"")         else:             sublime.status_message(""Item copied"")",if len ( items ) > 1 :,if item . is_selected ( ) :,6.742555929751843,6.742555929751843,0.0
"def get_icon(self):     if self.icon is not None:         # Load it from an absolute filename         <MASK>             try:                 return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24)             except GObject.GError as ge:                 pass         # Load it from the current icon theme         (icon_name, extension) = os.path.splitext(os.path.basename(self.icon))         theme = Gtk.IconTheme()         if theme.has_icon(icon_name):             return theme.load_icon(icon_name, 24, 0)",if os . path . exists ( self . icon ) :,"if self . icon . endswith ( "".png"" ) :",16.74765944848823,16.74765944848823,0.0
"def setup_logger():     """"""Set up logger and add stdout handler""""""     logging.setLoggerClass(IPDLogger)     logger = logging.getLogger(""icloudpd"")     has_stdout_handler = False     for handler in logger.handlers:         <MASK>             has_stdout_handler = True     if not has_stdout_handler:         formatter = logging.Formatter(             fmt=""%(asctime)s %(levelname)-8s %(message)s"", datefmt=""%Y-%m-%d %H:%M:%S""         )         stdout_handler = logging.StreamHandler(stream=sys.stdout)         stdout_handler.setFormatter(formatter)         stdout_handler.name = ""stdoutLogger""         logger.addHandler(stdout_handler)     return logger","if handler . name == ""stdoutLogger"" :","if handler . name == ""stdoutLogger"" :",100.00000000000004,100.00000000000004,1.0
"def process_extra_fields(self):     if self.instance.pk is not None:         if self.cleaned_data.get(""initialize"", None):             self.instance.initialize()         <MASK>             self.instance.update_from_templates()","if self . cleaned_data . get ( ""update"" , None ) or not self . instance . stores . count ( ) :","if self . cleaned_data . get ( ""update_from_templates"" , None ) :",46.5684173837697,46.5684173837697,0.0
"def testFunctions(self):     from zim.formats.wiki import match_url, is_url     for input, input_is_url, tail in self.examples:         if input_is_url:             <MASK>                 self.assertEqual(match_url(input), input[: -len(tail)])                 self.assertFalse(is_url(input))             else:                 self.assertEqual(match_url(input), input)                 self.assertTrue(is_url(input))         else:             self.assertEqual(match_url(input), None)             self.assertFalse(is_url(input))",if tail :,if tail :,100.00000000000004,0.0,1.0
"def _SetUser(self, users):     for user in users.items():         username = user[0]         settings = user[1]         room = settings[""room""][""name""] if ""room"" in settings else None         file_ = settings[""file""] if ""file"" in settings else None         <MASK>             if ""joined"" in settings[""event""]:                 self._client.userlist.addUser(username, room, file_)             elif ""left"" in settings[""event""]:                 self._client.removeUser(username)         else:             self._client.userlist.modUser(username, room, file_)","if ""event"" in settings :","if ""right"" in settings [ ""event"" ] :",23.29769207594981,23.29769207594981,0.0
"def restoreTerminals(self, state):     for name in list(self.terminals.keys()):         <MASK>             self.removeTerminal(name)     for name, opts in state.items():         if name in self.terminals:             term = self[name]             term.setOpts(**opts)             continue         try:             opts = strDict(opts)             self.addTerminal(name, **opts)         except:             printExc(""Error restoring terminal %s (%s):"" % (str(name), str(opts)))",if name not in state :,if name in state :,40.93653765389909,40.93653765389909,0.0
"def htmlify(path, text):     fname = os.path.basename(path)     if any((fnmatch.fnmatchcase(fname, p) for p in _patterns)):         # Get file_id, skip if not in database         sql = ""SELECT files.id FROM files WHERE path = ? LIMIT 1""         row = _conn.execute(sql, (path,)).fetchone()         <MASK>             return ClangHtmlifier(_tree, _conn, path, text, row[0])     return None",if row :,if row :,100.00000000000004,0.0,1.0
"def autoformat_filter_conv2d(fsize, in_depth, out_depth):     if isinstance(fsize, int):         return [fsize, fsize, in_depth, out_depth]     elif isinstance(fsize, (tuple, list, tf.TensorShape)):         <MASK>             return [fsize[0], fsize[1], in_depth, out_depth]         else:             raise Exception(                 ""filter length error: ""                 + str(len(fsize))                 + "", only a length of 2 is supported.""             )     else:         raise Exception(""filter format error: "" + str(type(fsize)))",if len ( fsize ) == 2 :,if len ( fsize ) == 2 :,100.00000000000004,100.00000000000004,1.0
"def _rle_encode(string):     new = b""""     count = 0     for cur in string:         <MASK>             count += 1         else:             if count:                 new += b""\0"" + bytes([count])                 count = 0             new += bytes([cur])     return new",if not cur :,"if cur == b""\0"" :",5.522397783539471,5.522397783539471,0.0
"def is_clean(self):     acceptable_statuses = {""external"", ""unversioned""}     root = self._capture_output(""status"", ""--quiet"")     for elem in root.findall(""./target/entry""):         status = elem.find(""./wc-status"")         <MASK>             continue         log.debug(""Path %s is %s"", elem.get(""path""), status.get(""item""))         return False     return True","if status . get ( ""item"" , None ) in acceptable_statuses :","if not status or status . get ( ""item"" ) not in acceptable_statuses :",54.9995519064483,54.9995519064483,0.0
"def process(self, body, message):     try:         <MASK>             raise TypeError(                 'Received an unexpected type ""%s"" for payload.' % type(body)             )         response = self._handler.pre_ack_process(body)         self._dispatcher.dispatch(self._process_message, response)     except:         LOG.exception(""%s failed to process message: %s"", self.__class__.__name__, body)     finally:         # At this point we will always ack a message.         message.ack()","if not isinstance ( body , self . _handler . message_type ) :","if not isinstance ( body , dict ) :",30.35117977459125,30.35117977459125,0.0
"def page_file(self, page):     try:         page = self.notebook.get_page(page)         <MASK>             return page.source         else:             return None     except PageNotFoundError:         return None","if hasattr ( page , ""source"" ) and isinstance ( page . source , File ) :",if page :,0.19159732247644734,0.0,0.0
"def _optimize(self, solutions):     best_a = None     best_silhouette = None     best_k = None     for a, silhouette, k in solutions():         <MASK>             pass         elif silhouette <= best_silhouette:             break         best_silhouette = silhouette         best_a = a         best_k = k     return best_a, best_silhouette, best_k",if best_silhouette is None :,if a <= best_silhouette :,23.356898886410015,23.356898886410015,0.0
"def _cancel_tasks_for_partitions(self, to_cancel_partitions):     # type: (Iterable[str]) -> None     with self._lock:         _LOGGER.debug(             ""EventProcessor %r tries to cancel partitions %r"",             self._id,             to_cancel_partitions,         )         for partition_id in to_cancel_partitions:             <MASK>                 self._consumers[partition_id].stop = True                 _LOGGER.info(                     ""EventProcessor %r has cancelled partition %r"",                     self._id,                     partition_id,                 )",if partition_id in self . _consumers :,if partition_id in self . _consumers :,100.00000000000004,100.00000000000004,1.0
"def get_intersect_all(self, refine=False):     result = None     for source, parts in self._per_source.items():         <MASK>             result = parts         else:             result.intersection_update(parts)     if not result:         return None     elif len(result) == 1:         return list(result)[0].item     else:         solids = [p.item for p in result]         solid = solids[0].fuse(solids[1:])         if refine:             solid = solid.removeSplitter()         return solid",if result is None :,if source == self . _per_source :,4.456882760699063,4.456882760699063,0.0
"def geli_detach(self, pool, clear=False):     failed = 0     for ed in self.middleware.call_sync(         ""datastore.query"",         ""storage.encrypteddisk"",         [(""encrypted_volume"", ""="", pool[""id""])],     ):         dev = ed[""encrypted_provider""]         try:             self.geli_detach_single(dev)         except Exception as ee:             self.logger.warn(str(ee))             failed += 1         <MASK>             try:                 self.geli_clear(dev)             except Exception as e:                 self.logger.warn(""Failed to clear %s: %s"", dev, e)     return failed",if clear :,if clear :,100.00000000000004,0.0,1.0
def compute_lengths(batch_sizes):     tmp_batch_sizes = np.copy(batch_sizes)     lengths = []     while True:         c = np.count_nonzero(tmp_batch_sizes > 0)         <MASK>             break         lengths.append(c)         tmp_batch_sizes = np.array([b - 1 for b in tmp_batch_sizes])     return np.array(lengths),if c == 0 :,if c < 0 :,24.736929544091932,24.736929544091932,0.0
"def _render_raw_list(bytes_items):     flatten_items = []     for item in bytes_items:         <MASK>             flatten_items.append(b"""")         elif isinstance(item, bytes):             flatten_items.append(item)         elif isinstance(item, int):             flatten_items.append(str(item).encode())         elif isinstance(item, list):             flatten_items.append(_render_raw_list(item))     return b""\n"".join(flatten_items)",if item is None :,"if isinstance ( item , bytes ) :",7.267884212102741,7.267884212102741,0.0
"def update(self, new_config):     jsonschema.validate(new_config, self.schema)     config = {}     for k, v in new_config.items():         <MASK>             config[k] = self[k]         else:             config[k] = v     self._config = config     self.changed()","if k in self . schema . get ( ""secret"" , [ ] ) and v == SECRET_PLACEHOLDER :",if v is None :,0.3145002372781782,0.3145002372781782,0.0
"def _encode_numpy(values, uniques=None, encode=False, check_unknown=True):     # only used in _encode below, see docstring there for details     if uniques is None:         if encode:             uniques, encoded = np.unique(values, return_inverse=True)             return uniques, encoded         else:             # unique sorts             return np.unique(values)     if encode:         <MASK>             diff = _encode_check_unknown(values, uniques)             if diff:                 raise ValueError(""y contains previously unseen labels: %s"" % str(diff))         encoded = np.searchsorted(uniques, values)         return uniques, encoded     else:         return uniques",if check_unknown :,if check_unknown :,100.00000000000004,100.00000000000004,1.0
"def restore_dtype_and_merge(arr, input_dtype):     if isinstance(arr, list):         arr = [restore_dtype_and_merge(arr_i, input_dtype) for arr_i in arr]         shapes = [arr_i.shape for arr_i in arr]         <MASK>             arr = np.array(arr)     if ia.is_np_array(arr):         arr = iadt.restore_dtypes_(arr, input_dtype)     return arr",if len ( set ( shapes ) ) == 1 :,if shapes :,1.9758011175389976,0.0,0.0
"def proc_minute(d):     if expanded[0][0] != ""*"":         diff_min = nearest_diff_method(d.minute, expanded[0], 60)         if diff_min is not None and diff_min != 0:             <MASK>                 d += relativedelta(minutes=diff_min, second=59)             else:                 d += relativedelta(minutes=diff_min, second=0)             return True, d     return False, d",if is_prev :,if diff_min < 60 :,8.643019616048525,8.643019616048525,0.0
"def _populate_tree(self, element, d):     """"""Populates an etree with attributes & elements, given a dict.""""""     for k, v in d.iteritems():         <MASK>             self._populate_dict(element, k, v)         elif isinstance(v, list):             self._populate_list(element, k, v)         elif isinstance(v, bool):             self._populate_bool(element, k, v)         elif isinstance(v, basestring):             self._populate_str(element, k, v)         elif type(v) in [int, float, long, complex]:             self._populate_number(element, k, v)","if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",100.00000000000004,100.00000000000004,1.0
"def __createItemAttribute(self, item, function, preload):     """"""Create the new widget, add it, and remove the old one""""""     try:         self.__stack.addWidget(function(item, preload))         # Remove the widget         <MASK>             oldWidget = self.__stack.widget(0)             self.__stack.removeWidget(oldWidget)             oldWidget.setParent(QtWidgets.QWidget())     except Exception as e:         list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",if self . __stack . count ( ) > 1 :,if oldWidget is not None :,3.005799339448764,3.005799339448764,0.0
"def download_main(     download, download_playlist, urls, playlist, output_dir, merge, info_only ):     for url in urls:         <MASK>             url = url[8:]         if not url.startswith(""http://""):             url = ""http://"" + url         if playlist:             download_playlist(                 url, output_dir=output_dir, merge=merge, info_only=info_only             )         else:             download(url, output_dir=output_dir, merge=merge, info_only=info_only)","if url . startswith ( ""https://"" ) :",if len ( url ) > 8 :,4.420141128732569,4.420141128732569,0.0
"def add_enc_zero(obj, enc_zero):     if isinstance(obj, np.ndarray):         return obj + enc_zero     elif isinstance(obj, Iterable):         return type(obj)(             EncryptModeCalculator.add_enc_zero(o, enc_zero)             <MASK>             else o + enc_zero             for o in obj         )     else:         return obj + enc_zero","if isinstance ( o , Iterable )",if o is not None :,8.170609724417774,8.170609724417774,0.0
"def ensemble(self, pairs, other_preds):     """"""Ensemble the dict with statistical model predictions.""""""     lemmas = []     assert len(pairs) == len(other_preds)     for p, pred in zip(pairs, other_preds):         w, pos = p         if (w, pos) in self.composite_dict:             lemma = self.composite_dict[(w, pos)]         elif w in self.word_dict:             lemma = self.word_dict[w]         else:             lemma = pred         <MASK>             lemma = w         lemmas.append(lemma)     return lemmas",if lemma is None :,if pred is None :,42.72870063962342,42.72870063962342,0.0
"def replace_to_6hex(color):     """"""Validate and replace 3hex colors to 6hex ones.""""""     if match(r""^#(?:[0-9a-fA-F]{3}){1,2}$"", color):         <MASK>             color = ""#{0}{0}{1}{1}{2}{2}"".format(color[1], color[2], color[3])         return color     else:         exit(_(""Invalid color {}"").format(color))",if len ( color ) == 4 :,"if match ( r""^#[0-9a-fA-F]{3}"" , color ) :",4.814971807094068,4.814971807094068,0.0
"def computeMachineName(self):     """"""Return the name of the current machine, i.e, HOSTNAME.""""""     # This is prepended to leoSettings.leo or myLeoSettings.leo     # to give the machine-specific setting name.     # How can this be worth doing??     try:         import os         name = os.getenv(""HOSTNAME"")         <MASK>             name = os.getenv(""COMPUTERNAME"")         if not name:             import socket             name = socket.gethostname()     except Exception:         name = """"     return name",if not name :,if not name :,100.00000000000004,100.00000000000004,1.0
"def _git_dirty_working_directory(q, include_untracked):     try:         cmd = [""git"", ""status"", ""--porcelain""]         if include_untracked:             cmd += [""--untracked-files=normal""]         else:             cmd += [""--untracked-files=no""]         status = _run_git_cmd(cmd)         <MASK>             q.put(bool(status))         else:             q.put(None)     except (subprocess.CalledProcessError, OSError, FileNotFoundError):         q.put(None)",if status is not None :,if status :,23.174952587773145,0.0,0.0
"def runAndWaitWork(server, work):     work.touch()     thr = threading.Thread(target=workThread, args=(server, work))     thr.setDaemon(True)     thr.start()     # Wait around for done or timeout     while True:         if work.isTimedOut():             break         # If the thread is done, lets get out.         if not thr.isAlive():             break         # If our parent, or some thread closes stdin,         # time to pack up and go.         <MASK>             break         time.sleep(2)",if sys . stdin . closed :,if not thr . isAlive ( ) :,7.267884212102741,7.267884212102741,0.0
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True):     try:         return self._read(count, timeout)     except usb.USBError as e:         if DEBUG_COMM:             log.info(                 ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s""                 % (e.errno, e.strerror, e.message, repr(e))             )         if ignore_timeouts and is_timeout(e):             return []         <MASK>             return []         raise",if ignore_non_errors and is_noerr ( e ) :,if ignore_non_errors and not e . errno :,46.48035271845195,46.48035271845195,0.0
"def PrintHeader(self):  # print the header array     if self.draw == False:         return     for val in self.parent.header:         self.SetPrintFont(val[""Font""])         header_indent = val[""Indent""] * self.pwidth         text = val[""Text""]         htype = val[""Type""]         <MASK>             addtext = self.GetDate()         elif htype == ""Date & Time"":             addtext = self.GetDateTime()         else:             addtext = """"         self.OutTextPageWidth(             text + addtext, self.pheader_margin, val[""Align""], header_indent, True         )","if htype == ""Date"" :","if htype == ""Date & Time"" :",58.14307369682194,58.14307369682194,0.0
"def get_intersect_all(self, refine=False):     result = None     for source, parts in self._per_source.items():         if result is None:             result = parts         else:             result.intersection_update(parts)     if not result:         return None     elif len(result) == 1:         return list(result)[0].item     else:         solids = [p.item for p in result]         solid = solids[0].fuse(solids[1:])         <MASK>             solid = solid.removeSplitter()         return solid",if refine :,if refine :,100.00000000000004,0.0,1.0
"def captured_updateNode(self, context):     if not self.updating_name_from_pointer:         font_datablock = self.get_bpy_data_from_name(self.fontname, bpy.data.fonts)         <MASK>             self.font_pointer = font_datablock             updateNode(self, context)",if font_datablock :,if font_datablock :,100.00000000000004,100.00000000000004,1.0
"def __add__(self, other):     if isinstance(other, Vector2):         # Vector + Vector -> Vector         # Vector + Point -> Point         # Point + Point -> Vector         <MASK>             _class = Vector2         else:             _class = Point2         return _class(self.x + other.x, self.y + other.y)     else:         assert hasattr(other, ""__len__"") and len(other) == 2         return Vector2(self.x + other[0], self.y + other[1])",if self . __class__ is other . __class__ :,"if isinstance ( other , Vector2 ) :",2.3595365419339505,2.3595365419339505,0.0
"def _flatten_settings_from_form(self, settings, form, form_values):     """"""Take a nested dict and return a flat dict of setting values.""""""     setting_values = {}     for field in form.c:         <MASK>             setting_values.update(                 self._flatten_settings_from_form(                     settings, field, form_values[field._name]                 )             )         elif field._name in settings:             setting_values[field._name] = form_values[field._name]     return setting_values","if isinstance ( field , _ContainerMixin ) :",if field . _name in settings :,6.892168295481103,6.892168295481103,0.0
"def add_include_dirs(self, args):     ids = []     for a in args:         # FIXME same hack, forcibly unpack from holder.         if hasattr(a, ""includedirs""):             a = a.includedirs         <MASK>             raise InvalidArguments(                 ""Include directory to be added is not an include directory object.""             )         ids.append(a)     self.include_dirs += ids","if not isinstance ( a , IncludeDirs ) :","if not isinstance ( a , ( list , tuple ) ) :",42.803206067505954,42.803206067505954,0.0
"def _clip_array(array, config):     if ""threshold"" in config.keys():         threshold = config[""threshold""]     else:         abs_array = np.max(np.abs(array))         <MASK>             return array         threshold = np.percentile(np.abs(array), 99.99)     return np.clip(array, -threshold, threshold)",if abs_array < 1.0 :,if abs_array < 0 :,64.34588841607616,64.34588841607616,0.0
def dfs(v: str) -> Iterator[Set[str]]:     index[v] = len(stack)     stack.append(v)     boundaries.append(index[v])     for w in edges[v]:         <MASK>             yield from dfs(w)         elif w not in identified:             while index[w] < boundaries[-1]:                 boundaries.pop()     if boundaries[-1] == index[v]:         boundaries.pop()         scc = set(stack[index[v] :])         del stack[index[v] :]         identified.update(scc)         yield scc,if w not in index :,if w in identified :,20.80119537801062,20.80119537801062,0.0
"def create_balancer(     self, name, members, protocol=""http"", port=80, algorithm=DEFAULT_ALGORITHM ):     balancer = self.ex_create_balancer_nowait(name, members, protocol, port, algorithm)     timeout = 60 * 20     waittime = 0     interval = 2 * 15     if balancer.id is not None:         return balancer     else:         while waittime < timeout:             balancers = self.list_balancers()             for i in balancers:                 <MASK>                     return i             waittime += interval             time.sleep(interval)     raise Exception(""Failed to get id"")",if i . name == balancer . name and i . id is not None :,if i . id is not None :,30.70373468463369,30.70373468463369,0.0
"def handle(self, scope: Scope, receive: Receive, send: Send) -> None:     if self.methods and scope[""method""] not in self.methods:         <MASK>             raise HTTPException(status_code=405)         else:             response = PlainTextResponse(""Method Not Allowed"", status_code=405)         await response(scope, receive, send)     else:         await self.app(scope, receive, send)","if ""app"" in scope :","if scope [ ""method"" ] not in self . methods :",4.834632845440431,4.834632845440431,0.0
"def convert(data):     result = []     for d in data:         # noinspection PyCompatibility         if isinstance(d, tuple) and len(d) == 2:             result.append((d[0], None, d[1]))         <MASK>             result.append(d)     return result","elif isinstance ( d , basestring ) :","if isinstance ( d , list ) :",41.11336169005198,41.11336169005198,0.0
"def register_adapters():     global adapters_registered     if adapters_registered is True:         return     try:         import pkg_resources         packageDir = pkg_resources.resource_filename(""pyamf"", ""adapters"")     except:         packageDir = os.path.dirname(__file__)     for f in glob.glob(os.path.join(packageDir, ""*.py"")):         mod = os.path.basename(f).split(os.path.extsep, 1)[0]         <MASK>             continue         try:             register_adapter(mod[1:].replace(""_"", "".""), PackageImporter(mod))         except ImportError:             pass     adapters_registered = True","if mod == ""__init__"" or not mod . startswith ( ""_"" ) :","if mod == ""adapters"" :",9.772026964347612,9.772026964347612,0.0
"def load_modules(     to_load, load, attr, modules_dict, excluded_aliases, loading_message=None ):     if loading_message:         print(loading_message)     for name in to_load:         module = load(name)         if module is None or not hasattr(module, attr):             continue         cls = getattr(module, attr)         if hasattr(cls, ""initialize"") and not cls.initialize():             continue         if hasattr(module, ""aliases""):             for alias in module.aliases():                 <MASK>                     modules_dict[alias] = module         else:             modules_dict[name] = module     if loading_message:         print()",if alias not in excluded_aliases :,if alias not in excluded_aliases :,100.00000000000004,100.00000000000004,1.0
"def clean_items(event, items, variations):     for item in items:         <MASK>             raise ValidationError(_(""One or more items do not belong to this event.""))         if item.has_variations:             if not any(var.item == item for var in variations):                 raise ValidationError(                     _(                         ""One or more items has variations but none of these are in the variations list.""                     )                 )",if event != item . event :,if not event . is_active :,7.809849842300637,7.809849842300637,0.0
"def __get_file_by_num(self, num, file_list, idx=0):     for element in file_list:         <MASK>             return element         if element[3] and element[4]:             i = self.__get_file_by_num(num, element[3], idx + 1)             if not isinstance(i, int):                 return i             idx = i         else:             idx += 1     return idx",if idx == num :,if idx == 0 :,53.7284965911771,53.7284965911771,0.0
"def check(chip, xeddb, chipdb):     all_inst = []     undoc = []     for inst in xeddb.recs:         <MASK>             if inst.undocumented:                 undoc.append(inst)             else:                 all_inst.append(inst)     return (all_inst, undoc)",if inst . isa_set in chipdb [ chip ] :,if inst . id == chipdb . id :,14.530346490115708,14.530346490115708,0.0
"def get_all_topic_src_files(self):     """"""Retrieves the file paths of all the topics in directory""""""     topic_full_paths = []     topic_names = os.listdir(self.topic_dir)     for topic_name in topic_names:         # Do not try to load hidden files.         <MASK>             topic_full_path = os.path.join(self.topic_dir, topic_name)             # Ignore the JSON Index as it is stored with topic files.             if topic_full_path != self.index_file:                 topic_full_paths.append(topic_full_path)     return topic_full_paths","if not topic_name . startswith ( ""."" ) :",if not os . path . isfile ( topic_full_path ) :,10.074708078532293,10.074708078532293,0.0
"def _get_element(dom_msi, tag_name, name=None, id_=None):     """"""Get a xml element defined on Product.""""""     product = dom_msi.getElementsByTagName(""Product"")[0]     elements = product.getElementsByTagName(tag_name)     for element in elements:         <MASK>             if (                 element.getAttribute(""Name"") == name                 and element.getAttribute(""Id"") == id_             ):                 return element         elif id_:             if element.getAttribute(""Id"") == id_:                 return element",if name and id_ :,if name :,23.174952587773145,0.0,0.0
"def __init__(self, *models):     super().__init__()     self.models = ModuleList(models)     for m in models:         <MASK>             raise ValueError(                 ""IndependentModelList currently only supports models that have a likelihood (e.g. ExactGPs)""             )     self.likelihood = LikelihoodList(*[m.likelihood for m in models])","if not hasattr ( m , ""likelihood"" ) :",if not m . likelihood :,8.871198783083607,8.871198783083607,0.0
"def _sniff(filename, oxlitype):     try:         with open(filename, ""rb"") as fileobj:             header = fileobj.read(4)             <MASK>                 fileobj.read(1)  # skip the version number                 ftype = fileobj.read(1)                 if binascii.hexlify(ftype) == oxlitype:                     return True         return False     except OSError:         return False","if header == b""OXLI"" :",if header == oxlitype :,32.58798048281462,32.58798048281462,0.0
"def convert_port_bindings(port_bindings):     result = {}     for k, v in six.iteritems(port_bindings):         key = str(k)         if ""/"" not in key:             key += ""/tcp""         <MASK>             result[key] = [_convert_port_binding(binding) for binding in v]         else:             result[key] = [_convert_port_binding(v)]     return result","if isinstance ( v , list ) :",if v :,7.49553326588684,0.0,0.0
"def input_data(self):     gen = self.config.generator     # don't try running the generator if we specify an output file explicitly,     # otherwise generator may segfault and we end up returning the output file anyway     if gen and (not self.config[""out""] or not self.config[""in""]):         <MASK>             self._run_generator(gen, args=self.config.generator_args)         if self._generated[0]:             return self._generated[0]     # in file is optional     return (         self._normalize(self.problem.problem_data[self.config[""in""]])         if self.config[""in""]         else b""""     )",if self . _generated is None :,"if self . config [ ""out"" ] :",16.784459625186194,16.784459625186194,0.0
"def __new__(cls, *tasks, **kwargs):     # This forces `chain(X, Y, Z)` to work the same way as `X | Y | Z`     if not kwargs and tasks:         <MASK>             tasks = tasks[0] if len(tasks) == 1 else tasks             return reduce(operator.or_, tasks)     return super(chain, cls).__new__(cls, *tasks, **kwargs)",if len ( tasks ) != 1 or is_list ( tasks [ 0 ] ) :,if len ( tasks ) == 1 :,17.47261628019723,17.47261628019723,0.0
"def get_file_sources():     global _file_sources     if _file_sources is None:         from galaxy.files import ConfiguredFileSources         file_sources = None         if os.path.exists(""file_sources.json""):             file_sources_as_dict = None             with open(""file_sources.json"", ""r"") as f:                 file_sources_as_dict = json.load(f)             if file_sources_as_dict is not None:                 file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict)         <MASK>             ConfiguredFileSources.from_dict([])         _file_sources = file_sources     return _file_sources",if file_sources is None :,if file_sources is None :,100.00000000000004,100.00000000000004,1.0
"def InitializeColours(self):     """"""Initializes the 16 custom colours in :class:`CustomPanel`.""""""     curr = self._colourData.GetColour()     self._colourSelection = -1     for i in range(16):         c = self._colourData.GetCustomColour(i)         <MASK>             self._customColours[i] = self._colourData.GetCustomColour(i)         else:             self._customColours[i] = wx.WHITE         if c == curr:             self._colourSelection = i",if c . IsOk ( ) :,if c == curr :,15.207218222740094,15.207218222740094,0.0
"def convert_obj_into_marshallable(self, obj):     if isinstance(obj, self.marshalable_types):         return obj     if isinstance(obj, array.array):         if obj.typecode == ""c"":             return obj.tostring()         <MASK>             return obj.tounicode()         return obj.tolist()     return self.class_to_dict(obj)","if obj . typecode == ""u"" :","if obj . typecode == ""t"" :",70.71067811865478,70.71067811865478,0.0
"def run(self):     self.run_command(""egg_info"")     from glob import glob     for pattern in self.match:         pattern = self.distribution.get_name() + ""*"" + pattern         files = glob(os.path.join(self.dist_dir, pattern))         files = [(os.path.getmtime(f), f) for f in files]         files.sort()         files.reverse()         log.info(""%d file(s) matching %s"", len(files), pattern)         files = files[self.keep :]         for (t, f) in files:             log.info(""Deleting %s"", f)             <MASK>                 os.unlink(f)",if not self . dry_run :,if t == self . keep :,13.134549472120788,13.134549472120788,0.0
"def render_token_list(self, tokens):     result = []     vars = []     for token in tokens:         if token.token_type == TOKEN_TEXT:             result.append(token.contents.replace(""%"", ""%%""))         <MASK>             result.append(""%%(%s)s"" % token.contents)             vars.append(token.contents)     return """".join(result), vars",elif token . token_type == TOKEN_VAR :,if token . token_type == TOKEN_NUMBER :,72.92571723872932,72.92571723872932,0.0
"def _handle_raise(self, values, is_NAs, origins):     for is_NA, origin in zip(is_NAs, origins):         <MASK>             msg = (                 ""Missing values detected. If you want rows with missing ""                 ""values to be automatically deleted in a list-wise ""                 ""manner (not recommended), please set dropna=True in ""                 ""the Bambi Model initialization.""             )             raise PatsyError(msg, origin)     return values",if np . any ( is_NA ) :,if not is_NA :,17.625328548379716,17.625328548379716,0.0
"def add_node_data(node_array, ntwk):     node_ntwk = nx.Graph()     newdata = {}     for idx, data in ntwk.nodes(data=True):         <MASK>             newdata[""value""] = node_array[int(idx) - 1]             data.update(newdata)             node_ntwk.add_node(int(idx), **data)     return node_ntwk",if not int ( idx ) == 0 :,if idx < node_array . count ( ) :,5.604233375480572,5.604233375480572,0.0
"def safe_parse_date(date_hdr):     """"""Parse a Date: or Received: header into a unix timestamp.""""""     try:         if "";"" in date_hdr:             date_hdr = date_hdr.split("";"")[-1].strip()         msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr)))         <MASK>             return None         else:             return msg_ts     except (ValueError, TypeError, OverflowError):         return None",if ( msg_ts > ( time . time ( ) + 24 * 3600 ) ) or ( msg_ts < 1 ) :,if msg_ts is None :,1.5952593645088629,1.5952593645088629,0.0
"def _route_db(self, model, **hints):     chosen_db = None     for router in self.routers:         try:             method = getattr(router, action)         except AttributeError:             # If the router doesn't have a method, skip to the next one.             pass         else:             chosen_db = method(model, **hints)             <MASK>                 return chosen_db     try:         return hints[""instance""]._state.db or DEFAULT_DB_ALIAS     except KeyError:         return DEFAULT_DB_ALIAS",if chosen_db :,if chosen_db :,100.00000000000004,100.00000000000004,1.0
"def get_keys(struct, ignore_first_level=False):     res = []     if isinstance(struct, dict):         if not ignore_first_level:             keys = [x.split(""("")[0] for x in struct.keys()]             res.extend(keys)         for key in struct:             <MASK>                 logging.debug(""Ignored: %s: %s"", key, struct[key])                 continue             res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL))     elif isinstance(struct, list):         for item in struct:             res.extend(get_keys(item))     return res",if key in IGNORED_KEYS :,if key in IGNORED_FIRST_LEVEL :,46.713797772819994,46.713797772819994,0.0
"def launch_app(self, fs_id):     if fs_id in self.app_infos:         row = self.get_row_by_fsid(fs_id)         <MASK>             return         app_info = self.app_infos[fs_id]         filepath = os.path.join(row[SAVEDIR_COL], row[SAVENAME_COL])         gfile = Gio.File.new_for_path(filepath)         app_info.launch(             [                 gfile,             ],             None,         )         self.app_infos.pop(fs_id, None)",if not row :,if not row :,100.00000000000004,100.00000000000004,1.0
"def create_skipfile(files_changed, skipfile):     # File is likely to contain some garbage values at start,     # only the corresponding json should be parsed.     json_pattern = re.compile(r""^\{.*\}"")     for line in files_changed.readlines():         <MASK>             for filename in json.loads(line):                 if ""/COMMIT_MSG"" in filename:                     continue                 skipfile.write(""+*/%s\n"" % filename)     skipfile.write(""-*\n"")","if re . match ( json_pattern , line ) :",if line :,1.9758011175389976,0.0,0.0
"def zscore(self, client, request, N):     check_input(request, N != 2)     key = request[1]     db = client.db     value = db.get(key)     if value is None:         client.reply_bulk(None)     elif not isinstance(value, self.zset_type):         client.reply_wrongtype()     else:         score = value.score(request[2], None)         <MASK>             score = str(score).encode(""utf-8"")         client.reply_bulk(score)",if score is not None :,if score :,23.174952587773145,0.0,0.0
"def _list_cases(suite):     for test in suite:         if isinstance(test, unittest.TestSuite):             _list_cases(test)         elif isinstance(test, unittest.TestCase):             <MASK>                 print(test.id())",if support . match_test ( test ) :,if test . id ( ) == test . id ( ) :,7.768562846380172,7.768562846380172,0.0
"def Run(self):     """"""The main run method of the client.""""""     for thread in self._threads.values():         thread.start()     logging.info(START_STRING)     while True:         dead_threads = [tn for (tn, t) in self._threads.items() if not t.isAlive()]         <MASK>             raise FatalError(                 ""These threads are dead: %r. Shutting down..."" % dead_threads             )         time.sleep(10)",if dead_threads :,if dead_threads :,100.00000000000004,100.00000000000004,1.0
"def _slice_queryset(queryset, order_by, per_page, start):     page_len = int(per_page) + 1     if start:         <MASK>             filter_name = ""%s__lte"" % order_by[1:]         else:             filter_name = ""%s__gte"" % order_by         return queryset.filter(**{filter_name: start})[:page_len]     return queryset[:page_len]","if order_by . startswith ( ""-"" ) :",if page_len > 1 :,4.231118166423695,4.231118166423695,0.0
"def compute_timer_precision(timer):     precision = None     points = 0     timeout = timeout_timer() + 1.0     previous = timer()     while timeout_timer() < timeout or points < 5:         for _ in XRANGE(10):             t1 = timer()             t2 = timer()             dt = t2 - t1             <MASK>                 break         else:             dt = t2 - previous             if dt <= 0.0:                 continue         if precision is not None:             precision = min(precision, dt)         else:             precision = dt         points += 1         previous = timer()     return precision",if 0 < dt :,if dt <= 0.0 :,11.478744233307168,11.478744233307168,0.0
"def findWorkingDir():     frozen = getattr(sys, ""frozen"", """")     if not frozen:         path = os.path.dirname(__file__)     elif frozen in (""dll"", ""console_exe"", ""windows_exe"", ""macosx_app""):         path = os.path.dirname(             os.path.dirname(os.path.dirname(os.path.dirname(__file__)))         )     elif frozen:  # needed for PyInstaller         <MASK>             path = getattr(sys, ""_MEIPASS"", """")  # --onefile         else:             path = os.path.dirname(sys.executable)  # --onedir     else:         path = """"     return path","if getattr ( sys , ""_MEIPASS"" , """" ) is not None :",if os . path . isdir ( os . path . dirname ( sys . executable ) :,5.401157445454033,5.401157445454033,0.0
"def CreateDataType(vmodlName, wsdlName, parent, version, props):     with _lazyLock:         dic = [vmodlName, wsdlName, parent, version, props]         names = vmodlName.split(""."")         <MASK>             vmodlName = ""."".join(name[0].lower() + name[1:] for name in names)         _AddToDependencyMap(names)         typeNs = GetWsdlNamespace(version)         _dataDefMap[vmodlName] = dic         _wsdlDefMap[(typeNs, wsdlName)] = dic         _wsdlTypeMapNSs.add(typeNs)",if _allowCapitalizedNames :,if len ( names ) > 1 :,6.567274736060395,6.567274736060395,0.0
"def ParseResponses(     self,     knowledge_base: rdf_client.KnowledgeBase,     responses: Iterable[rdfvalue.RDFValue], ) -> Iterator[rdf_client.User]:     for response in responses:         if not isinstance(response, rdf_client_fs.StatEntry):             raise TypeError(f""Unexpected response type: `{type(response)}`"")         # TODO: `st_mode` has to be an `int`, not `StatMode`.         if stat.S_ISDIR(int(response.st_mode)):             homedir = response.pathspec.path             username = os.path.basename(homedir)             <MASK>                 yield rdf_client.User(username=username, homedir=homedir)",if username not in self . _ignore_users :,"if not isinstance ( response , rdf_client_fs . StatEntry ) :",4.112982349983277,4.112982349983277,0.0
"def process_question(qtxt):     question = """"     skip = False     for letter in qtxt:         if letter == ""<"":             skip = True         if letter == "">"":             skip = False         if skip:             continue         <MASK>             if letter == "" "":                 letter = ""_""             question += letter.lower()     return question","if letter . isalnum ( ) or letter == "" "" :","if letter == "" "" :",37.34955222566951,37.34955222566951,0.0
"def process_all(self, lines, times=1):     gap = False     for _ in range(times):         for line in lines:             if gap:                 self.write("""")             self.process(line)             <MASK>                 gap = True     return 0",if not is_command ( line ) :,"if line . strip ( ) == ""\n"" :",4.6192151051305474,4.6192151051305474,0.0
"def _get(self, domain):     with self.lock:         try:             record = self.cache[domain]             time_now = time.time()             if time_now - record[""update""] > self.ttl:                 record = None         except KeyError:             record = None         <MASK>             record = {""r"": ""unknown"", ""dns"": {}, ""g"": 1, ""query_count"": 0}         # self.cache[domain] = record         return record",if not record :,if not record :,100.00000000000004,100.00000000000004,1.0
"def gen_constant_folding(cw):     types = [""Int32"", ""Double"", ""BigInteger"", ""Complex""]     for cur_type in types:         cw.enter_block(""if (constLeft.Value.GetType() == typeof(%s))"" % (cur_type,))         cw.enter_block(""switch (_op)"")         for op in ops:             gen = getattr(op, ""genConstantFolding"", None)             <MASK>                 gen(cw, cur_type)         cw.exit_block()         cw.exit_block()",if gen is not None :,if gen :,23.174952587773145,0.0,0.0
"def unreferenced_dummy(self):     for g, base in zip(self.evgroups, self.evbases):         for ind, j in enumerate(g):             <MASK>                 debug_print(                     ""replacing unreferenced %d %s with dummy"" % ((base + ind), g[ind])                 )                 g[ind] = ""dummy""                 self.evnum[base + ind] = ""dummy""",if not self . indexobj [ base + ind ] :,if j == base :,4.642454187453896,4.642454187453896,0.0
"def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:     for cls in self.__class__.__mro__:         <MASK>             warnings.warn(                 ""PyDecoratorMixin is deprecated. ""                 ""Please check the implementation of %s"" % cls,                 RemovedInSphinx50Warning,                 stacklevel=2,             )             break     else:         warnings.warn(             ""PyDecoratorMixin is deprecated"", RemovedInSphinx50Warning, stacklevel=2         )     ret = super().handle_signature(sig, signode)  # type: ignore     signode.insert(0, addnodes.desc_addname(""@"", ""@""))     return ret","if cls . __name__ != ""DirectiveAdapter"" :",if cls . __name__ == sig :,54.88684910025905,54.88684910025905,0.0
"def _iter_lines(path=path, response=response, max_next=options.http_max_next):     path.responses = []     n = 0     while response:         path.responses.append(response)         yield from response.iter_lines(decode_unicode=True)         src = response.links.get(""next"", {}).get(""url"", None)         <MASK>             break         n += 1         if n > max_next:             vd.warning(f""stopping at max {max_next} pages"")             break         vd.status(f""fetching next page from {src}"")         response = requests.get(src, stream=True)",if not src :,if not src :,100.00000000000004,100.00000000000004,1.0
"def ordered_indices(self):     with data_utils.numpy_seed(self.seed, self.epoch):         # Used to store the order of indices of each dataset to use         indices = [             np.random.permutation(len(dataset)) for dataset in self.datasets.values()         ]         # Keep track of which samples we've  used for each dataset         counters = [0 for _ in self.datasets]         sampled_indices = [             self._sample(indices, counters) for _ in range(self.total_num_instances)         ]         <MASK>             sampled_indices.sort(key=lambda i: self.num_tokens(i))         return np.array(sampled_indices, dtype=np.int64)",if self . sort_indices :,if self . num_tokens ( ) > 0 :,15.851165692617148,15.851165692617148,0.0
"def _build_columns(self):     self.columns = [Column() for col in self.keys]     for row in self:         for (col_idx, col_val) in enumerate(row):             col = self.columns[col_idx]             col.append(col_val)             <MASK>                 col.is_quantity = False     for (idx, key_name) in enumerate(self.keys):         self.columns[idx].name = key_name     self.x = Column()     self.ys = []",if ( col_val is not None ) and ( not is_quantity ( col_val ) ) :,if col . is_quantity :,3.4106484601338583,3.4106484601338583,0.0
"def tearDown(self):     subprocess_list = self.subprocess_list     processes = subprocess_list.processes     self.schedule.reset()     del self.schedule     for proc in processes:         <MASK>             terminate_process(proc.pid, kill_children=True, slow_stop=True)     subprocess_list.cleanup()     processes = subprocess_list.processes     if processes:         for proc in processes:             if proc.is_alive():                 terminate_process(proc.pid, kill_children=True, slow_stop=False)         subprocess_list.cleanup()     processes = subprocess_list.processes     if processes:         log.warning(""Processes left running: %s"", processes)",if proc . is_alive ( ) :,if proc . is_alive ( ) :,100.00000000000004,100.00000000000004,1.0
"def colorNetwork(cls, network, nodesInNetwork, nodeByID=None):     for node in nodesInNetwork:         node.use_custom_color = True         neededCopies = sum(socket.execution.neededCopies for socket in node.outputs)         <MASK>             color = (0.7, 0.9, 0.7)         else:             color = (1.0, 0.3, 0.3)         node.color = color",if neededCopies == 0 :,if neededCopies > 1 :,19.3576934939088,19.3576934939088,0.0
"def _init_warmup_scheduler(self, optimizer, states):     updates_so_far = states.get(""number_training_updates"", 0)     if self.warmup_updates > 0 and (         updates_so_far <= self.warmup_updates or self.hard_reset     ):         self.warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, self._warmup_lr)         <MASK>             self.warmup_scheduler.load_state_dict(states[""warmup_scheduler""])     else:         self.warmup_scheduler = None","if states . get ( ""warmup_scheduler"" ) :",if self . warmup_scheduler . is_running ( ) :,16.26170171519489,16.26170171519489,0.0
"def inner(self, *iargs, **ikwargs):     try:         return getattr(super(VEXResilienceMixin, self), func)(*iargs, **ikwargs)     except excs as e:         for exc, handler in zip(excs, handlers):             if isinstance(e, exc):                 v = getattr(self, handler)(*iargs, **ikwargs)                 <MASK>                     raise                 return v         assert False, ""this should be unreachable if Python is working correctly""",if v is raiseme :,if not v :,16.37226966703825,16.37226966703825,0.0
"def unwrap_envelope(self, data, many):     if many:         if data[""items""]:             <MASK>                 self.context[""total""] = len(data)                 return data             else:                 self.context[""total""] = data[""total""]         else:             self.context[""total""] = 0             data = {""items"": []}         return data[""items""]     return data","if isinstance ( data , InstrumentedList ) or isinstance ( data , list ) :","if data [ ""items"" ] :",3.0297048914466935,3.0297048914466935,0.0
"def __subclasscheck__(self, cls):     if self.__origin__ is not None:         <MASK>             raise TypeError(                 ""Parameterized generics cannot be used with class "" ""or instance checks""             )         return False     if self is Generic:         raise TypeError(             ""Class %r cannot be used with class "" ""or instance checks"" % self         )     return super().__subclasscheck__(cls)","if sys . _getframe ( 1 ) . f_globals [ ""__name__"" ] not in [ ""abc"" , ""functools"" ] :",if self is Parameterized :,0.046975141313942724,0.046975141313942724,0.0
"def __init__(self, pyversions, coverage_service):     build_matrix = """"     for version in pyversions:         build_matrix += ""\n    {},"".format(             version             <MASK>             else ""py{}"".format("""".join(version.split(""."")))         )     coverage_package = """"     if coverage_service:         coverage_package += ""\n    {}"".format(coverage_service.package)     coverage_package += ""\n""     super(Tox, self).__init__(         ""tox.ini"",         TEMPLATE.format(build_matrix=build_matrix, coverage_package=coverage_package),     )","if version . startswith ( ""pypy"" )","if version . startswith ( ""py"" ) :",58.14307369682194,58.14307369682194,0.0
"def _get_app(self, body=None):     app = self._app     if app is None:         try:             tasks = self.tasks.tasks  # is a group         except AttributeError:             tasks = self.tasks         if len(tasks):             app = tasks[0]._app         <MASK>             app = body._app     return app if app is not None else current_app",if app is None and body is not None :,if body :,3.848335094984576,0.0,0.0
"def logic():     for v in [True, False, None, 0, True, None, None, 1]:         yield clk.posedge         xd.next = v         <MASK>             yd.next = zd.next = None         elif v:             yd.next = zd.next = 11         else:             yd.next = zd.next = 0",if v is None :,if v :,32.34325178227722,0.0,0.0
"def run(self):     eid = self.start_episode()     obs = self.env.reset()     while True:         <MASK>             action = self.env.action_space.sample()             self.log_action(eid, obs, action)         else:             action = self.get_action(eid, obs)         obs, reward, done, info = self.env.step(action)         self.log_returns(eid, reward, info=info)         if done:             self.end_episode(eid, obs)             obs = self.env.reset()             eid = self.start_episode()",if random . random ( ) < self . off_pol_frac :,if self . env . action_space :,6.443030905386945,6.443030905386945,0.0
"def tearDown(self):     os.chdir(self.orig_working_dir)     sys.argv = self.orig_argv     sys.stdout = self.orig_stdout     sys.stderr = self.orig_stderr     for dirname in [""lv_LV"", ""ja_JP""]:         locale_dir = os.path.join(self.datadir, ""project"", ""i18n"", dirname)         <MASK>             shutil.rmtree(locale_dir)",if os . path . isdir ( locale_dir ) :,if os . path . isdir ( locale_dir ) :,100.00000000000004,100.00000000000004,1.0
"def sentry_set_scope(process_context, entity, project, email=None, url=None):     # Using GLOBAL_HUB means these tags will persist between threads.     # Normally there is one hub per thread.     with sentry_sdk.hub.GLOBAL_HUB.configure_scope() as scope:         scope.set_tag(""process_context"", process_context)         scope.set_tag(""entity"", entity)         scope.set_tag(""project"", project)         <MASK>             scope.user = {""email"": email}         if url:             scope.set_tag(""url"", url)",if email :,if email :,100.00000000000004,0.0,1.0
"def getDataMax(self):     result = -Double.MAX_VALUE     nCurves = self.chart.getNCurves()     for i in range(nCurves):         c = self.getSystemCurve(i)         <MASK>             continue         if c.getYAxis() == Y_AXIS:             nPoints = c.getNPoints()             for j in range(nPoints):                 result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY())     if result == -Double.MAX_VALUE:         return Double.NaN     return result",if not c . isVisible ( ) :,if c is None :,7.715486568024961,7.715486568024961,0.0
"def handle_starttag(self, tag, attrs):     if tag == ""link"" and (""rel"", ""icon"") in attrs or (""rel"", ""shortcut icon"") in attrs:         href = None         icon_type = None         for attr, value in attrs:             if attr == ""href"":                 href = value             elif attr == ""type"":                 icon_type = value         <MASK>             try:                 mimetype = extension_to_mimetype(href.rpartition(""."")[2])             except KeyError:                 pass             else:                 icon_type = mimetype             if icon_type:                 self.icons.append((href, icon_type))",if href :,"if attr == ""type"" :",6.567274736060395,6.567274736060395,0.0
"def get_version(version_file=STATIC_VERSION_FILE):     version_info = get_static_version_info(version_file)     version = version_info[""version""]     if version == ""__use_git__"":         version = get_version_from_git()         <MASK>             version = get_version_from_git_archive(version_info)         if not version:             version = Version(""unknown"", None, None)         return pep440_format(version)     else:         return version",if not version :,if version :,45.13864405503391,0.0,0.0
"def _Sleep(self, seconds):     if threading.current_thread() is not self._worker_thread:         return self._original_sleep(seconds)     self._time += seconds     self._budget -= seconds     while self._budget < 0:         self._worker_thread_turn.clear()         self._owner_thread_turn.set()         self._worker_thread_turn.wait()         <MASK>             raise FakeTimeline._WorkerThreadExit()",if self . _worker_thread_done :,if self . _time > self . _budget :,25.965358893403383,25.965358893403383,0.0
"def validate_attributes(self):     if not (self.has_variants or self.variant_of):         return     if not self.variant_based_on:         self.variant_based_on = ""Item Attribute""     if self.variant_based_on == ""Item Attribute"":         attributes = []         <MASK>             frappe.throw(_(""Attribute table is mandatory""))         for d in self.attributes:             if d.attribute in attributes:                 frappe.throw(                     _(                         ""Attribute {0} selected multiple times in Attributes Table""                     ).format(d.attribute)                 )             else:                 attributes.append(d.attribute)",if not self . attributes :,if not self . attributes :,100.00000000000004,100.00000000000004,1.0
"def check_digest_auth(user, passwd):     """"""Check user authentication using HTTP Digest auth""""""     if request.headers.get(""Authorization""):         credentails = parse_authorization_header(request.headers.get(""Authorization""))         if not credentails:             return         response_hash = response(             credentails,             passwd,             dict(                 uri=request.script_root + request.path,                 body=request.data,                 method=request.method,             ),         )         <MASK>             return True     return False","if credentails . get ( ""response"" ) == response_hash :",if response_hash == user :,11.720937028376891,11.720937028376891,0.0
"def _get_index_type(return_index_type, ctx):     if return_index_type is None:  # pragma: no cover         if ctx.running_mode == RunningMode.local:             return_index_type = ""object""         <MASK>             return_index_type = ""filename""         else:             return_index_type = ""bytes""     return return_index_type",elif ctx . running_mode == RunningMode . local_cluster :,if ctx . running_mode == RunningMode . file :,61.73028691712478,61.73028691712478,0.0
"def iter_event_handlers(     self,     resource: resources_.Resource,     event: bodies.RawEvent, ) -> Iterator[handlers.ResourceWatchingHandler]:     warnings.warn(         ""SimpleRegistry.iter_event_handlers() is deprecated; use ""         ""ResourceWatchingRegistry.iter_handlers()."",         DeprecationWarning,     )     cause = _create_watching_cause(resource, event)     for handler in self._handlers:         if not isinstance(handler, handlers.ResourceWatchingHandler):             pass         <MASK>             yield handler","elif registries . match ( handler = handler , cause = cause , ignore_fields = True ) :",if cause is not None :,0.936015604744713,0.936015604744713,0.0
"def subprocess_post_check(     completed_process: subprocess.CompletedProcess, raise_error: bool = True ) -> None:     if completed_process.returncode:         <MASK>             print(completed_process.stdout, file=sys.stdout, end="""")         if completed_process.stderr is not None:             print(completed_process.stderr, file=sys.stderr, end="""")         if raise_error:             raise PipxError(                 f""{' '.join([str(x) for x in completed_process.args])!r} failed""             )         else:             logger.info(f""{' '.join(completed_process.args)!r} failed"")",if completed_process . stdout is not None :,if completed_process . stdout is not None :,100.00000000000004,100.00000000000004,1.0
"def __pow__(self, power):     if power == 1:         return self     if power == -1:         # HACK: break cycle         from cirq.devices import line_qubit         decomposed = protocols.decompose_once_with_qubits(             self, qubits=line_qubit.LineQid.for_gate(self), default=None         )         <MASK>             return NotImplemented         inverse_decomposed = protocols.inverse(decomposed, None)         if inverse_decomposed is None:             return NotImplemented         return _InverseCompositeGate(self)     return NotImplemented",if decomposed is None :,if decomposed is not None :,37.99178428257963,37.99178428257963,0.0
"def tearDown(self):     """"""Close the application after tests""""""     # set it back to it's old position so not to annoy users :-)     self.old_pos = self.dlg.rectangle     # close the application     self.dlg.menu_select(""File->Exit"")     try:         <MASK>             self.app.UntitledNotepad[""Do&n't Save""].click()             self.app.UntitledNotepad.wait_not(""visible"")     except Exception:         pass     finally:         self.app.kill()","if self . app . UntitledNotepad [ ""Do&n't Save"" ] . exists ( ) :",if self . app . UntitledNotepad :,15.14389796999661,15.14389796999661,0.0
"def terminate_subprocess(proc, timeout=0.1, log=None):     <MASK>         if log:             log.info(""Sending SIGTERM to %r"", proc)         proc.terminate()         timeout_time = time.time() + timeout         while proc.poll() is None and time.time() < timeout_time:             time.sleep(0.02)         if proc.poll() is None:             if log:                 log.info(""Sending SIGKILL to %r"", proc)             proc.kill()     return proc.returncode",if proc . poll ( ) is None :,if log :,4.691812222477093,0.0,0.0
"def validate(self, detection, expectation):     config = SigmaConfiguration()     self.basic_rule[""detection""] = detection     with patch(""yaml.safe_load_all"", return_value=[self.basic_rule]):         parser = SigmaCollectionParser(""any sigma io"", config, None)         backend = SQLiteBackend(config, self.table)         assert len(parser.parsers) == 1         for p in parser.parsers:             <MASK>                 self.assertEqual(expectation, backend.generate(p))             elif isinstance(expectation, Exception):                 self.assertRaises(type(expectation), backend.generate, p)","if isinstance ( expectation , str ) :","if isinstance ( p , str ) :",50.000000000000014,50.000000000000014,0.0
"def makelist(d):     """"""Convert d into a list if all the keys of d are integers.""""""     if isinstance(d, dict):         <MASK>             return [makelist(d[k]) for k in sorted(d, key=int)]         else:             return web.storage((k, makelist(v)) for k, v in d.items())     else:         return d",if all ( isint ( k ) for k in d ) :,"if isinstance ( d , list ) :",7.433761660133445,7.433761660133445,0.0
"def __share_local_dir(self, lpath, rpath, fast):     result = const.ENoError     for walk in self.__walk_normal_file(lpath):         (dirpath, dirnames, filenames) = walk         for filename in filenames:             rpart = os.path.relpath(dirpath, lpath)             if rpart == ""."":                 rpart = """"             subr = self.__share_local_file(                 joinpath(dirpath, filename),                 posixpath.join(rpath, rpart, filename),                 fast,             )             <MASK>                 result = subr     return result",if subr != const . ENoError :,if subr :,11.898417391331403,0.0,0.0
"def _targets(self, sigmaparser):     # build list of matching target mappings     targets = set()     for condfield in self.conditions:         <MASK>             rulefieldvalues = sigmaparser.values[condfield]             for condvalue in self.conditions[condfield]:                 if condvalue in rulefieldvalues:                     targets.update(self.conditions[condfield][condvalue])     return targets",if condfield in sigmaparser . values :,if condfield in sigmaparser . values :,100.00000000000004,100.00000000000004,1.0
"def _wrapped_view(request, *args, **kwargs):     # based on authority/decorators.py     user = request.user     if user.is_authenticated():         obj = _resolve_lookup(obj_lookup, kwargs)         perm_obj = _resolve_lookup(perm_obj_lookup, kwargs)         granted = access.has_perm_or_owns(user, perm, obj, perm_obj, owner_attr)         <MASK>             return view_func(request, *args, **kwargs)     # In all other cases, permission denied     return HttpResponseForbidden()",if granted or user . has_perm ( perm ) :,if granted :,3.136388772461349,0.0,0.0
"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint):     cleaned_parts = []     for earlier in earlier_parts:         earlier_part = earlier[""part""]         earlier_step = earlier[""step""]         found = False         for current in current_parts:             <MASK>                 found = True                 break         if not found:             cleaned_parts.append(dict(part=earlier_part, step=earlier_step))     self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint)     for expected in expected_parts:         self.assertThat(cleaned_parts, Contains(expected), hint)","if earlier_part == current [ ""part"" ] and earlier_step == current [ ""step"" ] :",if current . step == earlier_step :,6.486736672746918,6.486736672746918,0.0
"def show_image(self, wnd_name, img):     if wnd_name in self.named_windows:         if self.named_windows[wnd_name] == 0:             self.named_windows[wnd_name] = 1             self.on_create_window(wnd_name)             <MASK>                 self.capture_mouse(wnd_name)         self.on_show_image(wnd_name, img)     else:         print(""show_image: named_window "", wnd_name, "" not found."")",if wnd_name in self . capture_mouse_windows :,if self . capture_mouse :,28.379522624171575,28.379522624171575,0.0
"def readlines(self, hint=None):     # Again, allow hint but ignore     body = self._get_body()     rest = body[self.position :]     self.position = len(body)     result = []     while 1:         next = rest.find(""\r\n"")         <MASK>             result.append(rest)             break         result.append(rest[: next + 2])         rest = rest[next + 2 :]     return result",if next == - 1 :,if next < 0 :,15.848738972120703,15.848738972120703,0.0
"def __lt__(self, other):     olen = len(other)     for i in range(olen):         try:             c = self[i] < other[i]         except IndexError:             # self must be shorter             return True         if c:             return c         <MASK>             return False     return len(self) < olen",elif other [ i ] < self [ i ] :,if i == olen :,4.194930905450255,4.194930905450255,0.0
"def social_user(backend, uid, user=None, *args, **kwargs):     provider = backend.name     social = backend.strategy.storage.user.get_social_auth(provider, uid)     if social:         if user and social.user != user:             msg = ""This account is already in use.""             raise AuthAlreadyAssociated(backend, msg)         <MASK>             user = social.user     return {         ""social"": social,         ""user"": user,         ""is_new"": user is None,         ""new_association"": social is None,     }",elif not user :,if user is None :,12.703318703865365,12.703318703865365,0.0
"def markUVs(self, indices=None):     if isinstance(indices, tuple):         indices = indices[0]     ntexco = len(self.texco)     if indices is None:         self.utexc = True     else:         if self.utexc is False:             self.utexc = np.zeros(ntexco, dtype=bool)         <MASK>             self.utexc[indices] = True",if self . utexc is not True :,if indices in self . utexc :,24.0785655451027,24.0785655451027,0.0
"def destination(self, type, name, arglist):     classname = ""ResFunction""     listname = ""functions""     if arglist:         t, n, m = arglist[0]         <MASK>             classname = ""ResMethod""             listname = ""resmethods""     return classname, listname","if t == ""Handle"" and m == ""InMode"" :",if n == 1 :,4.3074986800341035,4.3074986800341035,0.0
"def select(self, regions, register):     self.view.sel().clear()     to_store = []     for r in regions:         self.view.sel().add(r)         if register:             to_store.append(self.view.substr(self.view.full_line(r)))     if register:         text = """".join(to_store)         <MASK>             text = text + ""\n""         state = State(self.view)         state.registers[register] = [text]","if not text . endswith ( ""\n"" ) :",if text :,1.9758011175389976,0.0,0.0
"def _skip_start(self):     start, stop = self.start, self.stop     for chunk in self.app_iter:         self._pos += len(chunk)         if self._pos < start:             continue         <MASK>             return b""""         else:             chunk = chunk[start - self._pos :]             if stop is not None and self._pos > stop:                 chunk = chunk[: stop - self._pos]                 assert len(chunk) == stop - start             return chunk     else:         raise StopIteration()",elif self . _pos == start :,if stop is None :,4.79981069911921,4.79981069911921,0.0
"def start(self):     self.on_config_change()     self.start_config_watch()     try:         if self.config[""MITMf""][""DNS""][""tcp""].lower() == ""on"":             self.startTCP()         else:             self.startUDP()     except socket.error as e:         <MASK>             shutdown(                 ""\n[DNS] Unable to start DNS server on port {}: port already in use"".format(                     self.config[""MITMf""][""DNS""][""port""]                 )             )","if ""Address already in use"" in e :",if e . errno == errno . EADDRINUSE :,5.522397783539471,5.522397783539471,0.0
"def ignore(self, other):     if isinstance(other, Suppress):         if other not in self.ignoreExprs:             super(ParseElementEnhance, self).ignore(other)             <MASK>                 self.expr.ignore(self.ignoreExprs[-1])     else:         super(ParseElementEnhance, self).ignore(other)         if self.expr is not None:             self.expr.ignore(self.ignoreExprs[-1])     return self",if self . expr is not None :,if self . expr is not None :,100.00000000000004,100.00000000000004,1.0
"def test_relative_deploy_path_override():     s = Site(TEST_SITE_ROOT)     s.load()     res = s.content.resource_from_relative_path(         ""blog/2010/december/merry-christmas.html""     )     res.relative_deploy_path = ""blog/2010/december/happy-holidays.html""     for page in s.content.walk_resources():         <MASK>             assert page.relative_deploy_path == ""blog/2010/december/happy-holidays.html""         else:             assert page.relative_deploy_path == Folder(page.relative_path)",if res . source_file == page . source_file :,"if page . relative_path == ""blog/2010/december/merry-christmas.html"" :",6.439931429457922,6.439931429457922,0.0
"def _parser(cls, buf):     tlvs = []     while buf:         tlv_type = LLDPBasicTLV.get_type(buf)         tlv = cls._tlv_parsers[tlv_type](buf)         tlvs.append(tlv)         offset = LLDP_TLV_SIZE + tlv.len         buf = buf[offset:]         <MASK>             break         assert len(buf) > 0     lldp_pkt = cls(tlvs)     assert lldp_pkt._tlvs_len_valid()     assert lldp_pkt._tlvs_valid()     return lldp_pkt, None, buf",if tlv . tlv_type == LLDP_TLV_END :,if not tlv :,1.7256245272235644,1.7256245272235644,0.0
"def _do_pull(self, repo, pull_kwargs, silent, ignore_pull_failures):     try:         output = self.client.pull(repo, **pull_kwargs)         if silent:             with open(os.devnull, ""w"") as devnull:                 yield from stream_output(output, devnull)         else:             yield from stream_output(output, sys.stdout)     except (StreamOutputError, NotFound) as e:         <MASK>             raise         else:             log.error(str(e))",if not ignore_pull_failures :,if ignore_pull_failures :,72.89545183625967,72.89545183625967,0.0
def _collect_bytecode(ordered_code):     bytecode_blocks = []     stack = [ordered_code]     while stack:         code = stack.pop()         bytecode_blocks.append(code.co_code)         for const in code.co_consts:             <MASK>                 stack.append(const)     return bytecode_blocks,"if isinstance ( const , blocks . OrderedCode ) :",if const . co_code == ordered_code :,4.789232204309912,4.789232204309912,0.0
"def displayhook(value):     if value is None:         return     builtins = modules[""builtins""]     # Set '_' to None to avoid recursion     builtins._ = None     text = repr(value)     try:         local_stdout = stdout     except NameError as e:         raise RuntimeError(""lost sys.stdout"") from e     try:         local_stdout.write(text)     except UnicodeEncodeError:         bytes = text.encode(local_stdout.encoding, ""backslashreplace"")         <MASK>             local_stdout.buffer.write(bytes)         else:             text = bytes.decode(local_stdout.encoding, ""strict"")             local_stdout.write(text)     local_stdout.write(""\n"")     builtins._ = value","if hasattr ( local_stdout , ""buffer"" ) :",if bytes :,1.7260212584862158,0.0,0.0
"def _analyze(self):     lines = open(self.log_path, ""r"").readlines()     prev_line = None     for line in lines:         if line.startswith(""ERROR:"") and prev_line and prev_line.startswith(""=""):             self.errors.append(line[len(""ERROR:"") :].strip())         <MASK>             self.failures.append(line[len(""FAIL:"") :].strip())         prev_line = line","elif line . startswith ( ""FAIL:"" ) and prev_line and prev_line . startswith ( ""="" ) :","if line . startswith ( ""FAIL:"" ) and prev_line . startswith ( ""="" ) :",79.29573390667649,79.29573390667649,0.0
"def _flush(self):     if self._data:         if self._last is not None:             text = """".join(self._data)             <MASK>                 assert self._last.tail is None, ""internal error (tail)""                 self._last.tail = text             else:                 assert self._last.text is None, ""internal error (text)""                 self._last.text = text         self._data = []",if self . _tail :,if self . _last . tail is not None :,25.965358893403383,25.965358893403383,0.0
"def write(self, chunk):     consumer = self._current_consumer     server_side = consumer.server_side     if server_side:         server_side.data_received(chunk)     else:         consumer.message += chunk         assert consumer.in_parser.execute(chunk, len(chunk)) == len(chunk)         <MASK>             consumer.finished()",if consumer . in_parser . is_message_complete ( ) :,if consumer . in_parser :,26.816738085462994,26.816738085462994,0.0
"def _api_change_cat(name, output, kwargs):     """"""API: accepts output, value(=nzo_id), value2(=category)""""""     value = kwargs.get(""value"")     value2 = kwargs.get(""value2"")     if value and value2:         nzo_id = value         cat = value2         <MASK>             cat = None         result = sabnzbd.NzbQueue.change_cat(nzo_id, cat)         return report(output, keyword=""status"", data=bool(result > 0))     else:         return report(output, _MSG_NO_VALUE)","if cat == ""None"" :",if cat :,11.898417391331403,0.0,0.0
"def get_allocated_address(     self, config: ActorPoolConfig, allocated: allocated_type ) -> str:     addresses = config.get_external_addresses(label=self.label)     for addr in addresses:         occupied = False         for strategy, _ in allocated.get(addr, dict()).values():             <MASK>                 occupied = True                 break         if not occupied:             return addr     raise NoIdleSlot(         f""No idle slot for creating actor "" f""with label {self.label}, mark {self.mark}""     )",if strategy == self :,if strategy == self . idle_slot :,41.11336169005198,41.11336169005198,0.0
"def schedule_logger(job_id=None, delete=False):     if not job_id:         return getLogger(""fate_flow_schedule"")     else:         <MASK>             with LoggerFactory.lock:                 try:                     for key in LoggerFactory.schedule_logger_dict.keys():                         if job_id in key:                             del LoggerFactory.schedule_logger_dict[key]                 except:                     pass             return True         key = job_id + ""schedule""         if key in LoggerFactory.schedule_logger_dict:             return LoggerFactory.schedule_logger_dict[key]         return LoggerFactory.get_schedule_logger(job_id)",if delete :,if delete :,100.00000000000004,0.0,1.0
"def quick_load(tool_file, async_load=True):     try:         tool = self.load_tool(tool_file, tool_cache_data_dir)         self.__add_tool(tool, load_panel_dict, elems)         # Always load the tool into the integrated_panel_dict, or it will not be included in the integrated_tool_panel.xml file.         key = ""tool_%s"" % str(tool.id)         integrated_elems[key] = tool         <MASK>             self._load_tool_panel()             self._save_integrated_tool_panel()         return tool.id     except Exception:         log.exception(""Failed to load potential tool %s."", tool_file)         return None",if async_load :,if async_load :,100.00000000000004,100.00000000000004,1.0
"def _get_default_ordering(self):     try:         ordering = super(DocumentChangeList, self)._get_default_ordering()     except AttributeError:         ordering = []         if self.model_admin.ordering:             ordering = self.model_admin.ordering         <MASK>             ordering = self.lookup_opts.ordering     return ordering",elif self . lookup_opts . ordering :,if self . lookup_opts :,48.35447404743731,48.35447404743731,0.0
"def names(self, persistent=None):     u = set()     result = []     for s in [         self.__storage(None),         self.__storage(self.__category),     ]:         for b in s:             <MASK>                 continue             if b.name.startswith(""__""):                 continue             if b.name not in u:                 result.append(b.name)                 u.add(b.name)     return result",if persistent is not None and b . persistent != persistent :,if b . name is None :,7.335725659408425,7.335725659408425,0.0
"def common_check_get_messages_query(     self, query_params: Dict[str, object], expected: str ) -> None:     user_profile = self.example_user(""hamlet"")     request = POSTRequestMock(query_params, user_profile)     with queries_captured() as queries:         get_messages_backend(request, user_profile)     for query in queries:         <MASK>             sql = str(query[""sql""]).replace("" /* get_messages */"", """")             self.assertEqual(sql, expected)             return     raise AssertionError(""get_messages query not found"")","if ""/* get_messages */"" in query [ ""sql"" ] :","if query [ ""sql"" ] in query :",26.90608636157671,26.90608636157671,0.0
"def _activate_only_current_top_active():     for i in range(0, len(current_sequence().tracks) - 1):         <MASK>             current_sequence().tracks[i].active = True         else:             current_sequence().tracks[i].active = False     gui.tline_column.widget.queue_draw()",if i == current_sequence ( ) . get_first_active_track ( ) . id :,if current_sequence . tracks [ i ] . active :,7.210152474818981,7.210152474818981,0.0
"def http_wrapper(self, url, postdata={}):     try:         if postdata != {}:             f = urllib.urlopen(url, postdata)         else:             f = urllib.urlopen(url)         response = f.read()     except:         import traceback         import logging, sys         cla, exc, tb = sys.exc_info()         logging.error(url)         <MASK>             logging.error(""with post data"")         else:             logging.error(""without post data"")         logging.error(exc.args)         logging.error(traceback.format_tb(tb))         response = """"     return response",if postdata :,if exc . args :,12.703318703865365,12.703318703865365,0.0
"def frequent_thread_switches():     """"""Make concurrency bugs more likely to manifest.""""""     interval = None     <MASK>         if hasattr(sys, ""getswitchinterval""):             interval = sys.getswitchinterval()             sys.setswitchinterval(1e-6)         else:             interval = sys.getcheckinterval()             sys.setcheckinterval(1)     try:         yield     finally:         if not sys.platform.startswith(""java""):             if hasattr(sys, ""setswitchinterval""):                 sys.setswitchinterval(interval)             else:                 sys.setcheckinterval(interval)","if not sys . platform . startswith ( ""java"" ) :","if sys . platform . startswith ( ""java"" ) :",84.96364166597652,84.96364166597652,0.0
"def iter_filters(filters, block_end=False):     queue = deque(filters)     while queue:         f = queue.popleft()         <MASK>             if block_end:                 queue.appendleft(None)             for gf in f.filters:                 queue.appendleft(gf)         yield f","if f is not None and f . type in ( ""or"" , ""and"" , ""not"" ) :",if not f . filters :,1.016274277419396,1.016274277419396,0.0
"def smartsplit(code):     """"""Split `code` at "" symbol, only if it is not escaped.""""""     strings = []     pos = 0     while pos < len(code):         <MASK>             word = """"  # new word             pos += 1             while pos < len(code):                 if code[pos] == '""':                     break                 if code[pos] == ""\\"":                     word += ""\\""                     pos += 1                 word += code[pos]                 pos += 1             strings.append('""%s""' % word)         pos += 1     return strings","if code [ pos ] == '""' :","if code [ pos ] == ""'"" :",67.0422683816333,67.0422683816333,0.0
"def get_folder_content(cls, name):     """"""Return (folders, files) for the given folder in the root dir.""""""     folders = set()     files = set()     for path in cls.LAYOUT:         <MASK>             continue         parts = path.split(""/"")         if len(parts) == 2:             files.add(parts[1])         else:             folders.add(parts[1])     folders = list(folders)     folders.sort()     files = list(files)     files.sort()     return (folders, files)","if not path . startswith ( name + ""/"" ) :",if not path . startswith ( name ) :,51.51425457345961,51.51425457345961,0.0
"def array_for(self, i):     if 0 <= i < self._cnt:         <MASK>             return self._tail         node = self._root         level = self._shift         while level > 0:             assert isinstance(node, Node)             node = node._array[(i >> level) & 0x01F]             level -= 5         assert isinstance(node, Node)         return node._array     affirm(False, u""Index out of Range"")",if i >= self . tailoff ( ) :,if i == self . _cnt :,20.90067144241745,20.90067144241745,0.0
"def __or__(self, other) -> ""MultiVector"":     r""""""``self | other``, the inner product :math:`M \cdot N`""""""     other, mv = self._checkOther(other)     if mv:         newValue = self.layout.imt_func(self.value, other.value)     else:         <MASK>             obj = self.__array__()             return obj | other         # l * M = M * l = 0 for scalar l         return self._newMV(dtype=np.result_type(self.value.dtype, other))     return self._newMV(newValue)","if isinstance ( other , np . ndarray ) :",if self . is_array :,5.630400552901077,5.630400552901077,0.0
"def parse_bzr_stats(status):     stats = RepoStats()     statustype = ""changed""     for statusline in status:         if statusline[:2] == ""  "":             setattr(stats, statustype, getattr(stats, statustype) + 1)         <MASK>             statustype = ""staged""         elif statusline == ""unknown:"":             statustype = ""new""         else:  # removed, missing, renamed, modified or kind changed             statustype = ""changed""     return stats","elif statusline == ""added:"" :","if statusline == ""staged:"" :",43.167001068522545,43.167001068522545,0.0
"def write(self, timestamps, actualValues, predictedValues, predictionStep=1):     assert len(timestamps) == len(actualValues) == len(predictedValues)     for index in range(len(self.names)):         timestamp = timestamps[index]         actual = actualValues[index]         prediction = predictedValues[index]         writer = self.outputWriters[index]         <MASK>             outputRow = [timestamp, actual, prediction]             writer.writerow(outputRow)             self.lineCounts[index] += 1",if timestamp is not None :,if predictionStep == 1 :,9.652434877402245,9.652434877402245,0.0
"def clean(self):     """"""Delete old files in ""tmp"".""""""     now = time.time()     for entry in os.listdir(os.path.join(self._path, ""tmp"")):         path = os.path.join(self._path, ""tmp"", entry)         <MASK>  # 60 * 60 * 36             os.remove(path)",if now - os . path . getatime ( path ) > 129600 :,if now - now > self . _time_stamp :,12.689698066272173,12.689698066272173,0.0
"def _get_info(self, path):     info = OrderedDict()     if not self._is_mac() or self._has_xcode_tools():         stdout = None         try:             stdout, stderr = Popen(                 [self._find_binary(), ""info"", os.path.realpath(path)],                 stdout=PIPE,                 stderr=PIPE,             ).communicate()         except OSError:             pass         else:             <MASK>                 for line in stdout.splitlines():                     line = u(line).split("": "", 1)                     if len(line) == 2:                         info[line[0]] = line[1]     return info",if stdout :,if not stdout :,35.35533905932737,35.35533905932737,0.0
"def add(meta_list, info_list=None):     if not info_list:         info_list = meta_list     if not isinstance(meta_list, (list, tuple)):         meta_list = (meta_list,)     if not isinstance(info_list, (list, tuple)):         info_list = (info_list,)     for info_f in info_list:         <MASK>             for meta_f in meta_list:                 metadata[meta_f] = info[info_f]             break",if info . get ( info_f ) is not None :,if info_f in metadata :,13.044969897820199,13.044969897820199,0.0
"def _compute_log_r(model_trace, guide_trace):     log_r = MultiFrameTensor()     stacks = get_plate_stacks(model_trace)     for name, model_site in model_trace.nodes.items():         <MASK>             log_r_term = model_site[""log_prob""]             if not model_site[""is_observed""]:                 log_r_term = log_r_term - guide_trace.nodes[name][""log_prob""]             log_r.add((stacks[name], log_r_term.detach()))     return log_r","if model_site [ ""type"" ] == ""sample"" :",if name in stacks :,1.719207234832579,1.719207234832579,0.0
"def pickline(file, key, casefold=1):     try:         f = open(file, ""r"")     except IOError:         return None     pat = re.escape(key) + "":""     prog = re.compile(pat, casefold and re.IGNORECASE)     while 1:         line = f.readline()         if not line:             break         if prog.match(line):             text = line[len(key) + 1 :]             while 1:                 line = f.readline()                 <MASK>                     break                 text = text + line             return text.strip()     return None",if not line or not line [ 0 ] . isspace ( ) :,if not line :,5.2447643832804935,5.2447643832804935,0.0
"def build_iterator(data, infinite=True):     """"""Build the iterator for inputs.""""""     index = 0     size = len(data[0])     while True:         if index + batch_size > size:             <MASK>                 index = 0             else:                 return         yield data[0][index : index + batch_size], data[1][index : index + batch_size]         index += batch_size",if infinite :,if infinite :,100.00000000000004,0.0,1.0
"def checkall(g, bg, dst_nodes, include_dst_in_src=True):     for etype in g.etypes:         ntype = g.to_canonical_etype(etype)[2]         <MASK>             check(g, bg, ntype, etype, dst_nodes[ntype], include_dst_in_src)         else:             check(g, bg, ntype, etype, None, include_dst_in_src)",if dst_nodes is not None and ntype in dst_nodes :,if ntype in dst_nodes :,30.93485033266056,30.93485033266056,0.0
"def minimalBases(classes):     """"""Reduce a list of base classes to its ordered minimum equivalent""""""     if not __python3:  # pragma: no cover         classes = [c for c in classes if c is not ClassType]     candidates = []     for m in classes:         for n in classes:             if issubclass(n, m) and m is not n:                 break         else:             # m has no subclasses in 'classes'             <MASK>                 candidates.remove(m)  # ensure that we're later in the list             candidates.append(m)     return candidates",if m in candidates :,if n is ClassType :,12.703318703865365,12.703318703865365,0.0
"def __keep_songs_enable(self, enabled):     config.set(""memory"", ""queue_keep_songs"", enabled)     if enabled:         self.queue.set_first_column_type(CurrentColumn)     else:         for col in self.queue.get_columns():             # Remove the CurrentColum if it exists             <MASK>                 self.queue.set_first_column_type(None)                 break","if isinstance ( col , CurrentColumn ) :",if col . type == CurrentColum :,7.267884212102741,7.267884212102741,0.0
"def outlineView_heightOfRowByItem_(self, tree, item) -> float:     default_row_height = self.rowHeight     if item is self:         return default_row_height     heights = [default_row_height]     for column in self.tableColumns:         value = getattr(item.attrs[""node""], str(column.identifier))         <MASK>             # if the cell value is a widget, use its height             heights.append(value._impl.native.intrinsicContentSize().height)     return max(heights)","if isinstance ( value , toga . Widget ) :",if value is not None :,5.484411595600381,5.484411595600381,0.0
"def condition(self):     if self.__condition is None:         <MASK>             # Avoid an extra indirection in the common case of only one condition.             self.__condition = self.flat_conditions[0]         elif len(self.flat_conditions) == 0:             # Possible, if unlikely, due to filter predicate rewriting             self.__condition = lambda _: True         else:             self.__condition = lambda x: all(cond(x) for cond in self.flat_conditions)     return self.__condition",if len ( self . flat_conditions ) == 1 :,if len ( self . flat_conditions ) == 1 :,100.00000000000004,100.00000000000004,1.0
"def _find_delimiter(f, block_size=2 ** 16):     delimiter = b""\n""     if f.tell() == 0:         return 0     while True:         b = f.read(block_size)         if not b:             return f.tell()         <MASK>             return f.tell() - len(b) + b.index(delimiter) + 1",elif delimiter in b :,if b [ 0 ] == delimiter :,6.27465531099474,6.27465531099474,0.0
"def serialize(self, name=None):     data = super(SimpleText, self).serialize(name)     data[""contentType""] = self.contentType     data[""content""] = self.content     if self.width:         <MASK>             raise InvalidWidthException(self.width)         data[""inputOptions""] = {}         data[""width""] = self.width     return data","if self . width not in [ 100 , 50 , 33 , 25 ] :",if self . width not in self . inputOptions :,29.48682411907622,29.48682411907622,0.0
"def inference(self):     self.attention_weight_dim = self.input_dims[0][-1]     if self.keep_dim:         self.output_dim = copy.deepcopy(self.input_dims[0])     else:         self.output_dim = []         for idx, dim in enumerate(self.input_dims[0]):             <MASK>                 self.output_dim.append(dim)     super(         LinearAttentionConf, self     ).inference()  # PUT THIS LINE AT THE END OF inference()",if idx != len ( self . input_dims [ 0 ] ) - 2 :,if idx == 1 :,2.6126300161123828,2.6126300161123828,0.0
"def __delete_hook(self, rpc):     try:         rpc.check_success()     except apiproxy_errors.Error:         return None     result = []     for status in rpc.response.delete_status_list():         if status == MemcacheDeleteResponse.DELETED:             result.append(DELETE_SUCCESSFUL)         <MASK>             result.append(DELETE_ITEM_MISSING)         else:             result.append(DELETE_NETWORK_FAILURE)     return result",elif status == MemcacheDeleteResponse . NOT_FOUND :,if status == MemcacheDeleteResponse . MISSING :,42.13952948452608,42.13952948452608,0.0
def identify_page_at_cursor(self):     for region in self.view.sel():         text_on_cursor = None         pos = region.begin()         scope_region = self.view.extract_scope(pos)         <MASK>             text_on_cursor = self.view.substr(scope_region)             return text_on_cursor.strip(string.punctuation)     return None,if not scope_region . empty ( ) :,if scope_region :,16.62083000646927,16.62083000646927,0.0
"def from_elem(cls, parent, when_elem):     """"""Loads the proper when by attributes of elem""""""     when_value = when_elem.get(""value"", None)     <MASK>         return ValueToolOutputActionConditionalWhen(parent, when_elem, when_value)     else:         when_value = when_elem.get(""datatype_isinstance"", None)         if when_value is not None:             return DatatypeIsInstanceToolOutputActionConditionalWhen(                 parent, when_elem, when_value             )     raise TypeError(""When type not implemented"")",if when_value is not None :,if when_value is not None :,100.00000000000004,100.00000000000004,1.0
"def test_insert_entity_empty_string_rk(     self, tables_cosmos_account_name, tables_primary_cosmos_account_key ):     # Arrange     await self._set_up(tables_cosmos_account_name, tables_primary_cosmos_account_key)     try:         entity = {""PartitionKey"": ""pk"", ""RowKey"": """"}         # Act         with pytest.raises(HttpResponseError):             await self.table.create_entity(entity=entity)             # Assert         #  assert resp is None     finally:         await self._tear_down()         <MASK>             sleep(SLEEP_DELAY)",if self . is_live :,if resp is None :,9.423716574733431,9.423716574733431,0.0
"def provider_uris(self):     login_urls = {}     continue_url = self.request.get(""continue_url"")     for provider in self.provider_info:         <MASK>             login_url = self.uri_for(                 ""social-login"", provider_name=provider, continue_url=continue_url             )         else:             login_url = self.uri_for(""social-login"", provider_name=provider)         login_urls[provider] = login_url     return login_urls",if continue_url :,if continue_url :,100.00000000000004,100.00000000000004,1.0
"def expand_extensions(existing):     for name in extension_names:         ext = (             im(""lizard_ext.lizard"" + name.lower()).LizardExtension()             <MASK>             else name         )         existing.insert(             len(existing) if not hasattr(ext, ""ordering_index"") else ext.ordering_index,             ext,         )     return existing","if isinstance ( name , str )","if not hasattr ( ext , ""lizard_ext"" ) :",4.368583925857938,4.368583925857938,0.0
"def wrapper(self, *args, **kwargs):     if not self.request.path.endswith(""/""):         if self.request.method in (""GET"", ""HEAD""):             uri = self.request.path + ""/""             <MASK>                 uri += ""?"" + self.request.query             self.redirect(uri, permanent=True)             return         raise HTTPError(404)     return method(self, *args, **kwargs)",if self . request . query :,if self . request . query :,100.00000000000004,100.00000000000004,1.0
"def subword_map_by_joiner(subwords, marker=SubwordMarker.JOINER):     """"""Return word id for each subword token (annotate by joiner).""""""     flags = [0] * len(subwords)     for i, tok in enumerate(subwords):         <MASK>             flags[i] = 1         if tok.startswith(marker):             assert i >= 1 and flags[i - 1] != 1, ""Sentence `{}` not correct!"".format(                 "" "".join(subwords)             )             flags[i - 1] = 1     marker_acc = list(accumulate([0] + flags[:-1]))     word_group = [(i - maker_sofar) for i, maker_sofar in enumerate(marker_acc)]     return word_group",if tok . endswith ( marker ) :,if tok . startswith ( marker ) :,50.000000000000014,50.000000000000014,0.0
"def next_item(self, direction):     """"""Selects next menu item, based on self._direction""""""     start, i = -1, 0     try:         start = self.items.index(self._selected)         i = start + direction     except:         pass     while True:         if i == start:             # Cannot find valid menu item             self.select(start)             break         if i >= len(self.items):             i = 0             continue         if i < 0:             i = len(self.items) - 1             continue         <MASK>             break         i += direction         if start < 0:             start = 0",if self . select ( i ) :,if i >= self . items :,13.888095170058955,13.888095170058955,0.0
"def get_config(cls):     # FIXME: Replace this as soon as we have a config module     config = {}     # Try to get iflytek_yuyin config from config     profile_path = dingdangpath.config(""profile.yml"")     if os.path.exists(profile_path):         with open(profile_path, ""r"") as f:             profile = yaml.safe_load(f)             <MASK>                 if ""vid"" in profile[""iflytek_yuyin""]:                     config[""vid""] = profile[""iflytek_yuyin""][""vid""]     return config","if ""iflytek_yuyin"" in profile :",iflytek_yuyin :,18.306026428729766,18.306026428729766,0.0
"def get_signed_in_user(test_case):     playback = not (test_case.is_live or test_case.in_recording)     if playback:         return MOCKED_USER_NAME     else:         account_info = test_case.cmd(""account show"").get_output_in_json()         <MASK>             return account_info[""user""][""name""]     return None","if account_info [ ""user"" ] [ ""type"" ] != ""servicePrincipal"" :",if account_info :,3.520477365831487,3.520477365831487,0.0
"def rename_project(self, project, new_name):     """"""Rename project, update the related projects if necessary""""""     old_name = project.name     for proj in self.projects:         relproj = proj.get_related_projects()         <MASK>             relproj[relproj.index(old_name)] = new_name             proj.set_related_projects(relproj)     project.rename(new_name)     self.save()",if old_name in relproj :,if relproj :,16.605579150202516,0.0,0.0
"def test_call_extern_c_fn(self):     global memcmp     memcmp = cffi_support.ExternCFunction(         ""memcmp"",         (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""),     )     @udf(BooleanVal(FunctionContext, StringVal, StringVal))     def fn(context, a, b):         if a.is_null != b.is_null:             return False         if a is None:             return True         if len(a) != b.len:             return False         <MASK>             return True         return memcmp(a.ptr, b.ptr, a.len) == 0",if a . ptr == b . ptr :,if len ( b ) != 0 :,6.033504141761816,6.033504141761816,0.0
"def parse_variable(self):     begin = self._pos     while True:         ch = self.read()         <MASK>             return ScriptVariable(self._text[begin : self._pos - 1])         elif ch is None:             self.__raise_eof()         elif not isidentif(ch) and ch != "":"":             self.__raise_char(ch)","if ch == ""%"" :","if ch == "":"" :",59.4603557501361,59.4603557501361,0.0
"def h_file(self):     filename = self.abspath()     st = os.stat(filename)     cache = self.ctx.hashes_md5_tstamp     if filename in cache and cache[filename][0] == st.st_mtime:         return cache[filename][1]     if STRONGEST:         ret = Utils.h_file(filename)     else:         <MASK>             raise IOError(""Not a file"")         ret = Utils.md5(str((st.st_mtime, st.st_size)).encode()).digest()     cache[filename] = (st.st_mtime, ret)     return ret",if stat . S_ISDIR ( st [ stat . ST_MODE ] ) :,if not st . st_size :,2.6809511148331087,2.6809511148331087,0.0
"def add_widgets(self, *widgets_or_spacings):     """"""Add widgets/spacing to dialog vertical layout""""""     layout = self.layout()     for widget_or_spacing in widgets_or_spacings:         <MASK>             layout.addSpacing(widget_or_spacing)         else:             layout.addWidget(widget_or_spacing)","if isinstance ( widget_or_spacing , int ) :",if widget_or_spacing . is_vertical :,34.84694488743309,34.84694488743309,0.0
"def _str_index(self):     idx = self[""index""]     out = []     if len(idx) == 0:         return out     out += ["".. index:: %s"" % idx.get(""default"", """")]     for section, references in idx.iteritems():         <MASK>             continue         elif section == ""refguide"":             out += [""   single: %s"" % ("", "".join(references))]         else:             out += [""   %s: %s"" % (section, "","".join(references))]     return out","if section == ""default"" :","if section == ""single"" :",59.4603557501361,59.4603557501361,0.0
"def dictify_CPPDEFINES(env):     cppdefines = env.get(""CPPDEFINES"", {})     if cppdefines is None:         return {}     if SCons.Util.is_Sequence(cppdefines):         result = {}         for c in cppdefines:             <MASK>                 result[c[0]] = c[1]             else:                 result[c] = None         return result     if not SCons.Util.is_Dict(cppdefines):         return {cppdefines: None}     return cppdefines",if SCons . Util . is_Sequence ( c ) :,"if c [ 0 ] == ""CPPDEFINES"" :",4.503733751056993,4.503733751056993,0.0
"def decoder(s):     r = []     decode = []     for c in s:         if c == ""&"" and not decode:             decode.append(""&"")         elif c == ""-"" and decode:             if len(decode) == 1:                 r.append(""&"")             else:                 r.append(modified_unbase64("""".join(decode[1:])))             decode = []         <MASK>             decode.append(c)         else:             r.append(c)     if decode:         r.append(modified_unbase64("""".join(decode[1:])))     bin_str = """".join(r)     return (bin_str, len(s))",elif decode :,if len ( decode ) == 1 :,5.669791110976001,5.669791110976001,0.0
"def optimize(self, graph: Graph):     MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE     flag_changed = False     for v in traverse.listup_variables(graph):         <MASK>             continue         height, width = TextureShape.get(v)         if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE:             continue         if not v.has_attribute(SplitTarget):             flag_changed = True             v.attributes.add(SplitTarget())     return graph, flag_changed",if not Placeholder . check_resolved ( v . size ) :,if not v . is_shape :,9.252550052807573,9.252550052807573,0.0
"def one_gpr_reg_one_mem_scalable(ii):     n, r = 0, 0     for op in _gen_opnds(ii):         if op_agen(op) or (op_mem(op) and op.oc2 in [""v""]):             n += 1         <MASK>             r += 1         else:             return False     return n == 1 and r == 1",elif op_gprv ( op ) :,if op_mem ( op ) :,41.11336169005198,41.11336169005198,0.0
"def get_genome_dir(gid, galaxy_dir, data):     """"""Return standard location of genome directories.""""""     if galaxy_dir:         refs = genome.get_refs(gid, None, galaxy_dir, data)         seq_file = tz.get_in([""fasta"", ""base""], refs)         if seq_file and os.path.exists(seq_file):             return os.path.dirname(os.path.dirname(seq_file))     else:         gdirs = glob.glob(os.path.join(_get_data_dir(), ""genomes"", ""*"", gid))         <MASK>             return gdirs[0]",if len ( gdirs ) == 1 and os . path . exists ( gdirs [ 0 ] ) :,if len ( gdirs ) == 1 :,23.43746816281914,23.43746816281914,0.0
"def __modules(self):     raw_output = self.__module_avail_output().decode(""utf-8"")     for line in StringIO(raw_output):         line = line and line.strip()         if not line or line.startswith(""-""):             continue         line_modules = line.split()         for module in line_modules:             <MASK>                 module = module[0 : -len(self.default_indicator)].strip()             module_parts = module.split(""/"")             module_version = None             if len(module_parts) == 2:                 module_version = module_parts[1]             module_name = module_parts[0]             yield module_name, module_version",if module . endswith ( self . default_indicator ) :,if self . default_indicator :,32.73762387107808,32.73762387107808,0.0
"def save(self):     updates = self.cinder_obj_get_changes()     if updates:         <MASK>             metadata = updates.pop(""metadata"", None)             self.metadata = db.backup_metadata_update(                 self._context, self.id, metadata, True             )         updates.pop(""parent"", None)         db.backup_update(self._context, self.id, updates)     self.obj_reset_changes()","if ""metadata"" in updates :",if self . metadata :,9.423716574733431,9.423716574733431,0.0
"def test_set_tag(association_obj, sagemaker_session):     tag = {""Key"": ""foo"", ""Value"": ""bar""}     association_obj.set_tag(tag)     while True:         actual_tags = sagemaker_session.sagemaker_client.list_tags(             ResourceArn=association_obj.source_arn         )[""Tags""]         <MASK>             break         time.sleep(5)     # When sagemaker-client-config endpoint-url is passed as argument to hit some endpoints,     # length of actual tags will be greater than 1     assert len(actual_tags) > 0     assert actual_tags[0] == tag",if actual_tags :,if not actual_tags :,53.7284965911771,53.7284965911771,0.0
"def test_error_stream(environ, start_response):     writer = start_response(""200 OK"", [])     wsgi_errors = environ[""wsgi.errors""]     error_msg = None     for method in [         ""flush"",         ""write"",         ""writelines"",     ]:         if not hasattr(wsgi_errors, method):             error_msg = ""wsgi.errors has no '%s' attr"" % method         <MASK>             error_msg = ""wsgi.errors.%s attr is not callable"" % method         if error_msg:             break     return_msg = error_msg or ""success""     writer(return_msg)     return []","if not error_msg and not callable ( getattr ( wsgi_errors , method ) ) :",if not callable ( wsgi_errors [ method ] ) :,20.347556624849783,20.347556624849783,0.0
"def current_dict(cursor_offset, line):     """"""If in dictionary completion, return the dict that should be used""""""     for m in current_dict_re.finditer(line):         <MASK>             return LinePart(m.start(1), m.end(1), m.group(1))     return None",if m . start ( 2 ) <= cursor_offset and m . end ( 2 ) >= cursor_offset :,if m . group ( 0 ) == cursor_offset :,16.63956412397367,16.63956412397367,0.0
"def show_file_browser(self):     """"""Show/hide the file browser.""""""     if self.show_file_browser_action.isChecked():         sizes = self.panel.sizes()         <MASK>             sizes[0] = sum(sizes) // 4             self.panel.setSizes(sizes)         self.file_browser.show()     else:         self.file_browser.hide()",if sizes [ 0 ] == 0 :,if sizes :,8.525588607164655,0.0,0.0
"def run(self, paths=[]):     items = []     for item in SideBarSelection(paths).getSelectedItems():         items.append(item.nameEncoded())     if len(items) > 0:         sublime.set_clipboard(""\n"".join(items))         <MASK>             sublime.status_message(""Items copied"")         else:             sublime.status_message(""Item copied"")",if len ( items ) > 1 :,if item . is_selected ( ) :,6.742555929751843,6.742555929751843,0.0
"def prepend(self, value):     """"""prepend value to nodes""""""     root, root_text = self._get_root(value)     for i, tag in enumerate(self):         <MASK>             tag.text = """"         if len(root) > 0:             root[-1].tail = tag.text             tag.text = root_text         else:             tag.text = root_text + tag.text         if i > 0:             root = deepcopy(list(root))         tag[:0] = root         root = tag[: len(root)]     return self",if not tag . text :,if i == 0 :,9.652434877402245,9.652434877402245,0.0
"def getLabel(self, address=None):     if address is None:         address = self.address     label = address     if shared.config.has_section(address):         label = shared.config.get(address, ""label"")     queryreturn = sqlQuery(""""""select label from addressbook where address=?"""""", address)     <MASK>         for row in queryreturn:             (label,) = row     else:         queryreturn = sqlQuery(             """"""select label from subscriptions where address=?"""""", address         )         if queryreturn != []:             for row in queryreturn:                 (label,) = row     return label",if queryreturn != [ ] :,if queryreturn != [ ] :,100.00000000000004,100.00000000000004,1.0
"def _parse(self, engine):     """"""Parse the layer.""""""     if isinstance(self.args, dict):         if ""axis"" in self.args:             self.axis = engine.evaluate(self.args[""axis""], recursive=True)             if not isinstance(self.axis, int):                 raise ParsingError('""axis"" must be an integer.')         <MASK>             self.momentum = engine.evaluate(self.args[""momentum""], recursive=True)             if not isinstance(self.momentum, (int, float)):                 raise ParsingError('""momentum"" must be numeric.')","if ""momentum"" in self . args :","if ""momentum"" in self . args :",100.00000000000004,100.00000000000004,1.0
"def urlquote(*args, **kwargs):     new_kwargs = dict(kwargs)     if not PY3:         new_kwargs = dict(kwargs)         if ""encoding"" in new_kwargs:             del new_kwargs[""encoding""]         <MASK>             del new_kwargs[""errors""]     return quote(*args, **new_kwargs)","if ""errors"" in kwargs :","if ""errors"" in new_kwargs :",51.33450480401705,51.33450480401705,0.0
"def setNextFormPrevious(self, backup=STARTING_FORM):     try:         if self._THISFORM.FORM_NAME == self._FORM_VISIT_LIST[-1]:             self._FORM_VISIT_LIST.pop()  # Remove the current form. if it is at the end of the list         <MASK>             # take no action if it looks as if someone has already set the next form.             self.setNextForm(                 self._FORM_VISIT_LIST.pop()             )  # Switch to the previous form if one exists     except IndexError:         self.setNextForm(backup)",if self . _THISFORM . FORM_NAME == self . NEXT_ACTIVE_FORM :,if self . _THISFORM . FORM_NAME == self . _FORM_VISIT_LIST [ 0 ] :,58.1562356837876,58.1562356837876,0.0
"def iter_chars_to_words(self, chars):     current_word = []     for char in chars:         if not self.keep_blank_chars and char[""text""].isspace():             if current_word:                 yield current_word                 current_word = []         <MASK>             yield current_word             current_word = [char]         else:             current_word.append(char)     if current_word:         yield current_word","elif current_word and self . char_begins_new_word ( current_word , char ) :","if char [ ""text"" ] == "" "" :",1.7502348496399052,1.7502348496399052,0.0
"def get(self):     """"""return a secret by name""""""     results = self._get(""secrets"", self.name)     results[""decoded""] = {}     results[""exists""] = False     if results[""returncode""] == 0 and results[""results""][0]:         results[""exists""] = True         <MASK>             if ""data"" in results[""results""][0]:                 for sname, value in results[""results""][0][""data""].items():                     results[""decoded""][sname] = base64.b64decode(value)     if results[""returncode""] != 0 and '""%s"" not found' % self.name in results[""stderr""]:         results[""returncode""] = 0     return results",if self . decode :,"if ""data"" in results [ ""results"" ] :",4.02724819242185,4.02724819242185,0.0
"def insert_use(self, edit):     if self.is_first_use():         for location in [r""^\s*namespace\s+[\w\\]+[;{]"", r""<\?php""]:             inserted = self.insert_first_use(location, edit)             <MASK>                 break     else:         self.insert_use_among_others(edit)",if inserted :,if inserted :,100.00000000000004,0.0,1.0
"def _new_rsa_key(spec):     if ""name"" not in spec:         <MASK>             (head, tail) = os.path.split(spec[""key""])             spec[""path""] = head             spec[""name""] = tail         else:             spec[""name""] = spec[""key""]     return rsa_init(spec)","if ""/"" in spec [ ""key"" ] :","if ""path"" in spec :",15.749996500436227,15.749996500436227,0.0
"def mimeData(self, indexes):     if len(indexes) == 1:         index = indexes[0]         model = song = index.data(Qt.UserRole)         <MASK>             try:                 model = song.album             except (ProviderIOError, Exception):                 model = None         return ModelMimeData(model)",if index . column ( ) == Column . album :,if not model :,2.570814443273602,2.570814443273602,0.0
"def get(self, url, **kwargs):     app, url = self._prepare_call(url, kwargs)     if app:         if url.endswith(""ping"") and self._first_ping:             self._first_ping = False             return EmptyCapabilitiesResponse()         <MASK>             return ErrorApiResponse()         else:             response = app.get(url, **kwargs)             return TestingResponse(response)     else:         return requests.get(url, **kwargs)","elif ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url :",if response is None :,0.6495837404474224,0.6495837404474224,0.0
"def handle_noargs(self, **options):     self.style = color_style()     print(""Running Django's own validation:"")     self.validate(display_num_errors=True)     for model in loading.get_models():         if hasattr(model, ""_create_content_base""):             self.validate_base_model(model)         <MASK>             self.validate_content_type(model)","if hasattr ( model , ""_feincms_content_models"" ) :","if hasattr ( model , ""_create_content_type"" ) :",57.73502691896262,57.73502691896262,0.0
"def test_rules_widget(self):     subreddit = self.reddit.subreddit(pytest.placeholders.test_subreddit)     widgets = subreddit.widgets     with self.use_cassette(""TestSubredditWidgets.fetch_widgets""):         rules = None         for widget in widgets.sidebar:             <MASK>                 rules = widget                 break         assert isinstance(rules, RulesWidget)         assert rules == rules         assert rules.id == rules         assert rules.display         assert len(rules) > 0         assert subreddit == rules.subreddit","if isinstance ( widget , RulesWidget ) :",if rules is None :,6.9717291216921975,6.9717291216921975,0.0
"def __init__(self, exception):     message = str(exception)     with contextlib.suppress(IndexError):         underlying_exception = exception.args[0]         <MASK>             message = (                 ""maximum retries exceeded trying to reach the store.\n""                 ""Check your network connection, and check the store ""                 ""status at {}"".format(_STORE_STATUS_URL)             )     super().__init__(message=message)","if isinstance ( underlying_exception , urllib3 . exceptions . MaxRetryError ) :",if underlying_exception . errno == errno . ECONNREFUSED :,13.188274750399428,13.188274750399428,0.0
"def wrapped(self, request):     try:         return self._finished     except AttributeError:         if self.node_ids:             <MASK>                 log.debug(                     ""%s is still going to be used, not terminating it. ""                     ""Still in use on:\n%s"",                     self,                     pprint.pformat(list(self.node_ids)),                 )                 return         log.debug(""Finish called on %s"", self)         try:             return func(request)         finally:             self._finished = True",if not request . session . shouldfail and not request . session . shouldstop :,if self . node_ids :,2.75631563063758,2.75631563063758,0.0
"def get_min_vertical_scroll() -> int:     # Make sure that the cursor line is not below the bottom.     # (Calculate how many lines can be shown between the cursor and the .)     used_height = 0     prev_lineno = ui_content.cursor_position.y     for lineno in range(ui_content.cursor_position.y, -1, -1):         used_height += get_line_height(lineno)         <MASK>             return prev_lineno         else:             prev_lineno = lineno     return 0",if used_height > height - scroll_offsets_bottom :,if used_height > 0 :,27.30664777474173,27.30664777474173,0.0
"def cookies(self):     # strip cookie_suffix from all cookies in the request, return result     cookies = flask.Request.cookies.__get__(self)     result = {}     desuffixed = {}     for key, value in cookies.items():         <MASK>             desuffixed[key[: -len(self.cookie_suffix)]] = value         else:             result[key] = value     result.update(desuffixed)     return result",if key . endswith ( self . cookie_suffix ) :,if self . cookie_suffix :,32.73762387107808,32.73762387107808,0.0
"def update_vars(state1, state2):     ops = []     for name in state1._fields:         state1_vs = getattr(state1, name)         <MASK>             ops += [                 tf.assign(_v1, _v2)                 for _v1, _v2 in zip(state1_vs, getattr(state2, name))             ]         else:             ops += [tf.assign(state1_vs, getattr(state2, name))]     return tf.group(*ops)","if isinstance ( state1_vs , list ) :",if state2 . _fields :,5.484411595600381,5.484411595600381,0.0
"def manifest(self):     """"""The current manifest dictionary.""""""     if self.reload:         <MASK>             return {}         mtime = self.getmtime(self.manifest_path)         if self._mtime is None or mtime > self._mtime:             self._manifest = self.get_manifest()             self._mtime = mtime     return self._manifest",if not self . exists ( self . manifest_path ) :,if self . _manifest is None :,7.780436171361459,7.780436171361459,0.0
"def csvtitle(self):     if isinstance(self.name, six.string_types):         return '""' + self.name + '""' + char[""sep""] * (len(self.nick) - 1)     else:         ret = """"         for i, name in enumerate(self.name):             ret = ret + '""' + name + '""' + char[""sep""] * (len(self.nick) - 1)             <MASK>                 ret = ret + char[""sep""]         return ret",if i + 1 != len ( self . name ) :,if i == 0 :,6.011598678897526,6.011598678897526,0.0
"def cache_dst(self):     final_dst = None     final_linenb = None     for linenb, assignblk in enumerate(self):         for dst, src in viewitems(assignblk):             <MASK>                 if final_dst is not None:                     raise ValueError(""Multiple destinations!"")                 final_dst = src                 final_linenb = linenb     self._dst = final_dst     self._dst_linenb = final_linenb     return final_dst","if dst . is_id ( ""IRDst"" ) :",if linenb == linenb :,3.550932348642477,3.550932348642477,0.0
"def _ProcessName(self, name, dependencies):     """"""Retrieve a module name from a node name.""""""     module_name, dot, base_name = name.rpartition(""."")     if dot:         <MASK>             if module_name in dependencies:                 dependencies[module_name].add(base_name)             else:                 dependencies[module_name] = {base_name}         else:             # If we have a relative import that did not get qualified (usually due             # to an empty package_name), don't insert module_name='' into the             # dependencies; we get a better error message if we filter it out here             # and fail later on.             logging.warning(""Empty package name: %s"", name)",if module_name :,if base_name :,42.72870063962342,42.72870063962342,0.0
"def get_aa_from_codonre(re_aa):     aas = []     m = 0     for i in re_aa:         if i == ""["":             m = -1             aas.append("""")         elif i == ""]"":             m = 0             continue         elif m == -1:             aas[-1] = aas[-1] + i         <MASK>             aas.append(i)     return aas",elif m == 0 :,if m == 1 :,32.46679154750991,32.46679154750991,0.0
"def logic():     count = intbv(0, min=0, max=MAXVAL + 1)     while True:         yield clock.posedge, reset.posedge         if reset == 1:             count[:] = 0         else:             flag.next = 0             <MASK>                 flag.next = 1                 count[:] = 0             else:                 count += 1",if count == MAXVAL :,if count == 0 :,53.7284965911771,53.7284965911771,0.0
"def _history_define_metric(     self, hkey: str ) -> Optional[wandb_internal_pb2.MetricRecord]:     """"""check for hkey match in glob metrics, return defined metric.""""""     # Dont define metric for internal metrics     if hkey.startswith(""_""):         return None     for k, mglob in six.iteritems(self._metric_globs):         if k.endswith(""*""):             <MASK>                 m = wandb_internal_pb2.MetricRecord()                 m.CopyFrom(mglob)                 m.ClearField(""glob_name"")                 m.name = hkey                 return m     return None",if hkey . startswith ( k [ : - 1 ] ) :,if k . startswith ( hkey ) :,15.56087234272292,15.56087234272292,0.0
"def optimize_models(args, use_cuda, models):     """"""Optimize ensemble for generation""""""     for model in models:         model.make_generation_fast_(             beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,             need_attn=args.print_alignment,         )         <MASK>             model.half()         if use_cuda:             model.cuda()",if args . fp16 :,if args . half :,42.72870063962342,42.72870063962342,0.0
"def _Dynamic_Rollback(self, transaction, transaction_response):     txid = transaction.handle()     self.__local_tx_lock.acquire()     try:         <MASK>             raise apiproxy_errors.ApplicationError(                 datastore_pb.Error.BAD_REQUEST, ""Transaction %d not found."" % (txid,)             )         txdata = self.__transactions[txid]         assert (             txdata.thread_id == thread.get_ident()         ), ""Transactions are single-threaded.""         del self.__transactions[txid]     finally:         self.__local_tx_lock.release()",if txid not in self . __transactions :,if txdata is None :,4.673289785800722,4.673289785800722,0.0
"def get_job_dirs(path):     regex = re.compile(""[1-9][0-9]*-"")     jobdirs = []     for d in os.listdir(path):         # skip directories not matching the job result dir pattern         <MASK>             continue         d = os.path.join(options.resultsdir, d)         if os.path.isdir(d) and not os.path.exists(os.path.join(d, PUBLISH_FLAGFILE)):             jobdirs.append(d)     return jobdirs",if not regex . match ( d ) :,if not regex . match ( d ) :,100.00000000000004,100.00000000000004,1.0
"def traverse(node, functions=[]):     if hasattr(node, ""grad_fn""):         node = node.grad_fn     if hasattr(node, ""variable""):         node = graph.nodes_by_id.get(id(node.variable))         if node:             node.functions = list(functions)             del functions[:]     if hasattr(node, ""next_functions""):         functions.append(type(node).__name__)         for f in node.next_functions:             <MASK>                 functions.append(type(f[0]).__name__)                 traverse(f[0], functions)     if hasattr(node, ""saved_tensors""):         for t in node.saved_tensors:             traverse(t)",if f [ 0 ] :,"if hasattr ( f , ""next_functions"" ) :",4.456882760699063,4.456882760699063,0.0
"def get_all_snap_points(self, forts):     points = []     radius = Constants.MAX_DISTANCE_FORT_IS_REACHABLE     for i in range(0, len(forts)):         for j in range(i + 1, len(forts)):             c1, c2 = self.get_enclosing_circles(forts[i], forts[j], radius)             <MASK>                 points.append((c1, c2, forts[i], forts[j]))     return points",if c1 and c2 :,ifc1 != c2 :,21.3643503198117,21.3643503198117,0.0
"def doDir(elem):     for child in elem.childNodes:         if not isinstance(child, minidom.Element):             continue         if child.tagName == ""Directory"":             doDir(child)         elif child.tagName == ""Component"":             for grandchild in child.childNodes:                 <MASK>                     continue                 if grandchild.tagName != ""File"":                     continue                 files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))","if not isinstance ( grandchild , minidom . Element ) :","if grandchild . tagName != ""File"" :",5.369488567517933,5.369488567517933,0.0
"def computeLeadingWhitespaceWidth(s, tab_width):     w = 0     for ch in s:         if ch == "" "":             w += 1         <MASK>             w += abs(tab_width) - (w % abs(tab_width))         else:             break     return w","elif ch == ""\t"" :","if ch == "" "" :",36.74145494215666,36.74145494215666,0.0
"def test_avg_group_by(self):     ret = (         await Book.annotate(avg=Avg(""rating""))         .group_by(""author_id"")         .values(""author_id"", ""avg"")     )     for item in ret:         author_id = item.get(""author_id"")         avg = item.get(""avg"")         <MASK>             self.assertEqual(avg, 4.5)         elif author_id == self.a2.pk:             self.assertEqual(avg, 2.0)",if author_id == self . a1 . pk :,if author_id == self . a1 . pk :,100.00000000000004,100.00000000000004,1.0
"def open_session(self, app, request):     sid = request.cookies.get(app.session_cookie_name)     if sid:         stored_session = self.cls.objects(sid=sid).first()         <MASK>             expiration = stored_session.expiration             if not expiration.tzinfo:                 expiration = expiration.replace(tzinfo=utc)             if expiration > datetime.datetime.utcnow().replace(tzinfo=utc):                 return MongoEngineSession(                     initial=stored_session.data, sid=stored_session.sid                 )     return MongoEngineSession(sid=str(uuid.uuid4()))",if stored_session :,if stored_session :,100.00000000000004,100.00000000000004,1.0
"def one_line_description(self):     MAX_LINE_LENGTH = 120     desc = util.remove_html_tags(self.description or """")     desc = re.sub(""\s+"", "" "", desc).strip()     if not desc:         return _(""No description available"")     else:         # Decode the description to avoid gPodder bug 1277         desc = util.convert_bytes(desc).strip()         <MASK>             return desc[:MAX_LINE_LENGTH] + ""...""         else:             return desc",if len ( desc ) > MAX_LINE_LENGTH :,if MAX_LINE_LENGTH < desc . count ( ) :,34.38931217657843,34.38931217657843,0.0
"def setInnerHTML(self, html):     log.HTMLClassifier.classify(         log.ThugLogging.url if log.ThugOpts.local else log.last_url, html     )     self.tag.clear()     for node in bs4.BeautifulSoup(html, ""html.parser"").contents:         self.tag.append(node)         name = getattr(node, ""name"", None)         if name is None:             continue         handler = getattr(log.DFT, ""handle_%s"" % (name,), None)         <MASK>             handler(node)",if handler :,if handler is not None :,17.965205598154213,17.965205598154213,0.0
def get_supported_period_type_map(cls):     if cls.supported_period_map is None:         cls.supported_period_map = {}         cls.supported_period_map.update(cls.period_type_map)         try:             from dateutil import relativedelta             <MASK>                 cls.supported_period_map.update(cls.optional_period_type_map)         except Exception:             pass     return cls.supported_period_map,if relativedelta is not None :,if cls . optional_period_type_map :,4.456882760699063,4.456882760699063,0.0
"def _compare_single_run(self, compares_done):     try:         compare_id, redo = self.in_queue.get(             timeout=float(self.config[""ExpertSettings""][""block_delay""])         )     except Empty:         pass     else:         <MASK>             if redo:                 self.db_interface.delete_old_compare_result(compare_id)             compares_done.add(compare_id)             self._process_compare(compare_id)             if self.callback:                 self.callback()","if self . _decide_whether_to_process ( compare_id , redo , compares_done ) :",if compare_id :,1.234488517472643,1.234488517472643,0.0
"def _get_field_actual(cant_be_number, raw_string, field_names):     for line in raw_string.splitlines():         for field_name in field_names:             field_name = field_name.lower()             if "":"" in line:                 left, right = line.split("":"", 1)                 left = left.strip().lower()                 right = right.strip()                 if left == field_name and len(right) > 0:                     if cant_be_number:                         <MASK>                             return right                     else:                         return right     return None",if not right . isdigit ( ) :,if len ( left ) > 0 :,7.809849842300637,7.809849842300637,0.0
"def _p_basicstr_content(s, content=_basicstr_re):     res = []     while True:         res.append(s.expect_re(content).group(0))         if not s.consume(""\\""):             break         <MASK>             pass         elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re):             res.append(_chr(int(s.last().group(1), 16)))         else:             s.expect_re(_escapes_re)             res.append(_escapes[s.last().group(0)])     return """".join(res)",if s . consume_re ( _newline_esc_re ) :,"if s . consume_re ( "" "" ) :",43.90217885737765,43.90217885737765,0.0
"def removedir(self, path):     # type: (Text) -> None     _path = self.validatepath(path)     if _path == ""/"":         raise errors.RemoveRootError()     with ftp_errors(self, path):         try:             self.ftp.rmd(_encode(_path, self.ftp.encoding))         except error_perm as error:             code, _ = _parse_ftp_error(error)             if code == ""550"":                 if self.isfile(path):                     raise errors.DirectoryExpected(path)                 <MASK>                     raise errors.DirectoryNotEmpty(path)             raise  # pragma: no cover",if not self . isempty ( path ) :,if self . isdir ( path ) :,37.70794596593207,37.70794596593207,0.0
"def _normalize_store_path(self, resource_store):     if resource_store[""type""] == ""filesystem"":         <MASK>             resource_store[""base_directory""] = os.path.join(                 self.root_directory, resource_store[""base_directory""]             )     return resource_store","if not os . path . isabs ( resource_store [ ""base_directory"" ] ) :",if self . root_directory :,2.5784177834288906,2.5784177834288906,0.0
"def _apply_nested(name, val, nested):     parts = name.split(""."")     cur = nested     for i in range(0, len(parts) - 1):         cur = cur.setdefault(parts[i], {})         <MASK>             conflicts_with = ""."".join(parts[0 : i + 1])             raise ValueError(                 ""%r cannot be nested: conflicts with {%r: %s}""                 % (name, conflicts_with, cur)             )     cur[parts[-1]] = val","if not isinstance ( cur , dict ) :",if parts [ i ] != val :,5.669791110976001,5.669791110976001,0.0
"def build_packages(targeted_packages, distribution_directory, is_dev_build=False):     # run the build and distribution     for package_root in targeted_packages:         service_hierarchy = os.path.join(os.path.basename(package_root))         <MASK>             verify_update_package_requirement(package_root)         print(""Generating Package Using Python {}"".format(sys.version))         run_check_call(             [                 sys.executable,                 build_packing_script_location,                 ""--dest"",                 os.path.join(distribution_directory, service_hierarchy),                 package_root,             ],             root_dir,         )",if is_dev_build :,if is_dev_build :,100.00000000000004,100.00000000000004,1.0
"def resolve_root_node_address(self, root_node):     if ""["" in root_node:         name, numbers = root_node.split(""["", maxsplit=1)         number = numbers.split("","", maxsplit=1)[0]         <MASK>             number = number.split(""-"")[0]         number = re.sub(""[^0-9]"", """", number)         root_node = name + number     return root_node","if ""-"" in number :","if ""-"" in number :",100.00000000000004,100.00000000000004,1.0
"def _map_args(maps: dict, **kwargs):     # maps: key=old name, value= new name     output = {}     for name, val in kwargs.items():         if name in maps:             assert isinstance(maps[name], str)             output.update({maps[name]: val})         else:             output.update({name: val})     for keys in maps.keys():         <MASK>             pass     return output",if keys not in output . keys ( ) :,if keys [ 0 ] == key :,9.442944296079734,9.442944296079734,0.0
"def next_item(self, direction):     """"""Selects next menu item, based on self._direction""""""     start, i = -1, 0     try:         start = self.items.index(self._selected)         i = start + direction     except:         pass     while True:         if i == start:             # Cannot find valid menu item             self.select(start)             break         <MASK>             i = 0             continue         if i < 0:             i = len(self.items) - 1             continue         if self.select(i):             break         i += direction         if start < 0:             start = 0",if i >= len ( self . items ) :,if i > self . items . count ( ) :,27.455024338805618,27.455024338805618,0.0
"def detect_reentrancy(self, contract):     for function in contract.functions_and_modifiers_declared:         <MASK>             if self.KEY in function.context:                 continue             self._explore(function.entry_point, [])             function.context[self.KEY] = True",if function . is_implemented :,if self . KEY not in function . context :,9.980099403873663,9.980099403873663,0.0
"def load_model(self):     if not os.path.exists(self.get_filename(absolute=True)):         <MASK>             return {}, {}         error(             ""Model file with pre-trained convolution layers not found. Download it here..."",             ""https://github.com/alexjc/neural-enhance/releases/download/v%s/%s""             % (__version__, self.get_filename()),         )     print(""  - Loaded file `{}` with trained model."".format(self.get_filename()))     return pickle.load(bz2.open(self.get_filename(), ""rb""))",if args . train :,if not self . _trained :,8.643019616048525,8.643019616048525,0.0
"def get_nonexisting_check_definition_extends(definition, indexed_oval_defs):     # TODO: handle multiple levels of referrals.     # OVAL checks that go beyond one level of extend_definition won't be properly identified     for extdefinition in definition.findall("".//{%s}extend_definition"" % oval_ns):         # Verify each extend_definition in the definition         extdefinitionref = extdefinition.get(""definition_ref"")         # Search the OVAL tree for a definition with the referred ID         referreddefinition = indexed_oval_defs.get(extdefinitionref)         <MASK>             # There is no oval satisfying the extend_definition referal             return extdefinitionref     return None",if referreddefinition is None :,if referreddefinition :,32.34325178227722,0.0,0.0
"def pause(self):     if self.is_playing:         self.state = MusicPlayerState.PAUSED         <MASK>             self._current_player.pause()         self.emit(""pause"", player=self, entry=self.current_entry)         return     elif self.is_paused:         return     raise ValueError(""Cannot pause a MusicPlayer in state %s"" % self.state)",if self . _current_player :,if self . current_entry :,27.890014303843827,27.890014303843827,0.0
"def setNextFormPrevious(self, backup=STARTING_FORM):     try:         <MASK>             self._FORM_VISIT_LIST.pop()  # Remove the current form. if it is at the end of the list         if self._THISFORM.FORM_NAME == self.NEXT_ACTIVE_FORM:             # take no action if it looks as if someone has already set the next form.             self.setNextForm(                 self._FORM_VISIT_LIST.pop()             )  # Switch to the previous form if one exists     except IndexError:         self.setNextForm(backup)",if self . _THISFORM . FORM_NAME == self . _FORM_VISIT_LIST [ - 1 ] :,if self . _THISFORM . FORM_NAME == self . NEXT_ACTIVE_FORM :,54.95528514675272,54.95528514675272,0.0
"def get_expr_referrers(schema: s_schema.Schema, obj: so.Object) -> Dict[so.Object, str]:     """"""Return schema referrers with refs in expressions.""""""     refs = schema.get_referrers_ex(obj)     result = {}     for (mcls, fn), referrers in refs.items():         field = mcls.get_field(fn)         <MASK>             result.update({ref: fn for ref in referrers})     return result","if issubclass ( field . type , ( Expression , ExpressionList ) ) :",if field . is_ref :,5.746166391236874,5.746166391236874,0.0
"def _fields_to_index(cls):     fields = []     for field in cls._meta.sorted_fields:         <MASK>             continue         requires_index = any(             (field.index, field.unique, isinstance(field, ForeignKeyField))         )         if requires_index:             fields.append(field)     return fields",if field . primary_key :,if not field . index :,16.341219448835542,16.341219448835542,0.0
"def ident_values(self):     value = self._ident_values     if value is False:         value = None         # XXX: how will this interact with orig_prefix ?         #      not exposing attrs for now if orig_prefix is set.         if not self.orig_prefix:             wrapped = self.wrapped             idents = getattr(wrapped, ""ident_values"", None)             <MASK>                 value = [self._wrap_hash(ident) for ident in idents]             ##else:             ##    ident = self.ident             ##    if ident is not None:             ##        value = [ident]         self._ident_values = value     return value",if idents :,if idents is not None :,17.965205598154213,17.965205598154213,0.0
"def apply_incpaths_ml(self):     inc_lst = self.includes.split()     lst = self.incpaths_lst     for dir in inc_lst:         node = self.path.find_dir(dir)         if not node:             error(""node not found: "" + str(dir))             continue         <MASK>             lst.append(node)         self.bld_incpaths_lst.append(node)",if not node in lst :,"if node . name == ""bld"" :",5.522397783539471,5.522397783539471,0.0
"def application_openFiles_(self, nsapp, filenames):     # logging.info('[osx] file open')     # logging.info('[osx] file : %s' % (filenames))     for filename in filenames:         logging.info(""[osx] receiving from macOS : %s"", filename)         if os.path.exists(filename):             <MASK>                 sabnzbd.add_nzbfile(filename, keep=True)",if sabnzbd . filesystem . get_ext ( filename ) in VALID_ARCHIVES + VALID_NZB_FILES :,"if nsapp . macOS == ""mac"" :",1.6633142482788623,1.6633142482788623,0.0
"def check(self, xp, nout):     input = xp.asarray(self.x).astype(numpy.float32)     with warnings.catch_warnings():         if self.ignore_warning:             warnings.simplefilter(""ignore"", self.ignore_warning)         <MASK>             self.check_positive(xp, self.func, input, self.eps, nout)         else:             self.check_negative(xp, self.func, input, self.eps, nout)",if self . result :,if self . positive :,42.72870063962342,42.72870063962342,0.0
"def _set_scheme(url, newscheme):     scheme = _get_scheme(url)     newscheme = newscheme or """"     newseparator = "":"" if newscheme in COLON_SEPARATED_SCHEMES else ""://""     if scheme == """":  # Protocol relative URL.         url = ""%s:%s"" % (newscheme, url)     elif scheme is None and url:  # No scheme.         url = """".join([newscheme, newseparator, url])     elif scheme:  # Existing scheme.         remainder = url[len(scheme) :]         <MASK>             remainder = remainder[3:]         elif remainder.startswith("":""):             remainder = remainder[1:]         url = """".join([newscheme, newseparator, remainder])     return url","if remainder . startswith ( ""://"" ) :",if remainder . startswith ( newscheme ) :,36.06452879987793,36.06452879987793,0.0
"def parquet(tables, data_directory, ignore_missing_dependency, **params):     try:         import pyarrow as pa  # noqa: F401         import pyarrow.parquet as pq  # noqa: F401     except ImportError:         msg = ""PyArrow dependency is missing""         <MASK>             logger.warning(""Ignored: %s"", msg)             return 0         else:             raise click.ClickException(msg)     data_directory = Path(data_directory)     for table, df in read_tables(tables, data_directory):         arrow_table = pa.Table.from_pandas(df)         target_path = data_directory / ""{}.parquet"".format(table)         pq.write_table(arrow_table, str(target_path))",if ignore_missing_dependency :,if ignore_missing_dependency :,100.00000000000004,100.00000000000004,1.0
"def h2i(self, pkt, s):     t = ()     if type(s) is str:         t = time.strptime(s)         t = t[:2] + t[2:-3]     else:         <MASK>             y, m, d, h, min, sec, rest, rest, rest = time.gmtime(time.time())             t = (y, m, d, h, min, sec)         else:             t = s     return t",if not s :,if type ( s ) is int :,7.267884212102741,7.267884212102741,0.0
"def filter_episodes(self, batch, cross_entropy):     """"""Filter the episodes for the cross_entropy method""""""     accumulated_reward = [sum(rewards) for rewards in batch[""rewards""]]     percentile = cross_entropy * 100     reward_bound = np.percentile(accumulated_reward, percentile)     # we save the batch with reward above the bound     result = {k: [] for k in self.data_keys}     episode_kept = 0     for i in range(len(accumulated_reward)):         <MASK>             for k in self.data_keys:                 result[k].append(batch[k][i])             episode_kept += 1     return result",if accumulated_reward [ i ] >= reward_bound :,if reward_bound > episode_kept :,14.06401411379081,14.06401411379081,0.0
"def _readenv(var, msg):     match = _ENV_VAR_PAT.match(var)     if match and match.groups():         envvar = match.groups()[0]         if envvar in os.environ:             value = os.environ[envvar]             <MASK>                 value = value.decode(""utf8"")             return value         else:             raise InvalidConfigException(                 ""{} - environment variable '{}' not set"".format(msg, var)             )     else:         raise InvalidConfigException(             ""{} - environment variable name '{}' does not match pattern '{}'"".format(                 msg, var, _ENV_VAR_PAT_STR             )         )",if six . PY2 :,if value :,17.799177396293473,0.0,0.0
"def _allocate_nbd(self):     if not os.path.exists(""/sys/block/nbd0""):         self.error = _(""nbd unavailable: module not loaded"")         return None     while True:         if not self._DEVICES:             # really want to log this info, not raise             self.error = _(""No free nbd devices"")             return None         device = self._DEVICES.pop()         <MASK>             break     return device","if not os . path . exists ( ""/sys/block/%s/pid"" % os . path . basename ( device ) ) :",if device is None :,0.09472565111320143,0.09472565111320143,0.0
"def _expand_deps_java_generation(self):     """"""Ensure that all multilingual dependencies such as proto_library generate java code.""""""     queue = collections.deque(self.deps)     keys = set()     while queue:         k = queue.popleft()         <MASK>             keys.add(k)             dep = self.target_database[k]             if ""generate_java"" in dep.attr:  # Has this attribute                 dep.attr[""generate_java""] = True                 queue.extend(dep.deps)",if k not in keys :,if k not in keys :,100.00000000000004,100.00000000000004,1.0
"def load_syntax(syntax):     context = _create_scheme() or {}     partition_scanner = PartitionScanner(syntax.get(""partitions"", []))     scanners = {}     for part_name, part_scanner in list(syntax.get(""scanner"", {}).items()):         scanners[part_name] = Scanner(part_scanner)     formats = []     for fname, fstyle in list(syntax.get(""formats"", {}).items()):         if isinstance(fstyle, basestring):             <MASK>                 key = fstyle[2:-2]                 fstyle = context[key]             else:                 fstyle = fstyle % context         formats.append((fname, fstyle))     return partition_scanner, scanners, formats","if fstyle . startswith ( ""%("" ) and fstyle . endswith ( "")s"" ) :",if context :,0.08593353840063979,0.0,0.0
"def rollback(self):     for operation, values in self.current_transaction_state[::-1]:         <MASK>             values.remove()         elif operation == ""update"":             old_value, new_value = values             if new_value.full_filename != old_value.full_filename:                 os.unlink(new_value.full_filename)             old_value.write()     self._post_xact_cleanup()","if operation == ""insert"" :","if operation == ""delete"" :",59.4603557501361,59.4603557501361,0.0
"def _buildOffsets(offsetDict, localeData, indexStart):     o = indexStart     for key in localeData:         <MASK>             for k in key.split(""|""):                 offsetDict[k] = o         else:             offsetDict[key] = o         o += 1","if ""|"" in key :",if o % 2 :,8.51528917838043,8.51528917838043,0.0
"def _check_start_pipeline_execution_errors(     graphene_info, execution_params, execution_plan ):     if execution_params.step_keys:         for step_key in execution_params.step_keys:             <MASK>                 raise UserFacingGraphQLError(                     graphene_info.schema.type_named(""InvalidStepError"")(                         invalid_step_key=step_key                     )                 )",if not execution_plan . has_step ( step_key ) :,if step_key not in execution_plan . step_keys :,27.392758081541032,27.392758081541032,0.0
"def __setattr__(self, option_name, option_value):     if option_name in self._options:         # type checking         sort = self.OPTIONS[self.arch.name][option_name][0]         <MASK>             self._options[option_name] = option_value         else:             raise ValueError(                 'Value for option ""%s"" must be of type %s' % (option_name, sort)             )     else:         super(CFGArchOptions, self).__setattr__(option_name, option_value)","if sort is None or isinstance ( option_value , sort ) :",if sort is not None :,9.049145405312009,9.049145405312009,0.0
"def value(self):     quote = False     if self.defects:         quote = True     else:         for x in self:             <MASK>                 quote = True     if quote:         pre = post = """"         if self[0].token_type == ""cfws"" or self[0][0].token_type == ""cfws"":             pre = "" ""         if self[-1].token_type == ""cfws"" or self[-1][-1].token_type == ""cfws"":             post = "" ""         return pre + quote_string(self.display_name) + post     else:         return super(DisplayName, self).value","if x . token_type == ""quoted-string"" :","if x . token_type == ""cfws"" :",76.91605673134588,76.91605673134588,0.0
"def __init__(self, patch_files, patch_directories):     files = []     files_data = {}     for filename_data in patch_files:         <MASK>             filename, data = filename_data         else:             filename = filename_data             data = None         if not filename.startswith(os.sep):             filename = ""{0}{1}"".format(FakeState.deploy_dir, filename)         files.append(filename)         if data:             files_data[filename] = data     self.files = files     self.files_data = files_data     self.directories = patch_directories","if isinstance ( filename_data , list ) :","if isinstance ( filename_data , ( str , unicode ) ) :",47.855439210937384,47.855439210937384,0.0
"def _evaluateStack(s):     op = s.pop()     if op in ""+-*/@^"":         op2 = _evaluateStack(s)         op1 = _evaluateStack(s)         result = opn[op](op1, op2)         <MASK>             print(result)         return result     else:         return op",if debug_flag :,if result :,17.799177396293473,0.0,0.0
"def reconnect_user(self, user_id, host_id, server_id):     if host_id == settings.local.host_id:         return     if server_id and self.server.id != server_id:         return     for client in self.clients.find({""user_id"": user_id}):         self.clients.update_id(             client[""id""],             {                 ""ignore_routes"": True,             },         )         <MASK>             self.instance.disconnect_wg(client[""id""])         else:             self.instance_com.client_kill(client[""id""])","if len ( client [ ""id"" ] ) > 32 :","if client [ ""id"" ] == self . server . id :",34.79159475128448,34.79159475128448,0.0
"def _get_library(self, name, args):     library_database = self._library_manager.get_new_connection_to_library_database()     try:         last_updated = library_database.get_library_last_updated(name, args)         if last_updated:             <MASK>                 self._library_manager.fetch_keywords(                     name, args, self._libraries_need_refresh_listener                 )             return library_database.fetch_library_keywords(name, args)         return self._library_manager.get_and_insert_keywords(name, args)     finally:         library_database.close()",if time . time ( ) - last_updated > 10.0 :,if self . _libraries_need_refresh_listener :,4.406306339938217,4.406306339938217,0.0
"def get_paths(self, path, commit):     """"""Return a generator of all filepaths under path at commit.""""""     _check_path_is_repo_relative(path)     git_path = _get_git_path(path)     tree = self.gl_repo.git_repo[commit.tree[git_path].id]     assert tree.type == pygit2.GIT_OBJ_TREE     for tree_entry in tree:         tree_entry_path = os.path.join(path, tree_entry.name)         <MASK>             for fp in self.get_paths(tree_entry_path, commit):                 yield fp         else:             yield tree_entry_path","if tree_entry . type == ""tree"" :",if tree_entry_path :,21.28139770959968,21.28139770959968,0.0
"def scan_resource_conf(self, conf):     if ""properties"" in conf:         if ""attributes"" in conf[""properties""]:             <MASK>                 if conf[""properties""][""attributes""][""exp""]:                     return CheckResult.PASSED     return CheckResult.FAILED","if ""exp"" in conf [ ""properties"" ] [ ""attributes"" ] :","if ""exp"" in conf [ ""properties"" ] [ ""attributes"" ] :",100.00000000000004,100.00000000000004,1.0
"def _set_parse_context(self, tag, tag_attrs):     # special case: script or style parse context     if not self._wb_parse_context:         if tag == ""style"":             self._wb_parse_context = ""style""         elif tag == ""script"":             <MASK>                 self._wb_parse_context = ""script""",if self . _allow_js_type ( tag_attrs ) :,"if tag == ""script"" :",3.0297048914466935,3.0297048914466935,0.0
"def modified(self):     paths = set()     dictionary_list = []     for op_list in self._operations:         if not isinstance(op_list, list):             op_list = (op_list,)         for item in chain(*op_list):             <MASK>                 continue             dictionary = item.dictionary             if dictionary.path in paths:                 continue             paths.add(dictionary.path)             dictionary_list.append(dictionary)     return dictionary_list",if item is None :,if not item . path :,10.682175159905853,10.682175159905853,0.0
def preorder(root):     res = []     if not root:         return res     stack = []     stack.append(root)     while stack:         root = stack.pop()         res.append(root.val)         <MASK>             stack.append(root.right)         if root.left:             stack.append(root.left)     return res,if root . right :,if root . right :,100.00000000000004,100.00000000000004,1.0
"def create(exported_python_target):     if exported_python_target not in created:         self.context.log.info(             ""Creating setup.py project for {}"".format(exported_python_target)         )         subject = self.derived_by_original.get(             exported_python_target, exported_python_target         )         setup_dir, dependencies = self.create_setup_py(subject, dist_dir)         created[exported_python_target] = setup_dir         if self._recursive:             for dep in dependencies:                 <MASK>                     create(dep)",if is_exported_python_target ( dep ) :,if dep . name == setup_dir :,4.85851417160653,4.85851417160653,0.0
"def test_array_interface(self, data):     result = np.array(data)     np.testing.assert_array_equal(result[0], data[0])     result = np.array(data, dtype=object)     expected = np.array(list(data), dtype=object)     for a1, a2 in zip(result, expected):         <MASK>             assert np.isnan(a1) and np.isnan(a2)         else:             tm.assert_numpy_array_equal(a2, a1)",if np . isscalar ( a1 ) :,if a1 == a2 :,7.654112967106117,7.654112967106117,0.0
"def valueChanged(plug):     changed = plug.getInput() is not None     if not changed and isinstance(plug, Gaffer.ValuePlug):         <MASK>             changed = not Gaffer.NodeAlgo.isSetToUserDefault(plug)         else:             changed = not plug.isSetToDefault()     return changed",if Gaffer . NodeAlgo . hasUserDefault ( plug ) :,if plug . isUser ( ) :,11.260801105802155,11.260801105802155,0.0
"def process_tag(hive_name, company, company_key, tag, default_arch):     with winreg.OpenKeyEx(company_key, tag) as tag_key:         version = load_version_data(hive_name, company, tag, tag_key)         <MASK>  # if failed to get version bail             major, minor, _ = version             arch = load_arch_data(hive_name, company, tag, tag_key, default_arch)             if arch is not None:                 exe_data = load_exe(hive_name, company, company_key, tag)                 if exe_data is not None:                     exe, args = exe_data                     return company, major, minor, arch, exe, args",if version is not None :,if version is None :,40.93653765389909,40.93653765389909,0.0
"def __iter__(self):     for name, value in self.__class__.__dict__.items():         if isinstance(value, alias_flag_value):             continue         <MASK>             yield (name, self._has_flag(value.flag))","if isinstance ( value , flag_value ) :","if isinstance ( value , alias_flag_value ) :",63.40466277046863,63.40466277046863,0.0
"def connect(self):     self.sock = sockssocket()     self.sock.setproxy(*proxy_args)     if type(self.timeout) in (int, float):         self.sock.settimeout(self.timeout)     self.sock.connect((self.host, self.port))     if isinstance(self, compat_http_client.HTTPSConnection):         <MASK>  # Python > 2.6             self.sock = self._context.wrap_socket(self.sock, server_hostname=self.host)         else:             self.sock = ssl.wrap_socket(self.sock)","if hasattr ( self , ""_context"" ) :",if self . _context :,8.871198783083607,8.871198783083607,0.0
"def frequent_thread_switches():     """"""Make concurrency bugs more likely to manifest.""""""     interval = None     if not sys.platform.startswith(""java""):         if hasattr(sys, ""getswitchinterval""):             interval = sys.getswitchinterval()             sys.setswitchinterval(1e-6)         else:             interval = sys.getcheckinterval()             sys.setcheckinterval(1)     try:         yield     finally:         if not sys.platform.startswith(""java""):             <MASK>                 sys.setswitchinterval(interval)             else:                 sys.setcheckinterval(interval)","if hasattr ( sys , ""setswitchinterval"" ) :","if sys . platform . startswith ( ""java"" ) :",15.580105704117443,15.580105704117443,0.0
"def vars(self):     ret = []     if op.intlist:         varlist = op.intlist     else:         varlist = self.discover         for name in varlist:             if name in (""0"", ""1"", ""2"", ""8"", ""CPU0"", ""ERR"", ""LOC"", ""MIS"", ""NMI""):                 varlist.remove(name)         if not op.full and len(varlist) > 3:             varlist = varlist[-3:]     for name in varlist:         if name in self.discover:             ret.append(name)         <MASK>             ret.append(self.intmap[name.lower()])     return ret",elif name . lower ( ) in self . intmap :,if self . intmap :,20.1420709131299,20.1420709131299,0.0
"def deleteDuplicates(gadgets, callback=None):     toReturn = []     inst = set()     count = 0     added = False     len_gadgets = len(gadgets)     for i, gadget in enumerate(gadgets):         inst.add(gadget._gadget)         <MASK>             count = len(inst)             toReturn.append(gadget)             added = True         if callback:             callback(gadget, added, float(i + 1) / (len_gadgets))             added = False     return toReturn",if len ( inst ) > count :,if count == len_gadgets :,7.809849842300637,7.809849842300637,0.0
"def ident(self):     value = self._ident     if value is False:         value = None         # XXX: how will this interact with orig_prefix ?         #      not exposing attrs for now if orig_prefix is set.         if not self.orig_prefix:             wrapped = self.wrapped             ident = getattr(wrapped, ""ident"", None)             <MASK>                 value = self._wrap_hash(ident)         self._ident = value     return value",if ident is not None :,if ident is not None :,100.00000000000004,100.00000000000004,1.0
"def _flatten_settings_from_form(self, settings, form, form_values):     """"""Take a nested dict and return a flat dict of setting values.""""""     setting_values = {}     for field in form.c:         if isinstance(field, _ContainerMixin):             setting_values.update(                 self._flatten_settings_from_form(                     settings, field, form_values[field._name]                 )             )         <MASK>             setting_values[field._name] = form_values[field._name]     return setting_values",elif field . _name in settings :,if field . _name in form_values :,41.11336169005198,41.11336169005198,0.0
"def _decorator(cls):     for name, meth in inspect.getmembers(cls, inspect.isroutine):         if name not in cls.__dict__:             continue         if name != ""__init__"":             <MASK>                 continue         if name in butnot:             continue         setattr(cls, name, decorator(meth))     return cls","if not private and name . startswith ( ""_"" ) :",if meth is not None :,3.3264637832151163,3.3264637832151163,0.0
"def _do_cmp(f1, f2):     bufsize = BUFSIZE     with open(f1, ""rb"") as fp1, open(f2, ""rb"") as fp2:         while True:             b1 = fp1.read(bufsize)             b2 = fp2.read(bufsize)             <MASK>                 return False             if not b1:                 return True",if b1 != b2 :,if not b2 :,21.444097124017667,21.444097124017667,0.0
"def _memoized(*args):     now = time.time()     try:         value, last_update = self.cache[args]         age = now - last_update         if self._call_count > self.ctl or age > self.ttl:             self._call_count = 0             raise AttributeError         <MASK>             self._call_count += 1         return value     except (KeyError, AttributeError):         value = func(*args)         if value:             self.cache[args] = (value, now)         return value     except TypeError:         return func(*args)",if self . ctl :,if age > self . ttl :,15.619699684601283,15.619699684601283,0.0
"def check(self, hyperlinks: Dict[str, Hyperlink]) -> Generator[CheckResult, None, None]:     self.invoke_threads()     total_links = 0     for hyperlink in hyperlinks.values():         <MASK>             yield CheckResult(                 hyperlink.uri, hyperlink.docname, hyperlink.lineno, ""ignored"", """", 0             )         else:             self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, hyperlink), False)             total_links += 1     done = 0     while done < total_links:         yield self.rqueue.get()         done += 1     self.shutdown_threads()",if self . is_ignored_uri ( hyperlink . uri ) :,if hyperlink . is_ignored :,18.693159143202898,18.693159143202898,0.0
"def remove_subscriber(self, topic, subscriber):     if subscriber in self.subscribers[topic]:         <MASK>             subscriber._pyroRelease()         if hasattr(subscriber, ""_pyroUri""):             try:                 proxy = self.proxy_cache[subscriber._pyroUri]                 proxy._pyroRelease()                 del self.proxy_cache[subscriber._pyroUri]             except KeyError:                 pass         self.subscribers[topic].discard(subscriber)","if hasattr ( subscriber , ""_pyroRelease"" ) :","if hasattr ( subscriber , ""_pyroRelease"" ) :",100.00000000000004,100.00000000000004,1.0
"def delete_arc(collection, document, origin, target, type):     directory = collection     real_dir = real_directory(directory)     mods = ModificationTracker()     projectconf = ProjectConfiguration(real_dir)     document = path_join(real_dir, document)     with TextAnnotations(document) as ann_obj:         # bail as quick as possible if read-only         <MASK>             raise AnnotationsIsReadOnlyError(ann_obj.get_document())         _delete_arc_with_ann(origin, target, type, mods, ann_obj, projectconf)         mods_json = mods.json_response()         mods_json[""annotations""] = _json_from_ann(ann_obj)         return mods_json",if ann_obj . _read_only :,if ann_obj . get_document ( ) != document :,28.917849332325716,28.917849332325716,0.0
"def _select_from(self, parent_path, is_dir, exists, listdir):     if not is_dir(parent_path):         return     with _cached(listdir) as listdir:         yielded = set()         try:             successor_select = self.successor._select_from             for starting_point in self._iterate_directories(                 parent_path, is_dir, listdir             ):                 for p in successor_select(starting_point, is_dir, exists, listdir):                     <MASK>                         yield p                         yielded.add(p)         finally:             yielded.clear()",if p not in yielded :,if p . parent_path == parent_path :,7.495553473355845,7.495553473355845,0.0
"def _fractional_part(self, n, expr, evaluation):     n_sympy = n.to_sympy()     if n_sympy.is_constant():         <MASK>             positive_integer_part = (                 Expression(""Floor"", n).evaluate(evaluation).to_python()             )             result = n - positive_integer_part         else:             negative_integer_part = (                 Expression(""Ceiling"", n).evaluate(evaluation).to_python()             )             result = n - negative_integer_part     else:         return expr     return from_python(result)",if n_sympy >= 0 :,if n_sympy . is_positive ( ) :,24.808415001701817,24.808415001701817,0.0
"def check_bounds(geometry):     if isinstance(geometry[0], (list, tuple)):         return list(map(check_bounds, geometry))     else:         if geometry[0] > 180 or geometry[0] < -180:             raise ValueError(                 ""Longitude is out of bounds, check your JSON format or data""             )         <MASK>             raise ValueError(                 ""Latitude is out of bounds, check your JSON format or data""             )",if geometry [ 1 ] > 90 or geometry [ 1 ] < - 90 :,if geometry [ 0 ] > - 180 :,11.152372863964953,11.152372863964953,0.0
"def get_absolute_path(self, root, path):     # find the first absolute path that exists     self.root = self.roots[0]     for root in self.roots:         abspath = os.path.abspath(os.path.join(root, path))         <MASK>             self.root = root  # make sure all the other methods in the base class know how to find the file             break     return abspath",if os . path . exists ( abspath ) :,if abspath == self . root :,6.082317172853824,6.082317172853824,0.0
"def do_setflow(self, l=""""):     try:         <MASK>             l = str(self.flow_slider.GetValue())         else:             l = l.lower()         flow = int(l)         if self.p.online:             self.p.send_now(""M221 S"" + l)             self.log(_(""Setting print flow factor to %d%%."") % flow)         else:             self.logError(_(""Printer is not online.""))     except Exception as x:         self.logError(_(""You must enter a flow. (%s)"") % (repr(x),))","if not isinstance ( l , str ) or not len ( l ) :",if self . flow_slider :,2.4906123264252495,2.4906123264252495,0.0
"def sources():     for d in os.listdir(base):         #        if d.startswith('talis'):         #            continue         <MASK>             continue         if d == ""indcat"":             continue         if not os.path.isdir(base + d):             continue         yield d","if d . endswith ( ""old"" ) :","if d == ""talis"" :",10.816059393812111,10.816059393812111,0.0
"def create_accumulator(self) -> tf_metric_accumulators.TFCompilableMetricsAccumulator:     configs = zip(self._metric_configs, self._loss_configs)     padding_options = None     if self._eval_config is not None:         model_spec = model_util.get_model_spec(self._eval_config, self._model_name)         <MASK>             padding_options = model_spec.padding_options     return tf_metric_accumulators.TFCompilableMetricsAccumulator(         padding_options,         [len(m) + len(l) for m, l in configs],         desired_batch_size=self._desired_batch_size,     )","if model_spec is not None and model_spec . HasField ( ""padding_options"" ) :",if model_spec is not None :,17.120323028183236,17.120323028183236,0.0
"def parseImpl(self, instring, loc, doActions=True):     try:         loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)     except (ParseException, IndexError):         <MASK>             if self.expr.resultsName:                 tokens = ParseResults([self.defaultValue])                 tokens[self.expr.resultsName] = self.defaultValue             else:                 tokens = [self.defaultValue]         else:             tokens = []     return loc, tokens",if self . defaultValue is not self . __optionalNotMatched :,if self . expr . resultsName :,13.597602315271134,13.597602315271134,0.0
"def handleConnection(self):     # connection handshake     try:         <MASK>             return True         self.csock.close()     except:         ex_t, ex_v, ex_tb = sys.exc_info()         tb = util.formatTraceback(ex_t, ex_v, ex_tb)         log.warning(""error during connect/handshake: %s; %s"", ex_v, ""\n"".join(tb))         self.csock.close()     return False",if self . daemon . _handshake ( self . csock ) :,if self . csock . connect ( ) :,20.564695254383224,20.564695254383224,0.0
"def getProc(su, innerTarget):     if len(su) == 1:  # have a one element wedge         proc = (""first"", ""last"")     else:         <MASK>             proc = (""first"", ""last"")  # same element can be first and last         elif su.isFirst(innerTarget):             proc = (""first"",)         elif su.isLast(innerTarget):             proc = (""last"",)         else:             proc = ()     return proc",if su . isFirst ( innerTarget ) and su . isLast ( innerTarget ) :,if su . isFirst ( innerTarget ) :,41.686201967850856,41.686201967850856,0.0
"def get_color_dtype(data, column_names):     has_color = all(column in data[""points""] for column in column_names)     if has_color:         color_data_types = [             data[""points""][column_name].dtype for column_name in column_names         ]         <MASK>             raise TypeError(                 f""Data types of color values are inconsistent: got {color_data_types}""             )         color_data_type = color_data_types[0]     else:         color_data_type = None     return color_data_type",if len ( set ( color_data_types ) ) > 1 :,if len ( color_data_types ) != 1 :,52.30193450457853,52.30193450457853,0.0
"def close(self):     children = []     for children_part, line_offset, last_line_offset_leaf in self.children_groups:         <MASK>             try:                 _update_positions(children_part, line_offset, last_line_offset_leaf)             except _PositionUpdatingFinished:                 pass         children += children_part     self.tree_node.children = children     # Reset the parents     for node in children:         node.parent = self.tree_node",if line_offset != 0 :,if line_offset_leaf :,37.68499164492418,37.68499164492418,0.0
"def get_multi(self, keys, index=None):     with self._lmdb.begin() as txn:         result = []         for key in keys:             packed = txn.get(key.encode())             <MASK>                 result.append((key, cbor.loads(packed)))     return result",if packed is not None :,if packed :,23.174952587773145,0.0,0.0
"def get_directory_info(prefix, pth, recursive):     res = []     directory = os.listdir(pth)     directory.sort()     for p in directory:         if p[0] != ""."":             subp = os.path.join(pth, p)             p = os.path.join(prefix, p)             <MASK>                 res.append([p, get_directory_info(prefix, subp, 1)])             else:                 res.append([p, None])     return res",if recursive and os . path . isdir ( subp ) :,if subp :,1.9758011175389976,0.0,0.0
"def __schedule(self, workflow_scheduler_id, workflow_scheduler):     invocation_ids = self.__active_invocation_ids(workflow_scheduler_id)     for invocation_id in invocation_ids:         log.debug(""Attempting to schedule workflow invocation [%s]"", invocation_id)         self.__attempt_schedule(invocation_id, workflow_scheduler)         <MASK>             return",if not self . monitor_running :,if self . __is_scheduled ( ) :,9.425159511373677,9.425159511373677,0.0
"def write(self, data):     self.size -= len(data)     passon = None     if self.size > 0:         self.data.append(data)     else:         if self.size:             data, passon = data[: self.size], data[self.size :]         else:             passon = b""""         <MASK>             self.data.append(data)     return passon",if data :,if data :,100.00000000000004,0.0,1.0
"def __getstate__(self):     try:         store_func, load_func = self.store_function, self.load_function         self.store_function, self.load_function = None, None         # ignore analyses. we re-initialize analyses when restoring from pickling so that we do not lose any newly         # added analyses classes         d = dict(             (k, v)             for k, v in self.__dict__.items()             <MASK>             not in {                 ""analyses"",             }         )         return d     finally:         self.store_function, self.load_function = store_func, load_func",if k,"if k not in [ ""analyses"" , ""analyses"" ] :",5.679677445135579,5.679677445135579,0.0
"def mouse_down(self, event):     if event.button == 1:         if self.scrolling:             p = event.local             <MASK>                 self.scroll_up()                 return             elif self.scroll_down_rect().collidepoint(p):                 self.scroll_down()                 return     if event.button == 4:         self.scroll_up()     if event.button == 5:         self.scroll_down()     GridView.mouse_down(self, event)",if self . scroll_up_rect ( ) . collidepoint ( p ) :,if p :,0.5208155200691346,0.0,0.0
"def on_api_command(self, command, data):     if command == ""select"":         if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can():             return flask.abort(403, ""Insufficient permissions"")         if self._prompt is None:             return flask.abort(409, ""No active prompt"")         choice = data[""choice""]         <MASK>             return flask.abort(                 400, ""{!r} is not a valid value for choice"".format(choice)             )         self._answer_prompt(choice)","if not isinstance ( choice , int ) or not self . _prompt . validate_choice ( choice ) :",if choice not in self . _prompt :,8.372412376046533,8.372412376046533,0.0
"def register_predictors(self, model_data_arr):     for integration in self._get_integrations():         <MASK>             integration.register_predictors(model_data_arr)         else:             logger.warning(                 f""There is no connection to {integration.name}. predictor wouldn't be registred.""             )",if integration . check_connection ( ) :,if integration . connection is not None :,20.612390921238426,20.612390921238426,0.0
"def _pack_shears(shearData):     shears = list()     vidxs = list()     for e_idx, entry in enumerate(shearData):         # Should be 3 entries         <MASK>             shears.extend([float(""nan""), float(""nan"")])             vidxs.extend([0, 0])         else:             vidx1, vidx2, shear1, shear2 = entry             shears.extend([shear1, shear2])             vidxs.extend([vidx1, vidx2])     return (np.asarray(shears, dtype=np.float32), np.asarray(vidxs, dtype=np.uint32))",if entry is None :,if e_idx == 3 :,6.567274736060395,6.567274736060395,0.0
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]:     yield ""Core"", ""0""     for _dir in data_manager.cog_data_path().iterdir():         fpath = _dir / ""settings.json""         if not fpath.exists():             continue         with fpath.open() as f:             try:                 data = json.load(f)             except json.JSONDecodeError:                 continue         if not isinstance(data, dict):             continue         cog_name = _dir.stem         for cog_id, inner in data.items():             <MASK>                 continue             yield cog_name, cog_id","if not isinstance ( inner , dict ) :",if inner is None :,6.316906128202129,6.316906128202129,0.0
"def subFeaName(m, newNames, state):     try:         int(m[3], 16)     except:         return m[0]     name = m[2]     if name in newNames:         # print('sub %r => %r' % (m[0], m[1] + newNames[name] + m[4]))         <MASK>             print(""sub %r => %r"" % (m[0], m[1] + newNames[name] + m[4]))         state[""didChange""] = True         return m[1] + newNames[name] + m[4]     return m[0]","if name == ""uni0402"" :","if state [ ""didChange"" ] :",7.809849842300637,7.809849842300637,0.0
"def log_graph(self, model: LightningModule, input_array=None):     if self._log_graph:         if input_array is None:             input_array = model.example_input_array         <MASK>             input_array = model._apply_batch_transfer_handler(input_array)             self.experiment.add_graph(model, input_array)         else:             rank_zero_warn(                 ""Could not log computational graph since the""                 "" `model.example_input_array` attribute is not set""                 "" or `input_array` was not given"",                 UserWarning,             )",if input_array is not None :,if model . batch_transfer_handler is not None :,23.462350320527996,23.462350320527996,0.0
"def apply(self, db, person):     for family_handle in person.get_family_handle_list():         family = db.get_family_from_handle(family_handle)         if family:             for event_ref in family.get_event_ref_list():                 if event_ref:                     event = db.get_event_from_handle(event_ref.ref)                     <MASK>                         return True                     if not event.get_date_object():                         return True     return False",if not event . get_place_handle ( ) :,if not event . get_date_object ( ) :,54.52469119630866,54.52469119630866,0.0
"def format(m):     if m > 1000:         <MASK>             return (str(int(m / 1000)), ""km"")         else:             return (str(round(m / 1000, 1)), ""km"")     return (str(m), ""m"")",if m % 1000 == 0 :,if m < 1000 :,13.943458243384402,13.943458243384402,0.0
"def previous(self):     try:         idx = _jump_list_index         next_index = idx + 1         <MASK>             next_index = 100         next_index = min(len(_jump_list) - 1, next_index)         _jump_list_index = next_index         return _jump_list[next_index]     except (IndexError, KeyError) as e:         return None",if next_index > 100 :,if next_index > 100 :,100.00000000000004,100.00000000000004,1.0
"def _validate_and_set_default_hyperparameters(self):     """"""Placeholder docstring""""""     # Check if all the required hyperparameters are set. If there is a default value     # for one, set it.     for name, definition in self.hyperparameter_definitions.items():         if name not in self.hyperparam_dict:             spec = definition[""spec""]             <MASK>                 self.hyperparam_dict[name] = spec[""DefaultValue""]             elif ""IsRequired"" in spec and spec[""IsRequired""]:                 raise ValueError(""Required hyperparameter: %s is not set"" % name)","if ""DefaultValue"" in spec :","if ""DefaultValue"" in spec :",100.00000000000004,100.00000000000004,1.0
"def _actions_read(self, c):     self.action_input.handle_read(c)     if c in [curses.KEY_ENTER, util.KEY_ENTER2]:         # take action         if self.action_input.selected_index == 0:  # Cancel             self.back_to_parent()         <MASK>  # Apply             self._apply_prefs()             client.core.get_config().addCallback(self._update_preferences)         elif self.action_input.selected_index == 2:  # OK             self._apply_prefs()             self.back_to_parent()",elif self . action_input . selected_index == 1 :,"if c in [ util . KEY_ENTER , util . KEY_ENTER2 ] :",3.4197980307804725,3.4197980307804725,0.0
"def _split_anonymous_function(s):     # Regex is not sufficient to handle differences between anonymous     # functions and YAML encoded lists. We perform a sniff test to see     # if it might be an anonymous function and then confirm by     # decoding it as YAML and testing the result.     if s[:1] == ""["" and s[-1:] == ""]"" and "":"" in s:         try:             l = yaml_util.decode_yaml(s)         except Exception:             return None, s[1:-1]         else:             <MASK>                 return None, s[1:-1]     return None","if len ( l ) == 1 and isinstance ( l [ 0 ] , ( six . string_types , int ) ) :",if l != s :,0.34662804081990395,0.34662804081990395,0.0
"def test_source_address(self):     for addr, is_ipv6 in VALID_SOURCE_ADDRESSES:         <MASK>             warnings.warn(""No IPv6 support: skipping."", NoIPv6Warning)             continue         pool = HTTPConnectionPool(             self.host, self.port, source_address=addr, retries=False         )         self.addCleanup(pool.close)         r = pool.request(""GET"", ""/source_address"")         self.assertEqual(r.data, b(addr[0]))",if is_ipv6 and not HAS_IPV6_AND_DNS :,if not is_ipv6 :,9.471153562668167,9.471153562668167,0.0
"def vim_G(self):     """"""Put the cursor on the last character of the file.""""""     if self.is_text_wrapper(self.w):         <MASK>             self.do(""end-of-buffer-extend-selection"")         else:             self.do(""end-of-buffer"")         self.done()     else:         self.quit()","if self . state == ""visual"" :",if self . is_text_wrapper ( self . w ) :,11.359354890271161,11.359354890271161,0.0
"def backend_supported(module, manager, **kwargs):     if CollectionNodeModule.backend_supported(module, manager, **kwargs):         if ""tid"" not in kwargs:             return True         conn = manager.connection(did=kwargs[""did""])         template_path = ""partitions/sql/{0}/#{0}#{1}#"".format(             manager.server_type, manager.version         )         SQL = render_template(             ""/"".join([template_path, ""backend_support.sql""]), tid=kwargs[""tid""]         )         status, res = conn.execute_scalar(SQL)         # check if any errors         <MASK>             return internal_server_error(errormsg=res)         return res",if not status :,"if status != ""OK"" :",7.267884212102741,7.267884212102741,0.0
"def _get_regex_config(self, data_asset_name: Optional[str] = None) -> dict:     regex_config: dict = copy.deepcopy(self._default_regex)     asset: Optional[Asset] = None     if data_asset_name:         asset = self._get_asset(data_asset_name=data_asset_name)     if asset is not None:         # Override the defaults         <MASK>             regex_config[""pattern""] = asset.pattern         if asset.group_names:             regex_config[""group_names""] = asset.group_names     return regex_config",if asset . pattern :,if asset . pattern :,100.00000000000004,100.00000000000004,1.0
"def resolve(self, other):     if other == ANY_TYPE:         return self     elif isinstance(other, ComplexType):         f = self.first.resolve(other.first)         s = self.second.resolve(other.second)         <MASK>             return ComplexType(f, s)         else:             return None     elif self == ANY_TYPE:         return other     else:         return None",if f and s :,if f and s :,100.00000000000004,100.00000000000004,1.0
"def collect_pages(app):     new_images = {}     for full_path, basename in app.builder.images.iteritems():         base, ext = os.path.splitext(full_path)         retina_path = base + ""@2x"" + ext         <MASK>             new_images[retina_path] = app.env.images[retina_path][1]     app.builder.images.update(new_images)     return []",if retina_path in app . env . images :,"if basename == ""images"" :",4.995138898472386,4.995138898472386,0.0
"def has_bad_headers(self):     headers = [self.sender, self.reply_to] + self.recipients     for header in headers:         if _has_newline(header):             return True     if self.subject:         if _has_newline(self.subject):             for linenum, line in enumerate(self.subject.split(""\r\n"")):                 <MASK>                     return True                 if linenum > 0 and line[0] not in ""\t "":                     return True                 if _has_newline(line):                     return True                 if len(line.strip()) == 0:                     return True     return False",if not line :,if linenum == 0 :,9.652434877402245,9.652434877402245,0.0
"def reader():     try:         imgs = mp4_loader(video_path, seg_num, seglen, mode)         <MASK>             logger.error(                 ""{} frame length {} less than 1."".format(video_path, len(imgs))             )             yield None, None     except:         logger.error(""Error when loading {}"".format(mp4_path))         yield None, None     imgs_ret = imgs_transform(         imgs, mode, seg_num, seglen, short_size, target_size, img_mean, img_std     )     label_ret = video_path     yield imgs_ret, label_ret",if len ( imgs ) < 1 :,if len ( imgs ) > 1 :,59.4603557501361,59.4603557501361,0.0
"def translate_from_sortname(name, sortname):     """"""'Translate' the artist name by reversing the sortname.""""""     for c in name:         ctg = unicodedata.category(c)         <MASK>             for separator in ("" & "", ""; "", "" and "", "" vs. "", "" with "", "" y ""):                 if separator in sortname:                     parts = sortname.split(separator)                     break             else:                 parts = [sortname]                 separator = """"             return separator.join(map(_reverse_sortname, parts))     return name","if ctg [ 0 ] == ""L"" and unicodedata . name ( c ) . find ( ""LATIN"" ) == - 1 :","if ctg == ""y"" :",2.036793448355394,2.036793448355394,0.0
"def _to_local_path(path):     """"""Convert local path to SFTP path""""""     if sys.platform == ""win32"":  # pragma: no cover         path = os.fsdecode(path)         <MASK>             path = path[1:]         path = path.replace(""/"", ""\\"")     return path","if path [ : 1 ] == ""/"" and path [ 2 : 3 ] == "":"" :","if ""/"" in path :",2.563024872452784,2.563024872452784,0.0
"def __call__(self, text: str) -> str:     for t in self.cleaner_types:         if t == ""tacotron"":             text = tacotron_cleaner.cleaners.custom_english_cleaners(text)         <MASK>             text = jaconv.normalize(text)         elif t == ""vietnamese"":             if vietnamese_cleaners is None:                 raise RuntimeError(""Please install underthesea"")             text = vietnamese_cleaners.vietnamese_cleaner(text)         else:             raise RuntimeError(f""Not supported: type={t}"")     return text","elif t == ""jaconv"" :","if t == ""jaconv"" :",84.08964152537145,84.08964152537145,0.0
"def cb_syncthing_system_data(self, daemon, mem, cpu, d_failed, d_total):     if self.daemon.get_my_id() in self.devices:         # Update my device display         device = self.devices[self.daemon.get_my_id()]         device[""ram""] = sizeof_fmt(mem)         device[""cpu""] = ""%3.2f%%"" % (cpu)         <MASK>             device[""announce""] = _(""disabled"")         else:             device[""announce""] = ""%s/%s"" % (d_total - d_failed, d_total)",if d_total == 0 :,if d_failed == 0 :,50.000000000000014,50.000000000000014,0.0
"def update_kls(self, sampled_kls):     for i, kl in enumerate(sampled_kls):         <MASK>             self.kl_coeff_val[i] *= 0.5         elif kl > 1.5 * self.kl_target:             self.kl_coeff_val[i] *= 2.0     return self.kl_coeff_val",if kl < self . kl_target / 1.5 :,if kl < self . kl_target :,71.19674182275,71.19674182275,0.0
"def DeleteEmptyCols(self):     cols2delete = []     for c in range(0, self.GetCols()):         f = True         for r in range(0, self.GetRows()):             if self.FindItemAtPosition((r, c)) is not None:                 f = False         <MASK>             cols2delete.append(c)     for i in range(0, len(cols2delete)):         self.ShiftColsLeft(cols2delete[i] + 1)         cols2delete = [x - 1 for x in cols2delete]",if f :,if not f :,35.35533905932737,35.35533905932737,0.0
"def get_session(self):     if self._session is None:         session = super(ChildResourceManager, self).get_session()         <MASK>             session = session.get_session_for_resource(self.resource_type.resource)         self._session = session     return self._session",if self . resource_type . resource != constants . RESOURCE_ACTIVE_DIRECTORY :,if self . resource_type is not None :,24.141769716889275,24.141769716889275,0.0
"def _get_master_authorized_networks_config(self, raw_cluster):     if raw_cluster.get(""masterAuthorizedNetworksConfig""):         config = raw_cluster.get(""masterAuthorizedNetworksConfig"")         config[""includes_public_cidr""] = False         for block in config[""cidrBlocks""]:             <MASK>                 config[""includes_public_cidr""] = True         return config     else:         return {""enabled"": False, ""cidrBlocks"": [], ""includes_public_cidr"": False}","if block [ ""cidrBlock"" ] == ""0.0.0.0/0"" :",if block . enabled :,3.199805213077364,3.199805213077364,0.0
"def scan_folder(folder):     scanned_files = []     for root, dirs, files in os.walk(folder):         dirs[:] = [d for d in dirs if d != ""__pycache__""]         relative_path = os.path.relpath(root, folder)         for f in files:             <MASK>                 continue             relative_name = os.path.normpath(os.path.join(relative_path, f)).replace(                 ""\\"", ""/""             )             scanned_files.append(relative_name)     return sorted(scanned_files)","if f . endswith ( "".pyc"" ) :",if not os . path . isdir ( relative_path ) :,8.130850857597444,8.130850857597444,0.0
"def read_progress(self):     while True:         processed_file = self.queue.get()         self.threading_completed.append(processed_file)         total_number = len(self.file_list)         completed_number = len(self.threading_completed)         # Just for the record, this slows down book searching by about 20%         if _progress_emitter:  # Skip update in reading mode             _progress_emitter.update_progress(completed_number * 100 // total_number)         <MASK>             break",if total_number == completed_number :,if completed_number > total_number :,33.584386823726156,33.584386823726156,0.0
"def next_instruction_is_function_or_class(lines):     """"""Is the first non-empty, non-commented line of the cell either a function or a class?""""""     parser = StringParser(""python"")     for i, line in enumerate(lines):         if parser.is_quoted():             parser.read_line(line)             continue         parser.read_line(line)         if not line.strip():  # empty line             <MASK>                 return False             continue         if line.startswith(""def "") or line.startswith(""class ""):             return True         if line.startswith((""#"", ""@"", "" "", "")"")):             continue         return False     return False",if i > 0 and not lines [ i - 1 ] . strip ( ) :,if i == 0 :,3.086457674499703,3.086457674499703,0.0
def __next__(self):     try:         data = next(self.iter_loader)     except StopIteration:         self._epoch += 1         <MASK>             self._dataloader.sampler.set_epoch(self._epoch)         self.iter_loader = iter(self._dataloader)         data = next(self.iter_loader)     return data,"if hasattr ( self . _dataloader . sampler , ""set_epoch"" ) :",if not data :,0.736550669835346,0.736550669835346,0.0
"def dgl_mp_batchify_fn(data):     if isinstance(data[0], tuple):         data = zip(*data)         return [dgl_mp_batchify_fn(i) for i in data]     for dt in data:         if dt is not None:             <MASK>                 return [d for d in data if isinstance(d, dgl.DGLGraph)]             elif isinstance(dt, nd.NDArray):                 pad = Pad(axis=(1, 2), num_shards=1, ret_length=False)                 data_list = [dt for dt in data if dt is not None]                 return pad(data_list)","if isinstance ( dt , dgl . DGLGraph ) :","if isinstance ( dt , dgl . DGLGraph ) :",100.00000000000004,100.00000000000004,1.0
"def f(self, info):     for k in keys:         <MASK>             for k2 in list(info.keys()):                 if k(k2):                     info.pop(k2)         else:             info.pop(k, None)",if callable ( k ) :,if k in info :,11.51015341649912,11.51015341649912,0.0
"def create(path, binary=False):     for i in range(10):         try:             os.makedirs(os.path.dirname(path), exist_ok=True)             <MASK>                 return open(path, ""wb"")             else:                 return open(path, ""w"", encoding=""utf-8"")             if i > 0:                 log(True, f""Created {path} at attempt {i + 1}"")         except:             time.sleep(0.5)     else:         raise Error(f""Failed to create {path}"")",if binary :,if binary :,100.00000000000004,0.0,1.0
"def validate_update(self, update_query):     structure = DotCollapsedDict(self.doc_class.structure)     for op, fields in update_query.iteritems():         for field in fields:             if op != ""$unset"" and op != ""$rename"":                 <MASK>                     raise UpdateQueryError(                         ""'%s' not found in %s's structure""                         % (field, self.doc_class.__name__)                     )",if field not in structure :,if field not in structure :,100.00000000000004,100.00000000000004,1.0
"def check_enums_ATLAS_ISAEXT(lines):     for i, isaext in enumerate(ATLAS_ISAEXT):         got = lines.pop(0).strip()         <MASK>             expect = ""none: 1""         else:             expect = ""{0}: {1}"".format(isaext, 1 << i)         if got != expect:             raise RuntimeError(                 ""ATLAS_ISAEXT mismatch at position ""                 + str(i)                 + "": got >>""                 + got                 + ""<<, expected >>""                 + expect                 + ""<<""             )",if i == 0 :,if i == 0 :,100.00000000000004,100.00000000000004,1.0
"def _test_export_session_csv(self, test_session=None):     with self.app.test_request_context():         <MASK>             test_session = SessionFactory()         field_data = export_sessions_csv([test_session])         session_row = field_data[1]         self.assertEqual(session_row[0], ""example (accepted)"")         self.assertEqual(session_row[9], ""accepted"")",if not test_session :,if test_session is None :,27.77619034011791,27.77619034011791,0.0
"def get_report_to_platform(self, args, scan_reports):     if self.bc_api_key:         <MASK>             repo_id = self.get_repository(args)             self.setup_bridgecrew_credentials(                 bc_api_key=self.bc_api_key, repo_id=repo_id             )         if self.is_integration_configured():             self._upload_run(args, scan_reports)",if args . directory :,if self . is_integration_configured ( ) :,4.9323515694897075,4.9323515694897075,0.0
"def test_fvalue(self):     if not getattr(self, ""skip_f"", False):         rtol = getattr(self, ""rtol"", 1e-10)         assert_allclose(self.res1.fvalue, self.res2.F, rtol=rtol)         <MASK>             # only available with ivreg2             assert_allclose(self.res1.f_pvalue, self.res2.Fp, rtol=rtol)     else:         raise pytest.skip(""TODO: document why this test is skipped"")","if hasattr ( self . res2 , ""Fp"" ) :",if self . res2 . f_pvalue :,14.448814886766836,14.448814886766836,0.0
"def fix_repeating_arguments(self):     """"""Fix elements that should accumulate/increment values.""""""     either = [list(child.children) for child in transform(self).children]     for case in either:         for e in [child for child in case if case.count(child) > 1]:             if type(e) is Argument or type(e) is Option and e.argcount:                 if e.value is None:                     e.value = []                 <MASK>                     e.value = e.value.split()             if type(e) is Command or type(e) is Option and e.argcount == 0:                 e.value = 0     return self",elif type ( e . value ) is not list :,if e . value . strip ( ) :,16.89983564524027,16.89983564524027,0.0
"def touch(self):     if not self.exists():         try:             self.parent().touch()         except ValueError:             pass         node = self._fs.touch(self.pathnames, {})         if not node.isdir:             raise AssertionError(""Not a folder: %s"" % self.path)         <MASK>             self.watcher.emit(""created"", self)",if self . watcher :,if node . islink :,14.058533129758727,14.058533129758727,0.0
"def __init__(self, _inf=None, _tzinfos=None):     if _inf:         self._tzinfos = _tzinfos         self._utcoffset, self._dst, self._tzname = _inf     else:         _tzinfos = {}         self._tzinfos = _tzinfos         self._utcoffset, self._dst, self._tzname = self._transition_info[0]         _tzinfos[self._transition_info[0]] = self         for inf in self._transition_info[1:]:             <MASK>                 _tzinfos[inf] = self.__class__(inf, _tzinfos)",if not _tzinfos . has_key ( inf ) :,if inf in _tzinfos :,7.509307647752128,7.509307647752128,0.0
"def test_sample_output():     comment = ""SAMPLE OUTPUT""     skip_files = [""__init__.py""]     errors = []     for _file in sorted(MODULE_PATH.iterdir()):         <MASK>             with _file.open() as f:                 if comment not in f.read():                     errors.append((comment, _file))     if errors:         line = ""Missing sample error(s) detected!\n\n""         for error in errors:             line += ""`{}` is not in module `{}`\n"".format(*error)         print(line[:-1])         assert False","if _file . suffix == "".py"" and _file . name not in skip_files :",if _file not in skip_files :,17.63592542803416,17.63592542803416,0.0
"def http_get(url, target):     req = requests.get(url, stream=True)     content_length = req.headers.get(""Content-Length"")     total = int(content_length) if content_length is not None else None     progress = tqdm(unit=""B"", total=total)     with open(target, ""wb"") as target_file:         for chunk in req.iter_content(chunk_size=1024):             <MASK>  # filter out keep-alive new chunks                 progress.update(len(chunk))                 target_file.write(chunk)     progress.close()",if chunk :,if chunk :,100.00000000000004,0.0,1.0
"def _elements_to_datasets(self, elements, level=0):     for element in elements:         extra_kwds = {""identifier_%d"" % level: element[""name""]}         <MASK>             for inner_element in self._elements_to_datasets(                 element[""elements""], level=level + 1             ):                 dataset = extra_kwds.copy()                 dataset.update(inner_element)                 yield dataset         else:             dataset = extra_kwds             extra_kwds.update(element)             yield extra_kwds","if ""elements"" in element :","if element [ ""elements"" ] :",24.446151121745064,24.446151121745064,0.0
"def update_dict(a, b):     for key, value in b.items():         if value is None:             continue         <MASK>             a[key] = value         elif isinstance(a[key], dict) and isinstance(value, dict):             update_dict(a[key], value)         elif isinstance(a[key], list):             a[key].append(value)         else:             a[key] = [a[key], value]",if key not in a :,"if isinstance ( a [ key ] , list ) :",5.300156689756295,5.300156689756295,0.0
"def scan(self, targets):     for target in targets:         target.print_infos()         if self.is_interesting(target):             self.target[""other""].append(target)             <MASK>                 return target     return None",if self . match ( target ) :,if self . is_interesting ( target ) :,37.99178428257963,37.99178428257963,0.0
"def printConnections(switches):     ""Compactly print connected nodes to each switch""     for sw in switches:         output(""%s: "" % sw)         for intf in sw.intfList():             link = intf.link             <MASK>                 intf1, intf2 = link.intf1, link.intf2                 remote = intf1 if intf1.node != sw else intf2                 output(""%s(%s) "" % (remote.node, sw.ports[intf]))         output(""\n"")",if link :,if link :,100.00000000000004,0.0,1.0
"def __cut(sentence):     global emit_P     prob, pos_list = viterbi(sentence, ""BMES"", start_P, trans_P, emit_P)     begin, nexti = 0, 0     # print pos_list, sentence     for i, char in enumerate(sentence):         pos = pos_list[i]         if pos == ""B"":             begin = i         <MASK>             yield sentence[begin : i + 1]             nexti = i + 1         elif pos == ""S"":             yield char             nexti = i + 1     if nexti < len(sentence):         yield sentence[nexti:]","elif pos == ""E"" :",if begin < len ( sentence ) :,5.522397783539471,5.522397783539471,0.0
"def check_files(self, paths=None):     """"""Run all checks on the paths.""""""     if paths is None:         paths = self.paths     report = self.options.report     runner = self.runner     report.start()     try:         for path in paths:             <MASK>                 self.input_dir(path)             elif not self.excluded(path):                 runner(path)     except KeyboardInterrupt:         print(""... stopped"")     report.stop()     return report",if os . path . isdir ( path ) :,if self . input_dir ( path ) :,29.071536848410968,29.071536848410968,0.0
"def verts_of_loop(edge_loop):     verts = []     for e0, e1 in iter_pairs(edge_loop, False):         <MASK>             v0 = e0.shared_vert(e1)             verts += [e0.other_vert(v0), v0]         verts += [e1.other_vert(verts[-1])]     if len(verts) > 1 and verts[0] == verts[-1]:         return verts[:-1]     return verts",if not verts :,if len ( verts ) > 1 :,7.267884212102741,7.267884212102741,0.0
"def generator(self, data):     for task in data:         # Do we scan everything or just /bin/bash instances?         <MASK>             continue         for bucket in task.bash_hash_entries():             yield (                 0,                 [                     int(task.p_pid),                     str(task.p_comm),                     int(bucket.times_found),                     str(bucket.key),                     str(bucket.data.path),                 ],             )","if not ( self . _config . SCAN_ALL or str ( task . p_comm ) == ""bash"" ) :",if not task . p_pid :,3.974388264629921,3.974388264629921,0.0
"def __get_ratio(self):     """"""Return splitter ratio of the main splitter.""""""     c = self.c     free_layout = c.free_layout     if free_layout:         w = free_layout.get_main_splitter()         if w:             aList = w.sizes()             <MASK>                 n1, n2 = aList                 # 2017/06/07: guard against division by zero.                 ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2)                 return ratio     return 0.5",if len ( aList ) == 2 :,if aList :,5.370784274455332,0.0,0.0
"def geterrors(self):     """"""Get all error messages.""""""     notes = self.getnotes(origin=""translator"").split(""\n"")     errordict = {}     for note in notes:         <MASK>             error = note.replace(""(pofilter) "", """")             errorname, errortext = error.split("": "", 1)             errordict[errorname] = errortext     return errordict","if ""(pofilter) "" in note :","if note . startswith ( ""pofilter"" ) :",7.056995965394887,7.056995965394887,0.0
"def rename_path(self, path, new_path):     logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path))     dirs = self.readdir(path)     for d in dirs:         if d in [""."", ""..""]:             continue         d_path = """".join([path, ""/"", d])         d_new_path = """".join([new_path, ""/"", d])         attr = self.getattr(d_path)         <MASK>             self.rename_path(d_path, d_new_path)         else:             self.rename_item(d_path, d_new_path)     self.rename_item(path, new_path, dir=True)","if stat . S_ISDIR ( attr [ ""st_mode"" ] ) :",if attr is not None :,1.707863452144561,1.707863452144561,0.0
"def index(self, url_id: int) -> FlaskResponse:  # pylint: disable=no-self-use     url = db.session.query(models.Url).get(url_id)     if url and url.url:         explore_url = ""//superset/explore/?""         <MASK>             explore_url += f""r={url_id}""             return redirect(explore_url[1:])         return redirect(url.url[1:])     flash(""URL to nowhere..."", ""danger"")     return redirect(""/"")",if url . url . startswith ( explore_url ) :,"if url . url [ 0 ] == ""explore"" :",21.401603033752977,21.401603033752977,0.0
"def testShortCircuit(self):     """"""Test that creation short-circuits to reuse existing references""""""     sd = {}     for s in self.ss:         sd[s] = 1     for t in self.ts:         <MASK>             self.assertTrue(sd.has_key(safeRef(t.x)))             self.assertTrue(safeRef(t.x) in sd)         else:             self.assertTrue(sd.has_key(safeRef(t)))             self.assertTrue(safeRef(t) in sd)","if hasattr ( t , ""x"" ) :",if t . x :,5.557509463743763,5.557509463743763,0.0
"def wrapped(request, *args, **kwargs):     if not request.user.is_authenticated():         request.session[""_next""] = request.get_full_path()         <MASK>             redirect_uri = reverse(                 ""sentry-auth-organization"", args=[kwargs[""organization_slug""]]             )         else:             redirect_uri = get_login_url()         return HttpResponseRedirect(redirect_uri)     return func(request, *args, **kwargs)","if ""organization_slug"" in kwargs :","if kwargs . get ( ""organization_slug"" ) in kwargs :",36.6192636299943,36.6192636299943,0.0
"def read_info(reader, dump=None):     line_number_table_length = reader.read_u2()     <MASK>         reader.debug(             ""    "" * dump, ""Line numbers (%s total):"" % line_number_table_length         )     line_numbers = []     for i in range(0, line_number_table_length):         start_pc = reader.read_u2()         line_number = reader.read_u2()         if dump is not None:             reader.debug(""    "" * (dump + 1), ""%s: %s"" % (start_pc, line_number))         line_numbers.append((start_pc, line_number))     return LineNumberTable(line_numbers)",if dump is not None :,if dump is not None :,100.00000000000004,100.00000000000004,1.0
"def compute_timer_precision(timer):     precision = None     points = 0     timeout = timeout_timer() + 1.0     previous = timer()     while timeout_timer() < timeout or points < 5:         for _ in XRANGE(10):             t1 = timer()             t2 = timer()             dt = t2 - t1             if 0 < dt:                 break         else:             dt = t2 - previous             if dt <= 0.0:                 continue         <MASK>             precision = min(precision, dt)         else:             precision = dt         points += 1         previous = timer()     return precision",if precision is not None :,if precision :,23.174952587773145,0.0,0.0
def get_hi_lineno(self):     lineno = Node.get_hi_lineno(self)     if self.expr1 is None:         pass     else:         lineno = self.expr1.get_hi_lineno()         <MASK>             pass         else:             lineno = self.expr2.get_hi_lineno()             if self.expr3 is None:                 pass             else:                 lineno = self.expr3.get_hi_lineno()     return lineno,if self . expr2 is None :,if self . expr2 is None :,100.00000000000004,100.00000000000004,1.0
"def validate_cluster_resource_group(cmd, namespace):     if namespace.cluster_resource_group is not None:         client = get_mgmt_service_client(             cmd.cli_ctx, ResourceType.MGMT_RESOURCE_RESOURCES         )         <MASK>             raise InvalidArgumentValueError(                 ""Invalid --cluster-resource-group '%s': resource group must not exist.""                 % namespace.cluster_resource_group             )",if client . resource_groups . check_existence ( namespace . cluster_resource_group ) :,if client . cluster_resource_group not in namespace . cluster_resource_group :,40.30949313513571,40.30949313513571,0.0
"def find_word_bounds(self, text, index, allowed_chars):     right = left = index     done = False     while not done:         <MASK>             done = True         elif not self.word_boundary_char(text[left - 1]):             left -= 1         else:             done = True     done = False     while not done:         if right == len(text):             done = True         elif not self.word_boundary_char(text[right]):             right += 1         else:             done = True     return left, right",if left == 0 :,if left == len ( text ) :,31.55984539112946,31.55984539112946,0.0
"def _check_good_input(self, X, y=None):     if isinstance(X, dict):         lengths = [len(X1) for X1 in X.values()]         if len(set(lengths)) > 1:             raise ValueError(""Not all values of X are of equal length."")         x_len = lengths[0]     else:         x_len = len(X)     if y is not None:         <MASK>             raise ValueError(""X and y are not of equal length."")     if self.regression and y is not None and y.ndim == 1:         y = y.reshape(-1, 1)     return X, y",if len ( y ) != x_len :,if x_len != self . max_length :,17.242221289766636,17.242221289766636,0.0
"def _get_text_nodes(nodes, html_body):     text = []     open_tags = 0     for node in nodes:         if isinstance(node, HtmlTag):             if node.tag_type == OPEN_TAG:                 open_tags += 1             <MASK>                 open_tags -= 1         elif (             isinstance(node, HtmlDataFragment)             and node.is_text_content             and open_tags == 0         ):             text.append(html_body[node.start : node.end])     return text",elif node . tag_type == CLOSE_TAG :,if node . tag_type == CLOSE_TAG :,90.36020036098445,90.36020036098445,0.0
"def _get_spyne_type(cls_name, k, v):     try:         v = NATIVE_MAP.get(v, v)     except TypeError:         return     try:         subc = issubclass(v, ModelBase) or issubclass(v, SelfReference)     except:         subc = False     if subc:         if issubclass(v, Array) and len(v._type_info) != 1:             raise Exception(""Invalid Array definition in %s.%s."" % (cls_name, k))         <MASK>             raise Exception(""Please specify the number of dimensions"")         return v","elif issubclass ( v , Point ) and v . Attributes . dim is None :",if len ( v . _type_info ) != 2 :,7.362479602707965,7.362479602707965,0.0
"def customize(cls, **kwargs):     """"""return a class with some existing attributes customized""""""     for name, value in kwargs.iteritems():         <MASK>             raise TransportError(                 ""you cannot customize the protected attribute %s"" % name             )         if not hasattr(cls, name):             raise TransportError(""Transport has no attribute %s"" % name)     NewSubClass = type(""Customized_{}"".format(cls.__name__), (cls,), kwargs)     return NewSubClass","if name in [ ""cookie"" , ""circuit"" , ""upstream"" , ""downstream"" , ""stream"" ] :",if value is not None :,0.4067907049567216,0.4067907049567216,0.0
"def test_UNrelativize(self):     import URIlib     relative = self.relative + self.full_relativize     for base, rel, fullpath, common in relative:         URI = uriparse.UnRelativizeURL(base, rel)         fullURI = URIlib.URIParser(URI)         # We need to canonicalize the result from unrelativize         # compared to the original full path we expect to see.         <MASK>             fullpath = fullpath[:-1]         self.failUnlessSamePath(             os.path.normcase(fullURI.path), os.path.normcase(fullpath)         )","if fullpath [ - 1 ] in ( ""/"" , ""\\"" ) :","if common == ""relativize"" :",2.2375594425769316,2.2375594425769316,0.0
"def get_release_info(file_path=RELEASE_FILE):     RELEASE_TYPE_REGEX = re.compile(r""^[Rr]elease [Tt]ype: (major|minor|patch)$"")     with open(file_path, ""r"") as f:         line = f.readline()         match = RELEASE_TYPE_REGEX.match(line)         <MASK>             print(                 ""The file RELEASE.md should start with `Release type` ""                 ""and specify one of the following values: major, minor or patch.""             )             sys.exit(1)         type_ = match.group(1)         changelog = """".join([line for line in f.readlines()]).strip()     return type_, changelog",if not match :,if not match :,100.00000000000004,100.00000000000004,1.0
"def _get_next_history_entry(self):     if self._history:         hist_len = len(self._history) - 1         self.history_index = min(hist_len, self.history_index + 1)         index = self.history_index         <MASK>             self.history_index += 1         return self._history[index]     return """"",if self . history_index == hist_len :,if index < hist_len :,22.273858658245697,22.273858658245697,0.0
"def star_op(self):     """"""Put a '*' op, with special cases for *args.""""""     val = ""*""     if self.paren_level:         i = len(self.code_list) - 1         if self.code_list[i].kind == ""blank"":             i -= 1         token = self.code_list[i]         <MASK>             self.op_no_blanks(val)         elif token.value == "","":             self.blank()             self.add_token(""op-no-blanks"", val)         else:             self.op(val)     else:         self.op(val)","if token . kind == ""lt"" :","if token . value == ""*"" :",29.84745896009822,29.84745896009822,0.0
"def get_safe_settings():     ""Returns a dictionary of the settings module, with sensitive settings blurred out.""     settings_dict = {}     for k in dir(settings):         <MASK>             if HIDDEN_SETTINGS.search(k):                 settings_dict[k] = ""********************""             else:                 settings_dict[k] = getattr(settings, k)     return settings_dict",if k . isupper ( ) :,if k in settings_dict :,14.535768424205482,14.535768424205482,0.0
"def nextEditable(self):     """"""Moves focus of the cursor to the next editable window""""""     if self.currentEditable is None:         if len(self._editableChildren):             self._currentEditableRef = self._editableChildren[0]     else:         for ref in weakref.getweakrefs(self.currentEditable):             <MASK>                 cei = self._editableChildren.index(ref)                 nei = cei + 1                 if nei >= len(self._editableChildren):                     nei = 0                 self._currentEditableRef = self._editableChildren[nei]     return self.currentEditable",if ref in self . _editableChildren :,if cei > 0 :,6.9717291216921975,6.9717291216921975,0.0
"def _handle_dependents_type(types, type_str, type_name, rel_name, row):     if types[type_str[0]] is None:         <MASK>             type_name = ""index""             rel_name = row[""indname""] + "" ON "" + rel_name         elif type_str[0] == ""o"":             type_name = ""operator""             rel_name = row[""relname""]     else:         type_name = types[type_str[0]]     return type_name, rel_name","if type_str [ 0 ] == ""i"" :","if type_str [ 0 ] == ""i"" :",100.00000000000004,100.00000000000004,1.0
"def streamErrorHandler(self, conn, error):     name, text = ""error"", error.getData()     for tag in error.getChildren():         <MASK>             if tag.getName() == ""text"":                 text = tag.getData()             else:                 name = tag.getName()     if name in stream_exceptions.keys():         exc = stream_exceptions[name]     else:         exc = StreamError     raise exc((name, text))",if tag . getNamespace ( ) == NS_XMPP_STREAMS :,"if tag . getName ( ) == ""error"" :",26.83544415402699,26.83544415402699,0.0
"def _validate_names(self, settings: _SettingsType) -> None:     """"""Make sure all settings exist.""""""     unknown = []     for name in settings:         <MASK>             unknown.append(name)     if unknown:         errors = [             configexc.ConfigErrorDesc(                 ""While loading options"", ""Unknown option {}"".format(e)             )             for e in sorted(unknown)         ]         raise configexc.ConfigFileErrors(""autoconfig.yml"", errors)",if name not in configdata . DATA :,if name not in unknown :,38.49815007763549,38.49815007763549,0.0
"def can_haz(self, target, credentials):     """"""Check whether key-values in target are present in credentials.""""""     # TODO(termie): handle ANDs, probably by providing a tuple instead of a     #               string     for requirement in target:         key, match = requirement.split("":"", 1)         check = credentials.get(key)         <MASK>             check = [check]         if match in check:             return True","if check is None or isinstance ( check , basestring ) :",if not check :,2.845073863275343,2.845073863275343,0.0
"def _recursive_fx_apply(input: dict, fx):     for k, v in input.items():         <MASK>             v = torch.tensor(v)         if isinstance(v, torch.Tensor):             v = fx(v.float())             input[k] = v         else:             _recursive_fx_apply(v, fx)","if isinstance ( v , list ) :","if isinstance ( v , torch . Float ) :",45.180100180492246,45.180100180492246,0.0
"def get(self, url, **kwargs):     app, url = self._prepare_call(url, kwargs)     if app:         <MASK>             self._first_ping = False             return EmptyCapabilitiesResponse()         elif ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url:             return ErrorApiResponse()         else:             response = app.get(url, **kwargs)             return TestingResponse(response)     else:         return requests.get(url, **kwargs)","if url . endswith ( ""ping"" ) and self . _first_ping :",if self . _first_ping :,28.22664073782293,28.22664073782293,0.0
"def server_thread_fn():     server_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)     server_ctx.load_cert_chain(""trio-test-1.pem"")     server = server_ctx.wrap_socket(         server_sock,         server_side=True,         suppress_ragged_eofs=False,     )     while True:         data = server.recv(4096)         print(""server got:"", data)         <MASK>             print(""server waiting for client to finish everything"")             client_done.wait()             print(""server attempting to send back close-notify"")             server.unwrap()             print(""server ok"")             break         server.sendall(data)",if not data :,if not data :,100.00000000000004,100.00000000000004,1.0
"def find_hostnames(data):     # sends back an array of hostnames     hostnames = []     for i in re.finditer(hostname_regex, data):         h = string.lower(i.group(1))         tld = h.split(""."")[-1:][0]         <MASK>             hostnames.append(h)     return hostnames",if tld in tlds :,if tld :,32.34325178227722,0.0,0.0
"def Validate(self, win):     textCtrl = self.GetWindow()     text = textCtrl.GetValue().strip()     sChar = Character.getInstance()     try:         <MASK>             raise ValueError(_t(""You must supply a name for the Character!""))         elif text in [x.name for x in sChar.getCharacterList()]:             raise ValueError(                 _t(""Character name already in use, please choose another."")             )         return True     except ValueError as e:         pyfalog.error(e)         wx.MessageBox(""{}"".format(e), _t(""Error""))         textCtrl.SetFocus()         return False",if len ( text ) == 0 :,"if text == """" :",12.411264901419441,12.411264901419441,0.0
def get_random_user_agent(agent_list=UA_CACHE):     if not len(agent_list):         ua_file = file(UA_FILE)         for line in ua_file:             line = line.strip()             <MASK>                 agent_list.append(line)     ua = random.choice(UA_CACHE)     return ua,if line :,if not line :,35.35533905932737,35.35533905932737,0.0
"def _validate_action_like_for_prefixes(self, key):     for statement in self._statements:         <MASK>             if isinstance(statement[key], string_types):                 self._validate_action_prefix(statement[key])             else:                 for action in statement[key]:                     self._validate_action_prefix(action)",if key in statement :,if key in statement :,100.00000000000004,100.00000000000004,1.0
"def predict(self, X):     if self.regression:         return self.predict_proba(X)     else:         y_pred = np.argmax(self.predict_proba(X), axis=1)         <MASK>             y_pred = self.enc_.inverse_transform(y_pred)         return y_pred",if self . use_label_encoder :,if self . enc_ :,20.82186541080652,20.82186541080652,0.0
"def _threaded_request_tracker(self, builder):     while True:         event_type = self._read_q.get()         <MASK>             return         payload = {""body"": b""""}         request_id = builder.build_record(event_type, payload, """")         self._write_q.put_nowait(request_id)",if event_type is False :,if event_type is None :,64.34588841607616,64.34588841607616,0.0
"def __call__(self, value):     try:         super(EmailValidator, self).__call__(value)     except ValidationError as e:         # Trivial case failed. Try for possible IDN domain-part         <MASK>             parts = value.split(""@"")             try:                 parts[-1] = parts[-1].encode(""idna"").decode(""ascii"")             except UnicodeError:                 raise e             super(EmailValidator, self).__call__(""@"".join(parts))         else:             raise","if value and ""@"" in value :","if ""idna"" in parts :",12.411264901419441,12.411264901419441,0.0
"def PreprocessConditionalStatement(self, IfList, ReplacedLine):     while self:         if self.__Token:             x = 1         <MASK>             if self <= 2:                 continue             RegionSizeGuid = 3             if not RegionSizeGuid:                 RegionLayoutLine = 5                 continue             RegionLayoutLine = self.CurrentLineNumber     return 1",elif not IfList :,ifList :,18.393972058572114,0.0,0.0
"def _arg_with_type(self):     for t in self.d[""Args""]:         m = re.search(""([A-Za-z0-9_-]+)\s{0,4}(\(.+\))\s{0,4}:"", t)         <MASK>             self.args[m.group(1)] = m.group(2)     return self.args",if m :,if m :,100.00000000000004,0.0,1.0
"def get_palette_for_custom_classes(self, class_names, palette=None):     if self.label_map is not None:         # return subset of palette         palette = []         for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]):             if new_id != -1:                 palette.append(self.PALETTE[old_id])         palette = type(self.PALETTE)(palette)     elif palette is None:         <MASK>             palette = np.random.randint(0, 255, size=(len(class_names), 3))         else:             palette = self.PALETTE     return palette",if self . PALETTE is None :,if new_id in class_names :,5.669791110976001,5.669791110976001,0.0
"def Visit_star_expr(self, node):  # pylint: disable=invalid-name     # star_expr ::= '*' expr     for child in node.children:         self.Visit(child)         <MASK>             _AppendTokenSubtype(child, format_token.Subtype.UNARY_OPERATOR)             _AppendTokenSubtype(child, format_token.Subtype.VARARGS_STAR)","if isinstance ( child , pytree . Leaf ) and child . value == ""*"" :",if child . type == format_token . Type . VARARGS :,6.4638759065764395,6.4638759065764395,0.0
"def create_if_compatible(cls, typ: Type, *, root: ""RootNode"") -> Optional[""Node""]:     if cls.compatible_types:         target_type: Type = typ         <MASK>             target_type = getattr(typ, ""__origin__"", None) or typ         if cls._issubclass(target_type, cls.compatible_types):             return cls(typ, root=root)     return None",if cls . use_origin :,if target_type is None :,8.643019616048525,8.643019616048525,0.0
"def grep_full_py_identifiers(tokens):     global pykeywords     tokens = list(tokens)     i = 0     while i < len(tokens):         tokentype, token = tokens[i]         i += 1         if tokentype != ""id"":             continue         while (             i + 1 < len(tokens)             and tokens[i] == (""op"", ""."")             and tokens[i + 1][0] == ""id""         ):             token += ""."" + tokens[i + 1][1]             i += 2         <MASK>             continue         if token in pykeywords:             continue         if token[0] in "".0123456789"":             continue         yield token","if token == """" :",if token in pykeywords :,15.848738972120703,15.848738972120703,0.0
"def create_config_filepath(cls, visibility=None):     if cls.is_local(visibility):         # Local to this directory         base_path = os.path.join(""."")         <MASK>             # Add it to the current ""./.polyaxon""             base_path = os.path.join(base_path, "".polyaxon"")             cls._create_dir(base_path)     elif cls.CONFIG_PATH:  # Custom path         pass     else:  # Handle both global and all cases         base_path = polyaxon_user_path()         cls._create_dir(base_path)",if cls . IS_POLYAXON_DIR :,"if visibility == ""global"" :",5.795599612995366,5.795599612995366,0.0
"def test_len(self):     eq = self.assertEqual     eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol="""")))     for size in range(15):         if size == 0:             bsize = 0         <MASK>             bsize = 4         elif size <= 6:             bsize = 8         elif size <= 9:             bsize = 12         elif size <= 12:             bsize = 16         else:             bsize = 20         eq(base64MIME.base64_len(""x"" * size), bsize)",elif size <= 3 :,if size <= 1 :,32.46679154750991,32.46679154750991,0.0
"def as_dict(path="""", version=""latest"", section=""meta-data""):     result = {}     dirs = dir(path, version, section)     if not dirs:         return None     for item in dirs:         <MASK>             records = as_dict(path + item, version, section)             if records:                 result[item[:-1]] = records         elif is_dict.match(item):             idx, name = is_dict.match(item).groups()             records = as_dict(path + idx + ""/"", version, section)             if records:                 result[name] = records         else:             result[item] = valueconv(get(path + item, version, section))     return result","if item . endswith ( ""/"" ) :",if name in result :,4.673289785800722,4.673289785800722,0.0
"def api_read(self):     result = {}     files = [""my.cnf"", ""debian.cnf""]     directory_list = self.exec_payload(""mysql_config_directory"")[""directory""]     for _file in files:         for directory in directory_list:             mysql_conf = directory + _file             content = self.shell.read(mysql_conf)             <MASK>                 result[mysql_conf] = content     return result",if content :,if content :,100.00000000000004,0.0,1.0
"def generate(self, count=100):     self.pre_generate()     counter = iter(range(count))     created = 0     while True:         batch = list(islice(counter, self.batch_size))         <MASK>             break         self.do_generate(batch, self.batch_size)         from_size = created         created += len(batch)         print(""Generate %s: %s-%s"" % (self.resource, from_size, created))     self.after_generate()",if not batch :,if not batch :,100.00000000000004,100.00000000000004,1.0
"def _normalize_fields(self, document, loader):     # type: (Dict[Text, Text], Loader) -> None     # Normalize fields which are prefixed or full URIn to vocabulary terms     for d in list(document.keys()):         d2 = loader.expand_url(d, u"""", scoped_id=False, vocab_term=True)         <MASK>             document[d2] = document[d]             del document[d]",if d != d2 :,if d2 in document :,11.51015341649912,11.51015341649912,0.0
"def load_cache(filename, get_key=mangle_key):     cache = {}     if not os.path.exists(filename):         return cache     f = open(filename, ""rb"")     l = 0     for line in f.readlines():         l += 1         fields = line.split(b"" "")         <MASK>             sys.stderr.write(""Invalid file format in [%s], line %d\n"" % (filename, l))             continue         # put key:value in cache, key without ^:         cache[get_key(fields[0][1:])] = fields[1].split(b""\n"")[0]     f.close()     return cache","if fields == None or not len ( fields ) == 2 or fields [ 0 ] [ 0 : 1 ] != b"":"" :",if not fields :,0.026450229283321025,0.026450229283321025,0.0
"def __lshift__(self, other):     if not self.symbolic and type(other) is int:         return RegisterOffset(             self._bits, self.reg, self._to_signed(self.offset << other)         )     else:         <MASK>             return RegisterOffset(self._bits, self.reg, self.offset << other)         else:             return RegisterOffset(                 self._bits,                 self.reg,                 ArithmeticExpression(                     ArithmeticExpression.LShift,                     (                         self.offset,                         other,                     ),                 ),             )",if self . symbolic :,if other is not None :,9.652434877402245,9.652434877402245,0.0
"def SaveSettings(self, force=False):     if self.config is not None:         frame.ShellFrameMixin.SaveSettings(self)         <MASK>             frame.Frame.SaveSettings(self, self.config)             self.shell.SaveSettings(self.config)",if self . autoSaveSettings or force :,if force :,16.605579150202516,0.0,0.0
"def _parse_gene(element):     for genename_element in element:         if ""type"" in genename_element.attrib:             ann_key = ""gene_%s_%s"" % (                 genename_element.tag.replace(NS, """"),                 genename_element.attrib[""type""],             )             <MASK>                 self.ParsedSeqRecord.annotations[ann_key] = genename_element.text             else:                 append_to_annotations(ann_key, genename_element.text)","if genename_element . attrib [ ""type"" ] == ""primary"" :","if ""annotation"" in genename_element . attrib :",23.55289213618018,23.55289213618018,0.0
"def _write_pkg_file(self, file):     with TemporaryFile(mode=""w+"") as tmpfd:         _write_pkg_file_orig(self, tmpfd)         tmpfd.seek(0)         for line in tmpfd:             <MASK>                 file.write(""Metadata-Version: 2.1\n"")             elif line.startswith(""Description: ""):                 file.write(                     ""Description-Content-Type: %s; charset=UTF-8\n""                     % long_description_content_type                 )                 file.write(line)             else:                 file.write(line)","if line . startswith ( ""Metadata-Version: "" ) :","if line . startswith ( ""Metadata: "" ) :",70.16879391277372,70.16879391277372,0.0
"def get(self):     """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called.""""""     if self._exception is not _NONE:         <MASK>             return self.value         getcurrent().throw(*self._exception)  # pylint:disable=undefined-variable     else:         if self.greenlet is not None:             raise ConcurrentObjectUseError(                 ""This Waiter is already used by %r"" % (self.greenlet,)             )         self.greenlet = getcurrent()  # pylint:disable=undefined-variable         try:             return self.hub.switch()         finally:             self.greenlet = None",if self . _exception is None :,if self . value is not _NONE :,21.10534063187263,21.10534063187263,0.0
"def connect(self, *args):     """"""connects to the dropbox. args[0] is the username.""""""     if len(args) != 1:         return ""expected one argument!""     try:         dbci = get_dropbox_client(args[0], False, None, None)     except Exception as e:         return e.message     else:         <MASK>             return ""No Dropbox configured for '{u}'."".format(u=args[0])         else:             self.client = dbci         return True",if dbci is None :,if not dbci :,16.37226966703825,16.37226966703825,0.0
"def escape(text, newline=False):     """"""Escape special html characters.""""""     if isinstance(text, str):         if ""&"" in text:             text = text.replace(""&"", ""&amp;"")         if "">"" in text:             text = text.replace("">"", ""&gt;"")         if ""<"" in text:             text = text.replace(""<"", ""&lt;"")         if '""' in text:             text = text.replace('""', ""&quot;"")         if ""'"" in text:             text = text.replace(""'"", ""&quot;"")         if newline:             <MASK>                 text = text.replace(""\n"", ""<br>"")     return text","if ""\n"" in text :","if ""\n"" in text :",100.00000000000004,100.00000000000004,1.0
def t(ret):     with IPDB() as ipdb:         with ipdb.eventqueue() as evq:             for msg in evq:                 <MASK>                     ret.append(msg)                     return,"if msg . get_attr ( ""IFLA_IFNAME"" ) == ""test1984"" :","if msg . type == ""message"" :",12.135071233528631,12.135071233528631,0.0
"def check_stmt(self, stmt):     if is_future(stmt):         for name, asname in stmt.names:             <MASK>                 self.found[name] = 1             else:                 raise SyntaxError(""future feature %s is not defined"" % name)         stmt.valid_future = 1         return 1     return 0",if name in self . features :,if asname in self . found :,27.77619034011791,27.77619034011791,0.0
"def process_pypi_option(option, option_str, option_value, parser):     if option_str.startswith(""--no""):         setattr(parser.values, option.dest, [])     else:         indexes = getattr(parser.values, option.dest, [])         <MASK>             indexes.append(_PYPI)         setattr(parser.values, option.dest, indexes)",if _PYPI not in indexes :,if option_value :,9.423716574733431,9.423716574733431,0.0
"def modify_address(self, name, address, domain):     if not self.get_entries_by_name(name, domain):         raise exception.NotFound     infile = open(self.filename, ""r"")     outfile = tempfile.NamedTemporaryFile(""w"", delete=False)     for line in infile:         entry = self.parse_line(line)         <MASK>             outfile.write(                 ""%s   %s   %s\n"" % (address, self.qualify(name, domain), entry[""type""])             )         else:             outfile.write(line)     infile.close()     outfile.close()     shutil.move(outfile.name, self.filename)","if entry and entry [ ""name"" ] . lower ( ) == self . qualify ( name , domain ) . lower ( ) :","if entry [ ""type"" ] == ""address"" :",6.002156169065222,6.002156169065222,0.0
"def tms_to_quadkey(self, tms, google=False):     quadKey = """"     x, y, z = tms     # this algorithm works with google tiles, rather than tms, so convert     # to those first.     if not google:         y = (2 ** z - 1) - y     for i in range(z, 0, -1):         digit = 0         mask = 1 << (i - 1)         if (x & mask) != 0:             digit += 1         <MASK>             digit += 2         quadKey += str(digit)     return quadKey",if ( y & mask ) != 0 :,if y & mask :,16.62083000646927,16.62083000646927,0.0
"def add_if_unique(self, issuer, use, keys):     if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]:         for typ, key in keys:             flag = 1             for _typ, _key in self.issuer_keys[issuer][use]:                 if _typ == typ and key is _key:                     flag = 0                     break             <MASK>                 self.issuer_keys[issuer][use].append((typ, key))     else:         self.issuer_keys[issuer][use] = keys",if flag :,if not flag :,35.35533905932737,35.35533905932737,0.0
"def scan_error(self):     ""A string describing why the last scan failed, or None if it didn't.""     self.acquire_lock()     try:         <MASK>             try:                 self._load_buf_data_once()             except NotFoundInDatabase:                 pass         return self._scan_error_cache     finally:         self.release_lock()",if self . _scan_error_cache is None :,if self . _scan_error_cache is not None :,79.10665071754353,79.10665071754353,0.0
"def _query(self):     if self._mongo_query is None:         self._mongo_query = self._query_obj.to_query(self._document)         <MASK>             if ""_cls"" in self._mongo_query:                 self._mongo_query = {""$and"": [self._cls_query, self._mongo_query]}             else:                 self._mongo_query.update(self._cls_query)     return self._mongo_query",if self . _cls_query :,"if ""_cls"" in self . _mongo_query :",21.97281387499715,21.97281387499715,0.0
"def CountButtons(self):     """"""Returns the number of visible buttons in the docked pane.""""""     n = 0     if self.HasCaption() or self.HasCaptionLeft():         if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame):             return 1         if self.HasCloseButton():             n += 1         if self.HasMaximizeButton():             n += 1         if self.HasMinimizeButton():             n += 1         <MASK>             n += 1     return n",if self . HasPinButton ( ) :,if self . HasCloseButton ( ) :,41.11336169005196,41.11336169005196,0.0
"def testBind(self):     try:         with socket.socket(socket.PF_CAN, socket.SOCK_DGRAM, socket.CAN_J1939) as s:             addr = (                 self.interface,                 socket.J1939_NO_NAME,                 socket.J1939_NO_PGN,                 socket.J1939_NO_ADDR,             )             s.bind(addr)             self.assertEqual(s.getsockname(), addr)     except OSError as e:         <MASK>             self.skipTest(""network interface `%s` does not exist"" % self.interface)         else:             raise",if e . errno == errno . ENODEV :,if e . errno == errno . EADDRNOTAVAIL :,78.25422900366438,78.25422900366438,0.0
"def createFields(self):     while self.current_size < self.size:         pos = self.stream.searchBytes(             ""\0\0\1"", self.current_size, self.current_size + 1024 * 1024 * 8         )  # seek forward by at most 1MB         if pos is not None:             padsize = pos - self.current_size             <MASK>                 yield PaddingBytes(self, ""pad[]"", padsize // 8)         chunk = Chunk(self, ""chunk[]"")         try:             # force chunk to be processed, so that CustomFragments are complete             chunk[""content/data""]         except:             pass         yield chunk",if padsize :,if padsize > 1 :,23.643540225079384,23.643540225079384,0.0
"def index_modulemd_files(repo_path):     merger = Modulemd.ModuleIndexMerger()     for fn in sorted(os.listdir(repo_path)):         <MASK>             continue         yaml_path = os.path.join(repo_path, fn)         mmd = Modulemd.ModuleIndex()         mmd.update_from_file(yaml_path, strict=True)         merger.associate_index(mmd, 0)     return merger.resolve()","if not fn . endswith ( "".yaml"" ) :",if not os . path . isfile ( fn ) :,11.510518494396258,11.510518494396258,0.0
"def set_visible(self, visible=True):     self._visible = visible     if self._nswindow is not None:         <MASK>             # Not really sure why on_resize needs to be here,             # but it's what pyglet wants.             self.dispatch_event(""on_resize"", self._width, self._height)             self.dispatch_event(""on_show"")             self.dispatch_event(""on_expose"")             self._nswindow.makeKeyAndOrderFront_(None)         else:             self._nswindow.orderOut_(None)",if visible :,if visible :,100.00000000000004,0.0,1.0
"def __repr__(self):     if self._in_repr:         return ""<recursion>""     try:         self._in_repr = True         if self.is_computed():             status = ""computed, ""             if self.error() is None:                 <MASK>                     status += ""= self""                 else:                     status += ""= "" + repr(self.value())             else:                 status += ""error = "" + repr(self.error())         else:             status = ""isn't computed""         return ""%s (%s)"" % (type(self), status)     finally:         self._in_repr = False",if self . value ( ) is self :,if self . value ( ) is None :,75.06238537503395,75.06238537503395,0.0
"def _individual_get(self, segment, index_type, index, strictdoc):     if index_type == ""val"":         for key, value in segment.items():             if key == index[0]:                 return value             <MASK>                 if key.text == index[0]:                     return value         raise Exception(""Invalid state"")     elif index_type == ""index"":         return segment[index]     elif index_type == ""textslice"":         return segment[index[0] : index[1]]     elif index_type == ""key"":         return index[1] if strictdoc else index[0]     else:         raise Exception(""Invalid state"")","if hasattr ( key , ""text"" ) :",if strictdoc :,3.361830360737634,0.0,0.0
"def _makeSafeAbsoluteURI(base, rel=None):     # bail if ACCEPTABLE_URI_SCHEMES is empty     if not ACCEPTABLE_URI_SCHEMES:         return _urljoin(base, rel or u"""")     if not base:         return rel or u""""     if not rel:         try:             scheme = urlparse.urlparse(base)[0]         except ValueError:             return u""""         <MASK>             return base         return u""""     uri = _urljoin(base, rel)     if uri.strip().split("":"", 1)[0] not in ACCEPTABLE_URI_SCHEMES:         return u""""     return uri",if not scheme or scheme in ACCEPTABLE_URI_SCHEMES :,"if scheme == ""http"" :",4.408194605881708,4.408194605881708,0.0
"def _write_packet(self, packet):     # Immediately writes the given packet to the network. The caller must     # have the write lock acquired before calling this method.     try:         for listener in self.early_outgoing_packet_listeners:             listener.call_packet(packet)         <MASK>             packet.write(self.socket, self.options.compression_threshold)         else:             packet.write(self.socket)         for listener in self.outgoing_packet_listeners:             listener.call_packet(packet)     except IgnorePacket:         pass",if self . options . compression_enabled :,if self . options . compression_threshold :,75.06238537503395,75.06238537503395,0.0
"def rangelist_to_set(rangelist):     result = set()     if not rangelist:         return result     for x in rangelist.split("",""):         <MASK>             result.add(int(x))             continue         m = re.match(r""^(\d+)-(\d+)$"", x)         if m:             start = int(m.group(1))             end = int(m.group(2))             result.update(set(range(start, end + 1)))             continue         msg = ""Cannot understand data input: %s %s"" % (x, rangelist)         raise ValueError(msg)     return result","if re . match ( r""^(\d+)$"" , x ) :",if not x :,0.49439970462914884,0.49439970462914884,0.0
"def test_device_property_logfile_isinstance(self):     mock = MagicMock()     with patch(builtin_string + "".open"", mock):         <MASK>             builtin_file = ""io.TextIOWrapper""         else:             builtin_file = builtin_string + "".file""         with patch(builtin_file, MagicMock):             handle = open(""filename"", ""r"")             self.dev.logfile = handle             self.assertEqual(self.dev.logfile, handle)","if sys . version > ""3"" :",if mock . is_io :,6.495032985064742,6.495032985064742,0.0
"def _line_ranges(statements, lines):     """"""Produce a list of ranges for `format_lines`.""""""     statements = sorted(statements)     lines = sorted(lines)     pairs = []     start = None     lidx = 0     for stmt in statements:         if lidx >= len(lines):             break         if stmt == lines[lidx]:             lidx += 1             if not start:                 start = stmt             end = stmt         <MASK>             pairs.append((start, end))             start = None     if start:         pairs.append((start, end))     return pairs",elif start :,if end :,27.516060407455225,0.0,0.0
"def reset_parameters(self):     initialize = layers.get_initializer(self._hparams.initializer)     if initialize is not None:         # Do not re-initialize LayerNorm modules.         for name, param in self.named_parameters():             <MASK>                 initialize(param)","if name . split ( ""."" ) [ - 1 ] == ""weight"" and ""layer_norm"" not in name :",if name == self . name :,1.6319949169117587,1.6319949169117587,0.0
"def billing_invoice_show_validator(namespace):     from azure.cli.core.azclierror import (         RequiredArgumentMissingError,         MutuallyExclusiveArgumentError,     )     valid_combs = (         ""only --account-name, --name / --name / --name, --by-subscription is valid""     )     if namespace.account_name is not None:         if namespace.by_subscription is not None:             raise MutuallyExclusiveArgumentError(valid_combs)         <MASK>             raise RequiredArgumentMissingError(""--name is also required"")     if namespace.by_subscription is not None:         if namespace.name is None:             raise RequiredArgumentMissingError(""--name is also required"")",if namespace . name is None :,if namespace . name is None :,100.00000000000004,100.00000000000004,1.0
"def DeleteDocuments(self, document_ids, response):     """"""Deletes documents for the given document_ids.""""""     for document_id in document_ids:         <MASK>             document = self._documents[document_id]             self._inverted_index.RemoveDocument(document)             del self._documents[document_id]         delete_status = response.add_status()         delete_status.set_code(search_service_pb.SearchServiceError.OK)",if document_id in self . _documents :,if document_id in self . _documents :,100.00000000000004,100.00000000000004,1.0
"def generate_new_element(items, prefix, numeric=False):     """"""Creates a random string with prefix, that is not in 'items' list.""""""     while True:         <MASK>             candidate = prefix + generate_random_numeric(8)         else:             candidate = prefix + generate_random_alphanumeric(8)         if not candidate in items:             return candidate         LOG.debug(""Random collision on %s"" % candidate)",if numeric :,if numeric :,100.00000000000004,0.0,1.0
"def generate_text_for_vocab(self, data_dir, tmp_dir):     for i, sample in enumerate(         self.generate_samples(data_dir, tmp_dir, problem.DatasetSplit.TRAIN)     ):         if self.has_inputs:             yield sample[""inputs""]         yield sample[""targets""]         <MASK>             break",if self . max_samples_for_vocab and ( i + 1 ) >= self . max_samples_for_vocab :,if i == 0 :,0.2934143018886864,0.2934143018886864,0.0
"def _get_ccp(config=None, config_path=None, saltenv=""base""):     """""" """"""     if config_path:         config = __salt__[""cp.get_file_str""](config_path, saltenv=saltenv)         <MASK>             raise SaltException(""{} is not available"".format(config_path))     if isinstance(config, six.string_types):         config = config.splitlines()     ccp = ciscoconfparse.CiscoConfParse(config)     return ccp",if config is False :,if not config :,16.37226966703825,16.37226966703825,0.0
"def rpush(key, *vals, **kwargs):     ttl = kwargs.get(""ttl"")     cap = kwargs.get(""cap"")     if not ttl and not cap:         _client.rpush(key, *vals)     else:         pipe = _client.pipeline()         pipe.rpush(key, *vals)         <MASK>             pipe.ltrim(key, 0, cap)         if ttl:             pipe.expire(key, ttl)         pipe.execute()",if cap :,if cap :,100.00000000000004,0.0,1.0
"def check_apns_certificate(ss):     mode = ""start""     for s in ss.split(""\n""):         <MASK>             if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s:                 mode = ""key""         elif mode == ""key"":             if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s:                 mode = ""end""                 break             elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s:                 raise ImproperlyConfigured(                     ""Encrypted APNS private keys are not supported""                 )     if mode != ""end"":         raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")","if mode == ""start"" :","if mode == ""start"" :",100.00000000000004,100.00000000000004,1.0
"def _add_communication_type(apps, schema_editor, communication_type):     Worker = apps.get_model(""orchestra"", ""Worker"")     CommunicationPreference = apps.get_model(""orchestra"", ""CommunicationPreference"")     for worker in Worker.objects.all():         (             communication_preference,             created,         ) = CommunicationPreference.objects.get_or_create(             worker=worker, communication_type=communication_type         )         # By default set both Slack and Email notifications to True         <MASK>             communication_preference.methods.slack = True             communication_preference.methods.email = True         communication_preference.save()",if created :,if created :,100.00000000000004,0.0,1.0
"def get_postgresql_driver_name():     # pylint: disable=unused-variable     try:         driver = os.getenv(""CODECHECKER_DB_DRIVER"")         <MASK>             return driver         try:             # pylint: disable=W0611             import psycopg2             return ""psycopg2""         except Exception:             # pylint: disable=W0611             import pg8000             return ""pg8000""     except Exception as ex:         LOG.error(str(ex))         LOG.error(""Failed to import psycopg2 or pg8000 module."")         raise",if driver :,if driver :,100.00000000000004,0.0,1.0
"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None:     modules = getattr(env, ""_viewcode_modules"", {})     for modname, entry in list(modules.items()):         if entry is False:             continue         code, tags, used, refname = entry         for fullname in list(used):             if used[fullname] == docname:                 used.pop(fullname)         <MASK>             modules.pop(modname)",if len ( used ) == 0 :,"if code == ""doc"" :",11.59119922599073,11.59119922599073,0.0
"def do_query(data, q):     ret = []     if not q:         return ret     qkey = q[0]     for key, value in iterate(data):         if len(q) == 1:             if key == qkey:                 ret.append(value)             elif is_iterable(value):                 ret.extend(do_query(value, q))         else:             <MASK>                 continue             if key == qkey:                 ret.extend(do_query(value, q[1:]))             else:                 ret.extend(do_query(value, q))     return ret",if not is_iterable ( value ) :,if not value :,10.88482843823664,10.88482843823664,0.0
"def _get_bucket_for_key(self, key: bytes) -> Optional[_DBValueTuple]:     dbs: Iterable[PartitionDB]     try:         partition = self._key_index[key]         dbs = [PartitionDB(partition, self._dbs[partition])]     except KeyError:         dbs = cast(Iterable[PartitionDB], self._dbs.items())     for partition, db in dbs:         if db.key_may_exist(key)[0]:             value = db.get(key)             <MASK>                 self._key_index[key] = partition                 return _DBValueTuple(db, value)     return None",if value is not None :,if value is not None :,100.00000000000004,100.00000000000004,1.0
"def _clean(self):     logger.info(""Cleaning up..."")     if self._process is not None:         if self._process.poll() is None:             for _ in range(3):                 self._process.terminate()                 time.sleep(0.5)                 <MASK>                     break             else:                 self._process.kill()                 self._process.wait()                 logger.error(""KILLED"")     if os.path.exists(self._tmp_dir):         shutil.rmtree(self._tmp_dir)     self._process = None     self._ws = None     logger.info(""Cleanup complete"")",if self . _process . poll ( ) is not None :,if self . _process . wait ( ) :,44.360636895626136,44.360636895626136,0.0
"def _calculate_runtimes(states):     results = {""runtime"": 0.00, ""num_failed_states"": 0, ""num_passed_states"": 0}     for state, resultset in states.items():         if isinstance(resultset, dict) and ""duration"" in resultset:             # Count the pass vs failures             <MASK>                 results[""num_passed_states""] += 1             else:                 results[""num_failed_states""] += 1             # Count durations             results[""runtime""] += resultset[""duration""]     log.debug(""Parsed state metrics: {}"".format(results))     return results","if resultset [ ""result"" ] :","if state == ""pass"" :",7.809849842300637,7.809849842300637,0.0
"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None):     if next is not None and token.end_mark.line == next.start_mark.line:         spaces = next.start_mark.pointer - token.end_mark.pointer         if max != -1 and spaces > max:             return LintProblem(                 token.start_mark.line + 1, next.start_mark.column, max_desc             )         <MASK>             return LintProblem(                 token.start_mark.line + 1, next.start_mark.column + 1, min_desc             )",elif min != - 1 and spaces < min :,if min != - 1 and spaces < min_desc :,66.52049901111006,66.52049901111006,0.0
"def getfileinfo(name):     finfo = FInfo()     with io.open(name, ""rb"") as fp:         # Quick check for textfile         data = fp.read(512)         <MASK>             finfo.Type = ""TEXT""         fp.seek(0, 2)         dsize = fp.tell()     dir, file = os.path.split(name)     file = file.replace("":"", ""-"", 1)     return file, finfo, dsize, 0",if 0 not in data :,if data :,23.174952587773145,0.0,0.0
"def dict_to_XML(tag, dictionary, **kwargs):     """"""Return XML element converting dicts recursively.""""""     elem = Element(tag, **kwargs)     for key, val in dictionary.items():         if tag == ""layers"":             child = dict_to_XML(""layer"", val, name=key)         <MASK>             child = dict_to_XML(key, val)         else:             if tag == ""config"":                 child = Element(""variable"", name=key)             else:                 child = Element(key)             child.text = str(val)         elem.append(child)     return elem","elif isinstance ( val , MutableMapping ) :","if tag == ""config"" :",5.522397783539471,5.522397783539471,0.0
"def _read_bytes(self, length):     buffer = b""""     while length:         chunk = self.request.recv(length)         <MASK>             log.debug(""Connection closed"")             return False         length -= len(chunk)         buffer += chunk     return buffer","if chunk == b"""" :",if not chunk :,7.733712583165139,7.733712583165139,0.0
"def rec_deps(services, container_by_name, cnt, init_service):     deps = cnt[""_deps""]     for dep in deps.copy():         dep_cnts = services.get(dep)         <MASK>             continue         dep_cnt = container_by_name.get(dep_cnts[0])         if dep_cnt:             # TODO: avoid creating loops, A->B->A             if init_service and init_service in dep_cnt[""_deps""]:                 continue             new_deps = rec_deps(services, container_by_name, dep_cnt, init_service)             deps.update(new_deps)     return deps",if not dep_cnts :,if not dep_cnts or not dep_cnts [ 0 ] :,27.824623288353134,27.824623288353134,0.0
"def fix_repeating_arguments(self):     """"""Fix elements that should accumulate/increment values.""""""     either = [list(child.children) for child in transform(self).children]     for case in either:         for e in [child for child in case if case.count(child) > 1]:             <MASK>                 if e.value is None:                     e.value = []                 elif type(e.value) is not list:                     e.value = e.value.split()             if type(e) is Command or type(e) is Option and e.argcount == 0:                 e.value = 0     return self",if type ( e ) is Argument or type ( e ) is Option and e . argcount :,if type ( e ) is list :,17.878459274600207,17.878459274600207,0.0
"def do_cli(manager, options):     header = [""Name"", ""Description""]     table_data = [header]     for filter_name, filter in get_filters():         <MASK>             continue         filter_doc = inspect.getdoc(filter) or """"         table_data.append([filter_name, filter_doc])     try:         table = TerminalTable(options.table_type, table_data)     except TerminalTableError as e:         console(""ERROR: %s"" % str(e))     else:         console(table.output)",if options . name and not options . name in filter_name :,if not filter :,1.8543042305716586,1.8543042305716586,0.0
"def _do_cmp(f1, f2):     bufsize = BUFSIZE     with open(f1, ""rb"") as fp1, open(f2, ""rb"") as fp2:         while True:             b1 = fp1.read(bufsize)             b2 = fp2.read(bufsize)             if b1 != b2:                 return False             <MASK>                 return True",if not b1 :,if b1 != b2 :,10.682175159905853,10.682175159905853,0.0
"def apply(self, db, person):     families = person.get_parent_family_handle_list()     if families == []:         return True     for family_handle in person.get_parent_family_handle_list():         family = db.get_family_from_handle(family_handle)         if family:             father_handle = family.get_father_handle()             mother_handle = family.get_mother_handle()             <MASK>                 return True             if not mother_handle:                 return True     return False",if not father_handle :,if not father_handle :,100.00000000000004,100.00000000000004,1.0
"def caesar_cipher(s, k):     result = """"     for char in s:         n = ord(char)         if 64 < n < 91:             n = ((n - 65 + k) % 26) + 65         <MASK>             n = ((n - 97 + k) % 26) + 97         result = result + chr(n)     return result",if 96 < n < 123 :,if 64 < n < 91 :,27.77619034011791,27.77619034011791,0.0
"def title_by_index(self, trans, index, context):     d_type = self.get_datatype(trans, context)     for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()):         if i == index:             rval = composite_name             if composite_file.description:                 rval = ""{} ({})"".format(rval, composite_file.description)             <MASK>                 rval = ""%s [optional]"" % rval             return rval     if index < self.get_file_count(trans, context):         return ""Extra primary file""     return None",if composite_file . optional :,if composite_file . optional :,100.00000000000004,100.00000000000004,1.0
"def __str__(self):     t = ""    ""     if self._name != ""root"":         r = f""{t * (self._level-1)}{self._name}:\n""     else:         r = """"     level = self._level     for i, (k, v) in enumerate(self._pointer.items()):         <MASK>             r += f""{t * (self._level)}{v}\n""             self._level += 1         else:             r += f""{t * (self._level)}{k}: {v} ({type(v).__name__})\n""         self._level = level     return r[:-1]","if isinstance ( v , Config ) :",if i == level :,6.916271812933183,6.916271812933183,0.0
"def __get_securitygroups(vm_):     vm_securitygroups = config.get_cloud_config_value(         ""securitygroups"", vm_, __opts__, search_global=False     )     if not vm_securitygroups:         return []     securitygroups = list_securitygroups()     for i in range(len(vm_securitygroups)):         vm_securitygroups[i] = six.text_type(vm_securitygroups[i])         <MASK>             raise SaltCloudNotFound(                 ""The specified securitygroups '{0}' could not be found."".format(                     vm_securitygroups[i]                 )             )     return vm_securitygroups",if vm_securitygroups [ i ] not in securitygroups :,if i not in securitygroups :,24.439253249722206,24.439253249722206,0.0
"def assert_walk_snapshot(     self, field, filespecs_or_globs, paths, ignore_patterns=None, prepare=None ):     with self.mk_project_tree(ignore_patterns=ignore_patterns) as project_tree:         scheduler = self.mk_scheduler(             rules=create_fs_rules(), project_tree=project_tree         )         <MASK>             prepare(project_tree)         result = self.execute(scheduler, Snapshot, self.specs(filespecs_or_globs))[0]         self.assertEqual(sorted(getattr(result, field)), sorted(paths))",if prepare :,if prepare :,100.00000000000004,0.0,1.0
"def _parse_rowids(self, rowids):     xploded = []     rowids = [x.strip() for x in rowids.split("","")]     for rowid in rowids:         try:             <MASK>                 start = int(rowid.split(""-"")[0].strip())                 end = int(rowid.split(""-"")[-1].strip())                 xploded += range(start, end + 1)             else:                 xploded.append(int(rowid))         except ValueError:             continue     return sorted(list(set(xploded)))","if ""-"" in rowid :","if ""-"" in rowid :",100.00000000000004,100.00000000000004,1.0
"def ensemble(self, pairs, other_preds):     """"""Ensemble the dict with statistical model predictions.""""""     lemmas = []     assert len(pairs) == len(other_preds)     for p, pred in zip(pairs, other_preds):         w, pos = p         <MASK>             lemma = self.composite_dict[(w, pos)]         elif w in self.word_dict:             lemma = self.word_dict[w]         else:             lemma = pred         if lemma is None:             lemma = w         lemmas.append(lemma)     return lemmas","if ( w , pos ) in self . composite_dict :",if pos in self . composite_dict :,49.56678178292308,49.56678178292308,0.0
"def selectionToChunks(self, remove=False, add=False):     box = self.selectionBox()     if box:         <MASK>             self.selectedChunks = set(self.level.allChunks)             return         selectedChunks = self.selectedChunks         boxedChunks = set(box.chunkPositions)         if boxedChunks.issubset(selectedChunks):             remove = True         if remove and not add:             selectedChunks.difference_update(boxedChunks)         else:             selectedChunks.update(boxedChunks)     self.selectionTool.selectNone()",if box == self . level . bounds :,if box . level . allChunks :,20.95871245288356,20.95871245288356,0.0
"def _ensure_max_size(cls, image, max_size, interpolation):     if max_size is not None:         size = max(image.shape[0], image.shape[1])         <MASK>             resize_factor = max_size / size             new_height = int(image.shape[0] * resize_factor)             new_width = int(image.shape[1] * resize_factor)             image = ia.imresize_single_image(                 image, (new_height, new_width), interpolation=interpolation             )     return image",if size > max_size :,if size > max_size :,100.00000000000004,100.00000000000004,1.0
"def _1_0_cloud_ips(self, method, url, body, headers):     if method == ""GET"":         return self.test_response(httplib.OK, self.fixtures.load(""list_cloud_ips.json""))     elif method == ""POST"":         <MASK>             body = json.loads(body)         node = json.loads(self.fixtures.load(""create_cloud_ip.json""))         if ""reverse_dns"" in body:             node[""reverse_dns""] = body[""reverse_dns""]         return self.test_response(httplib.ACCEPTED, json.dumps(node))",if body :,"if ""reverse_dns"" in body :",10.552670315936318,10.552670315936318,0.0
"def get_formatted_stats(self):     """"""Get percentage or number of rar's done""""""     if self.cur_setname and self.cur_setname in self.total_volumes:         # This won't work on obfuscated posts         <MASK>             return ""%02d/%02d"" % (self.cur_volume, self.total_volumes[self.cur_setname])     return self.cur_volume",if self . total_volumes [ self . cur_setname ] >= self . cur_volume and self . cur_volume :,if self . cur_volume in self . total_volumes :,22.07061200651185,22.07061200651185,0.0
"def wdayset(self, year, month, day):     # We need to handle cross-year weeks here.     dset = [None] * (self.yearlen + 7)     i = datetime.date(year, month, day).toordinal() - self.yearordinal     start = i     for j in range(7):         dset[i] = i         i += 1         # if (not (0 <= i < self.yearlen) or         #    self.wdaymask[i] == self.rrule._wkst):         # This will cross the year boundary, if necessary.         <MASK>             break     return dset, start, i",if self . wdaymask [ i ] == self . rrule . _wkst :,if self . wdaymask [ i ] == self . rrule . _wkst :,100.00000000000004,100.00000000000004,1.0
"def do_acquire_read_lock(self, wait=True):     self.condition.acquire()     try:         # see if a synchronous operation is waiting to start         # or is already running, in which case we wait (or just         # give up and return)         <MASK>             while self.current_sync_operation is not None:                 self.condition.wait()         else:             if self.current_sync_operation is not None:                 return False         self.asynch += 1     finally:         self.condition.release()     if not wait:         return True",if wait :,if self . current_sync_operation is not None :,4.02724819242185,4.02724819242185,0.0
"def _blend(x, y):  # pylint: disable=invalid-name     """"""Implements the ""blend"" strategy for `deep_merge`.""""""     if isinstance(x, (dict, OrderedDict)):         <MASK>             return y         return _merge(x, y, recursion_func=_blend)     if isinstance(x, (list, tuple)):         if not isinstance(y, (list, tuple)):             return y         result = [_blend(*i) for i in zip(x, y)]         if len(x) > len(y):             result += x[len(y) :]         elif len(x) < len(y):             result += y[len(x) :]         return result     return y","if not isinstance ( y , ( dict , OrderedDict ) ) :","if not isinstance ( y , ( list , tuple ) ) :",59.230330720232516,59.230330720232516,0.0
"def update_forum_nums_topic_post(modeladmin, request, queryset):     for forum in queryset:         forum.num_topics = forum.count_nums_topic()         forum.num_posts = forum.count_nums_post()         <MASK>             forum.last_post = forum.topic_set.order_by(""-last_reply_on"")[0].last_post         else:             forum.last_post = """"         forum.save()",if forum . num_topics :,if forum . last_reply_on :,20.164945583740657,20.164945583740657,0.0
"def get_docname_for_node(self, node: Node) -> str:     while node:         if isinstance(node, nodes.document):             return self.env.path2doc(node[""source""])         <MASK>             return node[""docname""]         else:             node = node.parent     return None  # never reached here. only for type hinting","elif isinstance ( node , addnodes . start_of_file ) :",if node . docname :,2.3238598963754593,2.3238598963754593,0.0
"def _selected_machines(self, virtual_machines):     selected_machines = []     for machine in virtual_machines:         if self._args.host and self._args.host == machine.name:             selected_machines.append(machine)         if self.tags and self._tags_match(machine.tags, self.tags):             selected_machines.append(machine)         <MASK>             selected_machines.append(machine)     return selected_machines",if self . locations and machine . location in self . locations :,if self . tags :,8.626775877575973,8.626775877575973,0.0
"def transform_kwarg(self, name, value, split_single_char_options):     if len(name) == 1:         if value is True:             return [""-%s"" % name]         <MASK>             if split_single_char_options:                 return [""-%s"" % name, ""%s"" % value]             else:                 return [""-%s%s"" % (name, value)]     else:         if value is True:             return [""--%s"" % dashify(name)]         elif value is not False and value is not None:             return [""--%s=%s"" % (dashify(name), value)]     return []","elif value not in ( False , None ) :",if value is not None :,5.893383794376546,5.893383794376546,0.0
"def indent(elem, level=0):     i = ""\n"" + level * ""  ""     if len(elem):         if not elem.text or not elem.text.strip():             elem.text = i + ""  ""         if not elem.tail or not elem.tail.strip():             elem.tail = i         for elem in elem:             indent(elem, level + 1)         if not elem.tail or not elem.tail.strip():             elem.tail = i     else:         <MASK>             elem.tail = i",if level and ( not elem . tail or not elem . tail . strip ( ) ) :,if len ( elem ) > 0 :,2.0879268276081806,2.0879268276081806,0.0
"def _run_instances_op(self, op, instance_ids, **kwargs):     while instance_ids:         try:             return self.manager.retry(op, InstanceIds=instance_ids, **kwargs)         except ClientError as e:             <MASK>                 instance_ids.remove(extract_instance_id(e))             raise","if e . response [ ""Error"" ] [ ""Code"" ] == ""IncorrectInstanceState"" :",if e . status_code == 404 :,7.562380261607853,7.562380261607853,0.0
"def runTest(self):     self.poco(text=""wait UI"").click()     bomb_count = 0     while True:         blue_fish = self.poco(""fish_emitter"").child(""blue"")         yellow_fish = self.poco(""fish_emitter"").child(""yellow"")         bomb = self.poco(""fish_emitter"").child(""bomb"")         fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb])         <MASK>             bomb_count += 1             if bomb_count > 3:                 return         else:             fish.click()         time.sleep(2.5)",if fish is bomb :,if fish is None :,42.72870063962342,42.72870063962342,0.0
"def lineWidth(self, lw=None):     """"""Set/get width of mesh edges. Same as `lw()`.""""""     if lw is not None:         <MASK>             self.GetProperty().EdgeVisibilityOff()             self.GetProperty().SetRepresentationToSurface()             return self         self.GetProperty().EdgeVisibilityOn()         self.GetProperty().SetLineWidth(lw)     else:         return self.GetProperty().GetLineWidth()     return self",if lw == 0 :,if lw == self . _lw :,31.55984539112946,31.55984539112946,0.0
"def _current_date_updater(doc, field_name, value):     if isinstance(doc, dict):         <MASK>             # TODO(juannyg): get_current_timestamp should also be using helpers utcnow,             # as it currently using time.time internally             doc[field_name] = helpers.get_current_timestamp()         else:             doc[field_name] = mongomock.utcnow()","if value == { ""$type"" : ""timestamp"" } :",if value is not None :,4.008579202215618,4.008579202215618,0.0
"def fill_members(self):     if self._get_retrieve():         after = self.after.id if self.after else None         data = await self.get_members(self.guild.id, self.retrieve, after)         <MASK>             # no data, terminate             return         if len(data) < 1000:             self.limit = 0  # terminate loop         self.after = Object(id=int(data[-1][""user""][""id""]))         for element in reversed(data):             await self.members.put(self.create_member(element))",if not data :,if not data :,100.00000000000004,100.00000000000004,1.0
"def extract(self, page, start_index=0, end_index=None):     items = []     for extractor in self.extractors:         extracted = extractor.extract(             page, start_index, end_index, self.template.ignored_regions         )         for item in arg_to_iter(extracted):             if item:                 <MASK>                     item[u""_template""] = self.template.id                 items.append(item)     return items","if isinstance ( item , ( ItemProcessor , dict ) ) :","if not item [ ""id"" ] :",4.4959869933858485,4.4959869933858485,0.0
"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any:     fields = self.config[fields_key]     node_tags = self.provider.node_tags(node_id)     if TAG_RAY_USER_NODE_TYPE in node_tags:         node_type = node_tags[TAG_RAY_USER_NODE_TYPE]         if node_type not in self.available_node_types:             raise ValueError(f""Unknown node type tag: {node_type}."")         node_specific_config = self.available_node_types[node_type]         <MASK>             fields = node_specific_config[fields_key]     return fields",if fields_key in node_specific_config :,if node_specific_config :,47.486944442513455,47.486944442513455,0.0
"def _write_all(self, writer):     """"""Writes messages and insert comments here and there.""""""     # Note: we make no assumptions about the length of original_messages and original_comments     for msg, comment in zip_longest(         self.original_messages, self.original_comments, fillvalue=None     ):         # msg and comment might be None         <MASK>             print(""writing comment: "", comment)             writer.log_event(comment)  # we already know that this method exists         if msg is not None:             print(""writing message: "", msg)             writer(msg)",if comment is not None :,if comment is not None :,100.00000000000004,100.00000000000004,1.0
"def run_tests():     # type: () -> None     x = 5     with switch(x) as case:         <MASK>             print(""zero"")             print(""zero"")         elif case(1, 2):             print(""one or two"")         elif case(3, 4):             print(""three or four"")         else:             print(""default"")             print(""another"")",if case ( 0 ) :,"if case ( 0 , 1 ) :",41.11336169005198,41.11336169005198,0.0
"def date_to_format(value, target_format):     """"""Convert date to specified format""""""     if target_format == str:         if isinstance(value, datetime.date):             ret = value.strftime(""%d/%m/%y"")         <MASK>             ret = value.strftime(""%d/%m/%y"")         elif isinstance(value, datetime.time):             ret = value.strftime(""%H:%M:%S"")     else:         ret = value     return ret","elif isinstance ( value , datetime . datetime ) :","if isinstance ( value , datetime . date ) :",58.14307369682194,58.14307369682194,0.0
"def database_app(request):     if request.param == ""postgres_app"":         if not which(""initdb""):             pytest.skip(""initdb must be on PATH for postgresql fixture"")         <MASK>             pytest.skip(""psycopg2 must be installed for postgresql fixture"")     if request.param == ""sqlite_rabbitmq_app"":         if not os.environ.get(""GALAXY_TEST_AMQP_INTERNAL_CONNECTION""):             pytest.skip(                 ""rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset""             )     return request.getfixturevalue(request.param)",if not psycopg2 :,"if not which ( ""psycopg2"" ) :",11.339582221952005,11.339582221952005,0.0
"def poll_ms(self, timeout=-1):     s = bytearray(self.evbuf)     <MASK>         deadline = utime.ticks_add(utime.ticks_ms(), timeout)     while True:         n = epoll_wait(self.epfd, s, 1, timeout)         if not os.check_error(n):             break         if timeout >= 0:             timeout = utime.ticks_diff(deadline, utime.ticks_ms())             if timeout < 0:                 n = 0                 break     res = []     if n > 0:         vals = struct.unpack(epoll_event, s)         res.append((vals[1], vals[0]))     return res",if timeout >= 0 :,if n == 0 :,32.46679154750991,32.46679154750991,0.0
"def get_all_active_plugins(self) -> List[BotPlugin]:     """"""This returns the list of plugins in the callback ordered defined from the config.""""""     all_plugins = []     for name in self.plugins_callback_order:         # None is a placeholder for any plugin not having a defined order         if name is None:             all_plugins += [                 plugin                 for name, plugin in self.plugins.items()                 if name not in self.plugins_callback_order and plugin.is_activated             ]         else:             plugin = self.plugins[name]             <MASK>                 all_plugins.append(plugin)     return all_plugins",if plugin . is_activated :,if plugin . is_activated :,100.00000000000004,100.00000000000004,1.0
"def get_expected_sql(self):     sql_base_path = path.join(path.dirname(path.realpath(__file__)), ""sql"")     # Iterate the version mapping directories.     for version_mapping in get_version_mapping_directories(self.server[""type""]):         <MASK>             continue         complete_path = path.join(sql_base_path, version_mapping[""name""])         if not path.exists(complete_path):             continue         break     data_sql = """"     with open(path.join(complete_path, ""test_sql_output.sql"")) as fp:         data_sql = fp.read()     return data_sql","if version_mapping [ ""number"" ] > self . server_information [ ""server_version"" ] :","if not version_mapping [ ""name"" ] :",16.233562704120803,16.233562704120803,0.0
"def _validate_headers(self, headers):     if headers is None:         return headers     res = {}     for key, value in headers.items():         if isinstance(value, (int, float)):             value = str(value)         <MASK>             raise ScriptError(                 {                     ""message"": ""headers must be a table""                     "" with strings as keys and values.""                     ""Header: `{!r}:{!r}` is not valid"".format(key, value)                 }             )         res[key] = value     return res","if not isinstance ( key , ( bytes , str ) ) or not isinstance ( value , ( bytes , str ) ) :","if not isinstance ( value , ( str , unicode ) ) :",24.049391026906246,24.049391026906246,0.0
"def _get_literal_value(self, pyval):     if pyval == self.vm.lookup_builtin(""builtins.True""):         return True     elif pyval == self.vm.lookup_builtin(""builtins.False""):         return False     elif isinstance(pyval, str):         prefix, value = parser_constants.STRING_RE.match(pyval).groups()[:2]         value = value[1:-1]  # remove quotation marks         <MASK>             value = compat.bytestring(value)         elif ""u"" in prefix and self.vm.PY2:             value = compat.UnicodeType(value)         return value     else:         return pyval","if ""b"" in prefix and not self . vm . PY2 :","if ""b"" in prefix and self . vm . PY2 :",78.81929718099911,78.81929718099911,0.0
"def decode_query_ids(self, trans, conditional):     if conditional.operator == ""and"":         self.decode_query_ids(trans, conditional.left)         self.decode_query_ids(trans, conditional.right)     else:         left_base = conditional.left.split(""."")[0]         <MASK>             field = self.FIELDS[left_base]             if field.id_decode:                 conditional.right = trans.security.decode_id(conditional.right)",if left_base in self . FIELDS :,if left_base in self . FIELDS :,100.00000000000004,100.00000000000004,1.0
"def testLastPhrases(self):     for day in (11, 12, 13, 14, 15, 16, 17):         start = datetime.datetime(2012, 11, day, 9, 0, 0)         (yr, mth, dy, _, _, _, wd, yd, isdst) = start.timetuple()         n = 4 - wd         <MASK>             n -= 7         target = start + datetime.timedelta(days=n)         self.assertExpectedResult(             self.cal.parse(""last friday"", start.timetuple()),             (target.timetuple(), 1),             dateOnly=True,         )",if n >= 0 :,if n > 7 :,34.98330125272253,34.98330125272253,0.0
"def _convertNbCharsInNbBits(self, nbChars):     nbMinBit = None     nbMaxBit = None     if nbChars is not None:         if isinstance(nbChars, int):             nbMinBit = nbChars * 8             nbMaxBit = nbMinBit         else:             <MASK>                 nbMinBit = nbChars[0] * 8             if nbChars[1] is not None:                 nbMaxBit = nbChars[1] * 8     return (nbMinBit, nbMaxBit)",if nbChars [ 0 ] is not None :,if nbChars [ 0 ] is not None :,100.00000000000004,100.00000000000004,1.0
"def getpystone():     # Start calculation     maxpystone = 0     # Start with a short run, find the the pystone, and increase runtime until duration took > 0.1 second     for pyseed in [1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000]:         duration, pystonefloat = pystones(pyseed)         maxpystone = max(maxpystone, int(pystonefloat))         # Stop when pystone() has been running for at least 0.1 second         <MASK>             break     return maxpystone",if duration > 0.1 :,if duration > 0.1 :,100.00000000000004,100.00000000000004,1.0
"def _append_to_io_queue(self, data, stream_name):     # Make sure ANSI CSI codes and object links are stored as separate events     # TODO: try to complete previously submitted incomplete code     parts = re.split(OUTPUT_SPLIT_REGEX, data)     for part in parts:         <MASK>  # split may produce empty string in the beginning or start             # split the data so that very long lines separated             for block in re.split(                 ""(.{%d,})"" % (self._get_squeeze_threshold() + 1), part             ):                 if block:                     self._queued_io_events.append((block, stream_name))",if part :,if part :,100.00000000000004,0.0,1.0
"def qtTypeIdent(conn, *args):     # We're not using the conn object at the moment, but - we will     # modify the     # logic to use the server version specific keywords later.     res = None     value = None     for val in args:         # DataType doesn't have len function then convert it to string         <MASK>             val = str(val)         if len(val) == 0:             continue         value = val         if Driver.needsQuoting(val, True):             value = value.replace('""', '""""')             value = '""' + value + '""'         res = ((res and res + ""."") or """") + value     return res","if not hasattr ( val , ""__len__"" ) :",if len ( val ) == 1 :,6.443030905386945,6.443030905386945,0.0
"def SetVerbose(self, level):     """"""Sets the verbose level.""""""     try:         <MASK>             level = int(level)         if (level >= 0) and (level <= 3):             self._verbose = level             return     except ValueError:         pass     self.Error(""Verbose level (%s) must be between 0 and 3 inclusive."" % level)",if type ( level ) != types . IntType :,if level :,2.7574525891364066,0.0,0.0
"def step(self) -> None:     """"""Performs a single optimization step.""""""     for group in self.param_groups:         for p in group[""params""]:             <MASK>                 continue             p.add_(p.grad, alpha=(-group[""lr""] * self.num_data))     return None",if p . grad is None :,if not p . grad :,29.05925408079185,29.05925408079185,0.0
"def fill(self, values):     if lupa.lua_type(values) != ""table"":         raise ScriptError(             {                 ""argument"": ""values"",                 ""message"": ""element:fill values is not a table"",                 ""splash_method"": ""fill"",             }         )     # marking all tables as arrays by default     for key, value in values.items():         <MASK>             _mark_table_as_array(self.lua, value)     values = self.lua.lua2python(values)     return self.element.fill(values)","if lupa . lua_type ( value ) == ""table"" :","if key == ""array"" :",11.277832374502772,11.277832374502772,0.0
"def _gen_repr(self, buf):     print >> buf, ""    def __repr__(self):""     if self.argnames:         fmt = COMMA.join([""%s""] * self.nargs)         <MASK>             fmt = ""(%s)"" % fmt         vals = [""repr(self.%s)"" % name for name in self.argnames]         vals = COMMA.join(vals)         if self.nargs == 1:             vals = vals + "",""         print >> buf, '        return ""%s(%s)"" %% (%s)' % (self.name, fmt, vals)     else:         print >> buf, '        return ""%s()""' % self.name","if ""("" in self . args :",if self . nargs == 2 :,11.59119922599073,11.59119922599073,0.0
"def render_observation(self):     x = self.read_head_position     label = ""Observation Grid    : ""     x_str = """"     for j in range(-1, self.rows + 1):         <MASK>             x_str += "" "" * len(label)         for i in range(-2, self.input_width + 2):             if i == x[0] and j == x[1]:                 x_str += colorize(self._get_str_obs((i, j)), ""green"", highlight=True)             else:                 x_str += self._get_str_obs((i, j))         x_str += ""\n""     x_str = label + x_str     return x_str",if j != - 1 :,if i == self . input_width :,5.522397783539471,5.522397783539471,0.0
"def get_module_comment(self, attrname: str) -> Optional[List[str]]:     try:         analyzer = ModuleAnalyzer.for_module(self.modname)         analyzer.analyze()         key = ("""", attrname)         <MASK>             return list(analyzer.attr_docs[key])     except PycodeError:         pass     return None",if key in analyzer . attr_docs :,if analyzer . attr_docs :,63.191456189157286,63.191456189157286,0.0
"def tms_to_quadkey(self, tms, google=False):     quadKey = """"     x, y, z = tms     # this algorithm works with google tiles, rather than tms, so convert     # to those first.     if not google:         y = (2 ** z - 1) - y     for i in range(z, 0, -1):         digit = 0         mask = 1 << (i - 1)         <MASK>             digit += 1         if (y & mask) != 0:             digit += 2         quadKey += str(digit)     return quadKey",if ( x & mask ) != 0 :,if x & mask :,16.62083000646927,16.62083000646927,0.0
"def test_enumerate(app):     async with new_stream(app) as stream:         for i in range(100):             await stream.channel.deliver(message(key=i, value=i * 4))         async for i, value in stream.enumerate():             current_event = stream.current_event             assert i == current_event.key             assert value == i * 4             <MASK>                 break         assert await channel_empty(stream.channel)",if i >= 99 :,if current_event . key == i :,5.934202609760488,5.934202609760488,0.0
"def print_messages(self):     output_reports = self.config.get_output_report()     for report in output_reports:         output_format, output_files = report         self.summary[""formatter""] = output_format         formatter = FORMATTERS[output_format](             self.summary, self.messages, self.config.profile         )         <MASK>             self.write_to(formatter, sys.stdout)         for output_file in output_files:             with open(output_file, ""w+"") as target:                 self.write_to(formatter, target)",if not output_files :,if output_files :,57.89300674674101,57.89300674674101,0.0
"def eval_metrics(self):     for task in self.task_list:         <MASK>             return [                 metrics.Metrics.ACC,                 metrics.Metrics.NEG_LOG_PERPLEXITY,                 metrics.Metrics.ROUGE_2_F,                 metrics.Metrics.ROUGE_L_F,             ]     return [         metrics.Metrics.ACC,         metrics.Metrics.NEG_LOG_PERPLEXITY,     ]","if ""summarize"" in task . name :","if task . name == ""ROUGE"" :",19.304869754804493,19.304869754804493,0.0
"def _getBuildRequestForBrdict(self, brdict):     # Turn a brdict into a BuildRequest into a brdict. This is useful     # for API like 'nextBuild', which operate on BuildRequest objects.     breq = self.breqCache.get(brdict[""buildrequestid""])     if not breq:         breq = yield BuildRequest.fromBrdict(self.master, brdict)         <MASK>             self.breqCache[brdict[""buildrequestid""]] = breq     defer.returnValue(breq)",if breq :,if breq :,100.00000000000004,0.0,1.0
"def _stash_splitter(states):     keep, split = [], []     if state_func is not None:         for s in states:             ns = state_func(s)             <MASK>                 split.append(ns)             elif isinstance(ns, (list, tuple, set)):                 split.extend(ns)             else:                 split.append(s)     if stash_func is not None:         split = stash_func(states)     if to_stash is not stash:         keep = states     return keep, split","if isinstance ( ns , SimState ) :",if ns is not None :,7.654112967106117,7.654112967106117,0.0
"def sequence_to_text(sequence):     """"""Converts a sequence of IDs back to a string""""""     result = """"     for symbol_id in sequence:         <MASK>             s = _id_to_symbol[symbol_id]             # Enclose ARPAbet back in curly braces:             if len(s) > 1 and s[0] == ""@"":                 s = ""{%s}"" % s[1:]             result += s     return result.replace(""}{"", "" "")",if symbol_id in _id_to_symbol :,if symbol_id in _id_to_symbol :,100.00000000000004,100.00000000000004,1.0
"def get_code(self, fullname=None):     fullname = self._fix_name(fullname)     if self.code is None:         mod_type = self.etc[2]         <MASK>             source = self.get_source(fullname)             self.code = compile(source, self.filename, ""exec"")         elif mod_type == imp.PY_COMPILED:             self._reopen()             try:                 self.code = read_code(self.file)             finally:                 self.file.close()         elif mod_type == imp.PKG_DIRECTORY:             self.code = self._get_delegate().get_code()     return self.code",if mod_type == imp . PY_SOURCE :,if mod_type == imp . PY_COMPILED :,82.651681837938,82.651681837938,0.0
"def identwaf(self, findall=False):     detected = list()     try:         self.attackres = self.performCheck(self.centralAttack)     except RequestBlocked:         return detected     for wafvendor in self.checklist:         self.log.info(""Checking for %s"" % wafvendor)         if self.wafdetections[wafvendor](self):             detected.append(wafvendor)             <MASK>                 break     self.knowledge[""wafname""] = detected     return detected",if not findall :,if not self . attackres :,17.965205598154213,17.965205598154213,0.0
"def SessionId(self):     """"""Returns the Session ID of the process""""""     if self.Session.is_valid():         process_space = self.get_process_address_space()         <MASK>             return obj.Object(                 ""_MM_SESSION_SPACE"", offset=self.Session, vm=process_space             ).SessionId     return obj.NoneObject(""Cannot find process session"")",if process_space :,if process_space :,100.00000000000004,100.00000000000004,1.0
"def _convert_java_pattern_to_python(pattern):     """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`.""""""     s = list(pattern)     i = 0     while i < len(s) - 1:         c = s[i]         if c == ""$"" and s[i + 1] in ""0123456789"":             s[i] = ""\\""         <MASK>             s[i] = """"             i += 1         i += 1     return pattern[:0].join(s)","elif c == ""\\"" and s [ i + 1 ] == ""$"" :","if c == ""0123456789"" :",8.09570092366407,8.09570092366407,0.0
"def __init__(self, coverage):     self.coverage = coverage     self.config = self.coverage.config     self.source_paths = set()     if self.config.source:         for src in self.config.source:             <MASK>                 if not self.config.relative_files:                     src = files.canonical_filename(src)                 self.source_paths.add(src)     self.packages = {}     self.xml_out = None",if os . path . exists ( src ) :,if src not in self . source_paths :,5.934202609760488,5.934202609760488,0.0
"def populate_vol_format(self):     rhel6_file_whitelist = [""raw"", ""qcow2"", ""qed""]     model = self.widget(""vol-format"").get_model()     model.clear()     formats = self.vol_class.formats     if hasattr(self.vol_class, ""create_formats""):         formats = getattr(self.vol_class, ""create_formats"")     if self.vol_class == Storage.FileVolume and not self.conn.rhel6_defaults_caps():         newfmts = []         for f in rhel6_file_whitelist:             <MASK>                 newfmts.append(f)         formats = newfmts     for f in formats:         model.append([f, f])",if f in formats :,if not formats :,27.534765745159184,27.534765745159184,0.0
"def get_file_sources():     global _file_sources     if _file_sources is None:         from galaxy.files import ConfiguredFileSources         file_sources = None         <MASK>             file_sources_as_dict = None             with open(""file_sources.json"", ""r"") as f:                 file_sources_as_dict = json.load(f)             if file_sources_as_dict is not None:                 file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict)         if file_sources is None:             ConfiguredFileSources.from_dict([])         _file_sources = file_sources     return _file_sources","if os . path . exists ( ""file_sources.json"" ) :",if file_sources . json is not None :,23.45000810620359,23.45000810620359,0.0
"def _blend(x, y):  # pylint: disable=invalid-name     """"""Implements the ""blend"" strategy for `deep_merge`.""""""     if isinstance(x, (dict, OrderedDict)):         if not isinstance(y, (dict, OrderedDict)):             return y         return _merge(x, y, recursion_func=_blend)     if isinstance(x, (list, tuple)):         if not isinstance(y, (list, tuple)):             return y         result = [_blend(*i) for i in zip(x, y)]         <MASK>             result += x[len(y) :]         elif len(x) < len(y):             result += y[len(x) :]         return result     return y",if len ( x ) > len ( y ) :,if len ( x ) > len ( y ) :,100.00000000000004,100.00000000000004,1.0
"def copy_dicts(dct):     if ""_remote_data"" in dct:         dsindex = dct[""_remote_data""][""_content""].dsindex         newdct = dct.copy()         newdct[""_remote_data""] = {""_content"": dsindex}         return list(newdct.items())     elif ""_data"" in dct:         newdct = dct.copy()         newdata = copy_dicts(dct[""_data""])         <MASK>             newdct[""_data""] = newdata         return list(newdct.items())     return None",if newdata :,if newdata :,100.00000000000004,0.0,1.0
"def _import_epic_activity(self, project_data, taiga_epic, epic, options):     offset = 0     while True:         activities = self._client.get(             ""/projects/{}/epics/{}/activity"".format(                 project_data[""id""],                 epic[""id""],             ),             {""envelope"": ""true"", ""limit"": 300, ""offset"": offset},         )         offset += 300         for activity in activities[""data""]:             self._import_activity(taiga_epic, activity, options)         <MASK>             break","if len ( activities [ ""data"" ] ) < 300 :",if not activities :,2.215745752614824,2.215745752614824,0.0
"def __get__(self, instance, instance_type=None):     if instance:         <MASK>             rel_obj = self.get_obj(instance)             if rel_obj:                 instance._obj_cache[self.att_name] = rel_obj         return instance._obj_cache.get(self.att_name)     return self",if self . att_name not in instance . _obj_cache :,if instance . att_name in self . att_cache :,38.14156086719433,38.14156086719433,0.0
"def download_main(     download, download_playlist, urls, playlist, output_dir, merge, info_only ):     for url in urls:         if url.startswith(""https://""):             url = url[8:]         if not url.startswith(""http://""):             url = ""http://"" + url         <MASK>             download_playlist(                 url, output_dir=output_dir, merge=merge, info_only=info_only             )         else:             download(url, output_dir=output_dir, merge=merge, info_only=info_only)",if playlist :,if playlist :,100.00000000000004,0.0,1.0
"def _mksubs(self):     self._subs = {}     commit_dir = CommitDir(self, "".commit"")     self._subs["".commit""] = commit_dir     tag_dir = TagDir(self, "".tag"")     self._subs["".tag""] = tag_dir     for (name, sha) in git.list_refs():         <MASK>             name = name[11:]             date = git.rev_get_date(sha.encode(""hex""))             n1 = BranchList(self, name, sha)             n1.ctime = n1.mtime = date             self._subs[name] = n1","if name . startswith ( ""refs/heads/"" ) :",if len ( name ) > 11 :,4.420141128732569,4.420141128732569,0.0
"def readAtOffset(self, offset, size, shortok=False):     ret = b""""     self.fd.seek(offset)     while len(ret) != size:         rlen = size - len(ret)         x = self.fd.read(rlen)         <MASK>             if not shortok:                 return None             return ret         ret += x     return ret","if x == b"""" :",if x == 0 :,38.49815007763549,38.49815007763549,0.0
"def remove_indent(self):     """"""Remove one tab-width of blanks from the previous token.""""""     w = abs(self.tab_width)     if self.result:         s = self.result[-1]         <MASK>             self.result.pop()             s = s.replace(""\t"", "" "" * w)             if s.startswith(""\n""):                 s2 = s[1:]                 self.result.append(""\n"" + s2[:-w])             else:                 self.result.append(s[:-w])",if s . isspace ( ) :,"if s . startswith ( ""\t"" ) :",18.36028134946796,18.36028134946796,0.0
"def flush(self, *args, **kwargs):     with self._lock:         self._last_updated = time.time()         try:             if kwargs.get(""in_place"", False):                 self._locked_flush_without_tempfile()             else:                 mailbox.mbox.flush(self, *args, **kwargs)         except OSError:             <MASK>                 self._locked_flush_without_tempfile()             else:                 raise         self._last_updated = time.time()","if ""_create_temporary"" in traceback . format_exc ( ) :","if kwargs . get ( ""in_place"" , False ) :",8.527902588160915,8.527902588160915,0.0
"def _collect_manual_intervention_nodes(pipeline_tree):     for act in pipeline_tree[""activities""].values():         <MASK>             _collect_manual_intervention_nodes(act[""pipeline""])         elif act[""component""][""code""] in MANUAL_INTERVENTION_COMP_CODES:             manual_intervention_nodes.add(act[""id""])","if act [ ""type"" ] == ""SubProcess"" :","if act [ ""component"" ] [ ""code"" ] in MANUAL_INTERVENTION_PIPELINE_CODES :",14.962848372546667,14.962848372546667,0.0
"def banned():     if request.endpoint == ""views.themes"":         return     if authed():         user = get_current_user_attrs()         team = get_current_team_attrs()         <MASK>             return (                 render_template(                     ""errors/403.html"", error=""You have been banned from this CTF""                 ),                 403,             )         if team and team.banned:             return (                 render_template(                     ""errors/403.html"",                     error=""Your team has been banned from this CTF"",                 ),                 403,             )",if user and user . banned :,if user and user . banned :,100.00000000000004,100.00000000000004,1.0
"def remove(self, values):     if not isinstance(values, (list, tuple, set)):         values = [values]     for v in values:         v = str(v)         if isinstance(self._definition, dict):             self._definition.pop(v, None)         elif self._definition == ""ANY"":             if v == ""ANY"":                 self._definition = []         <MASK>             self._definition.remove(v)     if (         self._value is not None         and self._value not in self._definition         and self._not_any()     ):         raise ConanException(bad_value_msg(self._name, self._value, self.values_range))",elif v in self . _definition :,"if isinstance ( self . _definition , list ) :",24.808415001701817,24.808415001701817,0.0
"def save(self, learner, file_name):     """"""Save the model to location specified in file_name.""""""     with open(file_name, ""wb"") as f:         <MASK>             # don't store the large inference cache!             learner.inference_cache_, tmp = (None, learner.inference_cache_)             pickle.dump(learner, f, -1)             learner.inference_cache_ = tmp         else:             pickle.dump(learner, f, -1)","if hasattr ( learner , ""inference_cache_"" ) :",if learner . inference_cache_ :,21.283887316959504,21.283887316959504,0.0
"def __init__(self, exprs, savelist=False):     super(ParseExpression, self).__init__(savelist)     if isinstance(exprs, _generatorType):         exprs = list(exprs)     if isinstance(exprs, basestring):         self.exprs = [ParserElement._literalStringClass(exprs)]     elif isinstance(exprs, collections.Iterable):         exprs = list(exprs)         # if sequence of strings provided, wrap with Literal         <MASK>             exprs = map(ParserElement._literalStringClass, exprs)         self.exprs = list(exprs)     else:         try:             self.exprs = list(exprs)         except TypeError:             self.exprs = [exprs]     self.callPreparse = False","if all ( isinstance ( expr , basestring ) for expr in exprs ) :","if isinstance ( exprs , ( list , tuple ) ) :",9.136248401161414,9.136248401161414,0.0
"def find(self, back=False):     flags = 0     <MASK>         flags = QTextDocument.FindBackward     if self.csBox.isChecked():         flags = flags | QTextDocument.FindCaseSensitively     text = self.searchEdit.text()     if not self.findMain(text, flags):         if text in self.editBoxes[self.ind].toPlainText():             cursor = self.editBoxes[self.ind].textCursor()             if back:                 cursor.movePosition(QTextCursor.End)             else:                 cursor.movePosition(QTextCursor.Start)             self.editBoxes[self.ind].setTextCursor(cursor)             self.findMain(text, flags)",if back :,if self . csBox . isChecked ( ) :,5.669791110976001,5.669791110976001,0.0
"def _load_storage(self):     self._storage = {}     for row in self(""SELECT object, resource, amount FROM storage""):         ownerid = int(row[0])         <MASK>             self._storage[ownerid].append(row[1:])         else:             self._storage[ownerid] = [row[1:]]",if ownerid in self . _storage :,if ownerid >= 0 :,12.872632311973014,12.872632311973014,0.0
"def parse_chunked(self, unreader):     (size, rest) = self.parse_chunk_size(unreader)     while size > 0:         while size > len(rest):             size -= len(rest)             yield rest             rest = unreader.read()             <MASK>                 raise NoMoreData()         yield rest[:size]         # Remove \r\n after chunk         rest = rest[size:]         while len(rest) < 2:             rest += unreader.read()         if rest[:2] != b""\r\n"":             raise ChunkMissingTerminator(rest[:2])         (size, rest) = self.parse_chunk_size(unreader, data=rest[2:])",if not rest :,"if rest [ 0 ] != b""\r\n"" :",3.4585921141027356,3.4585921141027356,0.0
"def _augment_batch_(self, batch, random_state, parents, hooks):     for column in batch.columns:         <MASK>             for i, cbaoi in enumerate(column.value):                 column.value[i] = cbaoi.clip_out_of_image_()     return batch","if column . name in [ ""keypoints"" , ""bounding_boxes"" , ""polygons"" , ""line_strings"" ] :","if isinstance ( column , Image ) :",0.7264291938935746,0.7264291938935746,0.0
"def to_nim(self):     if self.is_pointer == 2:         s = ""cstringArray"" if self.type == ""GLchar"" else ""ptr pointer""     else:         s = self.type         <MASK>             default = ""ptr "" + s             s = self.NIM_POINTER_MAP.get(s, default)     return s",if self . is_pointer == 1 :,if self . is_pointer == 1 :,100.00000000000004,100.00000000000004,1.0
"def find(self, path):     if os.path.isfile(path) or os.path.islink(path):         self.num_files = self.num_files + 1         if self.match_function(path):             self.files.append(path)     elif os.path.isdir(path):         for content in os.listdir(path):             file = os.path.join(path, content)             <MASK>                 self.num_files = self.num_files + 1                 if self.match_function(file):                     self.files.append(file)             else:                 self.find(file)",if os . path . isfile ( file ) or os . path . islink ( file ) :,if os . path . isfile ( file ) :,40.656965974059936,40.656965974059936,0.0
"def remove(self, event):     try:         self._events_current_sweep.remove(event)         <MASK>             assert event.in_sweep == True             assert event.other.in_sweep == True             event.in_sweep = False             event.other.in_sweep = False         return True     except KeyError:         if USE_DEBUG:             assert event.in_sweep == False             assert event.other.in_sweep == False         return False",if USE_DEBUG :,if USE_DEBUG :,100.00000000000004,100.00000000000004,1.0
"def update_metadata(self):     for attrname in dir(self):         if attrname.startswith(""__""):             continue         attrvalue = getattr(self, attrname, None)         if attrvalue == 0:             continue         <MASK>             attrname = ""version""         if hasattr(self.metadata, ""set_{0}"".format(attrname)):             getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue)         elif hasattr(self.metadata, attrname):             try:                 setattr(self.metadata, attrname, attrvalue)             except AttributeError:                 pass","if attrname == ""salt_version"" :",if attrvalue == 1 :,9.911450612811139,9.911450612811139,0.0
"def _init_auxiliary_head(self, auxiliary_head):     """"""Initialize ``auxiliary_head``""""""     if auxiliary_head is not None:         <MASK>             self.auxiliary_head = nn.ModuleList()             for head_cfg in auxiliary_head:                 self.auxiliary_head.append(builder.build_head(head_cfg))         else:             self.auxiliary_head = builder.build_head(auxiliary_head)","if isinstance ( auxiliary_head , list ) :",if self . auxiliary_head is nn . ModuleList :,15.851165692617148,15.851165692617148,0.0
"def _str_param_list(self, name):     out = []     if self[name]:         out += self._str_header(name)         for param in self[name]:             parts = []             if param.name:                 parts.append(param.name)             <MASK>                 parts.append(param.type)             out += ["" : "".join(parts)]             if param.desc and """".join(param.desc).strip():                 out += self._str_indent(param.desc)         out += [""""]     return out",if param . type :,if param . type :,100.00000000000004,100.00000000000004,1.0
"def _set_handler(     self, name, handle=None, obj=None, constructor_args=(), constructor_kwds={} ):     if handle is None:         handle = obj is not None     if handle:         handler_class = self.handler_classes[name]         <MASK>             newhandler = handler_class(obj)         else:             newhandler = handler_class(*constructor_args, **constructor_kwds)     else:         newhandler = None     self._replace_handler(name, newhandler)",if obj is not None :,if obj is not None :,100.00000000000004,100.00000000000004,1.0
"def _extract_subtitles(src):     subtitles = {}     for caption in try_get(src, lambda x: x[""captions""], list) or []:         subtitle_url = url_or_none(caption.get(""uri""))         <MASK>             lang = caption.get(""language"", ""deu"")             subtitles.setdefault(lang, []).append(                 {                     ""url"": subtitle_url,                 }             )     return subtitles",if subtitle_url :,if subtitle_url :,100.00000000000004,100.00000000000004,1.0
"def get_keys(struct, ignore_first_level=False):     res = []     if isinstance(struct, dict):         <MASK>             keys = [x.split(""("")[0] for x in struct.keys()]             res.extend(keys)         for key in struct:             if key in IGNORED_KEYS:                 logging.debug(""Ignored: %s: %s"", key, struct[key])                 continue             res.extend(get_keys(struct[key], key in IGNORED_FIRST_LEVEL))     elif isinstance(struct, list):         for item in struct:             res.extend(get_keys(item))     return res",if not ignore_first_level :,if ignore_first_level :,72.89545183625967,72.89545183625967,0.0
"def create_dir(path):     curr_path = None     for p in path:         if curr_path is None:             curr_path = os.path.abspath(p)         else:             curr_path = os.path.join(curr_path, p)         <MASK>             os.mkdir(curr_path)",if not os . path . exists ( curr_path ) :,if not os . path . isdir ( curr_path ) :,76.11606003349888,76.11606003349888,0.0
"def dataToDumpFile(dumpFile, data):     try:         dumpFile.write(data)         dumpFile.flush()     except IOError as ex:         if ""No space left"" in getUnicode(ex):             errMsg = ""no space left on output device""             logger.error(errMsg)         <MASK>             errMsg = ""permission denied when flushing dump data""             logger.error(errMsg)         else:             errMsg = (                 ""error occurred when writing dump data to file ('%s')"" % getUnicode(ex)             )             logger.error(errMsg)","elif ""Permission denied"" in getUnicode ( ex ) :","if ""Permission denied"" in getUnicode ( ex ) :",89.31539818068698,89.31539818068698,0.0
"def elements(self, top):     res = []     # try:     #     string = ""== %s (%s)"" % (self.name,self.__class__)     # except AttributeError:     #     string = ""== (%s)"" % (self.__class__,)     # print(string)     for part in self.parts:         <MASK>             res.append(name_or_ref(part, top))         else:             if isinstance(part, Extension):                 res.append(part.base)             res.extend(part.elements(top))     return res","if isinstance ( part , Element ) :","if isinstance ( part , Element ) :",100.00000000000004,100.00000000000004,1.0
"def _parse_param_value(name, datatype, default):     if datatype == ""bool"":         if default.lower() == ""true"":             return True         <MASK>             return False         else:             _s = ""{}: Invalid default value '{}' for bool parameter {}""             raise SyntaxError(_s.format(self.name, default, p))     elif datatype == ""int"":         if type(default) == int:             return default         else:             return int(default, 0)     elif datatype == ""real"":         if type(default) == float:             return default         else:             return float(default)     else:         return str(default)","elif default . lower ( ) == ""false"" :","if default . lower ( ) == ""false"" :",90.36020036098445,90.36020036098445,0.0
"def dvmethod(c, dx, doAST=False):     for m in c.get_methods():         mx = dx.get_method(m)         ms = DvMethod(mx)         ms.process(doAST=doAST)         <MASK>             assert ms.get_ast() is not None             assert isinstance(ms.get_ast(), dict)             assert ""body"" in ms.get_ast()         else:             assert ms.get_source() is not None",if doAST :,"if ""body"" in ms . get_source ( ) :",3.673526562988939,3.673526562988939,0.0
"def _repr_pretty_(self, p, cycle):     if cycle:         return ""{{...}""     with p.group(2, ""{"", ""}""):         p.breakable("""")         for idx, key in enumerate(self._items):             <MASK>                 p.text("","")                 p.breakable()             value = self._items[key]             p.pretty(key)             p.text("": "")             if isinstance(value, bytes):                 value = trimmed_repr(value)             p.pretty(value)         p.breakable("""")",if idx :,if idx == 0 :,17.965205598154213,17.965205598154213,0.0
"def remove_rating(self, songs, librarian):     count = len(songs)     if count > 1 and config.getboolean(""browsers"", ""rating_confirm_multiple""):         parent = qltk.get_menu_item_top_parent(self)         dialog = ConfirmRateMultipleDialog(parent, _(""_Remove Rating""), count, None)         <MASK>             return     reset = []     for song in songs:         if ""~#rating"" in song:             del song[""~#rating""]             reset.append(song)     librarian.changed(reset)",if dialog . run ( ) != Gtk . ResponseType . YES :,if dialog . is_active ( ) :,13.40110063389608,13.40110063389608,0.0
"def get_or_create_place(self, place_name):     ""Return the requested place object tuple-packed with a new indicator.""     LOG.debug(""get_or_create_place: looking for: %s"", place_name)     for place_handle in self.db.iter_place_handles():         place = self.db.get_place_from_handle(place_handle)         place_title = place_displayer.display(self.db, place)         <MASK>             return (0, place)     place = Place()     place.set_title(place_name)     place.name = PlaceName(value=place_name)     self.db.add_place(place, self.trans)     return (1, place)",if place_title == place_name :,if place_title == place_name :,100.00000000000004,100.00000000000004,1.0
def _skip_trivial(constraint_data):     if skip_trivial_constraints:         <MASK>             if constraint_data.variables is None:                 return True         else:             if constraint_data.body.polynomial_degree() == 0:                 return True     return False,"if isinstance ( constraint_data , LinearCanonicalRepn ) :",if constraint_data . body is None :,18.04438612975343,18.04438612975343,0.0
"def get_other(self, data, items):     is_tuple = False     if type(data) == tuple:         data = list(data)         is_tuple = True     if type(data) == list:         m_items = items.copy()         for idx, item in enumerate(items):             if item < 0:                 m_items[idx] = len(data) - abs(item)         for i in sorted(set(m_items), reverse=True):             <MASK>                 del data[i]         if is_tuple:             return tuple(data)         else:             return data     else:         return None",if i < len ( data ) and i > - 1 :,if data [ i ] > 0 :,4.420141128732569,4.420141128732569,0.0
"def test_case_insensitivity(self):     with support.EnvironmentVarGuard() as env:         env.set(""PYTHONCASEOK"", ""1"")         <MASK>             self.skipTest(""os.environ changes not reflected in "" ""_os.environ"")         loader = self.find_module()         self.assertTrue(hasattr(loader, ""load_module""))","if b""PYTHONCASEOK"" not in _bootstrap . _os . environ :","if env . get ( ""PYTHONCASEOK"" , ""0"" ) != ""1"" :",8.961672320242714,8.961672320242714,0.0
def field_spec(self):     <MASK>         self.lazy_init_lock_.acquire()         try:             if self.field_spec_ is None:                 self.field_spec_ = FieldSpec()         finally:             self.lazy_init_lock_.release()     return self.field_spec_,if self . field_spec_ is None :,if self . field_spec_ is None :,100.00000000000004,100.00000000000004,1.0
"def reduce(self, f, init):     for x in range(self._idx, rt.count(self._w_array)):         <MASK>             return rt.deref(init)         init = f.invoke([init, rt.nth(self._w_array, rt.wrap(x))])     return init",if rt . reduced_QMARK_ ( init ) :,if x == self . _idx :,5.3990167242108145,5.3990167242108145,0.0
"def _find(event: E) -> None:     # We first check values after the selected value, then all values.     values = list(self.values)     for value in values[self._selected_index + 1 :] + values:         text = fragment_list_to_text(to_formatted_text(value[1])).lower()         <MASK>             self._selected_index = self.values.index(value)             return",if text . startswith ( event . data . lower ( ) ) :,if text :,1.6102756877232731,0.0,0.0
"def check_permissions():     if platform_os() != ""Windows"":         <MASK>             print(localization.lang_check_permissions[""permissions_granted""])         else:             print(localization.lang_check_permissions[""permissions_denied""])             exit()     else:         print(localization.lang_check_permissions[""windows_warning""])         exit()",if getuid ( ) == 0 :,"if platform_os ( ) != ""Windows"" :",8.516593018819643,8.516593018819643,0.0
"def _ProcessName(self, name, dependencies):     """"""Retrieve a module name from a node name.""""""     module_name, dot, base_name = name.rpartition(""."")     if dot:         if module_name:             <MASK>                 dependencies[module_name].add(base_name)             else:                 dependencies[module_name] = {base_name}         else:             # If we have a relative import that did not get qualified (usually due             # to an empty package_name), don't insert module_name='' into the             # dependencies; we get a better error message if we filter it out here             # and fail later on.             logging.warning(""Empty package name: %s"", name)",if module_name in dependencies :,if base_name :,17.030578356760866,17.030578356760866,0.0
"def _load_db(self):     try:         with open(self.db) as db:             content = db.read(8)             db.seek(0)             if content == (""Salted__""):                 data = StringIO()                 <MASK>                     self.encryptor.decrypt(db, data)                 else:                     raise EncryptionError(                         ""Encrpyted credential storage: {}"".format(self.db)                     )                 return json.loads(data.getvalue())             else:                 return json.load(db)     except:         return {""creds"": []}",if self . encryptor :,"if content == ""Salted"" :",6.567274736060395,6.567274736060395,0.0
"def _parse(self, stream, context):     obj = []     try:         context_for_subcon = context         if self.subcon.conflags & self.FLAG_COPY_CONTEXT:             context_for_subcon = context.__copy__()         while True:             subobj = self.subcon._parse(stream, context_for_subcon)             <MASK>                 break             obj.append(subobj)     except ConstructError as ex:         raise ArrayError(""missing terminator"", ex)     return obj","if self . predicate ( subobj , context ) :",if subobj is None :,5.171845311465849,5.171845311465849,0.0
"def is_active_for_user(self, user):     is_active = super(AbstractUserFlag, self).is_active_for_user(user)     if is_active:         return is_active     user_ids = self._get_user_ids()     if hasattr(user, ""pk"") and user.pk in user_ids:         return True     if hasattr(user, ""groups""):         group_ids = self._get_group_ids()         if group_ids:             user_groups = set(user.groups.all().values_list(""pk"", flat=True))             <MASK>                 return True     return None",if group_ids . intersection ( user_groups ) :,if user_groups . count ( ) == 1 :,16.108992769687397,16.108992769687397,0.0
"def lookup_member(self, member_name):     document_choices = self.choices or []     for document_choice in document_choices:         doc_and_subclasses = [document_choice] + document_choice.__subclasses__()         for doc_type in doc_and_subclasses:             field = doc_type._fields.get(member_name)             <MASK>                 return field",if field :,if field :,100.00000000000004,0.0,1.0
"def apply(self, db, person):     families = person.get_parent_family_handle_list()     if families == []:         return True     for family_handle in person.get_parent_family_handle_list():         family = db.get_family_from_handle(family_handle)         if family:             father_handle = family.get_father_handle()             mother_handle = family.get_mother_handle()             if not father_handle:                 return True             <MASK>                 return True     return False",if not mother_handle :,if not mother_handle :,100.00000000000004,100.00000000000004,1.0
"def init_weights(self):     for m in self.modules():         if isinstance(m, nn.Linear):             normal_init(m, std=0.01)         if isinstance(m, nn.Conv3d):             xavier_init(m, distribution=""uniform"")         <MASK>             constant_init(m, 1)","if isinstance ( m , nn . BatchNorm3d ) :","if isinstance ( m , nn . Constant ) :",70.71067811865478,70.71067811865478,0.0
"def _update_learning_params(self):     model = self.model     hparams = self.hparams     fd = self.runner.feed_dict     step_num = self.step_num     if hparams.model_type == ""resnet_tf"":         <MASK>             lrn_rate = hparams.mom_lrn         elif step_num < 30000:             lrn_rate = hparams.mom_lrn / 10         elif step_num < 35000:             lrn_rate = hparams.mom_lrn / 100         else:             lrn_rate = hparams.mom_lrn / 1000         fd[model.lrn_rate] = lrn_rate",if step_num < hparams . lrn_step :,if step_num < 20000 :,36.337289265247364,36.337289265247364,0.0
"def token_producer(source):     token = source.read_uint8()     while token is not None:         if is_push_data_token(token):             yield DataToken(read_data(token, source))         <MASK>             yield SmallIntegerToken(read_small_integer(token))         else:             yield Token(token)         token = source.read_uint8()",elif is_small_integer ( token ) :,if is_push_small_integer_token ( token ) :,38.058030016749456,38.058030016749456,0.0
"def user_info(oicsrv, userdb, sub, client_id="""", user_info_claims=None):     identity = userdb[sub]     if user_info_claims:         result = {}         for key, restr in user_info_claims[""claims""].items():             try:                 result[key] = identity[key]             except KeyError:                 <MASK>                     raise Exception(""Missing property '%s'"" % key)     else:         result = identity     return OpenIDSchema(**result)","if restr == { ""essential"" : True } :","if restr [ ""id"" ] == client_id :",10.04916995660316,10.04916995660316,0.0
"def _helpSlot(self, *args):     help_text = ""Filters are applied to packets in both direction.\n\n""     filter_nb = 0     for filter in self._filters:         help_text += ""{}: {}"".format(filter[""name""], filter[""description""])         filter_nb += 1         <MASK>             help_text += ""\n\n""     QtWidgets.QMessageBox.information(self, ""Help for filters"", help_text)",if len ( self . _filters ) != filter_nb :,if filter_nb > 0 :,10.218289380194191,10.218289380194191,0.0
"def find_user_theme(self, name: str) -> Theme:     """"""Find a theme named as *name* from latex_theme_path.""""""     for theme_path in self.theme_paths:         config_path = path.join(theme_path, name, ""theme.conf"")         <MASK>             try:                 return UserTheme(name, config_path)             except ThemeError as exc:                 logger.warning(exc)     return None",if path . isfile ( config_path ) :,if config_path :,16.62083000646927,16.62083000646927,0.0
"def decompress(self, value):     if value:         <MASK>             if value.country_code and value.national_number:                 return [                     ""+%d"" % value.country_code,                     national_significant_number(value),                 ]         else:             return value.split(""."")     return [None, """"]",if type ( value ) == PhoneNumber :,if value . number :,6.316906128202129,6.316906128202129,0.0
"def update_prevdoc_status(self, flag):     for quotation in list(set([d.prevdoc_docname for d in self.get(""items"")])):         <MASK>             doc = frappe.get_doc(""Quotation"", quotation)             if doc.docstatus == 2:                 frappe.throw(_(""Quotation {0} is cancelled"").format(quotation))             doc.set_status(update=True)             doc.update_opportunity()",if quotation :,if flag :,34.66806371753173,0.0,0.0
"def map(item):     if item.deleted:         return     exploration = exp_fetchers.get_exploration_from_model(item)     for state_name, state in exploration.states.items():         hints_length = len(state.interaction.hints)         <MASK>             exp_and_state_key = ""%s %s"" % (item.id, state_name.encode(""utf-8""))             yield (python_utils.UNICODE(hints_length), exp_and_state_key)",if hints_length > 0 :,if hints_length > 0 :,100.00000000000004,100.00000000000004,1.0
"def _selected_machines(self, virtual_machines):     selected_machines = []     for machine in virtual_machines:         if self._args.host and self._args.host == machine.name:             selected_machines.append(machine)         <MASK>             selected_machines.append(machine)         if self.locations and machine.location in self.locations:             selected_machines.append(machine)     return selected_machines","if self . tags and self . _tags_match ( machine . tags , self . tags ) :",if self . args . host :,3.759098586913923,3.759098586913923,0.0
"def _ripple_trim_compositors_move(self, delta):     comp_ids = self.multi_data.moved_compositors_destroy_ids     tracks_compositors = _get_tracks_compositors_list()     track_moved = self.multi_data.track_affected     for i in range(1, len(current_sequence().tracks) - 1):         if not track_moved[i - 1]:             continue         track_comps = tracks_compositors[i - 1]         for comp in track_comps:             <MASK>                 comp.move(delta)",if comp . destroy_id in comp_ids :,if comp . id in comp_ids :,59.86908497649472,59.86908497649472,0.0
"def stream_docker_log(log_stream):     async for line in log_stream:         if ""stream"" in line and line[""stream""].strip():             logger.debug(line[""stream""].strip())         <MASK>             logger.debug(line[""status""].strip())         elif ""error"" in line:             logger.error(line[""error""].strip())             raise DockerBuildError","elif ""status"" in line :","if ""status"" in line :",80.91067115702207,80.91067115702207,0.0
"def create_keyfile(self, keyfile, size=64, force=False):     if force or not os.path.exists(keyfile):         keypath = os.path.dirname(keyfile)         <MASK>             os.makedirs(keypath)         subprocess.run(             [""dd"", ""if=/dev/random"", f""of={keyfile}"", f""bs={size}"", ""count=1""],             check=True,             stdout=subprocess.DEVNULL,             stderr=subprocess.DEVNULL,         )",if not os . path . exists ( keypath ) :,if not os . path . isdir ( keypath ) :,70.16879391277372,70.16879391277372,0.0
"def calc(self, arg):     op = arg[""op""]     if op == ""C"":         self.clear()         return str(self.current)     num = decimal.Decimal(arg[""num""])     if self.op:         if self.op == ""+"":             self.current += num         elif self.op == ""-"":             self.current -= num         <MASK>             self.current *= num         elif self.op == ""/"":             self.current /= num         self.op = op     else:         self.op = op         self.current = num     res = str(self.current)     if op == ""="":         self.clear()     return res","elif self . op == ""*"" :","if self . op == ""/"" :",58.14307369682194,58.14307369682194,0.0
"def chop(expr, delta=10.0 ** (-10.0)):     if isinstance(expr, Real):         if -delta < expr.get_float_value() < delta:             return Integer(0)     elif isinstance(expr, Complex) and expr.is_inexact():         real, imag = expr.real, expr.imag         <MASK>             real = Integer(0)         if -delta < imag.get_float_value() < delta:             imag = Integer(0)         return Complex(real, imag)     elif isinstance(expr, Expression):         return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves])     return expr",if - delta < real . get_float_value ( ) < delta :,if - delta < real . get_float_value ( ) < delta :,100.00000000000004,100.00000000000004,1.0
"def get_file_sources():     global _file_sources     if _file_sources is None:         from galaxy.files import ConfiguredFileSources         file_sources = None         if os.path.exists(""file_sources.json""):             file_sources_as_dict = None             with open(""file_sources.json"", ""r"") as f:                 file_sources_as_dict = json.load(f)             <MASK>                 file_sources = ConfiguredFileSources.from_dict(file_sources_as_dict)         if file_sources is None:             ConfiguredFileSources.from_dict([])         _file_sources = file_sources     return _file_sources",if file_sources_as_dict is not None :,if file_sources_as_dict is not None :,100.00000000000004,100.00000000000004,1.0
"def _get_sort_map(tags):     """"""See TAG_TO_SORT""""""     tts = {}     for name, tag in tags.items():         <MASK>             if tag.user:                 tts[name] = ""%ssort"" % name             if tag.internal:                 tts[""~%s"" % name] = ""~%ssort"" % name     return tts",if tag . has_sort :,if tag . user :,28.641904579795423,28.641904579795423,0.0
"def __init__(self, **kwargs):     if self.name is None:         raise RuntimeError(""RenderPrimitive cannot be used directly"")     self.option_values = {}     for key, val in kwargs.items():         <MASK>             raise ValueError(                 ""primitive `{0}' has no option `{1}'"".format(self.name, key)             )         self.option_values[key] = val     # set up defaults     for name, (description, default) in self.options.items():         if not name in self.option_values:             self.option_values[name] = default",if not key in self . options :,if key not in self . options :,58.14307369682194,58.14307369682194,0.0
"def modify_bottle_params(self, output_stride=None):     if output_stride is not None and output_stride % 2 != 0:         raise Exception(""output stride must to be even number"")     if output_stride is None:         return     else:         stride = 2         for i, _cfg in enumerate(self.cfg):             stride = stride * _cfg[-1]             <MASK>                 s = 1                 self.cfg[i][-1] = s",if stride > output_stride :,if i % 2 == 0 :,6.567274736060395,6.567274736060395,0.0
"def do_query(data, q):     ret = []     if not q:         return ret     qkey = q[0]     for key, value in iterate(data):         if len(q) == 1:             if key == qkey:                 ret.append(value)             <MASK>                 ret.extend(do_query(value, q))         else:             if not is_iterable(value):                 continue             if key == qkey:                 ret.extend(do_query(value, q[1:]))             else:                 ret.extend(do_query(value, q))     return ret",elif is_iterable ( value ) :,if not is_iterable ( value ) :,72.59795291154772,72.59795291154772,0.0
"def make_shares(self, plaintext):     share_arrays = []     for i, p in enumerate(plaintext):         share_array = self.make_byte_shares(p)         for sa in share_array:             <MASK>                 share_arrays.append(array.array(""H""))             current_share_array = sa             current_share_array.append(sa)     return share_arrays",if i == 0 :,if i == 0 :,100.00000000000004,100.00000000000004,1.0
"def populate(self, item):     # log.message('populate: %s', item)     path = self.getItemPath(item)     # log.message('populate: path=%s', path)     value = self.getValue(path)     for name in sorted(value.__dict__.keys()):         <MASK>             continue         child = getattr(value, name, None)         if hasattr(child, ""__dict__""):             item.addChild(name, True)         else:             item.addChild(name, False)","if name [ : 2 ] == ""__"" and name [ - 2 : ] == ""__"" :",if name not in self . children :,1.2883026858770386,1.2883026858770386,0.0
"def __repr__(self):     try:         if self._semlock._is_mine():             name = current_process().name             <MASK>                 name += ""|"" + threading.current_thread().name         elif self._semlock._get_value() == 1:             name = ""None""         elif self._semlock._count() > 0:             name = ""SomeOtherThread""         else:             name = ""SomeOtherProcess""     except Exception:         name = ""unknown""     return ""<Lock(owner=%s)>"" % name","if threading . current_thread ( ) . name != ""MainThread"" :",if self . _semlock . _count ( ) > 0 :,6.708893362778472,6.708893362778472,0.0
"def buffer(self, lines, scroll_end=True, scroll_if_editing=False):     ""Add data to be displayed in the buffer.""     self.values.extend(lines)     if scroll_end:         <MASK>             self.start_display_at = len(self.values) - len(self._my_widgets)         elif scroll_if_editing:             self.start_display_at = len(self.values) - len(self._my_widgets)",if not self . editing :,if scroll_if_editing :,14.535768424205482,14.535768424205482,0.0
"def warehouses(self) -> tuple:     from ..repositories import WarehouseBaseRepo     repos = dict()     for dep in chain(self.dependencies, [self]):         if dep.repo is None:             continue         <MASK>             continue         for repo in dep.repo.repos:             if repo.from_config:                 continue             repos[repo.name] = repo     return tuple(repos.values())","if not isinstance ( dep . repo , WarehouseBaseRepo ) :",if dep . repo . from_config :,16.14682615668325,16.14682615668325,0.0
"def _apply_flag_attrs(src_flag, dest_flag):     # Use a baseline flag def to get default values for empty data.     baseline_flag = FlagDef("""", {}, None)     for name in dir(src_flag):         <MASK>             continue         dest_val = getattr(dest_flag, name, None)         baseline_val = getattr(baseline_flag, name, None)         if dest_val == baseline_val:             setattr(dest_flag, name, getattr(src_flag, name))","if name [ : 1 ] == ""_"" :","if name == ""baseline"" :",18.325568129983218,18.325568129983218,0.0
"def out(parent, attr, indent=0):     val = getattr(parent, attr)     prefix = ""%s%s:"" % ("" "" * indent, attr.replace(""_"", ""-""))     if val is None:         cli.out(prefix)     else:         <MASK>             val = [flag_util.encode_flag_val(c.value) for c in val]         cli.out(""%s %s"" % (prefix, flag_util.encode_flag_val(val)))","if attr == ""choices"" :","if isinstance ( val , list ) :",6.567274736060395,6.567274736060395,0.0
"def add_cand_to_check(cands):     for cand in cands:         x = cand.creator         if x is None:             continue         <MASK>             # `len(fan_out)` is in order to avoid comparing `x`             heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x))         fan_out[x] += 1",if x not in fan_out :,if x . rank == 1 :,12.22307556087252,12.22307556087252,0.0
"def task_tree_lines(task=None):     if task is None:         task = current_root_task()     rendered_children = []     nurseries = list(task.child_nurseries)     while nurseries:         nursery = nurseries.pop()         nursery_children = _rendered_nursery_children(nursery)         <MASK>             nested = _render_subtree(""(nested nursery)"", rendered_children)             nursery_children.append(nested)         rendered_children = nursery_children     return _render_subtree(task.name, rendered_children)",if rendered_children :,if nursery_children :,42.72870063962342,42.72870063962342,0.0
"def lock_workspace(build_dir):     _BUILDING_LOCK_FILE = "".blade.building.lock""     lock_file_fd, ret_code = lock_file(os.path.join(build_dir, _BUILDING_LOCK_FILE))     if lock_file_fd == -1:         <MASK>             console.fatal(""There is already an active building in current workspace."")         else:             console.fatal(""Lock exception, please try it later."")     return lock_file_fd",if ret_code == errno . EAGAIN :,if ret_code == 0 :,55.0695314903184,55.0695314903184,0.0
"def test_list(self):     self._create_locations()     response = self.client.get(self.geojson_boxedlocation_list_url)     self.assertEqual(response.status_code, 200)     self.assertEqual(len(response.data[""features""]), 2)     for feature in response.data[""features""]:         self.assertIn(""bbox"", feature)         fid = feature[""id""]         if fid == 1:             self.assertEqual(feature[""bbox""], self.bl1.bbox_geometry.extent)         <MASK>             self.assertEqual(feature[""bbox""], self.bl2.bbox_geometry.extent)         else:             self.fail(""Unexpected id: {0}"".format(fid))     BoxedLocation.objects.all().delete()",elif fid == 2 :,if fid == 2 :,75.98356856515926,75.98356856515926,0.0
"def result():     # ""global"" does not work here...     R, V = rays, virtual_rays     if V is not None:         if normalize:             V = normalize_rays(V, lattice)         if check:             R = PointCollection(V, lattice)             V = PointCollection(V, lattice)             d = lattice.dimension()             <MASK>                 raise ValueError(                     ""virtual rays must be linearly ""                     ""independent and with other rays span the ambient space.""                 )     return RationalPolyhedralFan(cones, R, lattice, is_complete, V)",if len ( V ) != d - R . dim ( ) or ( R + V ) . dim ( ) != d :,if d > 1 :,0.14131406583082426,0.14131406583082426,0.0
"def search_host(self, search_string):     results = []     for host_entry in self.config_data:         if host_entry.get(""type"") != ""entry"":             continue         if host_entry.get(""host"") == ""*"":             continue         searchable_information = host_entry.get(""host"")         for key, value in six.iteritems(host_entry.get(""options"")):             <MASK>                 value = "" "".join(value)             if isinstance(value, int):                 value = str(value)             searchable_information += "" "" + value         if search_string in searchable_information:             results.append(host_entry)     return results","if isinstance ( value , list ) :","if key == ""options"" :",6.567274736060395,6.567274736060395,0.0
"def test_async_iterator(app):     async with new_stream(app) as stream:         for i in range(100):             await stream.channel.deliver(message(key=i, value=i))         received = 0         async for value in stream:             assert value == received             received += 1             <MASK>                 break         assert await channel_empty(stream.channel)",if received >= 100 :,if received >= 100 :,100.00000000000004,100.00000000000004,1.0
"def has_google_credentials():     global _HAS_GOOGLE_CREDENTIALS     if _HAS_GOOGLE_CREDENTIALS is None:         provider = Provider(""google"")         <MASK>             _HAS_GOOGLE_CREDENTIALS = False         else:             _HAS_GOOGLE_CREDENTIALS = True     return _HAS_GOOGLE_CREDENTIALS",if provider . get_access_key ( ) is None or provider . get_secret_key ( ) is None :,if provider . is_available ( ) :,4.242001188566008,4.242001188566008,0.0
"def __cmp__(self, other):     if isinstance(other, date) or isinstance(other, datetime):         a = self._d.getTime()         b = other._d.getTime()         if a < b:             return -1         <MASK>             return 0     else:         raise TypeError(""expected date or datetime object"")     return 1",elif a == b :,if a > b :,19.3576934939088,19.3576934939088,0.0
"def validate_weight(self, weight):     try:         add_acl_to_obj(self.context[""user_acl""], self.category)     except AttributeError:         return weight  # don't validate weight further if category failed     if weight > self.category.acl.get(""can_pin_threads"", 0):         <MASK>             raise ValidationError(                 _(                     ""You don't have permission to pin threads globally ""                     ""in this category.""                 )             )         else:             raise ValidationError(                 _(""You don't have permission to pin threads in this category."")             )     return weight",if weight == 2 :,"if weight < self . category . acl . get ( ""can_pin_threads"" , 1 ) :",3.8229746997386345,3.8229746997386345,0.0
"def effective(line):     for b in line:         if not b.cond:             return         else:             try:                 val = 5                 <MASK>                     if b.ignore:                         b.ignore -= 1                     else:                         return (b, True)             except:                 return (b, False)     return",if val :,if val == 0 :,17.965205598154213,17.965205598154213,0.0
"def wheelEvent(self, event):     """"""Handle a wheel event.""""""     if QtCore.Qt.ControlModifier & event.modifiers():         d = {""c"": self.leo_c}         if isQt5:             point = event.angleDelta()             delta = point.y() or point.x()         else:             delta = event.delta()         <MASK>             zoom_out(d)         else:             zoom_in(d)         event.accept()         return     QtWidgets.QTextBrowser.wheelEvent(self, event)",if delta < 0 :,if delta > self . leo_c :,10.552670315936318,10.552670315936318,0.0
"def test_evname_in_mp_events_testcases():     ok = True     for evname in ins.mp_events:         if evname == ""version"":             continue         for i, args in enumerate(ins.mp_events[evname][""test_cases""]):             <MASK>                 msg = ""Error, for evname %s the testase #%d does not match evname""                 print(msg % (evname, i))                 ok = False     if ok:         print(""test_evname_in_mp_events_testcases: passed"")",if evname != args [ 0 ] :,"if args [ 0 ] != ""test_cases"" :",24.71244254525359,24.71244254525359,0.0
"def check_database():     if len(EmailAddress.objects.all()) > 0:         print(             ""Are you sure you want to wipe the existing development database and reseed it? (Y/N)""         )         <MASK>             destroy_database()         else:             return False     else:         return True","if raw_input ( ) . lower ( ) == ""y"" :",if not self . _is_admin :,3.097704314134564,3.097704314134564,0.0
"def _get_requested_databases(self):     """"""Returns a list of databases requested, not including ignored dbs""""""     requested_databases = []     if (self._requested_namespaces is not None) and (self._requested_namespaces != []):         for requested_namespace in self._requested_namespaces:             if requested_namespace[0] is ""*"":                 return []             <MASK>                 requested_databases.append(requested_namespace[0])     return requested_databases",elif requested_namespace [ 0 ] not in IGNORE_DBS :,if requested_namespace [ 0 ] in self . _ignored_dbs :,35.83129187641355,35.83129187641355,0.0
"def decorated(self, *args, **kwargs):     start_time = time.perf_counter()     stderr = """"     saved_exception = None     try:         yield from fn(self, *args, **kwargs)     except GitSavvyError as e:         stderr = e.stderr         saved_exception = e     finally:         end_time = time.perf_counter()         util.debug.log_git(args, None, ""<SNIP>"", stderr, end_time - start_time)         <MASK>             raise saved_exception from None",if saved_exception :,if saved_exception :,100.00000000000004,100.00000000000004,1.0
"def is_suppressed_warning(     type: str, subtype: str, suppress_warnings: List[str] ) -> bool:     """"""Check the warning is suppressed or not.""""""     if type is None:         return False     for warning_type in suppress_warnings:         if ""."" in warning_type:             target, subtarget = warning_type.split(""."", 1)         else:             target, subtarget = warning_type, None         <MASK>             if (                 subtype is None                 or subtarget is None                 or subtarget == subtype                 or subtarget == ""*""             ):                 return True     return False",if target == type :,if target == subtype :,53.7284965911771,53.7284965911771,0.0
"def talk(self, words):     if self.writeSentence(words) == 0:         return     r = []     while 1:         i = self.readSentence()         if len(i) == 0:             continue         reply = i[0]         attrs = {}         for w in i[1:]:             j = w.find(""="", 1)             if j == -1:                 attrs[w] = """"             else:                 attrs[w[:j]] = w[j + 1 :]         r.append((reply, attrs))         <MASK>             return r","if reply == ""!done"" :","if i [ 0 ] == ""="" :",18.36028134946796,18.36028134946796,0.0
"def encrypt(self, plaintext):     encrypted = []     for p in _string_to_bytes(plaintext):         <MASK>             self._remaining_block = self._aes.encrypt(self._last_precipherblock)             self._last_precipherblock = []         precipherbyte = self._remaining_block.pop(0)         self._last_precipherblock.append(precipherbyte)         cipherbyte = p ^ precipherbyte         encrypted.append(cipherbyte)     return _bytes_to_string(encrypted)",if len ( self . _remaining_block ) == 0 :,if p == self . _last_precipherblock :,15.020004628709785,15.020004628709785,0.0
"def find_symbol(self, r, globally=False):     query = self.view.substr(self.view.word(r))     fname = self.view.file_name().replace(""\\"", ""/"")     locations = self.view.window().lookup_symbol_in_index(query)     if not locations:         return     try:         <MASK>             location = [hit[2] for hit in locations if fname.endswith(hit[1])][0]             return location[0] - 1, location[1] - 1         else:             # TODO: There might be many symbols with the same name.             return locations[0]     except IndexError:         return",if not globally :,if not globally :,100.00000000000004,100.00000000000004,1.0
"def __getslice__(self, i, j):     try:         <MASK>             # handle the case where the right bound is unspecified             j = len(self)         if i < 0 or j < 0:             raise dns.exception.FormError         # If it's not an empty slice, access left and right bounds         # to make sure they're valid         if i != j:             super(WireData, self).__getitem__(i)             super(WireData, self).__getitem__(j - 1)         return WireData(super(WireData, self).__getslice__(i, j))     except IndexError:         raise dns.exception.FormError",if j == sys . maxint :,if i > j :,7.715486568024961,7.715486568024961,0.0
"def main():     r = redis.StrictRedis()     curr_memory = prev_memory = r.info()[""used_memory""]     while True:         <MASK>             print(                 ""Delta Memory : %d, Total Memory : %d""                 % ((curr_memory - prev_memory), curr_memory)             )         time.sleep(1)         prev_memory = curr_memory         curr_memory = r.info()[""used_memory""]",if prev_memory != curr_memory :,if curr_memory > prev_memory :,33.584386823726156,33.584386823726156,0.0
"def _visit(self, func):     fname = func[0]     if fname in self._flags:         if self._flags[fname] == 1:             logger.critical(""Fatal error! network ins not Dag."")             import sys             sys.exit(-1)         else:             return     else:         <MASK>             self._flags[fname] = 1         for output in func[3]:             for f in self._orig:                 for input in f[2]:                     if output == input:                         self._visit(f)     self._flags[fname] = 2     self._sorted.insert(0, func)",if fname not in self . _flags :,if self . _flags [ fname ] == 2 :,24.384183193426086,24.384183193426086,0.0
"def urls(self, version=None):     """"""Returns all URLS that are mapped to this interface""""""     urls = []     for _base_url, routes in self.api.http.routes.items():         for url, methods in routes.items():             for _method, versions in methods.items():                 for interface_version, interface in versions.items():                     <MASK>                         if not url in urls:                             urls.append(                                 (""/v{0}"".format(version) if version else """") + url                             )     return urls",if interface_version == version and interface == self :,if interface_version == self . version :,46.905662681356304,46.905662681356304,0.0
"def _handle_data(self, text):     if self._translate:         <MASK>             self._data.append(text)         else:             self._translate = False             self._data = []             self._comments = []","if not text . startswith ( ""gtk-"" ) :",if text :,2.7574525891364066,0.0,0.0
"def set_dir_modes(self, dirname, mode):     if not self.is_chmod_supported():         return     for dirpath, dirnames, fnames in os.walk(dirname):         <MASK>             continue         log.info(""changing mode of %s to %o"", dirpath, mode)         if not self.dry_run:             os.chmod(dirpath, mode)",if os . path . islink ( dirpath ) :,if not dirnames or not fnames :,5.08764122072739,5.08764122072739,0.0
"def language(self):     if self.lang_data:         lang_data = [s if s != ""None"" else None for s in self.lang_data]         <MASK>             return Language(lang_data[0], country=lang_data[1], script=lang_data[2])",if lang_data [ 0 ] :,if lang_data :,38.80684294761701,38.80684294761701,0.0
"def _addItemToLayout(self, sample, label):     col = self.layout.columnCount()     row = self.layout.rowCount()     if row:         row -= 1     nCol = self.columnCount * 2     # FIRST ROW FULL     if col == nCol:         for col in range(0, nCol, 2):             # FIND RIGHT COLUMN             <MASK>                 break         if col + 2 == nCol:             # MAKE NEW ROW             col = 0             row += 1     self.layout.addItem(sample, row, col)     self.layout.addItem(label, row, col + 1)","if not self . layout . itemAt ( row , col ) :",if col == nCol :,3.3264637832151163,3.3264637832151163,0.0
"def align_comments(tlist):     tidx, token = tlist.token_next_by(i=sql.Comment)     while token:         pidx, prev_ = tlist.token_prev(tidx)         <MASK>             tlist.group_tokens(sql.TokenList, pidx, tidx, extend=True)             tidx = pidx         tidx, token = tlist.token_next_by(i=sql.Comment, idx=tidx)","if isinstance ( prev_ , sql . TokenList ) :",if prev_ == sql . TokenList :,19.437571020720103,19.437571020720103,0.0
"def hook_GetVariable(ql, address, params):     if params[""VariableName""] in ql.env:         var = ql.env[params[""VariableName""]]         read_len = read_int64(ql, params[""DataSize""])         if params[""Attributes""] != 0:             write_int64(ql, params[""Attributes""], 0)         write_int64(ql, params[""DataSize""], len(var))         <MASK>             return EFI_BUFFER_TOO_SMALL         if params[""Data""] != 0:             ql.mem.write(params[""Data""], var)         return EFI_SUCCESS     return EFI_NOT_FOUND",if read_len < len ( var ) :,if read_len > 0 :,28.319415510892387,28.319415510892387,0.0
"def _PromptMySQL(self, config):     """"""Prompts the MySQL configuration, retrying if the configuration is invalid.""""""     while True:         self._PromptMySQLOnce(config)         if self._CheckMySQLConnection():             print(""Successfully connected to MySQL with the given configuration."")             return         else:             print(""Error: Could not connect to MySQL with the given configuration."")             retry = RetryBoolQuestion(""Do you want to retry MySQL configuration?"", True)             <MASK>                 raise ConfigInitError()",if not retry :,if not retry :,100.00000000000004,100.00000000000004,1.0
"def split_long_line_with_indent(line, max_per_line, indent):     """"""Split the `line` so that it doesn't go over `max_per_line` and adds `indent` to new lines.""""""     words = line.split("" "")     lines = []     current_line = words[0]     for word in words[1:]:         <MASK>             lines.append(current_line)             current_line = "" "" * indent + word         else:             current_line = f""{current_line} {word}""     lines.append(current_line)     return ""\n"".join(lines)","if len ( f""{current_line} {word}"" ) > max_per_line :",if max_per_line > indent :,11.840379131985358,11.840379131985358,0.0
"def gen_cli(docs_dir):     with open(os.path.join(docs_dir, ""CLI_template.md""), ""r"") as cli_temp_file:         temp_lines = cli_temp_file.readlines()     lines = []     for line in temp_lines:         matched = re.match(r""{onnx-tf.*}"", line)         <MASK>             command = matched.string.strip()[1:-1]             output = subprocess.check_output(command.split("" "")).decode(""UTF-8"")             lines.append(output)         else:             lines.append(line)     with open(os.path.join(docs_dir, ""CLI.md""), ""w"") as cli_file:         cli_file.writelines(lines)",if matched :,if matched :,100.00000000000004,0.0,1.0
"def read(self, size=None):     if size == 0:         return """"     data = list()     while size is None or size > 0:         line = self.readline(size or -1)         if not line:             break         <MASK>             size -= len(line)         data.append(line)     return """".join(data)",if size is not None :,if len ( line ) > 0 :,6.567274736060395,6.567274736060395,0.0
"def _get_format_and_pattern(file_path):     file_path = Path(file_path)     with file_path.open() as f:         first_line = f.readline().strip()         match = re.match(r""format *: *(.+)"", first_line)         <MASK>             return ""gztar"", first_line, 1         return match.group(1), f.readline().strip(), 2",if match is None :,if match is None :,100.00000000000004,100.00000000000004,1.0
"def remove_old_snapshot(install_dir):     logging.info(""Removing any old files in {}"".format(install_dir))     for file in glob.glob(""{}/*"".format(install_dir)):         try:             if os.path.isfile(file):                 os.unlink(file)             <MASK>                 shutil.rmtree(file)         except Exception as error:             logging.error(""Error: {}"".format(error))             sys.exit(1)",elif os . path . isdir ( file ) :,if os . path . isdir ( file ) :,88.01117367933934,88.01117367933934,0.0
"def _test_forever(self, tests):     while True:         for test_name in tests:             yield test_name             if self.bad:                 return             <MASK>                 return",if self . ns . fail_env_changed and self . environment_changed :,if self . test_done :,6.656592803413299,6.656592803413299,0.0
"def _swig_extract_dependency_files(self, src):     dep = []     for line in open(src):         if line.startswith(""#include"") or line.startswith(""%include""):             line = line.split("" "")[1].strip(""""""'""\r\n"""""")             <MASK>                 dep.append(line)     return [i for i in dep if os.path.exists(i)]","if not ( ""<"" in line or line in dep ) :",if line :,1.0144101175482476,0.0,0.0
"def update_service_key(kid, name=None, metadata=None):     try:         with db_transaction():             key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get()             <MASK>                 key.name = name             if metadata is not None:                 key.metadata.update(metadata)             key.save()     except ServiceKey.DoesNotExist:         raise ServiceKeyDoesNotExist",if name is not None :,if name is not None :,100.00000000000004,100.00000000000004,1.0
"def range(self, dimension, data_range=True, dimension_range=True):     if self.nodes and dimension in self.nodes.dimensions():         node_range = self.nodes.range(dimension, data_range, dimension_range)         <MASK>             path_range = self._edgepaths.range(dimension, data_range, dimension_range)             return max_range([node_range, path_range])         return node_range     return super(Graph, self).range(dimension, data_range, dimension_range)",if self . _edgepaths :,if node_range :,11.51015341649912,11.51015341649912,0.0
"def handler(chan, host, port):     sock = socket()     try:         sock.connect((host, port))     except Exception as e:         if verbose == True:             print(e)         return     while True:         r, w, x = select.select([sock, chan], [], [])         if sock in r:             data = sock.recv(1024)             <MASK>                 break             chan.send(data)         if chan in r:             data = chan.recv(1024)             if len(data) == 0:                 break             sock.send(data)     chan.close()     sock.close()",if len ( data ) == 0 :,if len ( data ) == 0 :,100.00000000000004,100.00000000000004,1.0
"def output_layer(self, features, **kwargs):     """"""Project features to the vocabulary size.""""""     if self.adaptive_softmax is None:         # project back to size of vocabulary         <MASK>             return F.linear(features, self.embed_tokens.weight)         else:             return F.linear(features, self.embed_out)     else:         return features",if self . share_input_output_embed :,if self . adaptive_softmax == self . adaptive_softmax :,12.571192676522521,12.571192676522521,0.0
"def generate(self, dest, vars):     util.ensure_dir(dest)     for relpath, src, template in self._file_templates:         file_dest = os.path.join(dest, relpath)         util.ensure_dir(os.path.dirname(file_dest))         <MASK>             shutil.copyfile(src, file_dest)         else:             _render_template(template, vars, file_dest)",if template is None :,if src :,17.799177396293473,0.0,0.0
"def _py_matching_callback(self, context, result, sender, device):     d = HIDDevice.get_device(c_void_p(device))     if d not in self.devices:         self.devices.add(d)         for x in self.matching_observers:             <MASK>                 x.device_discovered(d)","if hasattr ( x , ""device_discovered"" ) :",if x . device_discovered ( d ) :,18.0854275844538,18.0854275844538,0.0
"def urlquote(*args, **kwargs):     new_kwargs = dict(kwargs)     if not PY3:         new_kwargs = dict(kwargs)         <MASK>             del new_kwargs[""encoding""]         if ""errors"" in kwargs:             del new_kwargs[""errors""]     return quote(*args, **new_kwargs)","if ""encoding"" in new_kwargs :","if ""encoding"" in kwargs :",53.137468984124546,53.137468984124546,0.0
"def Set(self, attr, value):     hook = getattr(self, ""_set_%s"" % attr, None)     if hook:         # If there is a set hook we must use the context manager.         <MASK>             raise ValueError(                 ""Can only update attribute %s using the context manager."" % attr             )         if attr not in self._pending_hooks:             self._pending_hooks.append(attr)         self._pending_parameters[attr] = value     else:         super(Configuration, self).Set(attr, value)",if self . _lock > 0 :,if not hook . is_set :,7.809849842300637,7.809849842300637,0.0
"def on_profiles_loaded(self, profiles):     cb = self.builder.get_object(""cbProfile"")     model = cb.get_model()     model.clear()     for f in profiles:         name = f.get_basename()         if name.endswith("".mod""):             continue         <MASK>             name = name[0:-11]         model.append((name, f, None))     cb.set_active(0)","if name . endswith ( "".sccprofile"" ) :","if name . endswith ( "".mod"" ) :",70.16879391277372,70.16879391277372,0.0
"def get_eval_task(self, worker_id):     """"""Return next evaluation (task_id, Task) tuple""""""     with self._lock:         <MASK>             return -1, None         self._task_id += 1         task = self._eval_todo.pop()         self._doing[self._task_id] = (worker_id, task, time.time())         return self._task_id, task",if not self . _eval_todo :,if self . _task_id >= self . _eval_todo . count ( ) :,25.34743707366162,25.34743707366162,0.0
"def queries(self):     if DEV:         cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s)         <MASK>             if not cmd.stdout.strip():                 log_cmd = ShellCommand(                     ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT                 )                 if log_cmd.check(f""docker logs for {self.path.k8s}""):                     print(cmd.stdout)                 pytest.exit(f""container failed to start for {self.path.k8s}"")     return ()","if not cmd . check ( f""docker check for {self.path.k8s}"" ) :",if cmd . check ( ) :,6.87682893933032,6.87682893933032,0.0
"def disjoined(data):     # create marginalized distributions and multiple them together     data_disjoined = None     dim = len(data.shape)     for d in range(dim):         axes = list(range(dim))         axes.remove(d)         data1d = multisum(data, axes)         shape = [1 for k in range(dim)]         shape[d] = len(data1d)         data1d = data1d.reshape(tuple(shape))         <MASK>             data_disjoined = data1d         else:             data_disjoined = data_disjoined * data1d     return data_disjoined",if d == 0 :,if data_disjoined is None :,7.809849842300637,7.809849842300637,0.0
"def safe_repr(val):     try:         <MASK>             # We special case dicts to have a sorted repr. This makes testing             # significantly easier             val = _obj_with_safe_repr(val)         ret = repr(val)         if six.PY2:             ret = ret.decode(""utf-8"")     except UnicodeEncodeError:         ret = red(""a %r that cannot be represented"" % type(val))     else:         ret = green(ret)     return ret","if isinstance ( val , dict ) :","if isinstance ( val , dict ) :",100.00000000000004,100.00000000000004,1.0
"def wrapper(*args, **kwargs):     resp = view_func(*args, **kwargs)     if isinstance(resp, dict):         ctx_params = request.environ.get(""webrec.template_params"")         <MASK>             resp.update(ctx_params)         template = self.jinja_env.jinja_env.get_or_select_template(template_name)         return template.render(**resp)     else:         return resp",if ctx_params :,"if ctx_params : template_name = ctx_params [ ""template_name"" ]",18.951629567590746,18.951629567590746,0.0
"def post(self, request, *args, **kwargs):     contact_id = kwargs.get(""pk"")     self.object = get_object_or_404(Contact, id=contact_id)     if (         self.request.user.role != ""ADMIN""         and not self.request.user.is_superuser         and self.request.user != self.object.created_by     ) or self.object.company != self.request.company:         raise PermissionDenied     else:         if self.object.address_id:             self.object.address.delete()         self.object.delete()         <MASK>             return JsonResponse({""error"": False})         return redirect(""contacts:list"")",if self . request . is_ajax ( ) :,if not self . object . address_id :,9.993744303650718,9.993744303650718,0.0
"def escape(text, newline=False):     """"""Escape special html characters.""""""     if isinstance(text, str):         if ""&"" in text:             text = text.replace(""&"", ""&amp;"")         if "">"" in text:             text = text.replace("">"", ""&gt;"")         <MASK>             text = text.replace(""<"", ""&lt;"")         if '""' in text:             text = text.replace('""', ""&quot;"")         if ""'"" in text:             text = text.replace(""'"", ""&quot;"")         if newline:             if ""\n"" in text:                 text = text.replace(""\n"", ""<br>"")     return text","if ""<"" in text :","if ""<"" in text :",100.00000000000004,100.00000000000004,1.0
"def everythingIsUnicode(d):     """"""Takes a dictionary, recursively verifies that every value is unicode""""""     for k, v in d.iteritems():         if isinstance(v, dict) and k != ""headers"":             <MASK>                 return False         elif isinstance(v, list):             for i in v:                 if isinstance(i, dict) and not everythingIsUnicode(i):                     return False                 elif isinstance(i, _bytes):                     return False         elif isinstance(v, _bytes):             return False     return True",if not everythingIsUnicode ( v ) :,if not everythingIsUnicode ( v ) :,100.00000000000004,100.00000000000004,1.0
"def fill(self):     try:         while (             not self.stopping.wait(self.sample_wait)             and len(self.queue) < self.queue.maxlen         ):             self.queue.append(self.parent._read())             <MASK>                 self.parent._fire_events()         self.full.set()         while not self.stopping.wait(self.sample_wait):             self.queue.append(self.parent._read())             if isinstance(self.parent, EventsMixin):                 self.parent._fire_events()     except ReferenceError:         # Parent is dead; time to die!         pass","if self . partial and isinstance ( self . parent , EventsMixin ) :",if self . parent is None :,13.448025110102005,13.448025110102005,0.0
"def _SetListviewTextItems(self, items):     self.listview.DeleteAllItems()     index = -1     for item in items:         index = self.listview.InsertItem(index + 1, item[0])         data = item[1]         <MASK>             data = """"         self.listview.SetItemText(index, 1, data)",if data is None :,if data is None :,100.00000000000004,100.00000000000004,1.0
"def process_request(self, request):     for old, new in self.names_name:         request.uri = request.uri.replace(old, new)         if is_text_payload(request) and request.body:             body = six.ensure_str(request.body)             <MASK>                 request.body = body.replace(old, new)     return request",if old in body :,if body :,32.34325178227722,0.0,0.0
"def serialize(cls, value, *args, **kwargs):     if value is None:         return """"     value_as_string = six.text_type(value)     if SHOULD_NOT_USE_LOCALE:         return value_as_string     else:         grouping = kwargs.get(""grouping"", None)         has_decimal_places = value_as_string.find(""."") != -1         <MASK>             string_format = ""%d""         else:             decimal_places = len(value_as_string.split(""."")[1])             string_format = ""%.{}f"".format(decimal_places)         return locale.format(string_format, value, grouping=grouping)",if not has_decimal_places :,if not has_decimal_places :,100.00000000000004,100.00000000000004,1.0
"def review_link(request, path_obj):     try:         <MASK>             if check_permission(""translate"", request):                 text = _(""Review Suggestions"")             else:                 text = _(""View Suggestions"")             return {                 ""href"": dispatch.translate(                     request, path_obj.pootle_path, matchnames=[""hassuggestion""]                 ),                 ""text"": text,             }     except IOError:         pass",if path_obj . has_suggestions ( ) :,if request . user . is_authenticated ( ) :,16.59038701421971,16.59038701421971,0.0
"def _migrate_key(self, key):     """"""migrate key from old .dat file""""""     key_path = os.path.join(self.home_path, ""keys.dat"")     if os.path.exists(key_path):         try:             key_data = json.loads(open(key_path, ""rb"").read())             <MASK>                 self.add_key(key, key_data.get(key))         except:             self.error(f""Corrupt key file. Manual migration of '{key}' required."")",if key_data . get ( key ) :,if key_data :,26.013004751144457,26.013004751144457,0.0
"def gather_callback_args(self, obj, callbacks):     session = sa.orm.object_session(obj)     for callback in callbacks:         backref = callback.backref         root_objs = getdotattr(obj, backref) if backref else obj         if root_objs:             <MASK>                 root_objs = [root_objs]             with session.no_autoflush:                 for root_obj in root_objs:                     if root_obj:                         args = self.get_callback_args(root_obj, callback)                         if args:                             yield args","if not isinstance ( root_objs , Iterable ) :","if not isinstance ( root_objs , list ) :",74.19446627365011,74.19446627365011,0.0
"def GetDefFile(self, gyp_to_build_path):     """"""Returns the .def file from sources, if any.  Otherwise returns None.""""""     spec = self.spec     if spec[""type""] in (""shared_library"", ""loadable_module"", ""executable""):         def_files = [s for s in spec.get(""sources"", []) if s.endswith("".def"")]         <MASK>             return gyp_to_build_path(def_files[0])         elif len(def_files) > 1:             raise Exception(""Multiple .def files"")     return None",if len ( def_files ) == 1 :,if len ( def_files ) == 1 :,100.00000000000004,100.00000000000004,1.0
"def _validate_gallery(images):     for image in images:         image_path = image.get(""image_path"", """")         if image_path:             if not isfile(image_path):                 raise TypeError(f""{image_path!r} is not a valid image path."")         else:             raise TypeError(""'image_path' is required."")         <MASK>             raise TypeError(""Caption must be 180 characters or less."")","if not len ( image . get ( ""caption"" , """" ) ) <= 180 :","if 180 < image . get ( ""caption"" ) :",31.48426474727927,31.48426474727927,0.0
"def VType(self):     if ""DW_AT_type"" in self.attributes:         target = self.types[self.type_id]         target_type = target.VType()         <MASK>             target_type = [target_type, None]         return [""Pointer"", dict(target=target_type[0], target_args=target_type[1])]     return [""Pointer"", dict(target=""Void"")]","if not isinstance ( target_type , list ) :",if target_type is not None :,16.801577573679282,16.801577573679282,0.0
"def addInPlace(self, value1, value2):     for group in value2:         for key in value2[group]:             <MASK>                 value1[group][key] = value2[group][key]             else:                 value1[group][key] += value2[group][key]     return value1",if key not in value1 [ group ] :,if group [ key ] == self . default :,5.865587580131999,5.865587580131999,0.0
"def _mongo_query_and(self, queries):     if len(queries) == 1:         return queries[0]     query = {}     for q in queries:         for k, v in q.items():             <MASK>                 query[k] = {}             if isinstance(v, list):                 # TODO check exists of k in query, may be it should be update                 query[k] = v             else:                 query[k].update(v)     return query",if k not in query :,"if not isinstance ( v , dict ) :",6.27465531099474,6.27465531099474,0.0
"def _handled_eventtype(self, eventtype, handler):     if eventtype not in known_events:         log.error('The event ""%s"" is not known', eventtype)         return False     if known_events[eventtype].__module__.startswith(""deluge.event""):         <MASK>             return True         log.error(             ""You cannot register custom notification providers ""             ""for built-in event types.""         )         return False     return True",if handler . __self__ is self :,if handler is not None :,8.389861810900507,8.389861810900507,0.0
"def get_ax_arg(uri):     if not ax_ns:         return u""""     prefix = ""openid."" + ax_ns + "".type.""     ax_name = None     for name, values in self.request.arguments.iteritems():         <MASK>             part = name[len(prefix) :]             ax_name = ""openid."" + ax_ns + "".value."" + part             break     if not ax_name:         return u""""     return self.get_argument(ax_name, u"""")",if values [ - 1 ] == uri and name . startswith ( prefix ) :,if values [ 0 ] == prefix :,13.185679291149079,13.185679291149079,0.0
"def handle_starttag(self, tag, attrs):     if tag == ""base"":         self.base_url = dict(attrs).get(""href"")     if self.scan_tag(tag):         for attr, value in attrs:             <MASK>                 if self.strip:                     value = strip_html5_whitespace(value)                 url = self.process_attr(value)                 link = Link(url=url)                 self.links.append(link)                 self.current_link = link",if self . scan_attr ( attr ) :,"if attr == ""href"" :",5.660233915657916,5.660233915657916,0.0
"def test_long_steadystate_queue_popright(self):     for size in (0, 1, 2, 100, 1000):         d = deque(reversed(range(size)))         append, pop = d.appendleft, d.pop         for i in range(size, BIG):             append(i)             x = pop()             <MASK>                 self.assertEqual(x, i - size)         self.assertEqual(list(reversed(list(d))), list(range(BIG - size, BIG)))",if x != i - size :,if x < size :,16.58165975077607,16.58165975077607,0.0
"def _update_read(self):     """"""Update state when there is read event""""""     try:         msg = bytes(self._sock.recv(4096))         <MASK>             self.on_message(msg)             return True         # normal close, remote is closed         self.close()     except socket.error as err:         if err.args[0] in (errno.EAGAIN, errno.EWOULDBLOCK):             pass         else:             self.on_error(err)     return False",if msg :,if msg :,100.00000000000004,0.0,1.0
"def prepend(self, value):     """"""prepend value to nodes""""""     root, root_text = self._get_root(value)     for i, tag in enumerate(self):         if not tag.text:             tag.text = """"         <MASK>             root[-1].tail = tag.text             tag.text = root_text         else:             tag.text = root_text + tag.text         if i > 0:             root = deepcopy(list(root))         tag[:0] = root         root = tag[: len(root)]     return self",if len ( root ) > 0 :,if i > 0 :,23.4500081062036,23.4500081062036,0.0
"def cmp(self, other):     v_is_ptr = not isinstance(self, CTypesGenericPrimitive)     w_is_ptr = isinstance(other, CTypesData) and not isinstance(         other, CTypesGenericPrimitive     )     if v_is_ptr and w_is_ptr:         return cmpfunc(self._convert_to_address(None), other._convert_to_address(None))     elif v_is_ptr or w_is_ptr:         return NotImplemented     else:         if isinstance(self, CTypesGenericPrimitive):             self = self._value         <MASK>             other = other._value         return cmpfunc(self, other)","if isinstance ( other , CTypesGenericPrimitive ) :","if isinstance ( other , CTypesData ) :",59.4603557501361,59.4603557501361,0.0
"def get_external_addresses(self, label=None) -> List[str]:     result = []     for c in self._conf[""pools""].values():         <MASK>             if label == c[""label""]:                 result.append(c[""external_address""][0])         else:             result.append(c[""external_address""][0])     return result",if label is not None :,if label is not None :,100.00000000000004,100.00000000000004,1.0
"def coerce_text(v):     if not isinstance(v, basestring_):         <MASK>             attr = ""__unicode__""         else:             attr = ""__str__""         if hasattr(v, attr):             return unicode(v)         else:             return bytes(v)     return v",if sys . version_info [ 0 ] < 3 :,"if isinstance ( v , bytes_ ) :",4.4959869933858485,4.4959869933858485,0.0
"def check_localhost(self):     """"""Warn if any socket_host is 'localhost'. See #711.""""""     for k, v in cherrypy.config.items():         <MASK>             warnings.warn(                 ""The use of 'localhost' as a socket host can ""                 ""cause problems on newer systems, since ""                 ""'localhost' can map to either an IPv4 or an ""                 ""IPv6 address. You should use '127.0.0.1' ""                 ""or '[::1]' instead.""             )","if k == ""server.socket_host"" and v == ""localhost"" :",if k != 'localhost' :,2.2115435625398026,2.2115435625398026,0.0
"def add_songs(self, filenames, library):     changed = []     for i in range(len(self)):         <MASK>             song = library[self._list[i]]             self._list[i] = song             changed.append(song)     if changed:         self._emit_changed(changed, msg=""add"")     return bool(changed)","if isinstance ( self [ i ] , str ) and self . _list [ i ] in filenames :",if self . _list [ i ] in filenames :,36.85202394149233,36.85202394149233,0.0
"def _expand_deps_java_generation(self):     """"""Ensure that all multilingual dependencies such as proto_library generate java code.""""""     queue = collections.deque(self.deps)     keys = set()     while queue:         k = queue.popleft()         if k not in keys:             keys.add(k)             dep = self.target_database[k]             <MASK>  # Has this attribute                 dep.attr[""generate_java""] = True                 queue.extend(dep.deps)","if ""generate_java"" in dep . attr :","if dep . attr [ ""generate_java"" ] :",44.833867003844574,44.833867003844574,0.0
"def get(self):     name = request.args.get(""filename"")     if name is not None:         opts = dict()         opts[""type""] = ""episode""         result = guessit(name, options=opts)         res = dict()         <MASK>             res[""episode""] = result[""episode""]         else:             res[""episode""] = 0         if ""season"" in result:             res[""season""] = result[""season""]         else:             res[""season""] = 0         if ""subtitle_language"" in result:             res[""subtitle_language""] = str(result[""subtitle_language""])         return jsonify(data=res)     else:         return """", 400","if ""episode"" in result :","if ""episode"" in result :",100.00000000000004,100.00000000000004,1.0
def _get_error_file(self) -> Optional[str]:     error_file = None     min_timestamp = sys.maxsize     for replicas in self.role_replicas.values():         for replica in replicas:             <MASK>                 continue             mtime = os.path.getmtime(replica.error_file)             if mtime < min_timestamp:                 min_timestamp = mtime                 error_file = replica.error_file     return error_file,if not os . path . exists ( replica . error_file ) :,if not replica . error_file :,25.6281831113684,25.6281831113684,0.0
"def findChapterNameForPosition(self, p):     """"""Return the name of a chapter containing p or None if p does not exist.""""""     cc, c = self, self.c     if not p or not c.positionExists(p):         return None     for name in cc.chaptersDict:         <MASK>             theChapter = cc.chaptersDict.get(name)             if theChapter.positionIsInChapter(p):                 return name     return ""main""","if name != ""main"" :",if name . startswith ( p ) :,12.22307556087252,12.22307556087252,0.0
"def remove_files(folder, file_extensions):     for f in os.listdir(folder):         f_path = os.path.join(folder, f)         if os.path.isfile(f_path):             extension = os.path.splitext(f_path)[1]             <MASK>                 os.remove(f_path)",if extension in file_extensions :,if extension in file_extensions :,100.00000000000004,100.00000000000004,1.0
"def execute_uncomment(self, event):     cursor = self._editor.GetCurrentPos()     line, pos = self._editor.GetCurLine()     spaces = "" "" * self._tab_size     comment = ""Comment"" + spaces     cpos = cursor - len(comment)     lenline = len(line)     if lenline > 0:         idx = 0         while idx < lenline and line[idx] == "" "":             idx += 1         <MASK>             self._editor.DeleteRange(cursor - pos + idx, len(comment))             self._editor.SetCurrentPos(cpos)             self._editor.SetSelection(cpos, cpos)             self.store_position()",if ( line [ idx : len ( comment ) + idx ] ) . lower ( ) == comment . lower ( ) :,if idx < lenline :,0.21081581353052783,0.21081581353052783,0.0
"def test_batch_kwarg_path_relative_dot_slash_is_modified_and_found_in_a_code_cell(     critical_suite_with_citations, empty_data_context ):     obs = SuiteEditNotebookRenderer.from_data_context(empty_data_context).render(         critical_suite_with_citations, {""path"": ""./foo/data""}     )     assert isinstance(obs, dict)     found_expected = False     for cell in obs[""cells""]:         <MASK>             source_code = cell[""source""]             if 'batch_kwargs = {""path"": ""../.././foo/data""}' in source_code:                 found_expected = True                 break     assert found_expected","if cell [ ""cell_type"" ] == ""code"" :","if cell [ ""source"" ] :",17.81197244687178,17.81197244687178,0.0
"def _get_file(self):     if self._file is None:         self._file = SpooledTemporaryFile(             max_size=self._storage.max_memory_size,             suffix="".S3Boto3StorageFile"",             dir=setting(""FILE_UPLOAD_TEMP_DIR""),         )         if ""r"" in self._mode:             self._is_dirty = False             self.obj.download_fileobj(self._file)             self._file.seek(0)         <MASK>             self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0)     return self._file","if self . _storage . gzip and self . obj . content_encoding == ""gzip"" :","if ""w"" in self . _mode :",6.42603291593205,6.42603291593205,0.0
"def _parse_filters(f_strs):     filters = []     if not f_strs:         return filters     for f_str in f_strs:         <MASK>             fname, fopts = f_str.split("":"", 1)             filters.append((fname, _parse_options([fopts])))         else:             filters.append((f_str, {}))     return filters","if "":"" in f_str :","if "":"" in f_str :",100.00000000000004,100.00000000000004,1.0
"def update_completion(self):     """"""Update completion model with exist tags""""""     orig_text = self.widget.text()     text = "", "".join(orig_text.replace("", "", "","").split("","")[:-1])     tags = []     for tag in self.tags_list:         <MASK>             if orig_text[-1] not in ("","", "" ""):                 tags.append(""%s,%s"" % (text, tag))             tags.append(""%s, %s"" % (text, tag))         else:             tags.append(tag)     if tags != self.completer_model.stringList():         self.completer_model.setStringList(tags)","if "","" in orig_text :",if orig_text :,31.772355751081438,31.772355751081438,0.0
"def _get_startup_packages(lib_path: Path, packages) -> Set[str]:     names = set()     for path in lib_path.iterdir():         name = path.name         if name == ""__pycache__"":             continue         <MASK>             names.add(name.split(""."")[0])         elif path.is_dir() and ""."" not in name:             names.add(name)     if packages:         packages = {package.lower().replace(""-"", ""_"") for package in packages}         if len(names & packages) == len(packages):             return packages     return names","if name . endswith ( "".py"" ) :","if path . is_file ( ) and ""."" in name :",7.655122720591221,7.655122720591221,0.0
"def get_cloud_credential(self):     """"""Return the credential which is directly tied to the inventory source type.""""""     credential = None     for cred in self.credentials.all():         <MASK>             if cred.kind == self.source.replace(""ec2"", ""aws""):                 credential = cred                 break         else:             # these need to be returned in the API credential field             if cred.credential_type.kind != ""vault"":                 credential = cred                 break     return credential",if self . source in CLOUD_PROVIDERS :,"if cred . credential_type . kind == ""aws"" :",4.016138436407654,4.016138436407654,0.0
"def newickize(clade):     """"""Convert a node tree to a Newick tree string, recursively.""""""     label = clade.name or """"     if label:         unquoted_label = re.match(token_dict[""unquoted node label""], label)         <MASK>             label = ""'%s'"" % label.replace(""\\"", ""\\\\"").replace(""'"", ""\\'"")     if clade.is_terminal():  # terminal         return label + make_info_string(clade, terminal=True)     else:         subtrees = (newickize(sub) for sub in clade)         return ""(%s)%s"" % ("","".join(subtrees), label + make_info_string(clade))",if ( not unquoted_label ) or ( unquoted_label . end ( ) < len ( label ) ) :,if unquoted_label :,1.234488517472643,1.234488517472643,0.0
"def __iter__(self):     for name, value in self._vars.store.data.items():         source = self._sources[name]         prefix = self._get_prefix(value)         name = u""{0}{{{1}}}"".format(prefix, name)         <MASK>             yield ArgumentInfo(name, value)         else:             yield VariableInfo(name, value, source)",if source == self . ARGUMENT_SOURCE :,if source is None :,8.697972365316721,8.697972365316721,0.0
"def filepath_enumerate(paths):     """"""Enumerate the file paths of all subfiles of the list of paths""""""     out = []     for path in paths:         <MASK>             out.append(path)         else:             for root, dirs, files in os.walk(path):                 for name in files:                     out.append(os.path.normpath(os.path.join(root, name)))     return out",if os . path . isfile ( path ) :,if os . path . isdir ( path ) :,65.80370064762461,65.80370064762461,0.0
"def del_(self, key):     hash_ = self.hash(key)     node_ = self._table[hash_]     pre_node = None     while node_ is not None:         <MASK>             if pre_node is None:                 self._table[hash_] = node_.next             else:                 pre_node.next = node_.next             self._len -= 1         pre_node = node_         node_ = node_.next",if node_ . key == key :,if node_ . next is None :,32.260135189272866,32.260135189272866,0.0
"def _recurse(self, base_path, rel_source, rel_zip):     submodules_path = Path(base_path) / ""submodules""     if not submodules_path.is_dir():         return     for submodule in submodules_path.iterdir():         source_path = submodule / rel_source         <MASK>             continue         output_path = submodule / rel_zip         self._build_lambdas(source_path, output_path)         self._recurse(submodule, rel_source, rel_zip)",if not source_path . is_dir ( ) :,if not source_path or not source_path . endswith ( rel_zip ) :,31.46660996956415,31.46660996956415,0.0
"def find_test_functions(collections):     if not isinstance(collections, list):         collections = [collections]     functions = []     for collection in collections:         if not isinstance(collection, dict):             collection = vars(collection)         keys = collection.keys()         keys.sort()         for key in keys:             value = collection[key]             <MASK>                 functions.append(value)     return functions","if isinstance ( value , types . FunctionType ) and hasattr ( value , ""unittest"" ) :",if value is not None :,1.2237376376462188,1.2237376376462188,0.0
"def __init__(     self,     classifier,     layer_name=None,     transpose=None,     distance=None,     copy_weights=True, ):     super().__init__()     self.copy_weights = copy_weights     ### set layer weights ###     if layer_name is not None:         self.set_weights(getattr(classifier, layer_name))     else:         for x in self.possible_layer_names:             layer = getattr(classifier, x, None)             <MASK>                 self.set_weights(layer)                 break     ### set distance measure ###     self.distance = classifier.distance if distance is None else distance     self.transpose = transpose",if layer is not None :,if layer is not None :,100.00000000000004,100.00000000000004,1.0
def multi_dev_generator(self):     for data in self._data_loader():         if len(self._tail_data) < self._base_number:             self._tail_data += data         <MASK>             yield self._tail_data             self._tail_data = [],if len ( self . _tail_data ) == self . _base_number :,if len ( self . _tail_data ) > self . _base_number :,79.4069318995806,79.4069318995806,0.0
"def Resolve(self, updater=None):     if len(self.Conflicts):         for setting, edge in self.Conflicts:             answer = self.AskUser(self.Setting, setting)             if answer == Gtk.ResponseType.YES:                 value = setting.Value.split(""|"")                 value.remove(edge)                 setting.Value = ""|"".join(value)                 if updater:                     updater.UpdateSetting(setting)             <MASK>                 return False     return True",if answer == Gtk . ResponseType . NO :,if answer == Gtk . ResponseType . OK :,78.25422900366438,78.25422900366438,0.0
"def _post_process_ttl(zone):     for name in zone:         for record_type in zone[name]:             records = zone[name][record_type]             if isinstance(records, list):                 ttl = min([x[""ttl""] for x in records])                 for record in records:                     <MASK>                         logger.warning(                             ""Using lowest TTL {} for the record set. Ignoring value {}"".format(                                 ttl, record[""ttl""]                             )                         )                     record[""ttl""] = ttl","if record [ ""ttl"" ] != ttl :","if ttl < record [ ""ttl"" ] :",51.7679965241078,51.7679965241078,0.0
"def __init__(self, cmds, env, cleanup=[]):     self.handle = None     self.cmds = cmds     self.env = env     if cleanup:         <MASK>             cleanup = [cleanup]         else:             try:                 cleanup = [c for c in cleanup if callable(c)]             except:                 cleanup = []     self.cleanup = cleanup",if callable ( cleanup ) :,if not callable ( cleanup ) :,64.34588841607616,64.34588841607616,0.0
"def _parse_data_of_birth(cls, data_of_birth_string):     if data_of_birth_string:         format = ""%m/%d/%Y""         try:             parsed_date = datetime.datetime.strptime(data_of_birth_string, format)             return parsed_date         except ValueError:             # Facebook sometimes provides a partial date format             # ie 04/07 (ignore those)             <MASK>                 raise","if data_of_birth_string . count ( ""/"" ) != 1 :",if parsed_date is None :,1.5565413865942863,1.5565413865942863,0.0
"def process_lib(vars_, coreval):     for d in vars_:         var = d.upper()         if var == ""QTCORE"":             continue         value = env[""LIBPATH_"" + var]         if value:             core = env[coreval]             accu = []             for lib in value:                 <MASK>                     continue                 accu.append(lib)             env[""LIBPATH_"" + var] = accu",if lib in core :,if lib not in core :,37.99178428257963,37.99178428257963,0.0
"def throttle_status(server=None):     result = AmonStruct()     result.allow = False     last_check = server.get(""last_check"")     server_check_period = server.get(""check_every"", 60)     if last_check:         period_since_last_check = unix_utc_now() - last_check         # Add 15 seconds buffer, for statsd         period_since_last_check = period_since_last_check + 15         <MASK>             result.allow = True     else:         result.allow = True  # Never checked     return result",if period_since_last_check >= server_check_period :,if period_since_last_check > server_check_period :,81.96501312471537,81.96501312471537,0.0
"def fetch_scatter_outputs(self, task):     scatteroutputs = []     for var in task[""body""]:         # TODO variable support         if var.startswith(""call""):             <MASK>                 for output in self.tasks_dictionary[task[""body""][var][""task""]][                     ""outputs""                 ]:                     scatteroutputs.append(                         {""task"": task[""body""][var][""alias""], ""output"": output[0]}                     )     return scatteroutputs","if ""outputs"" in self . tasks_dictionary [ task [ ""body"" ] [ var ] [ ""task"" ] ] :","if var . startswith ( ""task"" ) :",3.5266816381867354,3.5266816381867354,0.0
"def _add_constant_node(self, source_node):     parent_ids = range(len(source_node.in_edges))     for idx in parent_ids:         parent_node = self.tf_graph.get_node(source_node.in_edges[idx])         <MASK>             self._rename_Const(parent_node)","if parent_node . type == ""Const"" :",if parent_node :,17.437038542312457,17.437038542312457,0.0
"def enableCtrls(self):     # Check if each ctrl has a requirement or an incompatibility,     # look it up, and enable/disable if so     for data in self.storySettingsData:         name = data[""name""]         if name in self.ctrls:             <MASK>                 set = self.getSetting(data[""requires""])                 for i in self.ctrls[name]:                     i.Enable(set not in [""off"", ""false"", ""0""])","if ""requires"" in data :","if data [ ""incompatibility"" ] :",8.25791079503452,8.25791079503452,0.0
"def update_realtime(self, stdout="""", stderr="""", delete=False):     wooey_cache = wooey_settings.WOOEY_REALTIME_CACHE     if delete == False and wooey_cache is None:         self.stdout = stdout         self.stderr = stderr         self.save()     elif wooey_cache is not None:         cache = django_cache[wooey_cache]         <MASK>             cache.delete(self.get_realtime_key())         else:             cache.set(                 self.get_realtime_key(),                 json.dumps({""stdout"": stdout, ""stderr"": stderr}),             )",if delete :,if delete == True :,17.965205598154213,17.965205598154213,0.0
"def _check_for_batch_clashes(xs):     """"""Check that batch names do not overlap with sample names.""""""     names = set([x[""description""] for x in xs])     dups = set([])     for x in xs:         batches = tz.get_in((""metadata"", ""batch""), x)         if batches:             if not isinstance(batches, (list, tuple)):                 batches = [batches]             for batch in batches:                 <MASK>                     dups.add(batch)     if len(dups) > 0:         raise ValueError(             ""Batch names must be unique from sample descriptions.\n""             ""Clashing batch names: %s"" % sorted(list(dups))         )",if batch in names :,if len ( names ) > 0 :,7.267884212102741,7.267884212102741,0.0
"def toggle(self, event=None):     if self.absolute:         if self.save == self.split:             self.save = 100         if self.split > 20:             self.save = self.split             self.split = 1         else:             self.split = self.save     else:         if self.save == self.split:             self.save = 0.3         <MASK>             self.split = self.save         elif self.split < 0.5:             self.split = self.min         else:             self.split = self.max     self.placeChilds()",if self . split <= self . min or self . split >= self . max :,if self . split > 0.5 :,11.588199822065299,11.588199822065299,0.0
"def can_read(self):     if hasattr(self.file, ""__iter__""):         iterator = iter(self.file)         head = next(iterator, None)         <MASK>             self.repaired = []             return True         if isinstance(head, str):             self.repaired = itertools.chain([head], iterator)             return True         else:             # We may have mangled a generator at this point, so just abort             raise IOSourceError(                 ""Could not open source: %r (mode: %r)""                 % (self.file, self.options[""mode""])             )     return False",if head is None :,if head is None :,100.00000000000004,100.00000000000004,1.0
"def _print_message_content(self, peer, data):     inheaders = 1     lines = data.splitlines()     for line in lines:         # headers first         if inheaders and not line:             peerheader = ""X-Peer: "" + peer[0]             <MASK>                 # decoded_data=false; make header match other binary output                 peerheader = repr(peerheader.encode(""utf-8""))             print(peerheader)             inheaders = 0         if not isinstance(data, str):             # Avoid spurious 'str on bytes instance' warning.             line = repr(line)         print(line)","if not isinstance ( data , str ) :",if not line :,10.129474235115733,10.129474235115733,0.0
"def connect(self):     # Makes connection with MySQL server     try:         <MASK>             connection = pymysql.connect(read_default_file=""/etc/mysql/conf.d/my.cnf"")         else:             connection = pymysql.connect(read_default_file=""~/.my.cnf"")         return connection     except ValueError as e:         Log.debug(self, str(e))         raise MySQLConnectionError     except pymysql.err.InternalError as e:         Log.debug(self, str(e))         raise MySQLConnectionError","if os . path . exists ( ""/etc/mysql/conf.d/my.cnf"" ) :","if self . config . get ( ""mysql_conf"" ) :",8.311361556475362,8.311361556475362,0.0
"def _copy_package_apps(     local_bin_dir: Path, app_paths: List[Path], suffix: str = """" ) -> None:     for src_unresolved in app_paths:         src = src_unresolved.resolve()         app = src.name         dest = Path(local_bin_dir / add_suffix(app, suffix))         if not dest.parent.is_dir():             mkdir(dest.parent)         if dest.exists():             logger.warning(f""{hazard}  Overwriting file {str(dest)} with {str(src)}"")             dest.unlink()         <MASK>             shutil.copy(src, dest)",if src . exists ( ) :,if src . is_file ( ) :,29.84745896009822,29.84745896009822,0.0
"def update(self, x, who=None, metadata=None):     self._retain_refs(metadata)     y = self._get_key(x)     if self.keep == ""last"":         # remove key if already present so that emitted value         # will reflect elements' actual relative ordering         self._buffer.pop(y, None)         self._metadata_buffer.pop(y, None)         self._buffer[y] = x         self._metadata_buffer[y] = metadata     else:  # self.keep == ""first""         <MASK>             self._buffer[y] = x             self._metadata_buffer[y] = metadata     return self.last",if y not in self . _buffer :,if who is not None :,6.4790667469036025,6.4790667469036025,0.0
"def resolve_credential_keys(m_keys, keys):     res = []     for k in m_keys:         if k[""c7n:match-type""] == ""credential"":             c_date = parse_date(k[""last_rotated""])             for ak in keys:                 if c_date == ak[""CreateDate""]:                     ak = dict(ak)                     ak[""c7n:match-type""] = ""access""                     <MASK>                         res.append(ak)         elif k not in res:             res.append(k)     return res",if ak not in res :,"if k [ ""c7n:match-type"" ] == ""access"" :",3.1251907639724417,3.1251907639724417,0.0
"def _apply_flag_attrs(src_flag, dest_flag):     # Use a baseline flag def to get default values for empty data.     baseline_flag = FlagDef("""", {}, None)     for name in dir(src_flag):         if name[:1] == ""_"":             continue         dest_val = getattr(dest_flag, name, None)         baseline_val = getattr(baseline_flag, name, None)         <MASK>             setattr(dest_flag, name, getattr(src_flag, name))",if dest_val == baseline_val :,if baseline_val :,26.013004751144457,26.013004751144457,0.0
"def _ws_keep_reading(self):     import websockets.exceptions     while not self._reader_stopped:         try:             data = await self._ws.recv()             <MASK>                 data = data.encode(""UTF-8"")             if len(data) == 0:                 self._error = ""EOF""                 break         except websockets.exceptions.ConnectionClosedError:             # TODO: try to reconnect in case of Ctrl+D             self._error = ""EOF""             break         self.num_bytes_received += len(data)         self._make_output_available(data, block=False)","if isinstance ( data , str ) :",if data :,7.49553326588684,0.0,0.0
"def to_dict(self) -> Dict[str, Any]:     result = {}     for field_name in self.API_FIELDS:         <MASK>             result[""stream_id""] = self.id             continue         elif field_name == ""date_created"":             result[""date_created""] = datetime_to_timestamp(self.date_created)             continue         result[field_name] = getattr(self, field_name)     result[""is_announcement_only""] = (         self.stream_post_policy == Stream.STREAM_POST_POLICY_ADMINS     )     return result","if field_name == ""id"" :","if field_name == ""id"" :",100.00000000000004,100.00000000000004,1.0
"def all_masks(     cls,     images,     run,     run_key,     step, ):     all_mask_groups = []     for image in images:         <MASK>             mask_group = {}             for k in image._masks:                 mask = image._masks[k]                 mask_group[k] = mask.to_json(run)             all_mask_groups.append(mask_group)         else:             all_mask_groups.append(None)     if all_mask_groups and not all(x is None for x in all_mask_groups):         return all_mask_groups     else:         return False",if image . _masks :,if run_key == step :,7.267884212102741,7.267884212102741,0.0
"def disconnect_all(listener):     """"""Disconnect from all signals""""""     for emitter in listener._signal_data.emitters:         for signal in emitter._signal_data.listeners:             emitter._signal_data.listeners[signal] = [                 i                 for i in emitter._signal_data.listeners[signal]                 <MASK>             ]","if getattr ( i , ""__self__"" , None ) != listener",if i . signal_id == listener . signal_id :,5.617199498008724,5.617199498008724,0.0
"def wait(self, timeout=None):     if self.returncode is None:         if timeout is None:             msecs = _subprocess.INFINITE         else:             msecs = max(0, int(timeout * 1000 + 0.5))         res = _subprocess.WaitForSingleObject(int(self._handle), msecs)         if res == _subprocess.WAIT_OBJECT_0:             code = _subprocess.GetExitCodeProcess(self._handle)             <MASK>                 code = -signal.SIGTERM             self.returncode = code     return self.returncode",if code == TERMINATE :,if code == _subprocess . WAIT_OBJECT_1 :,20.448007360218387,20.448007360218387,0.0
"def set_pbar_fraction(self, frac, progress, stage=None):     gtk.gdk.threads_enter()     try:         self.is_pulsing = False         self.set_stage_text(stage or _(""Processing...""))         self.pbar.set_text(progress)         <MASK>             frac = 1.0         if frac < 0:             frac = 0         self.pbar.set_fraction(frac)     finally:         gtk.gdk.threads_leave()",if frac > 1 :,if frac > 1.0 :,42.72870063962342,42.72870063962342,0.0
"def get_aa_from_codonre(re_aa):     aas = []     m = 0     for i in re_aa:         if i == ""["":             m = -1             aas.append("""")         <MASK>             m = 0             continue         elif m == -1:             aas[-1] = aas[-1] + i         elif m == 0:             aas.append(i)     return aas","elif i == ""]"" :","if i == ""]"" :",84.08964152537145,84.08964152537145,0.0
"def link(token, base_url):     """"""Validation for ``link``.""""""     if get_keyword(token) == ""none"":         return ""none""     parsed_url = get_url(token, base_url)     if parsed_url:         return parsed_url     function = parse_function(token)     if function:         name, args = function         prototype = (name, [a.type for a in args])         args = [getattr(a, ""value"", a) for a in args]         <MASK>             return (""attr()"", args[0])","if prototype == ( ""attr"" , [ ""ident"" ] ) :","if ""attr"" in prototype :",8.036914931946859,8.036914931946859,0.0
"def on_bt_search_clicked(self, widget):     if self.current_provider is None:         return     query = self.en_query.get_text()     @self.obtain_podcasts_with     def load_data():         if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH:             return self.current_provider.on_search(query)         <MASK>             return self.current_provider.on_url(query)         elif self.current_provider.kind == directory.Provider.PROVIDER_FILE:             return self.current_provider.on_file(query)",elif self . current_provider . kind == directory . Provider . PROVIDER_URL :,if self . current_provider . kind == directory . Provider . PROVIDER_URL :,93.91044157537529,93.91044157537529,0.0
"def test_handle_single(self):     self.skipTest(         ""Pops up windows and needs user input.. so disabled.""         ""Still worth keeping whilst we don't have unit tests ""         ""for all plugins.""     )     # Ignored...     for id_, plugin in self.plugins.items():         <MASK>             self.h.plugin_enable(plugin, None)             self.h.handle(id_, self.lib, self.parent, SONGS)             self.h.plugin_disable(plugin)",if self . h . plugin_handle ( plugin ) :,if self . parent :,10.536767850900915,10.536767850900915,0.0
"def __repr__(self):     attrs = []     for k in self._keydata:         <MASK>             attrs.append(""p(%d)"" % (self.size() + 1,))         elif hasattr(self, k):             attrs.append(k)     if self.has_private():         attrs.append(""private"")     # PY3K: This is meant to be text, do not change to bytes (data)     return ""<%s @0x%x %s>"" % (self.__class__.__name__, id(self), "","".join(attrs))","if k == ""p"" :","if k == ""p"" :",100.00000000000004,100.00000000000004,1.0
"def apply(self, node, code, required):     yield ""try:""     yield from self.iterIndented(code)     yield ""    pass""     yield ""except {}:"".format(self.exceptionString)     outputVariables = node.getOutputSocketVariables()     for i, s in enumerate(node.outputs):         <MASK>             if hasattr(s, ""getDefaultValueCode""):                 yield f""    {outputVariables[s.identifier]} = {s.getDefaultValueCode()}""             else:                 yield f""    {outputVariables[s.identifier]} = self.outputs[{i}].getDefaultValue()""     yield ""    pass""",if s . identifier in required :,if s . identifier in required :,100.00000000000004,100.00000000000004,1.0
"def __import__(name, globals=None, locals=None, fromlist=(), level=0):     module = orig___import__(name, globals, locals, fromlist, level)     if fromlist and module.__name__ in modules:         if ""*"" in fromlist:             fromlist = list(fromlist)             fromlist.remove(""*"")             fromlist.extend(getattr(module, ""__all__"", []))         for x in fromlist:             <MASK>                 from_name = ""{}.{}"".format(module.__name__, x)                 if from_name in modules:                     importlib.import_module(from_name)     return module","if isinstance ( getattr ( module , x , None ) , types . ModuleType ) :",if x in modules :,1.2753613517831104,1.2753613517831104,0.0
"def _consume_msg(self):     ws = self._ws     try:         while True:             r = await ws.recv()             <MASK>                 r = r.decode(""utf-8"")             msg = json.loads(r)             stream = msg.get(""stream"")             if stream is not None:                 await self._dispatch(stream, msg)     except websockets.WebSocketException as wse:         logging.warn(wse)         await self.close()         asyncio.ensure_future(self._ensure_ws())","if isinstance ( r , bytes ) :",if r is not None :,7.654112967106117,7.654112967106117,0.0
"def add_source(self, source, name=None):     """"""Adds a new data source to an existing provider.""""""     if self.randomize:         <MASK>             raise ValueError(                 ""Cannot add a non-shuffleable source to an ""                 ""already shuffled provider.""             )     super().add_source(source, name=name)     if self.randomize is True:         self._shuffle_len = self.entries",if not source . can_shuffle ( ) :,if self . shuffle_len :,6.3973701491755195,6.3973701491755195,0.0
"def __str__(self):     buf = [""""]     if self.fileName:         buf.append(self.fileName + "":"")     if self.line != -1:         if not self.fileName:             buf.append(""line "")         buf.append(str(self.line))         <MASK>             buf.append("":"" + str(self.column))         buf.append("":"")     buf.append("" "")     return str("""").join(buf)",if self . column != - 1 :,if not self . column :,20.82186541080652,20.82186541080652,0.0
"def has_bad_headers(self):     headers = [self.sender, self.reply_to] + self.recipients     for header in headers:         if _has_newline(header):             return True     if self.subject:         if _has_newline(self.subject):             for linenum, line in enumerate(self.subject.split(""\r\n"")):                 if not line:                     return True                 if linenum > 0 and line[0] not in ""\t "":                     return True                 if _has_newline(line):                     return True                 <MASK>                     return True     return False",if len ( line . strip ( ) ) == 0 :,if linenum == 0 :,16.731227054577023,16.731227054577023,0.0
"def scanHexEscape(self, prefix):     code = 0     leng = 4 if (prefix == ""u"") else 2     for i in xrange(leng):         <MASK>             ch = self.source[self.index]             self.index += 1             code = code * 16 + HEX_CONV[ch]         else:             return """"     return unichr(code)",if self . index < self . length and isHexDigit ( self . source [ self . index ] ) :,"if prefix == ""u"" :",1.2931730698038337,1.2931730698038337,0.0
"def _get_table_info(self, table_name):     table_addr = self.addr_space.profile.get_symbol(table_name)     table_size = self._get_table_info_distorm()     <MASK>         table_size = self._get_table_info_other(table_addr, table_name)         if table_size == 0:             debug.error(""Unable to get system call table size"")     return [table_addr, table_size]",if table_size == 0 :,if table_size == 0 :,100.00000000000004,100.00000000000004,1.0
"def format_file_path(filepath):     """"""Formats a path as absolute and with the correct platform separator.""""""     try:         is_windows_network_mount = WINDOWS_NETWORK_MOUNT_PATTERN.match(filepath)         filepath = os.path.realpath(os.path.abspath(filepath))         filepath = re.sub(BACKSLASH_REPLACE_PATTERN, ""/"", filepath)         is_windows_drive = WINDOWS_DRIVE_PATTERN.match(filepath)         <MASK>             filepath = filepath.capitalize()         if is_windows_network_mount:             # Add back a / to the front, since the previous modifications             # will have replaced any double slashes with single             filepath = ""/"" + filepath     except:         pass     return filepath",if is_windows_drive :,if is_windows_drive :,100.00000000000004,100.00000000000004,1.0
"def _match(self, cre, s):     # Run compiled regular expression match method on 's'.     # Save result, return success.     self.mo = cre.match(s)     if __debug__:         <MASK>             self._mesg(""\tmatched r'%r' => %r"" % (cre.pattern, self.mo.groups()))     return self.mo is not None",if self . mo is not None and self . debug >= 5 :,if __debug__ :,2.75631563063758,2.75631563063758,0.0
"def reload_sanitize_allowlist(self, explicit=True):     self.sanitize_allowlist = []     try:         with open(self.sanitize_allowlist_file) as f:             for line in f.readlines():                 if not line.startswith(""#""):                     self.sanitize_allowlist.append(line.strip())     except OSError:         <MASK>             log.warning(                 ""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."",                 self.sanitize_allowlist_file,             )",if explicit :,if explicit :,100.00000000000004,0.0,1.0
"def conj(self):     dtype = self.dtype     if issubclass(self.dtype.type, np.complexfloating):         if not self.flags.forc:             raise RuntimeError(                 ""only contiguous arrays may "" ""be used as arguments to this operation""             )         <MASK>             order = ""F""         else:             order = ""C""         result = self._new_like_me(order=order)         func = elementwise.get_conj_kernel(dtype)         func.prepared_async_call(             self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size         )         return result     else:         return self",if self . flags . f_contiguous :,if self . flags . forc :,48.35447404743731,48.35447404743731,0.0
"def scan_spec_conf(self, conf):     if ""metadata"" in conf:         if ""annotations"" in conf[""metadata""] and conf[""metadata""].get(""annotations""):             for annotation in conf[""metadata""][""annotations""]:                 for key in annotation:                     <MASK>                         if (                             ""docker/default"" in annotation[key]                             or ""runtime/default"" in annotation[key]                         ):                             return CheckResult.PASSED     return CheckResult.FAILED","if ""seccomp.security.alpha.kubernetes.io/defaultProfileName"" in key :","if ""docker/default"" in annotation [ key ] :",7.262721116700918,7.262721116700918,0.0
"def test_error_through_destructor(self):     # Test that the exception state is not modified by a destructor,     # even if close() fails.     rawio = self.CloseFailureIO()     with support.catch_unraisable_exception() as cm:         with self.assertRaises(AttributeError):             self.tp(rawio).xyzzy         if not IOBASE_EMITS_UNRAISABLE:             self.assertIsNone(cm.unraisable)         <MASK>             self.assertEqual(cm.unraisable.exc_type, OSError)",elif cm . unraisable is not None :,if cm . unraisable . exc_type is not None :,30.66148710292676,30.66148710292676,0.0
"def _dumpf(frame):     if frame is None:         return ""<None>""     else:         addn = ""(with trace!)""         <MASK>             addn = "" **No Trace Set **""         return ""Frame at %d, file %s, line: %d%s"" % (             id(frame),             frame.f_code.co_filename,             frame.f_lineno,             addn,         )",if frame . f_trace is None :,"if frame . f_code . co_code == ""trace"" :",24.903286388467727,24.903286388467727,0.0
"def containsBadbytes(self, value, bytecount=4):     for b in self.badbytes:         tmp = value         <MASK>             b = ord(b)         for i in range(bytecount):             if (tmp & 0xFF) == b:                 return True             tmp >>= 8     return False",if type ( b ) == str :,if b != 0 :,6.962210312500384,6.962210312500384,0.0
"def _set_peer_statuses(self):     """"""Set peer statuses.""""""     cutoff = time.time() - STALE_SECS     for peer in self.peers:         <MASK>             peer.status = PEER_BAD         elif peer.last_good > cutoff:             peer.status = PEER_GOOD         elif peer.last_good:             peer.status = PEER_STALE         else:             peer.status = PEER_NEVER",if peer . bad :,if peer . last_good < cutoff :,19.070828081828378,19.070828081828378,0.0
"def afterTest(self, test):     try:         # If the browser window is still open, close it now.         self.driver.quit()     except AttributeError:         pass     except Exception:         pass     if self.options.headless:         <MASK>             try:                 self.display.stop()             except AttributeError:                 pass             except Exception:                 pass",if self . headless_active :,if self . options . display :,26.269098944241588,26.269098944241588,0.0
"def _written_variables_in_proxy(self, contract):     variables = []     if contract.is_upgradeable:         variables_name_written_in_proxy = self._variable_written_in_proxy()         <MASK>             variables_in_contract = [                 contract.get_state_variable_from_name(v)                 for v in variables_name_written_in_proxy             ]             variables_in_contract = [v for v in variables_in_contract if v]             variables += variables_in_contract     return list(set(variables))",if variables_name_written_in_proxy :,if not variables_name_written_in_proxy :,82.651681837938,82.651681837938,0.0
"def _available_symbols(self, scoperef, expr):     cplns = []     found_names = set()     while scoperef:         elem = self._elem_from_scoperef(scoperef)         for child in elem:             name = child.get(""name"", """")             if name.startswith(expr):                 <MASK>                     found_names.add(name)                     ilk = child.get(""ilk"") or child.tag                     cplns.append((ilk, name))         scoperef = self.parent_scoperef_from_scoperef(scoperef)         if not scoperef:             break     return sorted(cplns, key=operator.itemgetter(1))",if name not in found_names :,if not found_names :,40.29351667284423,40.29351667284423,0.0
"def get_resource_public_actions(resource_class):     resource_class_members = inspect.getmembers(resource_class)     resource_methods = {}     for name, member in resource_class_members:         if not name.startswith(""_""):             <MASK>                 if not name.startswith(""wait_until""):                     if is_resource_action(member):                         resource_methods[name] = member     return resource_methods",if not name [ 0 ] . isupper ( ) :,"if not name . startswith ( ""action"" ) :",19.081654556856684,19.081654556856684,0.0
def UpdateControlState(self):     active = self.demoModules.GetActiveID()     # Update the radio/restore buttons     for moduleID in self.radioButtons:         btn = self.radioButtons[moduleID]         <MASK>             btn.SetValue(True)         else:             btn.SetValue(False)         if self.demoModules.Exists(moduleID):             btn.Enable(True)             if moduleID == modModified:                 self.btnRestore.Enable(True)         else:             btn.Enable(False)             if moduleID == modModified:                 self.btnRestore.Enable(False),if moduleID == active :,if active == self . active :,16.515821590069027,16.515821590069027,0.0
"def test_controlcharacters(self):     for i in range(128):         c = chr(i)         testString = ""string containing %s"" % c         if i >= 32 or c in ""\r\n\t"":             # \r, \n and \t are the only legal control chars in XML             data = plistlib.dumps(testString, fmt=plistlib.FMT_XML)             <MASK>                 self.assertEqual(plistlib.loads(data), testString)         else:             with self.assertRaises(ValueError):                 plistlib.dumps(testString, fmt=plistlib.FMT_XML)         plistlib.dumps(testString, fmt=plistlib.FMT_BINARY)","if c != ""\r"" :",if data :,4.691812222477093,0.0,0.0
"def remove_usernames(self, username: SLT[str]) -> None:     with self.__lock:         <MASK>             raise RuntimeError(                 f""Can't set {self.username_name} in conjunction with (already set) ""                 f""{self.chat_id_name}s.""             )         parsed_username = self._parse_username(username)         self._usernames -= parsed_username",if self . _chat_ids :,if self . username_name in self . _usernames :,19.72940627795883,19.72940627795883,0.0
"def get_size(self, shape_info):     # The size is the data, that have constant size.     state = np.random.RandomState().get_state()     size = 0     for elem in state:         if isinstance(elem, str):             size += len(elem)         elif isinstance(elem, np.ndarray):             size += elem.size * elem.itemsize         <MASK>             size += np.dtype(""int"").itemsize         elif isinstance(elem, float):             size += np.dtype(""float"").itemsize         else:             raise NotImplementedError()     return size","elif isinstance ( elem , int ) :","if isinstance ( elem , int ) :",84.08964152537145,84.08964152537145,0.0
"def before_step(self, step, feed_dict):     if step == 0:         for _type, mem in self.memories.items():             <MASK>                 self.gan.session.run(tf.assign(mem[""var""], mem[""source""]))","if ""var"" in mem and ""source"" in mem :","if _type == ""var"" :",12.929367642051732,12.929367642051732,0.0
"def write(self, *bits):     for bit in bits:         if not self.bytestream:             self.bytestream.append(0)         byte = self.bytestream[self.bytenum]         <MASK>             if self.bytenum == len(self.bytestream) - 1:                 byte = 0                 self.bytestream += bytes([byte])             self.bytenum += 1             self.bitnum = 0         mask = 2 ** self.bitnum         if bit:             byte |= mask         else:             byte &= ~mask         self.bytestream[self.bytenum] = byte         self.bitnum += 1",if self . bitnum == 8 :,if not byte :,6.988198185490689,6.988198185490689,0.0
"def _validate_parameter_range(self, value_hp, parameter_range):     """"""Placeholder docstring""""""     for (         parameter_range_key,         parameter_range_value,     ) in parameter_range.__dict__.items():         if parameter_range_key == ""scaling_type"":             continue         # Categorical ranges         <MASK>             for categorical_value in parameter_range_value:                 value_hp.validate(categorical_value)         # Continuous, Integer ranges         else:             value_hp.validate(parameter_range_value)","if isinstance ( parameter_range_value , list ) :",if parameter_range_value :,32.73762387107808,32.73762387107808,0.0
"def _trackA(self, tracks):     try:         track, start, end = self.featureA         assert track in tracks         return track     except TypeError:         for track in tracks:             for feature_set in track.get_sets():                 if hasattr(feature_set, ""features""):                     <MASK>                         return track         return None",if self . featureA in feature_set . features . values ( ) :,if feature_set . features [ start ] == end :,26.970156334232563,26.970156334232563,0.0
"def walk(directory, path_so_far):     for name in sorted(os.listdir(directory)):         if any(fnmatch(name, pattern) for pattern in basename_ignore):             continue         path = path_so_far + ""/"" + name if path_so_far else name         if any(fnmatch(path, pattern) for pattern in path_ignore):             continue         full_name = os.path.join(directory, name)         <MASK>             for file_path in walk(full_name, path):                 yield file_path         elif os.path.isfile(full_name):             yield path",if os . path . isdir ( full_name ) :,if os . path . isdir ( full_name ) :,100.00000000000004,100.00000000000004,1.0
"def _poll_ipc_requests(self) -> None:     try:         <MASK>             return         while not self._ipc_requests.empty():             args = self._ipc_requests.get()             try:                 for filename in args:                     if os.path.isfile(filename):                         self.get_editor_notebook().show_file(filename)             except Exception as e:                 logger.exception(""Problem processing ipc request"", exc_info=e)         self.become_active_window()     finally:         self.after(50, self._poll_ipc_requests)",if self . _ipc_requests . empty ( ) :,if not self . _ipc_requests :,45.22723475922432,45.22723475922432,0.0
"def test_read1(self):     self.test_write()     blocks = []     nread = 0     with gzip.GzipFile(self.filename, ""r"") as f:         while True:             d = f.read1()             <MASK>                 break             blocks.append(d)             nread += len(d)             # Check that position was updated correctly (see issue10791).             self.assertEqual(f.tell(), nread)     self.assertEqual(b"""".join(blocks), data1 * 50)",if not d :,if not d :,100.00000000000004,100.00000000000004,1.0
"def _target_generator(self):     if self._internal_target_generator is None:         <MASK>             return None         from ....model_zoo.rcnn.rpn.rpn_target import RPNTargetGenerator         self._internal_target_generator = RPNTargetGenerator(             num_sample=self._num_sample,             pos_iou_thresh=self._pos_iou_thresh,             neg_iou_thresh=self._neg_iou_thresh,             pos_ratio=self._pos_ratio,             stds=self._box_norm,             **self._kwargs         )         return self._internal_target_generator     else:         return self._internal_target_generator",if self . _net_none :,if self . _is_target :,38.260294162784454,38.260294162784454,0.0
"def time_left(self):     """"""Return how many seconds are left until the timeout expires""""""     if self.is_non_blocking:         return 0     elif self.is_infinite:         return None     else:         delta = self.target_time - self.TIME()         <MASK>             # clock jumped, recalculate             self.target_time = self.TIME() + self.duration             return self.duration         else:             return max(0, delta)",if delta > self . duration :,if delta < 0 :,15.848738972120703,15.848738972120703,0.0
"def _decorator(cls):     for name, meth in inspect.getmembers(cls, inspect.isroutine):         if name not in cls.__dict__:             continue         if name != ""__init__"":             if not private and name.startswith(""_""):                 continue         <MASK>             continue         setattr(cls, name, decorator(meth))     return cls",if name in butnot :,if meth is not None :,9.652434877402245,9.652434877402245,0.0
"def load_vocab(vocab_file: str) -> List:     """"""Loads a vocabulary file into a dictionary.""""""     vocab = collections.OrderedDict()     with io.open(vocab_file, ""r"", encoding=""UTF-8"") as file:         for num, line in enumerate(file):             items = convert_to_unicode(line.strip()).split(""\t"")             <MASK>                 break             token = items[0]             index = items[1] if len(items) == 2 else num             token = token.strip()             vocab[token] = int(index)         return vocab",if len ( items ) > 2 :,if not items :,7.733712583165139,7.733712583165139,0.0
"def slice_fill(self, slice_):     ""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true""     if isinstance(self.indexes, int):         new_slice_ = [0]         offset = 0     else:         new_slice_ = [slice_[0]]         offset = 1     for i in range(1, len(self.nums)):         <MASK>             new_slice_.append(0)         elif offset < len(slice_):             new_slice_.append(slice_[offset])             offset += 1     new_slice_ += slice_[offset:]     return new_slice_",if self . squeeze_dims [ i ] :,if i == 0 :,5.484411595600381,5.484411595600381,0.0
"def check_update_function(url, folder, update_setter, version_setter, auto):     remote_version = urllib.urlopen(url).read()     if remote_version.isdigit():         local_version = get_local_timestamp(folder)         if remote_version > local_version:             <MASK>                 update_setter.set_value(True)             version_setter.set_value(remote_version)             return True         else:             return False     else:         return False",if auto :,if auto :,100.00000000000004,0.0,1.0
"def iter_content(self, chunk_size_bytes):     while True:         try:             data = self._fp.read(chunk_size_bytes)         except IOError as e:             raise Fetcher.PermanentError(                 ""Problem reading chunk from {}: {}"".format(self._fp.name, e)             )         <MASK>             break         yield data",if not data :,if not data :,100.00000000000004,100.00000000000004,1.0
"def gvariant_args(args: List[Any]) -> str:     """"""Convert args into gvariant.""""""     gvariant = """"     for arg in args:         if isinstance(arg, bool):             gvariant += "" {}"".format(str(arg).lower())         elif isinstance(arg, (int, float)):             gvariant += f"" {arg}""         <MASK>             gvariant += f' ""{arg}""'         else:             gvariant += f"" {arg!s}""     return gvariant.lstrip()","elif isinstance ( arg , str ) :",if arg is not None :,6.916271812933183,6.916271812933183,0.0
"def _element_keywords(cls, backend, elements=None):     ""Returns a dictionary of element names to allowed keywords""     if backend not in Store.loaded_backends():         return {}     mapping = {}     backend_options = Store.options(backend)     elements = elements if elements is not None else backend_options.keys()     for element in elements:         <MASK>             continue         element = element if isinstance(element, tuple) else (element,)         element_keywords = []         options = backend_options[""."".join(element)]         for group in Options._option_groups:             element_keywords.extend(options[group].allowed_keywords)         mapping[element[0]] = element_keywords     return mapping","if ""."" in element :","if not isinstance ( element , ( list , tuple ) ) :",4.065425428798724,4.065425428798724,0.0
"def setup_parameter_node(self, param_node):     if param_node.bl_idname == ""SvNumberNode"":         if self.use_prop or self.get_prop_name():             value = self.sv_get()[0][0]             print(""V"", value)             <MASK>                 param_node.selected_mode = ""int""                 param_node.int_ = value             elif isinstance(value, float):                 param_node.selected_mode = ""float""                 param_node.float_ = value","if isinstance ( value , int ) :","if isinstance ( value , int ) :",100.00000000000004,100.00000000000004,1.0
"def _get_oshape(indices_shape, depth, axis):     oshape = []     true_axis = len(indices_shape) if axis == -1 else axis     ndim = len(indices_shape) + 1     indices_index = 0     for i in range(0, ndim):         <MASK>             oshape.append(depth)         else:             oshape.append(indices_shape[indices_index])             indices_index += 1     return oshape",if i == true_axis :,if indices_index >= true_axis :,41.11336169005198,41.11336169005198,0.0
"def check(self, value):     value = String.check(self, value)     if isinstance(value, str):         value = value.upper()         for prefix in (self.prefix, self.prefix.split(""_"", 1)[1]):             # e.g. PANGO_WEIGHT_BOLD --> BOLD but also WEIGHT_BOLD --> BOLD             <MASK>                 value = value[len(prefix) :]             value = value.lstrip(""_"")         if hasattr(self.group, value):             return getattr(self.group, value)         else:             raise ValueError(""No such constant: %s_%s"" % (self.prefix, value))     else:         return value",if value . startswith ( prefix ) :,"if prefix . startswith ( ""PANGO_WEIGHT_BOLD"" ) :",14.458924666162856,14.458924666162856,0.0
"def shuffle_unison_inplace(list_of_lists, random_state=None):     if list_of_lists:         assert all(len(l) == len(list_of_lists[0]) for l in list_of_lists)         <MASK>             random_state.permutation(len(list_of_lists[0]))         else:             p = np.random.permutation(len(list_of_lists[0]))         return [l[p] for l in list_of_lists]     return None",if random_state is not None :,if random_state :,38.80684294761701,38.80684294761701,0.0
"def _load_module(self):     spec = self.default_module_spec     module_identifier = self.module_identifier     if module_identifier:         impls = self.get_module_implementation_map()         <MASK>             raise ModuleNotFound(                 ""Invalid module identifier %r in %s""                 % (module_identifier, force_ascii(repr(self)))             )         spec = impls[module_identifier]     cls = load(         spec, context_explanation=""Loading module for %s"" % force_ascii(repr(self))     )     options = getattr(self, self.module_options_field, None) or {}     return cls(self, options)",if module_identifier not in impls :,if not impls :,13.97639637098138,13.97639637098138,0.0
"def get_data(self, state=None, request=None):     if self.load_in_memory:         data, shapes = self._in_memory_get_data(state, request)     else:         data, shapes = self._out_of_memory_get_data(state, request)     for i in range(len(data)):         <MASK>             if isinstance(request, numbers.Integral):                 data[i] = data[i].reshape(shapes[i])             else:                 for j in range(len(data[i])):                     data[i][j] = data[i][j].reshape(shapes[i][j])     return tuple(data)",if shapes [ i ] is not None :,if self . load_out_of_memory :,4.456882760699063,4.456882760699063,0.0
"def resolve_credential_keys(m_keys, keys):     res = []     for k in m_keys:         if k[""c7n:match-type""] == ""credential"":             c_date = parse_date(k[""last_rotated""])             for ak in keys:                 <MASK>                     ak = dict(ak)                     ak[""c7n:match-type""] = ""access""                     if ak not in res:                         res.append(ak)         elif k not in res:             res.append(k)     return res","if c_date == ak [ ""CreateDate"" ] :",if c_date < c_date :,20.2355539266737,20.2355539266737,0.0
"def _is_legacy_mode(self, node):     """"""Checks if the ``ast.Call`` node's keywords signal using legacy mode.""""""     script_mode = False     py_version = ""py2""     for kw in node.keywords:         <MASK>             script_mode = (                 bool(kw.value.value) if isinstance(kw.value, ast.NameConstant) else True             )         if kw.arg == ""py_version"":             py_version = kw.value.s if isinstance(kw.value, ast.Str) else ""py3""     return not (py_version.startswith(""py3"") or script_mode)","if kw . arg == ""script_mode"" :","if kw . arg == ""script_mode"" :",100.00000000000004,100.00000000000004,1.0
"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]:     statuses_by_refs = {u: [] for u in upstream}     events = self.events or []  # type: List[V1EventTrigger]     for e in events:         entity_ref = contexts_refs.get_entity_ref(e.ref)         if not entity_ref:             continue         <MASK>             continue         for kind in e.kinds:             status = V1EventKind.events_statuses_mapping.get(kind)             if status:                 statuses_by_refs[entity_ref].append(status)     return statuses_by_refs",if entity_ref not in statuses_by_refs :,if e . kinds is None :,3.823246852690463,3.823246852690463,0.0
"def items(self):     dict = {}     for userdir in self.XDG_DIRS.keys():         prefix = self.get(userdir).strip('""').split(""/"")[0]         <MASK>             path = (                 os.getenv(""HOME"")                 + ""/""                 + ""/"".join(self.get(userdir).strip('""').split(""/"")[1:])             )         else:             path = self.get(userdir).strip('""')         dict[userdir] = path     return dict.items()",if prefix :,"if prefix == ""HOME"" :",12.22307556087252,12.22307556087252,0.0
"def clean_objects(string, common_attributes):     """"""Return object and attribute lists""""""     string = clean_string(string)     words = string.split()     if len(words) > 1:         prefix_words_are_adj = True         for att in words[:-1]:             <MASK>                 prefix_words_are_adj = False         if prefix_words_are_adj:             return words[-1:], words[:-1]         else:             return [string], []     else:         return [string], []",if att not in common_attributes :,if att in common_attributes :,61.29752413741059,61.29752413741059,0.0
"def extract_custom(extractor, *args, **kw):     for match in extractor(*args, **kw):         msg = match[2]         <MASK>             unused = (                 ""<unused singular (hash=%s)>"" % md5(msg[1].encode(""utf8"")).hexdigest()             )             msg = (unused, msg[1], msg[2])             match = (match[0], match[1], msg, match[3])         yield match","if isinstance ( msg , tuple ) and msg [ 0 ] == """" :",if len ( msg ) == 3 :,6.376384919709712,6.376384919709712,0.0
"def test_convex_decomposition(self):     mesh = g.get_mesh(""quadknot.obj"")     engines = [(""vhacd"", g.trimesh.interfaces.vhacd.exists)]     for engine, exists in engines:         <MASK>             g.log.warning(""skipping convex decomposition engine %s"", engine)             continue         g.log.info(""Testing convex decomposition with engine %s"", engine)         meshes = mesh.convex_decomposition(engine=engine)         self.assertTrue(len(meshes) > 1)         for m in meshes:             self.assertTrue(m.is_watertight)         g.log.info(""convex decomposition succeeded with %s"", engine)",if not exists :,if not exists :,100.00000000000004,100.00000000000004,1.0
"def _to_string_infix(self, ostream, idx, verbose):     if verbose:         ostream.write("" , "")     else:         hasConst = not (             self._const.__class__ in native_numeric_types and self._const == 0         )         if hasConst:             idx -= 1         _l = self._coef[id(self._args[idx])]         _lt = _l.__class__         <MASK>             ostream.write("" - "")         else:             ostream.write("" + "")",if _lt is _NegationExpression or ( _lt in native_numeric_types and _l < 0 ) :,if _lt . __class__ == int :,6.808798745384609,6.808798745384609,0.0
"def get_other(self, data, items):     is_tuple = False     if type(data) == tuple:         data = list(data)         is_tuple = True     if type(data) == list:         m_items = items.copy()         for idx, item in enumerate(items):             if item < 0:                 m_items[idx] = len(data) - abs(item)         for i in sorted(set(m_items), reverse=True):             if i < len(data) and i > -1:                 del data[i]         <MASK>             return tuple(data)         else:             return data     else:         return None",if is_tuple :,if is_tuple :,100.00000000000004,100.00000000000004,1.0
"def process_error(self, data):     if data.get(""error""):         <MASK>             raise AuthCanceled(self, data.get(""error_description"", """"))         raise AuthFailed(self, data.get(""error_description"") or data[""error""])     elif ""denied"" in data:         raise AuthCanceled(self, data[""denied""])","if ""denied"" in data [ ""error"" ] or ""cancelled"" in data [ ""error"" ] :","if ""canceled"" in data :",3.2720332204836837,3.2720332204836837,0.0
"def tamper(payload, **kwargs):     junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`""     retval = """"     for i, char in enumerate(payload, start=1):         amount = random.randint(10, 15)         if char == "">"":             retval += "">""             for _ in range(amount):                 retval += random.choice(junk_chars)         elif char == ""<"":             retval += ""<""             for _ in range(amount):                 retval += random.choice(junk_chars)         <MASK>             for _ in range(amount):                 retval += random.choice(junk_chars)         else:             retval += char     return retval","elif char == "" "" :",if i == 0 :,15.207218222740094,15.207218222740094,0.0
"def retry_http_digest_auth(self, req, auth):     token, challenge = auth.split("" "", 1)     chal = parse_keqv_list(parse_http_list(challenge))     auth = self.get_authorization(req, chal)     if auth:         auth_val = ""Digest %s"" % auth         <MASK>             return None         req.add_unredirected_header(self.auth_header, auth_val)         resp = self.parent.open(req)         return resp","if req . headers . get ( self . auth_header , None ) == auth_val :",if token != self . auth_header :,14.705972278549249,14.705972278549249,0.0
"def close(self):     self.selector.close()     if self.sock:         sockname = None         try:             sockname = self.sock.getsockname()         except (socket.error, OSError):             pass         self.sock.close()         if type(sockname) is str:             # it was a Unix domain socket, remove it from the filesystem             <MASK>                 os.remove(sockname)     self.sock = None",if os . path . exists ( sockname ) :,if type ( sockname ) is str :,18.190371142855746,18.190371142855746,0.0
"def to_nurbs(self, curves):     result = []     for i, c in enumerate(curves):         nurbs = SvNurbsCurve.to_nurbs(c)         <MASK>             raise Exception(f""Curve #{i} - {c} - can not be converted to NURBS!"")         result.append(nurbs)     return result",if nurbs is None :,if nurbs != self . nurbs :,12.22307556087252,12.22307556087252,0.0
"def handle_1_roomid_raffle(self, i):     if i[1] in [""handle_1_room_TV"", ""handle_1_room_captain""]:         <MASK>             await self.notify(""post_watching_history"", i[0])             await self.notify(i[1], i[0], i[2])     else:         print(""hhjjkskddrsfvsfdfvdfvvfdvdvdfdfffdfsvh"", i)","if await self . notify ( ""check_if_normal_room"" , i [ 0 ] , - 1 ) :","if i [ 2 ] in [ ""handle_1_room_TV"" , ""handle_1_room_captain"" , ""handle_1_room_captain"" , ""handle_1_room_captain"" , ""handle_1_room_captain"" , ""handle_1_room_captain_captain"" ] :",2.2100755209813543,2.2100755209813543,0.0
"def init_ps_var_partition(self):     ps_vars = {}     for v in self._non_embed_vars.values():         if v.name not in self._var_to_ps:             self._var_to_ps[v.name] = string_to_id(v.name, self._ps_num)         ps_id = self._var_to_ps[v.name]         <MASK>             ps_vars[ps_id] = [v]         else:             ps_vars[ps_id].append(v)     self._ps_vars = ps_vars",if ps_id not in ps_vars :,if ps_id not in ps_vars :,100.00000000000004,100.00000000000004,1.0
"def get_files(d):     f = []     for root, dirs, files in os.walk(d):         for name in files:             if ""meta-environment"" in root or ""cross-canadian"" in root:                 continue             if ""qemux86copy-"" in root or ""qemux86-"" in root:                 continue             <MASK>                 f.append(os.path.join(root, name))     return f","if ""do_build"" not in name and ""do_populate_sdk"" not in name :","if ""qemux86"" in root :",2.235173392777356,2.235173392777356,0.0
"def setSelectedLabelState(self, p):  # selected, disabled     c = self.c     # g.trace(p,c.edit_widget(p))     if p and c.edit_widget(p):         <MASK>             g.trace(self.trace_n, c.edit_widget(p), p)             # g.trace(g.callers(6))             self.trace_n += 1         self.setDisabledHeadlineColors(p)",if 0 :,if p . is_selected ( ) :,5.669791110976001,5.669791110976001,0.0
"def filter_tasks(self, task_types=None, task_states=None, task_text=None):     tasks = self.api.tasks(self.id).get(""tasks"", {})     if tasks and tasks.get(""task""):         return [             Task(self, task)             for task in tasks.get(""task"", [])             <MASK>             and (not task_states or task[""state""].lower() in task_states)             and (not task_text or task_text.lower() in str(task).lower())         ]     else:         return []","if ( not task_types or task [ ""type"" ] . lower ( ) in task_types )","if task [ ""state"" ] in task_types :",15.247616063796919,15.247616063796919,0.0
"def GenerateVector(self, hits, vector, level):     """"""Generate possible hit vectors which match the rules.""""""     for item in hits.get(level, []):         <MASK>             if item < vector[-1]:                 continue             if item > self.max_separation + vector[-1]:                 break         new_vector = vector + [item]         if level + 1 == len(hits):             yield new_vector         elif level + 1 < len(hits):             for result in self.GenerateVector(hits, new_vector, level + 1):                 yield result",if vector :,if item > self . max_separation :,5.669791110976001,5.669791110976001,0.0
def _transmit_from_storage(self) -> None:     for blob in self.storage.gets():         # give a few more seconds for blob lease operation         # to reduce the chance of race (for perf consideration)         if blob.lease(self._timeout + 5):             envelopes = [TelemetryItem(**x) for x in blob.get()]             result = self._transmit(list(envelopes))             <MASK>                 blob.lease(1)             else:                 blob.delete(),if result == ExportResult . FAILED_RETRYABLE :,if result :,6.108851178104657,0.0,0.0
"def load_dictionary(file):     oui = {}     with open(file, ""r"") as f:         for line in f:             <MASK>                 data = line.split(""(hex)"")                 key = data[0].replace(""-"", "":"").lower().strip()                 company = data[1].strip()                 oui[key] = company     return oui","if ""(hex)"" in line :",if line :,8.525588607164655,0.0,0.0
"def _yield_minibatches_idx(self, rgen, n_batches, data_ary, shuffle=True):     indices = np.arange(data_ary.shape[0])     if shuffle:         indices = rgen.permutation(indices)     if n_batches > 1:         remainder = data_ary.shape[0] % n_batches         <MASK>             minis = np.array_split(indices[:-remainder], n_batches)             minis[-1] = np.concatenate((minis[-1], indices[-remainder:]), axis=0)         else:             minis = np.array_split(indices, n_batches)     else:         minis = (indices,)     for idx_batch in minis:         yield idx_batch",if remainder :,if remainder > 0 :,23.643540225079384,23.643540225079384,0.0
"def canonical_custom_headers(self, headers):     hoi = []     custom_headers = {}     for key in headers:         lk = key.lower()         if headers[key] is not None:             <MASK>                 custom_headers[lk] = "","".join(v.strip() for v in headers.get_all(key))     sorted_header_keys = sorted(custom_headers.keys())     for key in sorted_header_keys:         hoi.append(""%s:%s"" % (key, custom_headers[key]))     return ""\n"".join(hoi)","if lk . startswith ( ""x-amz-"" ) :",if lk in custom_headers :,9.469167282754096,9.469167282754096,0.0
"def validate(self, data):     if not data.get(""reason""):         # If reason is not provided, message is required and can not be         # null or blank.         message = data.get(""message"")         if not message:             if ""message"" not in data:                 msg = serializers.Field.default_error_messages[""required""]             <MASK>                 msg = serializers.Field.default_error_messages[""null""]             else:                 msg = serializers.CharField.default_error_messages[""blank""]             raise serializers.ValidationError({""message"": [msg]})     return data",elif message is None :,"if ""null"" in data :",6.567274736060395,6.567274736060395,0.0
def tearDown(self):     try:         os.chdir(self.cwd)         <MASK>             os.remove(self.pythonexe)         test_support.rmtree(self.parent_dir)     finally:         BaseTestCase.tearDown(self),if self . pythonexe != sys . executable :,if self . pythonexe :,26.013004751144457,26.013004751144457,0.0
"def update(self, value, label):     if self._disabled:         return     try:         self._progress.value = value         self._label.value = label         <MASK>             self._displayed = True             display_widget(self._widget)     except Exception as e:         self._disabled = True         logger.exception(e)         wandb.termwarn(""Unable to render progress bar, see the user log for details"")",if not self . _displayed :,if self . _displayed :,67.31821382417488,67.31821382417488,0.0
"def GetBinaryOperationBinder(self, op):     with self._lock:         <MASK>             return self._binaryOperationBinders[op]         b = runtime.SymplBinaryOperationBinder(op)         self._binaryOperationBinders[op] = b     return b",if self . _binaryOperationBinders . ContainsKey ( op ) :,if op in self . _binaryOperationBinders :,27.329052280893862,27.329052280893862,0.0
"def apply(self, l, b, evaluation):     ""FromDigits[l_, b_]""     if l.get_head_name() == ""System`List"":         value = Integer(0)         for leaf in l.leaves:             value = Expression(""Plus"", Expression(""Times"", value, b), leaf)         return value     elif isinstance(l, String):         value = FromDigits._parse_string(l.get_string_value(), b)         <MASK>             evaluation.message(""FromDigits"", ""nlst"")         else:             return value     else:         evaluation.message(""FromDigits"", ""nlst"")",if value is None :,if value == 0 :,17.965205598154213,17.965205598154213,0.0
"def hsconn_sender(self):     while not self.stop_event.is_set():         try:             # Block, but timeout, so that we can exit the loop gracefully             request = self.send_queue.get(True, 6.0)             if self.socket is not None:                 # Socket got closed and set to None in another thread...                 self.socket.sendall(request)             <MASK>                 self.send_queue.task_done()         except queue.Empty:             pass         except OSError:             self.stop_event.set()",if self . send_queue is not None :,if request :,3.361830360737634,0.0,0.0
"def check_expected(result, expected, contains=False):     if sys.version_info[0] >= 3:         if isinstance(result, str):             result = result.encode(""ascii"")         if isinstance(expected, str):             expected = expected.encode(""ascii"")     resultlines = result.splitlines()     expectedlines = expected.splitlines()     if len(resultlines) != len(expectedlines):         return False     for rline, eline in zip(resultlines, expectedlines):         if contains:             <MASK>                 return False         else:             if not rline.endswith(eline):                 return False     return True",if eline not in rline :,"if eline == """" :",14.535768424205482,14.535768424205482,0.0
"def init_weights(self):     """"""Initialize model weights.""""""     for _, m in self.multi_deconv_layers.named_modules():         <MASK>             normal_init(m, std=0.001)         elif isinstance(m, nn.BatchNorm2d):             constant_init(m, 1)     for m in self.multi_final_layers.modules():         if isinstance(m, nn.Conv2d):             normal_init(m, std=0.001, bias=0)","if isinstance ( m , nn . ConvTranspose2d ) :","if isinstance ( m , nn . Conv2d ) :",70.71067811865478,70.71067811865478,0.0
"def filter_rel_attrs(field_name, **rel_attrs):     clean_dict = {}     for k, v in rel_attrs.items():         <MASK>             splitted_key = k.split(""__"")             key = ""__"".join(splitted_key[1:])             clean_dict[key] = v         else:             clean_dict[k] = v     return clean_dict","if k . startswith ( field_name + ""__"" ) :","if field_name == ""class"" :",11.708995388048033,11.708995388048033,0.0
"def cancel(self):     with self._condition:         <MASK>             self._squash(                 state_root=self._previous_state_hash,                 context_ids=[self._previous_context_id],                 persist=False,                 clean_up=True,             )         self._cancelled = True         self._condition.notify_all()",if not self . _cancelled and not self . _final and self . _previous_context_id :,if self . _previous_state_hash is not None :,16.76172481487686,16.76172481487686,0.0
"def _get_level(levels, level_ref):     if level_ref in levels:         return levels.index(level_ref)     if isinstance(level_ref, six.integer_types):         <MASK>             level_ref += len(levels)         if not (0 <= level_ref < len(levels)):             raise PatsyError(""specified level %r is out of range"" % (level_ref,))         return level_ref     raise PatsyError(""specified level %r not found"" % (level_ref,))",if level_ref < 0 :,if level_ref < 0 :,100.00000000000004,100.00000000000004,1.0
"def parse_node(self, node, alias_map=None, conv=None):     sql, params, unknown = self._parse(node, alias_map, conv)     if unknown and conv and params:         params = [conv.db_value(i) for i in params]     if isinstance(node, Node):         if node._negated:             sql = ""NOT %s"" % sql         <MASK>             sql = "" "".join((sql, ""AS"", node._alias))         if node._ordering:             sql = "" "".join((sql, node._ordering))     return sql, params",if node . _alias :,if node . _alias :,100.00000000000004,100.00000000000004,1.0
"def parse_object_id(_, values):     if values:         for key in values:             <MASK>                 val = values[key]                 if len(val) > 10:                     try:                         values[key] = utils.ObjectIdSilent(val)                     except:                         values[key] = None","if key . endswith ( ""_id"" ) :",if key in values :,7.121297464907233,7.121297464907233,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             self.set_app_id(d.getPrefixedString())             continue         if tt == 16:             self.set_max_rows(d.getVarInt32())             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt == 8 :,53.7284965911771,53.7284965911771,0.0
"def has_invalid_cce(yaml_file, product_yaml=None):     rule = yaml.open_and_macro_expand(yaml_file, product_yaml)     if ""identifiers"" in rule and rule[""identifiers""] is not None:         for i_type, i_value in rule[""identifiers""].items():             <MASK>                 if not checks.is_cce_value_valid(""CCE-"" + str(i_value)):                     return True     return False","if i_type [ 0 : 3 ] == ""cce"" :","if i_type == ""CCE"" :",25.681706510882123,25.681706510882123,0.0
"def _generate_table(self, fromdesc, todesc, diffs):     if fromdesc or todesc:         yield (             simple_colorize(fromdesc, ""description""),             simple_colorize(todesc, ""description""),         )     for i, line in enumerate(diffs):         if line is None:             # mdiff yields None on separator lines; skip the bogus ones             # generated for the first line             <MASK>                 yield (                     simple_colorize(""---"", ""separator""),                     simple_colorize(""---"", ""separator""),                 )         else:             yield line",if i > 0 :,if i == 1 :,17.965205598154213,17.965205598154213,0.0
"def _getPatternTemplate(pattern, key=None):     if key is None:         key = pattern         if ""%"" not in pattern:             key = pattern.upper()     template = DD_patternCache.get(key)     if not template:         if key in (""EPOCH"", ""{^LN-BEG}EPOCH"", ""^EPOCH""):             template = DateEpoch(lineBeginOnly=(key != ""EPOCH""))         <MASK>             template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False))         else:             template = DatePatternRegex(pattern)     DD_patternCache.set(key, template)     return template","elif key in ( ""TAI64N"" , ""{^LN-BEG}TAI64N"" , ""^TAI64N"" ) :","if key in ( ""TAI64N"" , ""start"" ) :",33.545253510957444,33.545253510957444,0.0
"def ref_max_pooling_2d(x, kernel, stride, ignore_border, pad):     y = []     for xx in x.reshape((-1,) + x.shape[-3:]):         <MASK>             xx = xx[np.newaxis]         y += [             refs.pooling_2d(xx, ""max"", kernel, stride, pad, ignore_border)[np.newaxis]         ]     y = np.vstack(y)     if x.ndim == 2:         y = np.squeeze(y, 1)     return y.reshape(x.shape[:-3] + y.shape[1:])",if xx . ndim == 2 :,if xx . ndim == 1 :,70.71067811865478,70.71067811865478,0.0
"def show_topics():     """"""prints all available miscellaneous help topics.""""""     print(_stash.text_color(""Miscellaneous Topics:"", ""yellow""))     for pp in PAGEPATHS:         if not os.path.isdir(pp):             continue         content = os.listdir(pp)         for pn in content:             <MASK>                 name = pn[: pn.index(""."")]             else:                 name = pn             print(name)","if ""."" in pn :","if pn . startswith ( ""."" ) :",18.575057999133595,18.575057999133595,0.0
"def justify_toggle_auto(self, event=None):     c = self     if c.editCommands.autojustify == 0:         c.editCommands.autojustify = abs(c.config.getInt(""autojustify"") or 0)         <MASK>             g.es(""Autojustify on, @int autojustify == %s"" % c.editCommands.autojustify)         else:             g.es(""Set @int autojustify in @settings"")     else:         c.editCommands.autojustify = 0         g.es(""Autojustify off"")",if c . editCommands . autojustify :,if c . editCommands . autojustify :,100.00000000000004,100.00000000000004,1.0
"def render_token_list(self, tokens):     result = []     vars = []     for token in tokens:         <MASK>             result.append(token.contents.replace(""%"", ""%%""))         elif token.token_type == TOKEN_VAR:             result.append(""%%(%s)s"" % token.contents)             vars.append(token.contents)     return """".join(result), vars",if token . token_type == TOKEN_TEXT :,if token . token_type == TOKEN_STRING :,82.651681837938,82.651681837938,0.0
"def get_target_dimensions(self):     width, height = self.engine.size     for operation in self.operations:         if operation[""type""] == ""crop"":             width = operation[""right""] - operation[""left""]             height = operation[""bottom""] - operation[""top""]         <MASK>             width = operation[""width""]             height = operation[""height""]     return (width, height)","if operation [ ""type"" ] == ""resize"" :","if operation [ ""type"" ] == ""crop"" :",79.10665071754353,79.10665071754353,0.0
"def get_eval_matcher(self):     if isinstance(self.data[""match""], str):         <MASK>             values = [""explicitDeny"", ""implicitDeny""]         else:             values = [""allowed""]         vf = ValueFilter(             {""type"": ""value"", ""key"": ""EvalDecision"", ""value"": values, ""op"": ""in""}         )     else:         vf = ValueFilter(self.data[""match""])     vf.annotate = False     return vf","if self . data [ ""match"" ] == ""denied"" :","if self . data [ ""match"" ] == ""implicitDeny"" :",82.42367502646057,82.42367502646057,0.0
"def test_training(self):     if not self.model_tester.is_training:         return     config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()     config.return_dict = True     for model_class in self.all_model_classes:         <MASK>             continue         model = model_class(config)         model.to(torch_device)         model.train()         inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)         loss = model(**inputs).loss         loss.backward()",if model_class in MODEL_MAPPING . values ( ) :,if not model_class . is_training :,14.301399262246576,14.301399262246576,0.0
"def prehook(self, emu, op, eip):     if op in self.badops:         emu.stopEmu()         raise v_exc.BadOpBytes(op.va)     if op.mnem in STOS:         <MASK>             reg = emu.getRegister(envi.archs.i386.REG_EDI)         elif self.arch == ""amd64"":             reg = emu.getRegister(envi.archs.amd64.REG_RDI)         if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None:             self.vw.makePointer(reg, follow=True)","if self . arch == ""i386"" :","if self . arch == ""i386"" :",100.00000000000004,100.00000000000004,1.0
"def test_len(self):     eq = self.assertEqual     eq(base64mime.base64_len(""hello""), len(base64mime.encode(""hello"", eol="""")))     for size in range(15):         <MASK>             bsize = 0         elif size <= 3:             bsize = 4         elif size <= 6:             bsize = 8         elif size <= 9:             bsize = 12         elif size <= 12:             bsize = 16         else:             bsize = 20         eq(base64mime.base64_len(""x"" * size), bsize)",if size == 0 :,if size <= 1 :,19.304869754804482,19.304869754804482,0.0
"def __new__(cls, dependencies):     deps = check.list_param(dependencies, ""dependencies"", of_type=DependencyDefinition)     seen = {}     for dep in deps:         key = dep.solid + "":"" + dep.output         <MASK>             raise DagsterInvalidDefinitionError(                 'Duplicate dependencies on solid ""{dep.solid}"" output ""{dep.output}"" '                 ""used in the same MultiDependencyDefinition."".format(dep=dep)             )         seen[key] = True     return super(MultiDependencyDefinition, cls).__new__(cls, deps)",if key in seen :,if seen [ key ] :,11.478744233307168,11.478744233307168,0.0
"def get_explanation(self, spec):     """"""Expand an explanation.""""""     if spec:         try:             a = self.dns_txt(spec)             if len(a) == 1:                 return str(self.expand(to_ascii(a[0]), stripdot=False))         except PermError:             # RFC4408 6.2/4 syntax errors cause exp= to be ignored             <MASK>                 raise  # but report in harsh mode for record checking tools             pass     elif self.strict > 1:         raise PermError(""Empty domain-spec on exp="")     # RFC4408 6.2/4 empty domain spec is ignored     # (unless you give precedence to the grammar).     return None",if self . strict > 1 :,if self . strict == 0 :,36.55552228545123,36.55552228545123,0.0
"def build(self):     if self.args.get(""sle_id""):         self.process_sle_against_current_voucher()     else:         entries_to_fix = self.get_future_entries_to_fix()         i = 0         while i < len(entries_to_fix):             sle = entries_to_fix[i]             i += 1             self.process_sle(sle)             <MASK>                 self.get_dependent_entries_to_fix(entries_to_fix, sle)     if self.exceptions:         self.raise_exceptions()     self.update_bin()",if sle . dependant_sle_voucher_detail_no :,"if sle . id == self . args . get ( ""sle_id"" ) :",9.917720727091444,9.917720727091444,0.0
"def ValidateStopLatitude(self, problems):     if self.stop_lat is not None:         value = self.stop_lat         try:             if not isinstance(value, (float, int)):                 self.stop_lat = util.FloatStringToFloat(value, problems)         except (ValueError, TypeError):             problems.InvalidValue(""stop_lat"", value)             del self.stop_lat         else:             <MASK>                 problems.InvalidValue(""stop_lat"", value)",if self . stop_lat > 90 or self . stop_lat < - 90 :,if value < 0 :,1.044177559991939,1.044177559991939,0.0
"def set(self, obj, **kwargs):     """"""Check for missing event functions and substitute these with""""""     """"""the ignore method""""""     ignore = getattr(self, ""ignore"")     for k, v in kwargs.iteritems():         setattr(self, k, getattr(obj, v))         <MASK>             for k1 in self.combinations[k]:                 if not hasattr(self, k1):                     setattr(self, k1, ignore)",if k in self . combinations :,if not ignore :,8.9730240870212,8.9730240870212,0.0
"def split(self, duration, include_remainder=True):     # Convert seconds to timedelta, if appropriate.     duration = _seconds_or_timedelta(duration)     if duration <= timedelta(seconds=0):         raise ValueError(""cannot call split with a non-positive timedelta"")     start = self.start     while start < self.end:         if start + duration <= self.end:             yield MayaInterval(start, start + duration)         <MASK>             yield MayaInterval(start, self.end)         start += duration",elif include_remainder :,if include_remainder :,66.87403049764218,66.87403049764218,0.0
"def get_first_field(layout, clz):     for layout_object in layout.fields:         if issubclass(layout_object.__class__, clz):             return layout_object         <MASK>             gf = get_first_field(layout_object, clz)             if gf:                 return gf","elif hasattr ( layout_object , ""get_field_names"" ) :",if layout_object . __class__ is layout :,10.856421294065939,10.856421294065939,0.0
"def _getPatternTemplate(pattern, key=None):     if key is None:         key = pattern         if ""%"" not in pattern:             key = pattern.upper()     template = DD_patternCache.get(key)     if not template:         <MASK>             template = DateEpoch(lineBeginOnly=(key != ""EPOCH""))         elif key in (""TAI64N"", ""{^LN-BEG}TAI64N"", ""^TAI64N""):             template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False))         else:             template = DatePatternRegex(pattern)     DD_patternCache.set(key, template)     return template","if key in ( ""EPOCH"" , ""{^LN-BEG}EPOCH"" , ""^EPOCH"" ) :","if key in ( ""EPOCH"" , ""{^LN-BEG}EPOCH"" , ""^LN-BEG"" ) :",87.25129388059685,87.25129388059685,0.0
"def findOwningViewController(self, object):     while object:         <MASK>             description = fb.evaluateExpressionValue(object).GetObjectDescription()             print(""Found the owning view controller.\n{}"".format(description))             cmd = 'echo {} | tr -d ""\n"" | pbcopy'.format(object)             os.system(cmd)             return         else:             object = self.nextResponder(object)     print(""Could not find an owning view controller"")",if self . isViewController ( object ) :,if fb . evaluateExpressionValue ( object ) :,38.260294162784454,38.260294162784454,0.0
"def __get_file_by_num(self, num, file_list, idx=0):     for element in file_list:         if idx == num:             return element         if element[3] and element[4]:             i = self.__get_file_by_num(num, element[3], idx + 1)             <MASK>                 return i             idx = i         else:             idx += 1     return idx","if not isinstance ( i , int ) :",if i :,5.370784274455332,0.0,0.0
"def promtool(**kwargs):     key = ""prometheus:promtool""     try:         path = pathlib.Path(util.setting(key))     except TypeError:         yield checks.Warning(             ""Missing setting for %s in %s "" % (key, settings.PROMGEN_CONFIG_FILE),             id=""promgen.W001"",         )     else:         <MASK>             yield checks.Warning(""Unable to execute file %s"" % path, id=""promgen.W003"")","if not os . access ( path , os . X_OK ) :",if not os . path . isfile ( path ) :,22.69624786949032,22.69624786949032,0.0
"def parse_config(schema, config):     schemaparser = ConfigParser()     schemaparser.readfp(StringIO(schema))     cfgparser = ConfigParser()     cfgparser.readfp(StringIO(config))     result = {}     for section in cfgparser.sections():         result_section = {}         schema = {}         <MASK>             schema = dict(schemaparser.items(section))         for key, value in cfgparser.items(section):             converter = converters[schema.get(key, ""string"")]             result_section[key] = converter(value)         result[section] = result_section     return result",if section in schemaparser . sections ( ) :,if section in schemaparser :,31.772355751081438,31.772355751081438,0.0
"def validate_arguments(args):     if args.num_pss < 1:         print(""Value error: must have ore than one parameter servers."")         exit(1)     if not GPU_IDS:         num_cpus = multiprocessing.cpu_count()         <MASK>             print(                 ""Value error: there are %s available CPUs but you are requiring %s.""                 % (num_cpus, args.cpu_trainers)             )             exit(1)     if not os.path.isfile(args.file):         print(""Value error: model trainning file does not exist"")         exit(1)",if args . cpu_trainers > num_cpus :,if num_cpus > args . cpu_trainers :,50.81327481546147,50.81327481546147,0.0
"def infer_dataset_impl(path):     if IndexedRawTextDataset.exists(path):         return ""raw""     elif IndexedDataset.exists(path):         with open(index_file_path(path), ""rb"") as f:             magic = f.read(8)             if magic == IndexedDataset._HDR_MAGIC:                 return ""cached""             <MASK>                 return ""mmap""             else:                 return None     elif FastaDataset.exists(path):         return ""fasta""     else:         return None",elif magic == MMapIndexedDataset . Index . _HDR_MAGIC [ : 8 ] :,if magic == IndexedDataset . _MDR_MAGIC :,12.28742276615499,12.28742276615499,0.0
"def _add_resource_group(obj):     if isinstance(obj, list):         for array_item in obj:             _add_resource_group(array_item)     elif isinstance(obj, dict):         try:             if ""resourcegroup"" not in [x.lower() for x in obj.keys()]:                 <MASK>                     obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""]         except (KeyError, IndexError, TypeError):             pass         for item_key in obj:             if item_key != ""sourceVault"":                 _add_resource_group(obj[item_key])","if obj [ ""id"" ] :","if ""id"" in obj :",25.201472805660515,25.201472805660515,0.0
"def reformatBody(self, event=None):     """"""Reformat all paragraphs in the body.""""""     c, p = self, self.p     undoType = ""reformat-body""     w = c.frame.body.wrapper     c.undoer.beforeChangeGroup(p, undoType)     w.setInsertPoint(0)     while 1:         progress = w.getInsertPoint()         c.reformatParagraph(event, undoType=undoType)         ins = w.getInsertPoint()         s = w.getAllText()         w.setInsertPoint(ins)         <MASK>             break     c.undoer.afterChangeGroup(p, undoType)",if ins <= progress or ins >= len ( s ) :,if not s :,1.7256245272235644,1.7256245272235644,0.0
"def make_sources(project: RootDependency) -> str:     content = []     if project.readme:         content.append(project.readme.path.name)         <MASK>             content.append(project.readme.to_rst().path.name)     path = project.package.path     for fname in (""setup.cfg"", ""setup.py""):         if (path / fname).exists():             content.append(fname)     for package in chain(project.package.packages, project.package.data):         for fpath in package:             fpath = fpath.relative_to(project.package.path)             content.append(""/"".join(fpath.parts))     return ""\n"".join(content)","if project . readme . markup != ""rst"" :",if project . readme . exists ( ) :,33.471898740037666,33.471898740037666,0.0
"def __init__(self, response):     error = ""{} {}"".format(response.status_code, response.reason)     extra = []     try:         response_json = response.json()         <MASK>             error = "" "".join(error[""message""] for error in response_json[""error_list""])             extra = [                 error[""extra""]                 for error in response_json[""error_list""]                 if ""extra"" in error             ]     except JSONDecodeError:         pass     super().__init__(response=response, error=error, extra=extra)","if ""error_list"" in response_json :","if ""message"" in response_json :",52.899344348276884,52.899344348276884,0.0
"def handle_event(self, fileno=None, events=None):     if self._state == RUN:         <MASK>             self._it = self._process_result(0)  # non-blocking         try:             next(self._it)         except (StopIteration, CoroStop):             self._it = None",if self . _it is None :,if self . _state == WAIT :,31.55984539112946,31.55984539112946,0.0
"def find_query(self, needle, haystack):     try:         import pinyin         haystack_py = pinyin.get_initial(haystack, """")         needle_len = len(needle)         start = 0         result = []         while True:             found = haystack_py.find(needle, start)             <MASK>                 break             result.append((found, needle_len))             start = found + needle_len         return result     except:         return None",if found < 0 :,if found == - 1 :,14.535768424205482,14.535768424205482,0.0
"def decorated_function(*args, **kwargs):     rv = f(*args, **kwargs)     if ""Last-Modified"" not in rv.headers:         try:             result = date             if callable(result):                 result = result(rv)             if not isinstance(result, basestring):                 from werkzeug.http import http_date                 result = http_date(result)             <MASK>                 rv.headers[""Last-Modified""] = result         except Exception:             logging.getLogger(__name__).exception(                 ""Error while calculating the lastmodified value for response {!r}"".format(                     rv                 )             )     return rv",if result :,if result :,100.00000000000004,0.0,1.0
"def check_require(require_modules, require_lines):     for require_module in require_modules:         st = try_import(require_module)         if st == 0:             continue         <MASK>             print(                 ""installed {}: {}\n"".format(                     require_module, require_lines[require_module]                 )             )         elif st == 2:             print(                 ""failed installed {}: {}\n"".format(                     require_module, require_lines[require_module]                 )             )",elif st == 1 :,if st == 1 :,75.98356856515926,75.98356856515926,0.0
"def bundle_directory(self, dirpath):     """"""Bundle all modules/packages in the given directory.""""""     dirpath = os.path.abspath(dirpath)     for nm in os.listdir(dirpath):         nm = _u(nm)         if nm.startswith("".""):             continue         itempath = os.path.join(dirpath, nm)         if os.path.isdir(itempath):             if os.path.exists(os.path.join(itempath, ""__init__.py"")):                 self.bundle_package(itempath)         <MASK>             self.bundle_module(itempath)","elif nm . endswith ( "".py"" ) :",if os . path . exists ( itempath ) :,9.548450962056531,9.548450962056531,0.0
"def _find_root():     test_dirs = [""Src"", ""Build"", ""Package"", ""Tests"", ""Util""]     root = os.getcwd()     test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs])     while not test:         last_root = root         root = os.path.dirname(root)         <MASK>             raise Exception(""Root not found"")         test = all([os.path.exists(os.path.join(root, x)) for x in test_dirs])     return root",if root == last_root :,if last_root != root :,28.117066259517458,28.117066259517458,0.0
"def findMarkForUnitTestNodes(self):     """"""return the position of *all* non-ignored @mark-for-unit-test nodes.""""""     c = self.c     p, result, seen = c.rootPosition(), [], []     while p:         if p.v in seen:             p.moveToNodeAfterTree()         else:             seen.append(p.v)             if g.match_word(p.h, 0, ""@ignore""):                 p.moveToNodeAfterTree()             <MASK>                 result.append(p.copy())                 p.moveToNodeAfterTree()             else:                 p.moveToThreadNext()     return result","elif p . h . startswith ( ""@mark-for-unit-tests"" ) :",if p . h in result :,11.147892272337163,11.147892272337163,0.0
"def startTagFrameset(self, token):     self.parser.parseError(""unexpected-start-tag"", {""name"": ""frameset""})     if len(self.tree.openElements) == 1 or self.tree.openElements[1].name != ""body"":         assert self.parser.innerHTML     elif not self.parser.framesetOK:         pass     else:         <MASK>             self.tree.openElements[1].parent.removeChild(self.tree.openElements[1])         while self.tree.openElements[-1].name != ""html"":             self.tree.openElements.pop()         self.tree.insertElement(token)         self.parser.phase = self.parser.phases[""inFrameset""]",if self . tree . openElements [ 1 ] . parent :,if self . parser . framesetOK :,13.597602315271134,13.597602315271134,0.0
"def try_split(self, split_text: List[str]):     ret = []     for i in split_text:         <MASK>             continue         val = int(i, 2)         if val > 255 or val < 0:             return None         ret.append(val)     if len(ret) != 0:         ret = bytes(ret)         logger.debug(f""binary successful, returning {ret.__repr__()}"")         return ret",if len ( i ) == 0 :,if i == 0 or i == 1 :,16.59038701421971,16.59038701421971,0.0
"def generator(self, data):     for sock in data:         <MASK>             offset = sock.obj_offset         else:             offset = sock.obj_vm.vtop(sock.obj_offset)         yield (             0,             [                 Address(offset),                 int(sock.Pid),                 int(sock.LocalPort),                 int(sock.Protocol),                 str(protos.protos.get(sock.Protocol.v(), ""-"")),                 str(sock.LocalIpAddress),                 str(sock.CreateTime),             ],         )",if not self . _config . PHYSICAL_OFFSET :,if sock . obj_vm is None :,5.3990167242108145,5.3990167242108145,0.0
"def __init__(self, num_bits=4, always_apply=False, p=0.5):     super(Posterize, self).__init__(always_apply, p)     if isinstance(num_bits, (list, tuple)):         <MASK>             self.num_bits = [to_tuple(i, 0) for i in num_bits]         else:             self.num_bits = to_tuple(num_bits, 0)     else:         self.num_bits = to_tuple(num_bits, num_bits)",if len ( num_bits ) == 3 :,if num_bits :,13.607984667977698,13.607984667977698,0.0
"def tearDown(self):     """"""Just in case yn00 creates some junk files, do a clean-up.""""""     del_files = [self.out_file, ""2YN.dN"", ""2YN.dS"", ""2YN.t"", ""rst"", ""rst1"", ""rub""]     for filename in del_files:         <MASK>             os.remove(filename)     if os.path.exists(self.working_dir):         for filename in os.listdir(self.working_dir):             filepath = os.path.join(self.working_dir, filename)             os.remove(filepath)         os.rmdir(self.working_dir)",if os . path . exists ( filename ) :,if os . path . exists ( filename ) :,100.00000000000004,100.00000000000004,1.0
"def reverse_search_history(self, searchfor, startpos=None):     if startpos is None:         startpos = self.history_cursor     if _ignore_leading_spaces:         res = [             (idx, line.lstrip())             for idx, line in enumerate(self.history[startpos:0:-1])             <MASK>         ]     else:         res = [             (idx, line)             for idx, line in enumerate(self.history[startpos:0:-1])             if line.startswith(searchfor)         ]     if res:         self.history_cursor -= res[0][0]         return res[0][1].get_line_text()     return """"",if line . lstrip ( ) . startswith ( searchfor . lstrip ( ) ),if line . startswith ( searchfor ) :,20.843100983925424,20.843100983925424,0.0
"def ComboBoxDroppedHeightTest(windows):     ""Check if each combobox height is the same as the reference""     bugs = []     for win in windows:         if not win.ref:             continue         <MASK>             continue         if win.DroppedRect().height() != win.ref.DroppedRect().height():             bugs.append(                 (                     [                         win,                     ],                     {},                     testname,                     0,                 )             )     return bugs","if win . Class ( ) != ""ComboBox"" or win . ref . Class ( ) != ""ComboBox"" :",if win . ref is None :,2.793802837285314,2.793802837285314,0.0
"def get_changed(self):     if self._is_expression():         result = self._get_node_text(self.ast)         if result == self.source:             return None         return result     else:         collector = codeanalyze.ChangeCollector(self.source)         last_end = -1         for match in self.matches:             start, end = match.get_region()             <MASK>                 if not self._is_expression():                     continue             last_end = end             replacement = self._get_matched_text(match)             collector.add_change(start, end, replacement)         return collector.get_changed()",if start < last_end :,if end > last_end :,43.47208719449914,43.47208719449914,0.0
"def unpickle_from_file(file_path, gzip=False):     """"""Unpickle obj from file_path with gzipping.""""""     with tf.io.gfile.GFile(file_path, ""rb"") as f:         <MASK>             obj = pickle.load(f)         else:             with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:                 obj = pickle.load(gzipf)     return obj",if not gzip :,if gzip :,45.13864405503391,0.0,0.0
"def get_user_context(request, escape=False):     if isinstance(request, HttpRequest):         user = getattr(request, ""user"", None)         result = {""ip_address"": request.META[""REMOTE_ADDR""]}         if user and user.is_authenticated():             result.update(                 {                     ""email"": user.email,                     ""id"": user.id,                 }             )             <MASK>                 result[""name""] = user.name     else:         result = {}     return mark_safe(json.dumps(result))",if user . name :,if escape :,17.799177396293473,0.0,0.0
"def get_item_address(self, item):     """"""Get an item's address as a collection of names""""""     result = []     while True:         name = self.tree_ctrl.GetItemPyData(item)         <MASK>             break         else:             result.insert(0, name)             item = self.tree_ctrl.GetItemParent(item)     return result",if name is None :,if not name :,16.37226966703825,16.37226966703825,0.0
"def closest_unseen(self, row1, col1, filter=None):     # find the closest unseen from this row/col     min_dist = maxint     closest_unseen = None     for row in range(self.height):         for col in range(self.width):             if filter is None or (row, col) not in filter:                 <MASK>                     dist = self.distance(row1, col1, row, col)                     if dist < min_dist:                         min_dist = dist                         closest_unseen = (row, col)     return closest_unseen",if self . map [ row ] [ col ] == UNSEEN :,if closest_unseen is None :,2.8730831956184355,2.8730831956184355,0.0
"def log_graph(self, model: LightningModule, input_array=None):     if self._log_graph:         <MASK>             input_array = model.example_input_array         if input_array is not None:             input_array = model._apply_batch_transfer_handler(input_array)             self.experiment.add_graph(model, input_array)         else:             rank_zero_warn(                 ""Could not log computational graph since the""                 "" `model.example_input_array` attribute is not set""                 "" or `input_array` was not given"",                 UserWarning,             )",if input_array is None :,if input_array is None :,100.00000000000004,100.00000000000004,1.0
"def get_scene_exceptions_by_season(self, season=-1):     scene_exceptions = []     for scene_exception in self.scene_exceptions:         if not len(scene_exception) == 2:             continue         scene_name, scene_season = scene_exception.split(""|"")         <MASK>             scene_exceptions.append(scene_name)     return scene_exceptions",if season == scene_season :,if scene_season == season :,39.28146509005134,39.28146509005134,0.0
def _clean_temp_files():     for pattern in _temp_files:         for path in glob.glob(pattern):             <MASK>                 os.remove(path)             else:                 shutil.rmtree(path),if os . path . islink ( path ) or os . path . isfile ( path ) :,if os . path . isdir ( path ) :,26.753788181976983,26.753788181976983,0.0
"def wait_for_completion(self, job_id, offset, max_results, start_time, timeout):     """"""Wait for job completion and return the first page.""""""     while True:         result = self.get_query_results(             job_id=job_id, page_token=None, start_index=offset, max_results=max_results         )         <MASK>             return result         if (time.time() - start_time) > timeout:             raise Exception(                 ""Timeout: the query doesn't finish within %d seconds."" % timeout             )         time.sleep(1)","if result [ ""jobComplete"" ] :",if result :,11.898417391331403,0.0,0.0
"def get_data(self, element, ranges, style):     <MASK>         groups = element.groupby(element.kdims).items()     else:         groups = [(element.label, element)]     plots = []     axis = ""x"" if self.invert_axes else ""y""     for key, group in groups:         if element.kdims:             label = "","".join([d.pprint_value(v) for d, v in zip(element.kdims, key)])         else:             label = key         data = {axis: group.dimension_values(group.vdims[0]), ""name"": label}         plots.append(data)     return plots",if element . kdims :,if element . label :,42.72870063962342,42.72870063962342,0.0
"def get_files(self, dirname):     if not self._data.has_key(dirname):         self._create(dirname)     else:         new_time = self._changed(dirname)         <MASK>             self._update(dirname, new_time)             dcLog.debug(""==> "" + ""\t\n"".join(self._data[dirname][""flist""]))     return self._data[dirname][""flist""]",if new_time :,if new_time :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, dir):     self.module_names = set()     for name in os.listdir(dir):         <MASK>             self.module_names.add(name[:-3])         elif ""."" not in name:             self.module_names.add(name)","if name . endswith ( "".py"" ) :","if ""-"" in name :",5.545738798841781,5.545738798841781,0.0
"def logic():     for i in range(100):         yield clock.posedge, reset.negedge         <MASK>             count.next = 0         else:             if enable:                 count.next = (count + 1) % n     raise StopSimulation",if reset == ACTIVE_LOW :,if enable :,6.5479514338598115,0.0,0.0
"def sortkeypicker(keynames):     negate = set()     for i, k in enumerate(keynames):         if k[:1] == ""-"":             keynames[i] = k[1:]             negate.add(k[1:])     def getit(adict):         composite = [adict[k] for k in keynames]         for i, (k, v) in enumerate(zip(keynames, composite)):             <MASK>                 composite[i] = -v         return composite     return getit",if k in negate :,if k not in negate :,37.99178428257963,37.99178428257963,0.0
"def show_image(self, wnd_name, img):     if wnd_name in self.named_windows:         <MASK>             self.named_windows[wnd_name] = 1             self.on_create_window(wnd_name)             if wnd_name in self.capture_mouse_windows:                 self.capture_mouse(wnd_name)         self.on_show_image(wnd_name, img)     else:         print(""show_image: named_window "", wnd_name, "" not found."")",if self . named_windows [ wnd_name ] == 0 :,if self . named_windows [ wnd_name ] == 0 :,100.00000000000004,100.00000000000004,1.0
"def check_action_permitted(self):     if (         self._action == ""sts:GetCallerIdentity""     ):  # always allowed, even if there's an explicit Deny for it         return True     policies = self._access_key.collect_policies()     permitted = False     for policy in policies:         iam_policy = IAMPolicy(policy)         permission_result = iam_policy.is_action_permitted(self._action)         if permission_result == PermissionResult.DENIED:             self._raise_access_denied()         <MASK>             permitted = True     if not permitted:         self._raise_access_denied()",elif permission_result == PermissionResult . PERMITTED :,if permission_result == PermissionResult . PENDING :,66.06328636027612,66.06328636027612,0.0
"def _limit_value(key, value, config):     if config[key].get(""upper_limit""):         limit = config[key][""upper_limit""]         # auto handle datetime         if isinstance(value, datetime) and isinstance(limit, timedelta):             if config[key][""inverse""] is True:                 <MASK>                     value = datetime.now() - limit             else:                 if (datetime.now() + limit) < value:                     value = datetime.now() + limit         elif value > limit:             value = limit     return value",if ( datetime . now ( ) - limit ) > value :,if value < limit :,3.0500258614052496,3.0500258614052496,0.0
"def replace_dataset_ids(path, key, value):     """"""Exchanges dataset_ids (HDA, LDA, HDCA, not Dataset) in input_values with dataset ids used in job.""""""     current_case = input_values     if key == ""id"":         for i, p in enumerate(path):             if isinstance(current_case, (list, dict)):                 current_case = current_case[p]         <MASK>             return key, translate_values.get(current_case[""id""], value)     return key, value","if src == current_case . get ( ""src"" ) :",if i == 0 :,4.3074986800341035,4.3074986800341035,0.0
"def load_ext(name, funcs):     ExtModule = namedtuple(""ExtModule"", funcs)     ext_list = []     lib_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))     for fun in funcs:         <MASK>             ext_list.append(extension.load(fun, name, lib_dir=lib_root).op)         else:             ext_list.append(extension.load(fun, name, lib_dir=lib_root).op_)     return ExtModule(*ext_list)","if fun in [ ""nms"" , ""softnms"" ] :",if fun . __name__ == name :,6.8962421077164695,6.8962421077164695,0.0
"def execute_action(self):     selected_actions = self.model_action.get_selected_results_with_index()     if selected_actions and self.args_for_action:         for name, _, act_idx in selected_actions:             try:                 action = self.actions[act_idx]                 <MASK>                     action.act([arg for arg, _, _ in self.args_for_action], self)             except Exception as e:                 debug.log(""execute_action"", e)",if action :,if action :,100.00000000000004,0.0,1.0
"def __getattr__(self, attr):     proxy = self.__proxy     if proxy and hasattr(proxy, attr):         return getattr(proxy, attr)     attrmap = self.__attrmap     if attr in attrmap:         source = attrmap[attr]         <MASK>             value = source()         else:             value = _import_object(source)         setattr(self, attr, value)         self.__log.debug(""loaded lazy attr %r: %r"", attr, value)         return value     raise AttributeError(""'module' object has no attribute '%s'"" % (attr,))",if callable ( source ) :,if source is not None :,10.682175159905853,10.682175159905853,0.0
"def forward(self, x):     # BxT -> BxCxT     x = x.unsqueeze(1)     for conv in self.conv_layers:         residual = x         x = conv(x)         <MASK>             tsz = x.size(2)             r_tsz = residual.size(2)             residual = residual[..., :: r_tsz // tsz][..., :tsz]             x = (x + residual) * self.residual_scale     if self.log_compression:         x = x.abs()         x = x + 1         x = x.log()     return x",if self . skip_connections and x . size ( 1 ) == residual . size ( 1 ) :,if self . log_compression :,3.258679487977817,3.258679487977817,0.0
"def __Prefix_Step2a(self, token):     for prefix in self.__prefix_step2a:         <MASK>             token = token[len(prefix) :]             self.prefix_step2a_success = True             break     return token",if token . startswith ( prefix ) and len ( token ) > 5 :,if token [ : len ( prefix ) ] == prefix :,14.490659526673278,14.490659526673278,0.0
"def is_valid(sample):     if sample is None:         return False     if isinstance(sample, tuple):         for s in sample:             if s is None:                 return False             elif isinstance(s, np.ndarray) and s.size == 0:                 return False             <MASK>                 return False     return True","elif isinstance ( s , collections . abc . Sequence ) and len ( s ) == 0 :","if not isinstance ( s , np . ndarray ) :",11.9069129447401,11.9069129447401,0.0
"def get_all_comments(self, gallery_id, post_no, comment_cnt):     comment_page_cnt = (comment_cnt - 1) // self.options.comments_per_page + 1     comments = []     headers = {""X-Requested-With"": ""XMLHttpRequest""}     data = {""ci_t"": self._session.cookies[""ci_c""], ""id"": gallery_id, ""no"": post_no}     for i in range(comment_page_cnt):         data[""comment_page""] = i + 1         response = self.request_comment(headers, data)         batch = self.parse_comments(response.text)         <MASK>             break         comments = batch + comments     return comments",if not batch :,if not batch :,100.00000000000004,100.00000000000004,1.0
def run_on_module(self):     try:         self.module_base.disable(self.opts.module_spec)     except dnf.exceptions.MarkingErrors as e:         <MASK>             if e.no_match_group_specs or e.error_group_specs:                 raise e             if (                 e.module_depsolv_errors                 and e.module_depsolv_errors[1]                 != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS             ):                 raise e         logger.error(str(e)),if self . base . conf . strict :,if e . module_base :,6.979367151952678,6.979367151952678,0.0
"def find_field_notnull_differ(self, meta, table_description, table_name):     if not self.can_detect_notnull_differ:         return     for field in all_local_fields(meta):         attname = field.db_column or field.attname         <MASK>             continue         null = self.get_field_db_nullable(field, table_name)         if field.null != null:             action = field.null and ""DROP"" or ""SET""             self.add_difference(""notnull-differ"", table_name, attname, action)","if ( table_name , attname ) in self . new_db_fields :",if attname not in self . field_db_table_names :,17.654939729717295,17.654939729717295,0.0
"def _change_moving_module(self, changes, dest):     if not self.source.is_folder():         pymodule = self.pycore.resource_to_pyobject(self.source)         source = self.import_tools.relatives_to_absolutes(pymodule)         pymodule = self.tools.new_pymodule(pymodule, source)         source = self._change_occurrences_in_module(dest, pymodule)         source = self.tools.new_source(pymodule, source)         <MASK>             changes.add_change(ChangeContents(self.source, source))",if source != self . source . read ( ) :,if self . source . is_folder ( ) :,32.03558799120807,32.03558799120807,0.0
"def get(quality_name):     """"""Returns a quality object based on canonical quality name.""""""     found_components = {}     for part in quality_name.lower().split():         component = _registry.get(part)         <MASK>             raise ValueError(""`%s` is not a valid quality string"" % part)         if component.type in found_components:             raise ValueError(                 ""`%s` cannot be defined twice in a quality"" % component.type             )         found_components[component.type] = component     if not found_components:         raise ValueError(""No quality specified"")     result = Quality()     for type, component in found_components.items():         setattr(result, type, component)     return result",if not component :,if not component :,100.00000000000004,100.00000000000004,1.0
def _unselected(self):     selected = self._selected     k = 0     z = selected[k]     k += 1     for i in range(self._n):         if i == z:             <MASK>                 z = selected[k]                 k += 1             else:                 z = -1         else:             yield i,if k < len ( selected ) :,if i == - 1 :,6.770186228657864,6.770186228657864,0.0
"def render_headers(self) -> bytes:     if not hasattr(self, ""_headers""):         parts = [             b""Content-Disposition: form-data; "",             format_form_param(""name"", self.name),         ]         <MASK>             filename = format_form_param(""filename"", self.filename)             parts.extend([b""; "", filename])         if self.content_type is not None:             content_type = self.content_type.encode()             parts.extend([b""\r\nContent-Type: "", content_type])         parts.append(b""\r\n\r\n"")         self._headers = b"""".join(parts)     return self._headers",if self . filename :,if self . filename is not None :,36.55552228545123,36.55552228545123,0.0
"def app_middleware(next, root, info, **kwargs):     app_auth_header = ""HTTP_AUTHORIZATION""     prefix = ""bearer""     request = info.context     if request.path == API_PATH:         if not hasattr(request, ""app""):             request.app = None             auth = request.META.get(app_auth_header, """").split()             <MASK>                 auth_prefix, auth_token = auth                 if auth_prefix.lower() == prefix:                     request.app = SimpleLazyObject(lambda: get_app(auth_token))     return next(root, info, **kwargs)",if len ( auth ) == 2 :,if auth :,5.370784274455332,0.0,0.0
"def _shortest_hypernym_paths(self, simulate_root):     if self.offset == ""00000000"":         return {self: 0}     queue = deque([(self, 0)])     path = {}     while queue:         s, depth = queue.popleft()         <MASK>             continue         path[s] = depth         depth += 1         queue.extend((hyp, depth) for hyp in s._hypernyms())     if simulate_root:         root = Synset(self._wordnet_corpus_reader, None, self.pos(), ""00000000"", """")         path[root] = max(path.values()) + 1     return path",if s in path :,if s == self :,17.965205598154213,17.965205598154213,0.0
"def _populate_class_variables():     lookup = {}     reverse_lookup = {}     characters_for_re = []     for codepoint, name in list(codepoint2name.items()):         character = chr(codepoint)         <MASK>             # There's no point in turning the quotation mark into             # &quot;, unless it happens within an attribute value, which             # is handled elsewhere.             characters_for_re.append(character)             lookup[character] = name         # But we do want to turn &quot; into the quotation mark.         reverse_lookup[name] = character     re_definition = ""[%s]"" % """".join(characters_for_re)     return lookup, reverse_lookup, re.compile(re_definition)",if codepoint != 34 :,if character in lookup :,10.400597689005304,10.400597689005304,0.0
"def prepare_data_status(self, view: sublime.View, data: Dict[str, Any]) -> Any:     """"""Prepare the returned data for status""""""     if (         data[""success""]         and ""No docstring"" not in data[""doc""]         and data[""doc""] != ""list\n""     ):         self.signature = data[""doc""]         <MASK>             return         try:             self.signature = self.signature.splitlines()[2]         except KeyError:             return         return self._show_status(view)",if self . _signature_excluded ( self . signature ) :,if not self . signature :,10.690302152100664,10.690302152100664,0.0
"def _setup_once_tables(cls):     if cls.run_define_tables == ""once"":         cls.define_tables(cls.metadata)         <MASK>             cls.metadata.create_all(cls.bind)         cls.tables.update(cls.metadata.tables)","if cls . run_create_tables == ""once"" :","if cls . run_define_tables == ""once"" :",78.25422900366432,78.25422900366432,0.0
"def _send_recursive(self, files):     for base in files:         <MASK>             # filename mixed into the bunch             self._send_files([base])             continue         last_dir = asbytes(base)         for root, dirs, fls in os.walk(base):             self._chdir(last_dir, asbytes(root))             self._send_files([os.path.join(root, f) for f in fls])             last_dir = asbytes(root)         # back out of the directory         for i in range(len(os.path.split(last_dir))):             self._send_popd()",if not os . path . isdir ( base ) :,if not os . path . isdir ( base ) :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, *args, **kwargs):     super().__init__(*args, **kwargs)     # Automatically register models if required.     if not is_registered(self.model):         inline_fields = ()         for inline in self.inlines:             inline_model, follow_field = self._reversion_introspect_inline_admin(inline)             if inline_model:                 self._reversion_autoregister(inline_model, ())             <MASK>                 inline_fields += (follow_field,)         self._reversion_autoregister(self.model, inline_fields)",if follow_field :,if follow_field :,100.00000000000004,100.00000000000004,1.0
"def dispatch_hook(key, hooks, hook_data, **kwargs):     """"""Dispatches a hook dictionary on a given piece of data.""""""     hooks = hooks or dict()     hooks = hooks.get(key)     if hooks:         if hasattr(hooks, ""__call__""):             hooks = [hooks]         for hook in hooks:             _hook_data = hook(hook_data, **kwargs)             <MASK>                 hook_data = _hook_data     return hook_data",if _hook_data is not None :,if _hook_data :,48.23560797692261,48.23560797692261,0.0
"def __call__(self, image, crop=True):     if isinstance(image, PTensor):         return self.crop_to_output(             numpy_to_paddle(self(paddle_to_numpy(image), crop=False))         )     else:         warp = cv.warpAffine(             image,             self.transform_matrix,             image.shape[1::-1],             borderMode=cv.BORDER_REPLICATE,         )         <MASK>             return self.crop_to_output(warp)         else:             return warp",if crop :,if crop :,100.00000000000004,0.0,1.0
"def _analyze(self):     lines = open(self.log_path, ""r"").readlines()     prev_line = None     for line in lines:         <MASK>             self.errors.append(line[len(""ERROR:"") :].strip())         elif line.startswith(""FAIL:"") and prev_line and prev_line.startswith(""=""):             self.failures.append(line[len(""FAIL:"") :].strip())         prev_line = line","if line . startswith ( ""ERROR:"" ) and prev_line and prev_line . startswith ( ""="" ) :","if line . startswith ( ""ERROR:"" ) and prev_line . startswith ( ""="" ) :",83.37529180751808,83.37529180751808,0.0
"def end(self, name):     self.soup.endData()     completed_tag = self.soup.tagStack[-1]     namespace, name = self._getNsTag(name)     nsprefix = None     if namespace is not None:         for inverted_nsmap in reversed(self.nsmaps):             <MASK>                 nsprefix = inverted_nsmap[namespace]                 break     self.soup.handle_endtag(name, nsprefix)     if len(self.nsmaps) > 1:         # This tag, or one of its parents, introduced a namespace         # mapping, so pop it off the stack.         self.nsmaps.pop()",if inverted_nsmap is not None and namespace in inverted_nsmap :,"if completed_tag == ""namespace"" :",3.9778149665594618,3.9778149665594618,0.0
"def _bind_parameters(operation, parameters):     # inspired by MySQL Python Connector (conversion.py)     string_parameters = {}     for (name, value) in parameters.iteritems():         if value is None:             string_parameters[name] = ""NULL""         <MASK>             string_parameters[name] = ""'"" + _escape(value) + ""'""         else:             string_parameters[name] = str(value)     return operation % string_parameters","elif isinstance ( value , basestring ) :",if _escape ( name ) :,12.600736402830258,12.600736402830258,0.0
"def plugin_on_song_ended(self, song, skipped):     if song is not None:         rating = song(""~#rating"")         invrating = 1.0 - rating         delta = min(rating, invrating) / 2.0         <MASK>             rating -= delta         else:             rating += delta         song[""~#rating""] = rating",if skipped :,if delta > 0 :,12.703318703865365,12.703318703865365,0.0
"def on_activated_async(self, view):     if settings[""modified_lines_only""]:         self.freeze_last_version(view)     if settings[""enabled""]:         match_trailing_spaces(view)         # continuously watch view for changes to the visible region         <MASK>             # track             active_views[view.id()] = view.visible_region()             self.update_on_region_change(view)",if not view . id ( ) in active_views :,"if settings [ ""enabled"" ] :",3.983253478176822,3.983253478176822,0.0
"def _notin_text(term, text, verbose=False):     index = text.find(term)     head = text[:index]     tail = text[index + len(term) :]     correct_text = head + tail     diff = _diff_text(correct_text, text, verbose)     newdiff = [u(""%s is contained here:"") % py.io.saferepr(term, maxsize=42)]     for line in diff:         <MASK>             continue         if line.startswith(u(""- "")):             continue         if line.startswith(u(""+ "")):             newdiff.append(u(""  "") + line[2:])         else:             newdiff.append(line)     return newdiff","if line . startswith ( u ( ""Skipping"" ) ) :",if not line :,2.215745752614824,2.215745752614824,0.0
"def delete_all(path):     ppath = os.getcwd()     os.chdir(path)     for fn in glob.glob(""*""):         fn_full = os.path.join(path, fn)         if os.path.isdir(fn):             delete_all(fn_full)         elif fn.endswith("".png""):             os.remove(fn_full)         <MASK>             os.remove(fn_full)         elif DELETE_ALL_OLD:             os.remove(fn_full)     os.chdir(ppath)     os.rmdir(path)","elif fn . endswith ( "".md"" ) :","if fn . endswith ( "".png"" ) :",58.77283725105324,58.77283725105324,0.0
"def reward(self):     """"""Returns a tuple of sum of raw and processed rewards.""""""     raw_rewards, processed_rewards = 0, 0     for ts in self.time_steps:         # NOTE: raw_reward and processed_reward are None for the first time-step.         if ts.raw_reward is not None:             raw_rewards += ts.raw_reward         <MASK>             processed_rewards += ts.processed_reward     return raw_rewards, processed_rewards",if ts . processed_reward is not None :,if ts . processed_reward is not None :,100.00000000000004,100.00000000000004,1.0
"def formatmonthname(self, theyear, themonth, withyear=True):     with TimeEncoding(self.locale) as encoding:         s = month_name[themonth]         <MASK>             s = s.decode(encoding)         if withyear:             s = ""%s %s"" % (s, theyear)         return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s",if encoding is not None :,if encoding :,23.174952587773145,0.0,0.0
"def check_digest_auth(user, passwd):     """"""Check user authentication using HTTP Digest auth""""""     if request.headers.get(""Authorization""):         credentails = parse_authorization_header(request.headers.get(""Authorization""))         <MASK>             return         response_hash = response(             credentails,             passwd,             dict(                 uri=request.script_root + request.path,                 body=request.data,                 method=request.method,             ),         )         if credentails.get(""response"") == response_hash:             return True     return False",if not credentails :,"if credentails . get ( ""user"" ) == user :",4.065425428798724,4.065425428798724,0.0
"def wrapped(self, request):     try:         return self._finished     except AttributeError:         <MASK>             if not request.session.shouldfail and not request.session.shouldstop:                 log.debug(                     ""%s is still going to be used, not terminating it. ""                     ""Still in use on:\n%s"",                     self,                     pprint.pformat(list(self.node_ids)),                 )                 return         log.debug(""Finish called on %s"", self)         try:             return func(request)         finally:             self._finished = True",if self . node_ids :,if self . node_ids :,100.00000000000004,100.00000000000004,1.0
"def run_tests():     # type: () -> None     x = 5     with switch(x) as case:         if case(0):             print(""zero"")             print(""zero"")         elif case(1, 2):             print(""one or two"")         <MASK>             print(""three or four"")         else:             print(""default"")             print(""another"")","elif case ( 3 , 4 ) :","if case ( 2 , 3 ) :",17.286039232097043,17.286039232097043,0.0
"def task_done(self):     with self._cond:         <MASK>             raise ValueError(""task_done() called too many times"")         if self._unfinished_tasks._semlock._is_zero():             self._cond.notify_all()",if not self . _unfinished_tasks . acquire ( False ) :,if self . _unfinished_tasks . _semlock . _is_zero ( ) :,38.05371078682543,38.05371078682543,0.0
"def _set_uid(self, val):     if val is not None:         <MASK>             self.bus.log(""pwd module not available; ignoring uid."", level=30)             val = None         elif isinstance(val, text_or_bytes):             val = pwd.getpwnam(val)[2]     self._uid = val",if pwd is None :,if val is None :,42.72870063962342,42.72870063962342,0.0
"def process_tag(hive_name, company, company_key, tag, default_arch):     with winreg.OpenKeyEx(company_key, tag) as tag_key:         version = load_version_data(hive_name, company, tag, tag_key)         if version is not None:  # if failed to get version bail             major, minor, _ = version             arch = load_arch_data(hive_name, company, tag, tag_key, default_arch)             if arch is not None:                 exe_data = load_exe(hive_name, company, company_key, tag)                 <MASK>                     exe, args = exe_data                     return company, major, minor, arch, exe, args",if exe_data is not None :,if exe_data is not None :,100.00000000000004,100.00000000000004,1.0
"def run(algs):     for alg in algs:         vcs = alg.get(""variantcaller"")         if vcs:             if isinstance(vcs, dict):                 vcs = reduce(operator.add, vcs.values())             <MASK>                 vcs = [vcs]             return any(vc.startswith(prefix) for vc in vcs if vc)","if not isinstance ( vcs , ( list , tuple ) ) :",if len ( vcs ) > 1 :,7.433761660133445,7.433761660133445,0.0
"def wrapper(self, *args, **kwargs):     if not self.request.path.endswith(""/""):         <MASK>             uri = self.request.path + ""/""             if self.request.query:                 uri += ""?"" + self.request.query             self.redirect(uri, permanent=True)             return         raise HTTPError(404)     return method(self, *args, **kwargs)","if self . request . method in ( ""GET"" , ""HEAD"" ) :",if not self . request . path :,12.421298901804239,12.421298901804239,0.0
"def check_response(self, response):     """"""Specialized version of check_response().""""""     for line in response:         # Skip blank lines:         <MASK>             continue         if line.startswith(b""OK""):             return         elif line.startswith(b""Benutzer/Passwort Fehler""):             raise BadLogin(line)         else:             raise FailedPost(""Server returned '%s'"" % six.ensure_text(line))",if not line . strip ( ) :,if not line :,23.50540321304655,23.50540321304655,0.0
"def Walk(self, hMenu=None):     if not hMenu:         hMenu = self.handle     n = user32.GetMenuItemCount(hMenu)     mi = MENUITEMINFO()     for i in range(n):         mi.fMask = 2  #  MIIM_ID         user32.GetMenuItemInfoA(hMenu, i, 1, byref(mi))         handle = user32.GetSubMenu(hMenu, i)         <MASK>             yield handle, self.ListItems(handle)             for i in self.Walk(handle):                 yield i",if handle :,if handle :,100.00000000000004,0.0,1.0
"def setSelection(self, labels):     input = self.__validateInput(labels)     if len(input) == 0 and not self.__allowEmptySelection:         return     if self.__allowMultipleSelection:         self.__selectedLabels[:] = input         self.__selectionChanged()     else:         <MASK>             raise RuntimeError(                 ""Parameter must be single item or a list with one element.""             )         else:             self.__selectedLabels[:] = input             self.__selectionChanged()     # Remove all selected labels that are not in the menu, emit signals if necessary and update the button.     self.__validateState()",if len ( input ) > 1 :,if len ( input ) > 1 :,100.00000000000004,100.00000000000004,1.0
"def _parse(self, engine):     """"""Parse the layer.""""""     if isinstance(self.args, dict):         if ""axis"" in self.args:             self.axis = engine.evaluate(self.args[""axis""], recursive=True)             if not isinstance(self.axis, int):                 raise ParsingError('""axis"" must be an integer.')         if ""momentum"" in self.args:             self.momentum = engine.evaluate(self.args[""momentum""], recursive=True)             <MASK>                 raise ParsingError('""momentum"" must be numeric.')","if not isinstance ( self . momentum , ( int , float ) ) :","if not isinstance ( self . momentum , int ) :",52.81951634615037,52.81951634615037,0.0
"def get_order(self, aBuf):     if not aBuf:         return -1, 1     # find out current char's byte length     first_char = wrap_ord(aBuf[0])     if (0x81 <= first_char <= 0x9F) or (0xE0 <= first_char <= 0xFC):         charLen = 2     else:         charLen = 1     # return its order if it is hiragana     if len(aBuf) > 1:         second_char = wrap_ord(aBuf[1])         <MASK>             return second_char - 0x9F, charLen     return -1, charLen",if ( first_char == 202 ) and ( 0x9F <= second_char <= 0xF1 ) :,if second_char > 0x9F :,3.4106484601338583,3.4106484601338583,0.0
"def saveSpecial(self, **kwargs):     for kw in SPECIAL_BOOL_LIST + SPECIAL_VALUE_LIST + SPECIAL_LIST_LIST:         item = config.get_config(""misc"", kw)         value = kwargs.get(kw)         msg = item.set(value)         <MASK>             return badParameterResponse(msg)     config.save_config()     raise Raiser(self.__root)",if msg :,if msg :,100.00000000000004,0.0,1.0
"def sanitize_event_keys(kwargs, valid_keys):     # Sanity check: Don't honor keys that we don't recognize.     for key in list(kwargs.keys()):         <MASK>             kwargs.pop(key)     # Truncate certain values over 1k     for key in [""play"", ""role"", ""task"", ""playbook""]:         if isinstance(kwargs.get(""event_data"", {}).get(key), str):             if len(kwargs[""event_data""][key]) > 1024:                 kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars(                     1024                 )",if key not in valid_keys :,if key in valid_keys :,61.29752413741059,61.29752413741059,0.0
"def toggleFactorReload(self, value=None):     self.serviceFittingOptions[""useGlobalForceReload""] = (         value         if value is not None         else not self.serviceFittingOptions[""useGlobalForceReload""]     )     fitIDs = set()     for fit in set(self._loadedFits):         <MASK>             continue         if fit.calculated:             fit.factorReload = self.serviceFittingOptions[""useGlobalForceReload""]             fit.clearFactorReloadDependentData()             fitIDs.add(fit.ID)     return fitIDs",if fit is None :,if fit . ID not in fitIDs :,12.22307556087252,12.22307556087252,0.0
"def closest_unseen(self, row1, col1, filter=None):     # find the closest unseen from this row/col     min_dist = maxint     closest_unseen = None     for row in range(self.height):         for col in range(self.width):             <MASK>                 if self.map[row][col] == UNSEEN:                     dist = self.distance(row1, col1, row, col)                     if dist < min_dist:                         min_dist = dist                         closest_unseen = (row, col)     return closest_unseen","if filter is None or ( row , col ) not in filter :",if filter is not None :,9.471153562668167,9.471153562668167,0.0
"def getAlphaClone(lookfor, eager=None):     if isinstance(lookfor, int):         <MASK>             item = get_gamedata_session().query(AlphaClone).get(lookfor)         else:             item = (                 get_gamedata_session()                 .query(AlphaClone)                 .options(*processEager(eager))                 .filter(AlphaClone.ID == lookfor)                 .first()             )     else:         raise TypeError(""Need integer as argument"")     return item",if eager is None :,if eager :,32.34325178227722,0.0,0.0
"def _rle_encode(string):     new = b""""     count = 0     for cur in string:         if not cur:             count += 1         else:             <MASK>                 new += b""\0"" + bytes([count])                 count = 0             new += bytes([cur])     return new",if count :,if count < len ( string ) :,12.22307556087252,12.22307556087252,0.0
def result_iterator():     try:         for future in fs:             <MASK>                 yield future.result()             else:                 yield future.result(end_time - time.time())     finally:         for future in fs:             future.cancel(),if timeout is None :,if future . done ( ) :,7.809849842300637,7.809849842300637,0.0
"def _individual_get(self, segment, index_type, index, strictdoc):     if index_type == ""val"":         for key, value in segment.items():             <MASK>                 return value             if hasattr(key, ""text""):                 if key.text == index[0]:                     return value         raise Exception(""Invalid state"")     elif index_type == ""index"":         return segment[index]     elif index_type == ""textslice"":         return segment[index[0] : index[1]]     elif index_type == ""key"":         return index[1] if strictdoc else index[0]     else:         raise Exception(""Invalid state"")",if key == index [ 0 ] :,if key . text == index [ 0 ] :,63.15552371794039,63.15552371794039,0.0
"def _reset_sequences(self, db_name):     conn = connections[db_name]     if conn.features.supports_sequence_reset:         sql_list = conn.ops.sequence_reset_by_name_sql(             no_style(), conn.introspection.sequence_list()         )         <MASK>             try:                 cursor = conn.cursor()                 for sql in sql_list:                     cursor.execute(sql)             except Exception:                 transaction.rollback_unless_managed(using=db_name)                 raise             transaction.commit_unless_managed(using=db_name)",if sql_list :,if not sql_list :,53.7284965911771,53.7284965911771,0.0
"def translate_to_statements(self, statements, conditional_write_vars):     lines = []     for stmt in statements:         <MASK>             self.temporary_vars.add((stmt.var, stmt.dtype))         line = self.translate_statement(stmt)         if stmt.var in conditional_write_vars:             subs = {}             condvar = conditional_write_vars[stmt.var]             lines.append(""if %s:"" % condvar)             lines.append(indent(line))         else:             lines.append(line)     return lines","if stmt . op == "":="" and not stmt . var in self . variables :","if stmt . dtype == ""boolean"" :",10.980266522628492,10.980266522628492,0.0
"def _bytecode_filenames(self, py_filenames):     bytecode_files = []     for py_file in py_filenames:         # Since build_py handles package data installation, the         # list of outputs can contain more than just .py files.         # Make sure we only report bytecode for the .py files.         ext = os.path.splitext(os.path.normcase(py_file))[1]         if ext != PYTHON_SOURCE_EXTENSION:             continue         <MASK>             bytecode_files.append(py_file + ""c"")         if self.optimize > 0:             bytecode_files.append(py_file + ""o"")     return bytecode_files",if self . compile :,if self . optimize > 0 :,26.269098944241588,26.269098944241588,0.0
"def logic():     for i in range(100):         yield clock.posedge, reset.negedge         if reset == ACTIVE_LOW:             count.next = 0         else:             <MASK>                 count.next = (count + 1) % n     raise StopSimulation",if enable :,if reset == ACTIVE_HIGH :,6.567274736060395,6.567274736060395,0.0
"def _is_subnet_of(a, b):     try:         # Always false if one is v4 and the other is v6.         <MASK>             raise TypeError(""%s and %s are not of the same version"" % (a, b))         return (             b.network_address <= a.network_address             and b.broadcast_address >= a.broadcast_address         )     except AttributeError:         raise TypeError(             ""Unable to test subnet containment "" ""between %s and %s"" % (a, b)         )",if a . _version != b . _version :,if a . version != b . version :,47.269442068339785,47.269442068339785,0.0
"def _filter_paths(basename, path, is_dir, exclude):     """""".gitignore style file filtering.""""""     for item in exclude:         # Items ending in '/' apply only to directories.         if item.endswith(""/"") and not is_dir:             continue         # Items starting with '/' apply to the whole path.         # In any other cases just the basename is used.         match = path if item.startswith(""/"") else basename         <MASK>             return True     return False","if fnmatch . fnmatch ( match , item . strip ( ""/"" ) ) :",if match == basename :,1.707863452144561,1.707863452144561,0.0
"def __recv_null(self):     """"""Receive a null byte.""""""     while 1:         c = self.sock.recv(1)         if c == """":             self.close()             raise EOFError(""Socket Closed"")         <MASK>             return","if c == ""\0"" :",if c == 0 :,34.1077254951379,34.1077254951379,0.0
"def onMessage(self, payload, isBinary):     if isBinary:         self.result = ""Expected text message with payload, but got binary.""     else:         <MASK>             self.result = (                 ""Expected text message with payload of length %d, but got %d.""                 % (self.DATALEN, len(payload))             )         else:             ## FIXME : check actual content             ##             self.behavior = Case.OK             self.result = ""Received text message of length %d."" % len(payload)     self.p.createWirelog = True     self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)",if len ( payload ) != self . DATALEN :,if self . DATALEN :,21.297646969725616,21.297646969725616,0.0
"def rename_path(self, path, new_path):     logger.debug(""rename_path '%s' -> '%s'"" % (path, new_path))     dirs = self.readdir(path)     for d in dirs:         <MASK>             continue         d_path = """".join([path, ""/"", d])         d_new_path = """".join([new_path, ""/"", d])         attr = self.getattr(d_path)         if stat.S_ISDIR(attr[""st_mode""]):             self.rename_path(d_path, d_new_path)         else:             self.rename_item(d_path, d_new_path)     self.rename_item(path, new_path, dir=True)","if d in [ ""."" , "".."" ] :","if not stat . S_ISDIR ( attr [ ""st_mode"" ] ) :",6.986768364373987,6.986768364373987,0.0
"def dir_box_click(self, double):     if double:         name = self.list_box.get_selected_name()         path = os.path.join(self.directory, name)         suffix = os.path.splitext(name)[1]         <MASK>             self.directory = path         else:             self.double_click_file(name)     self.update()",if suffix not in self . suffixes and os . path . isdir ( path ) :,"if suffix == "".txt"" :",4.661841620661271,4.661841620661271,0.0
"def __getattr__(self, key):     try:         value = self.__parent.contents[key]     except KeyError:         pass     else:         if value is not None:             <MASK>                 return value.mod_ns             else:                 assert isinstance(value, _MultipleClassMarker)                 return value.attempt_get(self.__parent.path, key)     raise AttributeError(         ""Module %r has no mapped classes ""         ""registered under the name %r"" % (self.__parent.name, key)     )","if isinstance ( value , _ModuleMarker ) :","if isinstance ( value , _NamespaceMarker ) :",66.06328636027612,66.06328636027612,0.0
"def poll_thread():     time.sleep(0.5)     if process.wait() and process_state:         time.sleep(0.25)         <MASK>             stdout, stderr = process._communicate(None)             logger.error(                 ""Web server process exited unexpectedly"",                 ""app"",                 stdout=stdout,                 stderr=stderr,             )             time.sleep(1)             restart_server(1)",if not check_global_interrupt ( ) :,if process . returncode == 0 :,5.11459870708889,5.11459870708889,0.0
"def apply_dateparser_timezone(utc_datetime, offset_or_timezone_abb):     for name, info in _tz_offsets:         <MASK>             tz = StaticTzInfo(name, info[""offset""])             return utc_datetime.astimezone(tz)","if info [ ""regex"" ] . search ( "" %s"" % offset_or_timezone_abb ) :",if offset_or_timezone_abb :,14.601126233589126,14.601126233589126,0.0
"def _load_wordlist(filename):     if filename is None:         return {}     path = None     for dir in (CONFIG_DIR, ASSETS_DIR):         path = os.path.realpath(os.path.join(dir, filename))         <MASK>             break     words = {}     with open(path, encoding=""utf-8"") as f:         pairs = [word.strip().rsplit("" "", 1) for word in f]         pairs.sort(reverse=True, key=lambda x: int(x[1]))         words = {p[0]: int(p[1]) for p in pairs}     return words",if os . path . exists ( path ) :,if not path :,4.690733795095046,4.690733795095046,0.0
"def terminate_processes_matching_names(match_strings, kill=False):     """"""Terminates processes matching particular names (case sensitive).""""""     if isinstance(match_strings, str):         match_strings = [match_strings]     for process in psutil.process_iter():         try:             process_info = process.as_dict(attrs=[""name"", ""pid""])             process_name = process_info[""name""]         except (psutil.AccessDenied, psutil.NoSuchProcess, OSError):             continue         <MASK>             terminate_process(process_info[""pid""], kill)",if any ( x == process_name for x in match_strings ) :,if process_name in match_strings :,18.897243358570165,18.897243358570165,0.0
"def has_scheme(self, inp):     if ""://"" in inp:         return True     else:         authority = inp.replace(""/"", ""#"").replace(""?"", ""#"").split(""#"")[0]         <MASK>             _, host_or_port = authority.split("":"", 1)             # Assert it's not a port number             if re.match(r""^\d+$"", host_or_port):                 return False         else:             return False     return True","if "":"" in authority :",if authority :,16.605579150202516,0.0,0.0
"def close(self):     with BrowserContext._BROWSER_LOCK:         BrowserContext._BROWSER_REFCNT -= 1         <MASK>             logger.info(""Destroying browser main loop"")             BrowserContext._BROWSER_LOOP.destroy()             BrowserContext._BROWSER_LOOP = None",if BrowserContext . _BROWSER_REFCNT == 0 :,if BrowserContext . _BROWSER_REFCNT > 0 :,63.981667416455416,63.981667416455416,0.0
"def _mock_get_merge_ticks(self, order_book_id_list, trading_date, last_dt=None):     for tick in self._ticks:         <MASK>             continue         if (             self.env.data_proxy.get_future_trading_date(tick.datetime).date()             != trading_date.date()         ):             continue         if last_dt and tick.datetime <= last_dt:             continue         yield tick",if tick . order_book_id not in order_book_id_list :,if tick . order_book_id_list != order_book_id_list :,69.3395566222006,69.3395566222006,0.0
"def messageSourceStamps(self, source_stamps):     text = """"     for ss in source_stamps:         source = """"         <MASK>             source += ""[branch %s] "" % ss[""branch""]         if ss[""revision""]:             source += str(ss[""revision""])         else:             source += ""HEAD""         if ss[""patch""] is not None:             source += "" (plus patch)""         discriminator = """"         if ss[""codebase""]:             discriminator = "" '%s'"" % ss[""codebase""]         text += ""Build Source Stamp%s: %s\n"" % (discriminator, source)     return text","if ss [ ""branch"" ] :","if ss [ ""branch"" ] :",100.00000000000004,100.00000000000004,1.0
"def test_open_read_bytes(self, sftp):     """"""Test reading bytes from a file""""""     f = None     try:         self._create_file(""file"", ""xxx"")         f = yield from sftp.open(""file"", ""rb"")         self.assertEqual((yield from f.read()), b""xxx"")     finally:         <MASK>  # pragma: no branch             yield from f.close()         remove(""file"")",if f :,if f :,100.00000000000004,0.0,1.0
"def handler(chan, host, port):     sock = socket()     try:         sock.connect((host, port))     except Exception as e:         if verbose == True:             print(e)         return     while True:         r, w, x = select.select([sock, chan], [], [])         if sock in r:             data = sock.recv(1024)             if len(data) == 0:                 break             chan.send(data)         <MASK>             data = chan.recv(1024)             if len(data) == 0:                 break             sock.send(data)     chan.close()     sock.close()",if chan in r :,if chan in w :,42.72870063962342,42.72870063962342,0.0
"def detect(get_page):     retval = False     for vector in WAF_ATTACK_VECTORS:         page, headers, code = get_page(get=vector)         retval = re.search(r""url\('/ks-waf-error\.png'\)"", page, re.I) is not None         <MASK>             break     return retval",if retval :,if retval :,100.00000000000004,0.0,1.0
"def __init__(self, raw):     ticker_ticks = {}     for tick in raw[""results""]:         <MASK>             ticker_ticks[tick[""T""]].append(tick)         else:             ticker_ticks[tick[""T""]] = [tick]     super().__init__(         {ticker: Aggsv2({""results"": ticks}) for ticker, ticks in ticker_ticks.items()}     )","if ticker_ticks . get ( tick [ ""T"" ] ) :","if tick [ ""T"" ] in ticker_ticks :",43.48783281197403,43.48783281197403,0.0
"def _makefiles(self, f):     if isinstance(f, dict):         for k, v in list(f.items()):             <MASK>                 self.makedir(dirname=k, content=v)             elif isinstance(v, str):                 self.make_file(filename=k, content=v)             else:  # pragma: nocover                 raise ValueError(""Unexpected:"", k, v)     elif isinstance(f, str):         self._make_empty_file(f)     elif isinstance(f, list):         self.make_list(f)     else:  # pragma: nocover         raise ValueError(""Unknown type:"", f)","if isinstance ( v , list ) :","if isinstance ( v , ( str , unicode ) ) :",36.462858619364674,36.462858619364674,0.0
"def migrate_command_storage(apps, schema_editor):     model = apps.get_model(""terminal"", ""CommandStorage"")     init_storage_data(model)     setting = get_setting(apps, schema_editor, ""TERMINAL_COMMAND_STORAGE"")     if not setting:         return     values = get_storage_data(setting)     for name, meta in values.items():         tp = meta.pop(""TYPE"")         <MASK>             continue         model.objects.create(name=name, type=tp, meta=meta)","if not tp or name in [ ""default"" , ""null"" ] :",if tp is None :,1.5577298727187734,1.5577298727187734,0.0
"def build_vertices(self, ulines):     vertex_idx = 0     vertices = collections.OrderedDict()     for line in ulines:         for vt in line:             <MASK>                 continue             new_vertex = (vt.u, vt.v, 0.0)             if new_vertex in vertices:                 continue             vt.index = vertex_idx             vertex_idx += 1             vertices[new_vertex] = 1     return vertex_idx, list(vertices.keys())",if vt . replacement is not None :,if vt . index == vertex_idx :,16.784459625186194,16.784459625186194,0.0
"def get_quarantine_count(self):     """"""get obj/container/account quarantine counts""""""     qcounts = {""objects"": 0, ""containers"": 0, ""accounts"": 0}     qdir = ""quarantined""     for device in os.listdir(self.devices):         for qtype in qcounts:             qtgt = os.path.join(self.devices, device, qdir, qtype)             if os.path.exists(qtgt):                 linkcount = os.lstat(qtgt).st_nlink                 <MASK>                     qcounts[qtype] += linkcount - 2     return qcounts",if linkcount > 2 :,if linkcount > 2 :,100.00000000000004,100.00000000000004,1.0
"def _format_arg(self, name, trait_spec, value):     if name == ""mask_file"":         return """"     if name == ""op_string"":         <MASK>             if isdefined(self.inputs.mask_file):                 return self.inputs.op_string % self.inputs.mask_file             else:                 raise ValueError(""-k %s option in op_string requires mask_file"")     return super(ImageStats, self)._format_arg(name, trait_spec, value)","if ""-k %s"" in self . inputs . op_string :",if self . inputs . op_string :,45.64995457685804,45.64995457685804,0.0
"def _update_theme_style(self, *args):     self.line_color_normal = self.theme_cls.divider_color     if not any([self.error, self._text_len_error]):         if not self.focus:             self._current_hint_text_color = self.theme_cls.disabled_hint_text_color             self._current_right_lbl_color = self.theme_cls.disabled_hint_text_color             <MASK>                 self._current_error_color = self.theme_cls.disabled_hint_text_color","if self . helper_text_mode == ""persistent"" :",if self . focus :,7.063006710882745,7.063006710882745,0.0
"def createFields(self):     for item in self.format:         <MASK>             yield item[0](self, *item[1:-1], **item[-1])         else:             yield item[0](self, *item[1:])","if isinstance ( item [ - 1 ] , dict ) :","if item [ 0 ] == ""field"" :",8.606119900909883,8.606119900909883,0.0
"def execute(self, statement, arguments=None):     while True:         try:             <MASK>                 self.cursor.execute(statement, arguments)             else:                 self.cursor.execute(statement)         except sqlite3.OperationalError as ex:             if ""locked"" not in getSafeExString(ex):                 raise         else:             break     if statement.lstrip().upper().startswith(""SELECT""):         return self.cursor.fetchall()",if arguments :,if arguments :,100.00000000000004,0.0,1.0
"def set_income_account_for_fixed_assets(self):     disposal_account = depreciation_cost_center = None     for d in self.get(""items""):         <MASK>             if not disposal_account:                 (                     disposal_account,                     depreciation_cost_center,                 ) = get_disposal_account_and_cost_center(self.company)             d.income_account = disposal_account             if not d.cost_center:                 d.cost_center = depreciation_cost_center",if d . is_fixed_asset :,if d . income_account :,20.873176328735713,20.873176328735713,0.0
"def _convertNbCharsInNbBits(self, nbChars):     nbMinBit = None     nbMaxBit = None     if nbChars is not None:         if isinstance(nbChars, int):             nbMinBit = nbChars * 8             nbMaxBit = nbMinBit         else:             if nbChars[0] is not None:                 nbMinBit = nbChars[0] * 8             <MASK>                 nbMaxBit = nbChars[1] * 8     return (nbMinBit, nbMaxBit)",if nbChars [ 1 ] is not None :,if nbChars [ 1 ] is not None :,100.00000000000004,100.00000000000004,1.0
"def _get_service_full_name(self, name, help_command_table):     if help_command_table and name not in self._NON_SERVICE_COMMANDS:         <MASK>             return self._HIGH_LEVEL_SERVICE_FULL_NAMES[name]         service = help_command_table.get(name)         if service:             return service.service_model.metadata[""serviceFullName""]",if name in self . _HIGH_LEVEL_SERVICE_FULL_NAMES :,if name in self . _HIGH_LEVEL_SERVICE_FULL_NAMES :,100.00000000000004,100.00000000000004,1.0
"def print_addresses(self):     p = 3     tmp_str = ""[""     if self.get_len() >= 7:  # at least one complete IP address         while 1:             <MASK>                 tmp_str += ""#""             tmp_str += self.get_ip_address(p)             p += 4             if p >= self.get_len():                 break             else:                 tmp_str += "", ""     tmp_str += ""] ""     if self.get_ptr() % 4:  # ptr field should be a multiple of 4         tmp_str += ""nonsense ptr field: %d "" % self.get_ptr()     return tmp_str",if p + 1 == self . get_ptr ( ) :,if p < self . get_len ( ) :,28.790785149783648,28.790785149783648,0.0
"def run(self):     for _ in range(self.n):         error = True         try:             self.collection.insert_one({""test"": ""insert""})             error = False         except:             if not self.expect_exception:                 raise         <MASK>             assert error",if self . expect_exception :,if self . expect_exception :,100.00000000000004,100.00000000000004,1.0
"def create_composite_mounter_by_args(args):     """"""Creates a CompositeMounter by the images in given args.""""""     logging.info(""Mount images..."")     mounter = composite_mounter.CompositeMounter()     for partition in composite_mounter.SUPPORTED_PARTITIONS:         image_source = vars(args)[partition]         <MASK>             logging.info(""  %s=%s"", partition, image_source)             mounter.add_by_mount_target(partition, image_source)     if mounter.is_empty():         raise RuntimeError(""Must give at least one image source."")     return mounter",if image_source :,if image_source :,100.00000000000004,100.00000000000004,1.0
"def _get_containing_class(self, pyname):     if isinstance(pyname, pynames.DefinedName):         scope = pyname.get_object().get_scope()         parent = scope.parent         <MASK>             return parent.pyobject","if parent is not None and parent . get_kind ( ) == ""Class"" :",if parent is not None :,9.110529534877564,9.110529534877564,0.0
"def test_chunkcoding(self):     tstring_lines = []     for b in self.tstring:         lines = b.split(b""\n"")         last = lines.pop()         assert last == b""""         lines = [line + b""\n"" for line in lines]         tstring_lines.append(lines)     for native, utf8 in zip(*tstring_lines):         u = self.decode(native)[0]         self.assertEqual(u, utf8.decode(""utf-8""))         <MASK>             self.assertEqual(native, self.encode(u)[0])",if self . roundtriptest :,if u :,17.799177396293473,0.0,0.0
"def set_default_variants(apps, schema_editor):     Product = apps.get_model(""product"", ""Product"")     for product in Product.objects.iterator():         first_variant = product.variants.first()         <MASK>             product.default_variant = first_variant             product.save(update_fields=[""default_variant"", ""updated_at""])",if first_variant :,if first_variant :,100.00000000000004,100.00000000000004,1.0
"def json(self):     try:         if self.is_json():             raw_data = self.raw_data()             <MASK>                 raw_data = raw_data.decode(""utf-8"")             return json.loads(raw_data)     except ValueError:         pass","if not isinstance ( raw_data , text_type ) :","if raw_data . decode ( ""utf-8"" ) :",15.8636093934526,15.8636093934526,0.0
"def clear_react(self, message: discord.Message, emoji: MutableMapping = None) -> None:     try:         await message.clear_reactions()     except discord.Forbidden:         <MASK>             return         with contextlib.suppress(discord.HTTPException):             async for key in AsyncIter(emoji.values(), delay=0.2):                 await message.remove_reaction(key, self.bot.user)     except discord.HTTPException:         return",if not emoji :,if self . bot . user is None :,5.669791110976001,5.669791110976001,0.0
"def check(self, value):     value = String.check(self, value)     if isinstance(value, str):         value = value.upper()         for prefix in (self.prefix, self.prefix.split(""_"", 1)[1]):             # e.g. PANGO_WEIGHT_BOLD --> BOLD but also WEIGHT_BOLD --> BOLD             if value.startswith(prefix):                 value = value[len(prefix) :]             value = value.lstrip(""_"")         <MASK>             return getattr(self.group, value)         else:             raise ValueError(""No such constant: %s_%s"" % (self.prefix, value))     else:         return value","if hasattr ( self . group , value ) :",if self . group :,16.62083000646927,16.62083000646927,0.0
"def value(self):     quote = False     if self.defects:         quote = True     else:         for x in self:             if x.token_type == ""quoted-string"":                 quote = True     if quote:         pre = post = """"         if self[0].token_type == ""cfws"" or self[0][0].token_type == ""cfws"":             pre = "" ""         <MASK>             post = "" ""         return pre + quote_string(self.display_name) + post     else:         return super(DisplayName, self).value","if self [ - 1 ] . token_type == ""cfws"" or self [ - 1 ] [ - 1 ] . token_type == ""cfws"" :","if self [ 0 ] . token_type == ""cfws"" :",21.096214602921574,21.096214602921574,0.0
"def get_drive(self, root_path="""", volume_guid_path=""""):     for drive in self.drives:         if root_path:             config_root_path = drive.get(""root_path"")             if config_root_path and root_path == config_root_path:                 return drive         <MASK>             config_volume_guid_path = drive.get(""volume_guid_path"")             if config_volume_guid_path and config_volume_guid_path == volume_guid_path:                 return drive",elif volume_guid_path :,if volume_guid_path :,80.91067115702207,80.91067115702207,0.0
"def parse_edges(self, pcb):     edges = []     drawings = list(pcb.GetDrawings())     bbox = None     for m in pcb.GetModules():         for g in m.GraphicalItems():             drawings.append(g)     for d in drawings:         if d.GetLayer() == pcbnew.Edge_Cuts:             parsed_drawing = self.parse_drawing(d)             <MASK>                 edges.append(parsed_drawing)                 if bbox is None:                     bbox = d.GetBoundingBox()                 else:                     bbox.Merge(d.GetBoundingBox())     if bbox:         bbox.Normalize()     return edges, bbox",if parsed_drawing :,if parsed_drawing :,100.00000000000004,100.00000000000004,1.0
"def to_key(literal_or_identifier):     """"""returns string representation of this object""""""     if literal_or_identifier[""type""] == ""Identifier"":         return literal_or_identifier[""name""]     elif literal_or_identifier[""type""] == ""Literal"":         k = literal_or_identifier[""value""]         if isinstance(k, float):             return unicode(float_repr(k))         elif ""regex"" in literal_or_identifier:             return compose_regex(k)         elif isinstance(k, bool):             return ""true"" if k else ""false""         <MASK>             return ""null""         else:             return unicode(k)",elif k is None :,"if ""null"" in literal_or_identifier :",3.7477767366779213,3.7477767366779213,0.0
"def find_multiple_stats(stats, name, _found=None, _on_found=None):     if _found is None:         _found = []     for child_stats in stats:         if child_stats.name == name:             _found.append(child_stats)             <MASK>                 _on_found(_found)         find_multiple_stats(child_stats, name, _found)     return _found",if callable ( _on_found ) :,if _on_found is not None :,33.03164318013809,33.03164318013809,0.0
"def _run_generated_code(     self,     code,     globs,     locs,     fails_under_py3k=True, ):     import warnings     from zope.interface._compat import PYTHON3     with warnings.catch_warnings(record=True) as log:         warnings.resetwarnings()         <MASK>             exec(code, globs, locs)             self.assertEqual(len(log), 0)  # no longer warn             return True         else:             try:                 exec(code, globs, locs)             except TypeError:                 return False             else:                 if fails_under_py3k:                     self.fail(""Didn't raise TypeError"")",if not PYTHON3 :,if log :,24.840753130578644,0.0,0.0
"def _get_node(self, node_id):     self.non_terminated_nodes({})  # Side effect: updates cache     with self.lock:         <MASK>             return self.cached_nodes[node_id]         instance = (             self.compute.instances()             .get(                 project=self.provider_config[""project_id""],                 zone=self.provider_config[""availability_zone""],                 instance=node_id,             )             .execute()         )         return instance",if node_id in self . cached_nodes :,if node_id in self . cached_nodes :,100.00000000000004,100.00000000000004,1.0
"def skip_to_close_match(self):     nestedCount = 1     while 1:         tok = self.tokenizer.get_next_token()         ttype = tok[""style""]         if ttype == SCE_PL_UNUSED:             return         elif self.classifier.is_index_op(tok):             tval = tok[""text""]             <MASK>                 if self.opHash[tval][1] == 1:                     nestedCount += 1                 else:                     nestedCount -= 1                     if nestedCount <= 0:                         break",if self . opHash . has_key ( tval ) :,if tval in self . opHash :,14.231728394642222,14.231728394642222,0.0
"def _create_or_get_helper(self, infer_mode: Optional[bool] = None, **kwargs) -> Helper:     # Prefer creating a new helper when at least one kwarg is specified.     prefer_new = len(kwargs) > 0     kwargs.update(infer_mode=infer_mode)     is_training = not infer_mode if infer_mode is not None else self.training     helper = self._train_helper if is_training else self._infer_helper     if prefer_new or helper is None:         helper = self.create_helper(**kwargs)         if is_training and self._train_helper is None:             self._train_helper = helper         <MASK>             self._infer_helper = helper     return helper",elif not is_training and self . _infer_helper is None :,if prefer_new and self . _infer_helper is None :,64.53174978135057,64.53174978135057,0.0
"def get_ldset(self, ldsets):     ldset = None     if self._properties[""ldset_name""] == """":         nldset = len(ldsets)         if nldset == 0:             msg = _(""Logical Disk Set could not be found."")             raise exception.NotFound(msg)         else:             ldset = None     else:         <MASK>             msg = (                 _(""Logical Disk Set `%s` could not be found."")                 % self._properties[""ldset_name""]             )             raise exception.NotFound(msg)         ldset = ldsets[self._properties[""ldset_name""]]     return ldset","if self . _properties [ ""ldset_name"" ] not in ldsets :",if nldset > 1 :,1.407567834071592,1.407567834071592,0.0
"def calc_fractal_serial(q, maxiter):     # calculate z using pure python on a numpy array     # note that, unlike the other two implementations,     # the number of iterations per point is NOT constant     z = np.zeros(q.shape, complex)     output = np.resize(         np.array(             0,         ),         q.shape,     )     for i in range(len(q)):         for iter in range(maxiter):             z[i] = z[i] * z[i] + q[i]             <MASK>                 output[i] = iter                 break     return output",if abs ( z [ i ] ) > 2.0 :,if iter < maxiter :,3.8261660656802645,3.8261660656802645,0.0
"def _verifySubs(self):     for inst in self.subs:         if not isinstance(inst, (_Block, _Instantiator, Cosimulation)):             raise BlockError(_error.ArgType % (self.name,))         <MASK>             if not inst.modctxt:                 raise BlockError(_error.InstanceError % (self.name, inst.callername))","if isinstance ( inst , ( _Block , _Instantiator ) ) :",if not inst . argtype :,2.8157908010020885,2.8157908010020885,0.0
"def walks_generator():     if filelist is not None:         bucket = []         for filename in filelist:             with io.open(filename) as inf:                 for line in inf:                     walk = [int(x) for x in line.strip(""\n"").split("" "")]                     bucket.append(walk)                     <MASK>                         yield bucket                         bucket = []         if len(bucket):             yield bucket     else:         for _ in range(epoch):             for nodes in graph.node_batch_iter(batch_size):                 walks = graph.random_walk(nodes, walk_len)                 yield walks",if len ( bucket ) == batch_size :,if len ( bucket ) > 0 :,37.1880042464665,37.1880042464665,0.0
def _traverse(op):     if op in visited:         return     visited.add(op)     if tag.is_injective(op.tag):         if op not in s.outputs:             s[op].compute_inline()         for tensor in op.input_tensors:             <MASK>                 _traverse(tensor.op)     callback(op),"if isinstance ( tensor . op , tvm . te . ComputeOp ) :",if tensor . op is not None :,11.033017809693943,11.033017809693943,0.0
"def unwatch_run(self, run_id, handler):     with self._dict_lock:         <MASK>             self._handlers_dict[run_id] = [                 (start_cursor, callback)                 for (start_cursor, callback) in self._handlers_dict[run_id]                 if callback != handler             ]         if not self._handlers_dict[run_id]:             del self._handlers_dict[run_id]             run_id_dict = self._run_id_dict             del run_id_dict[run_id]             self._run_id_dict = run_id_dict",if run_id in self . _run_id_dict :,if run_id in self . _run_id_dict :,100.00000000000004,100.00000000000004,1.0
"def _PromptMySQL(self, config):     """"""Prompts the MySQL configuration, retrying if the configuration is invalid.""""""     while True:         self._PromptMySQLOnce(config)         <MASK>             print(""Successfully connected to MySQL with the given configuration."")             return         else:             print(""Error: Could not connect to MySQL with the given configuration."")             retry = RetryBoolQuestion(""Do you want to retry MySQL configuration?"", True)             if not retry:                 raise ConfigInitError()",if self . _CheckMySQLConnection ( ) :,if self . _Connection . is_alive ( ) :,30.66148710292676,30.66148710292676,0.0
"def get_courses_without_topic(topic):     data = []     for entry in frappe.db.get_all(""Course""):         course = frappe.get_doc(""Course"", entry.name)         topics = [t.topic for t in course.topics]         <MASK>             data.append(course.name)     return data",if not topics or topic not in topics :,if topic in topics :,20.300727612812874,20.300727612812874,0.0
"def _error_handler(action, **keywords):     if keywords:         file_type = keywords.get(""file_type"", None)         if file_type:             raise exceptions.FileTypeNotSupported(                 constants.FILE_TYPE_NOT_SUPPORTED_FMT % (file_type, action)             )         else:             <MASK>                 keywords.pop(""on_demand"")             msg = ""Please check if there were typos in ""             msg += ""function parameters: %s. Otherwise ""             msg += ""unrecognized parameters were given.""             raise exceptions.UnknownParameters(msg % keywords)     else:         raise exceptions.UnknownParameters(""No parameters found!"")","if ""on_demand"" in keywords :",if keywords :,8.525588607164655,0.0,0.0
"def select(self, regions, register):     self.view.sel().clear()     to_store = []     for r in regions:         self.view.sel().add(r)         <MASK>             to_store.append(self.view.substr(self.view.full_line(r)))     if register:         text = """".join(to_store)         if not text.endswith(""\n""):             text = text + ""\n""         state = State(self.view)         state.registers[register] = [text]",if register :,if self . view . full_line ( r ) :,4.02724819242185,4.02724819242185,0.0
"def has_actor(self, message: HasActorMessage) -> ResultMessage:     actor_ref = message.actor_ref     # lookup allocated     for address, item in self._allocated_actors.items():         ref = create_actor_ref(address, actor_ref.uid)         <MASK>             return ResultMessage(message.message_id, True, protocol=message.protocol)     return ResultMessage(message.message_id, False, protocol=message.protocol)",if ref in item :,if ref . id == item . id :,9.980099403873663,9.980099403873663,0.0
"def toggleMetaButton(self, event):     """"""Process clicks on toggle buttons""""""     clickedBtn = event.EventObject     if wx.GetMouseState().GetModifiers() == wx.MOD_CONTROL:         activeBtns = [btn for btn in self.metaButtons if btn.GetValue()]         <MASK>             clickedBtn.setUserSelection(clickedBtn.GetValue())             self.itemView.filterItemStore()         else:             # Do 'nothing' if we're trying to turn last active button off             # Keep button in the same state             clickedBtn.setUserSelection(True)     else:         for btn in self.metaButtons:             btn.setUserSelection(btn == clickedBtn)         self.itemView.filterItemStore()",if activeBtns :,if activeBtns :,100.00000000000004,0.0,1.0
"def __init__(self, hub=None):  # pylint: disable=unused-argument     if resolver._resolver is None:         _resolver = resolver._resolver = _DualResolver()         if config.resolver_nameservers:             _resolver.network_resolver.nameservers[:] = config.resolver_nameservers         <MASK>             _resolver.network_resolver.lifetime = config.resolver_timeout     # Different hubs in different threads could be sharing the same     # resolver.     assert isinstance(resolver._resolver, _DualResolver)     self._resolver = resolver._resolver",if config . resolver_timeout :,if config . resolver_timeout :,100.00000000000004,100.00000000000004,1.0
"def sub_paragraph(self, li):     """"""Search for checkbox in sub-paragraph.""""""     found = False     if len(li):         first = list(li)[0]         <MASK>             m = RE_CHECKBOX.match(first.text)             if m is not None:                 first.text = self.markdown.htmlStash.store(                     get_checkbox(m.group(""state"")), safe=True                 ) + m.group(""line"")                 found = True     return found","if first . tag == ""p"" and first . text is not None :",if first . text :,5.3941217755126925,5.3941217755126925,0.0
"def _check_mswin_locale(locale):     msloc = None     try:         msloc = _LOCALE_NAMES[locale[:5]][:2]         locale = locale[:5]     except KeyError:         try:             msloc = _LOCALE_NAMES[locale[:2]][:2]             locale = locale[:2]         except KeyError:             # US English is the outlier, all other English locales want             # real English:             <MASK>                 return (""en_GB"", ""1252"")             return (None, None)     return (locale, msloc)","if locale [ : 2 ] == ( ""en"" ) and locale [ : 5 ] != ""en_US"" :","if locale [ : 2 ] == ""US"" :",22.09225818602599,22.09225818602599,0.0
"def setLabel(self, s, protect=False):     """"""Set the label of the minibuffer.""""""     c, k, w = self.c, self, self.w     if w:         # Support for the curses gui.         <MASK>             g.app.gui.set_minibuffer_label(c, s)         w.setAllText(s)         n = len(s)         w.setSelectionRange(n, n, insert=n)         if protect:             k.mb_prefix = s","if hasattr ( g . app . gui , ""set_minibuffer_label"" ) :",if c . is_terminal ( ) :,4.6166333787043365,4.6166333787043365,0.0
"def getProc(su, innerTarget):     if len(su) == 1:  # have a one element wedge         proc = (""first"", ""last"")     else:         if su.isFirst(innerTarget) and su.isLast(innerTarget):             proc = (""first"", ""last"")  # same element can be first and last         <MASK>             proc = (""first"",)         elif su.isLast(innerTarget):             proc = (""last"",)         else:             proc = ()     return proc",elif su . isFirst ( innerTarget ) :,if su . isFirst ( innerTarget ) :,84.08964152537145,84.08964152537145,0.0
"def await_test_end(self):     iterations = 0     while True:         if iterations > 100:             self.log.debug(""Await: iteration limit reached"")             return         status = self.master.get_status()         <MASK>             return         iterations += 1         time.sleep(1.0)","if status . get ( ""status"" ) == ""ENDED"" :","if status == ""RUNNING"" :",12.59496650349099,12.59496650349099,0.0
"def _handle_autocomplete_request_for_text(text):     if not hasattr(text, ""autocompleter""):         if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text():             <MASK>                 text.autocompleter = Completer(text)             elif isinstance(text, ShellText):                 text.autocompleter = ShellCompleter(text)             text.bind(""<1>"", text.autocompleter.on_text_click)         else:             return     text.autocompleter.handle_autocomplete_request()","if isinstance ( text , CodeViewText ) :",if text . is_text_click ( ) :,9.425159511373677,9.425159511373677,0.0
"def validate_party_details(self):     if self.party:         <MASK>             frappe.throw(_(""Invalid {0}: {1}"").format(self.party_type, self.party))         if self.party_account and self.party_type in (""Customer"", ""Supplier""):             self.validate_account_type(                 self.party_account, [erpnext.get_party_account_type(self.party_type)]             )","if not frappe . db . exists ( self . party_type , self . party ) :","if self . party_type not in ( ""Customer"" , ""Supplier"" ) :",24.491638846284875,24.491638846284875,0.0
"def format(self, formatstr):     pieces = []     for i, piece in enumerate(re_formatchars.split(force_text(formatstr))):         <MASK>             pieces.append(force_text(getattr(self, piece)()))         elif piece:             pieces.append(re_escaped.sub(r""\1"", piece))     return """".join(pieces)",if i % 2 :,if i == 0 :,17.965205598154213,17.965205598154213,0.0
"def _convert_java_pattern_to_python(pattern):     """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`.""""""     s = list(pattern)     i = 0     while i < len(s) - 1:         c = s[i]         <MASK>             s[i] = ""\\""         elif c == ""\\"" and s[i + 1] == ""$"":             s[i] = """"             i += 1         i += 1     return pattern[:0].join(s)","if c == ""$"" and s [ i + 1 ] in ""0123456789"" :","if c == ""\\"" :",16.898959014073654,16.898959014073654,0.0
"def download(self, url, filename, **kwargs):     try:         r = self.get(url, timeout=10, stream=True, **kwargs)         <MASK>             return False         with open(filename, ""wb"") as f:             for chunk in r.iter_content(chunk_size=1024):                 if chunk:                     f.write(chunk)         helpers.chmod_as_parent(filename)     except Exception as e:         sickrage.app.log.debug(             ""Failed to download file from {} - ERROR: {}"".format(url, e)         )         if os.path.exists(filename):             os.remove(filename)         return False     return True",if r . status_code >= 400 :,if not r :,4.690733795095046,4.690733795095046,0.0
"def run(self, paths=[]):     items = []     for item in SideBarSelection(paths).getSelectedFilesWithExtension(""js""):         items.append(             '<script type=""text/javascript"" src=""'             + item.pathAbsoluteFromProjectEncoded()             + '""></script>'         )     if len(items) > 0:         sublime.set_clipboard(""\n"".join(items))         <MASK>             sublime.status_message(""Items copied"")         else:             sublime.status_message(""Item copied"")",if len ( items ) > 1 :,if item . is_file ( ) :,6.742555929751843,6.742555929751843,0.0
"def work(self):     while True:         timeout = self.timeout         if idle.is_set():             timeout = self.idle_timeout         log.debug(""Wait for {}"".format(timeout))         fetch.wait(timeout)         <MASK>             log.info(""Stop fetch worker"")             break         self.fetch()",if shutting_down . is_set ( ) :,if not self . fetch_done :,5.367626065580593,5.367626065580593,0.0
"def check_apns_certificate(ss):     mode = ""start""     for s in ss.split(""\n""):         if mode == ""start"":             <MASK>                 mode = ""key""         elif mode == ""key"":             if ""END RSA PRIVATE KEY"" in s or ""END PRIVATE KEY"" in s:                 mode = ""end""                 break             elif s.startswith(""Proc-Type"") and ""ENCRYPTED"" in s:                 raise ImproperlyConfigured(                     ""Encrypted APNS private keys are not supported""                 )     if mode != ""end"":         raise ImproperlyConfigured(""The APNS certificate doesn't contain a private key"")","if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s :","if s . startswith ( ""RSA PRIVATE KEY"" ) :",15.291812286526515,15.291812286526515,0.0
"def compare_lists(self, l1, l2, key):     l2_lookup = {o.get(key): o for o in l2}     for obj1 in l1:         obj2 = l2_lookup.get(obj1.get(key))         for k in obj1:             <MASK>                 self.assertEqual(obj1.get(k), obj2.get(k))","if k not in ""id"" and obj1 . get ( k ) :",if obj2 is None :,1.719207234832579,1.719207234832579,0.0
"def before_get_object(self, view_kwargs):     if view_kwargs.get(""id"") is not None:         try:             user_favourite_event = find_user_favourite_event_by_id(                 event_id=view_kwargs[""id""]             )         except NoResultFound:             raise ObjectNotFound(                 {""source"": ""/data/relationships/event""}, ""Object: not found""             )         else:             <MASK>                 view_kwargs[""id""] = user_favourite_event.id             else:                 view_kwargs[""id""] = None",if user_favourite_event is not None :,if user_favourite_event :,54.77927682341229,54.77927682341229,0.0
"def close(self):     super().close()     if not sys.is_finalizing():         for sig in list(self._signal_handlers):             self.remove_signal_handler(sig)     else:         <MASK>             warnings.warn(                 f""Closing the loop {self!r} ""                 f""on interpreter shutdown ""                 f""stage, skipping signal handlers removal"",                 ResourceWarning,                 source=self,             )             self._signal_handlers.clear()",if self . _signal_handlers :,if self . _signal_handlers :,100.00000000000004,100.00000000000004,1.0
"def install_script(self, script, install_options=None):     try:         fname = utils.do_script(             script,             python_exe=osp.join(self.target, ""python.exe""),             architecture=self.architecture,             verbose=self.verbose,             install_options=install_options,         )     except RuntimeError:         <MASK>             print(""Failed!"")             raise",if not self . verbose :,if fname != self . target :,13.134549472120788,13.134549472120788,0.0
"def GetRouterForUser(self, username):     """"""Returns a router corresponding to a given username.""""""     for index, router in enumerate(self.routers):         router_id = str(index)         <MASK>             logging.debug(                 ""Matched router %s to user %s"", router.__class__.__name__, username             )             return router     logging.debug(         ""No router ACL rule match for user %s. Using default "" ""router %s"",         username,         self.default_router.__class__.__name__,     )     return self.default_router","if self . auth_manager . CheckPermissions ( username , router_id ) :",if router_id == username :,8.993236413460203,8.993236413460203,0.0
"def charset(self):     """"""The charset from the content type.""""""     header = self.environ.get(""CONTENT_TYPE"")     if header:         ct, options = parse_options_header(header)         charset = options.get(""charset"")         <MASK>             if is_known_charset(charset):                 return charset             return self.unknown_charset(charset)     return self.default_charset",if charset :,"if ct == ""text/plain"" :",4.990049701936832,4.990049701936832,0.0
def isFinished(self):     # returns true if episode timesteps has reached episode length and resets the task     if self.count > self.epiLen:         self.res()         return True     else:         if self.count == 1:             self.pertGlasPos(0)         <MASK>             self.env.reset()             self.pertGlasPos(1)         self.count += 1         return False,if self . count == self . epiLen / 2 + 1 :,if self . count == 2 :,34.53521209125189,34.53521209125189,0.0
"def mtimes_of_files(dirnames: List[str], suffix: str) -> Iterator[float]:     for dirname in dirnames:         for root, dirs, files in os.walk(dirname):             for sfile in files:                 <MASK>                     try:                         yield path.getmtime(path.join(root, sfile))                     except OSError:                         pass",if sfile . endswith ( suffix ) :,if suffix == sfile :,8.22487964923291,8.22487964923291,0.0
"def get_all_hashes(self):     event_hashes = []     sample_hashes = []     for a in self.event.attributes:         h = None         if a.type in (""md5"", ""sha1"", ""sha256""):             h = a.value             event_hashes.append(h)         elif a.type in (""filename|md5"", ""filename|sha1"", ""filename|sha256""):             h = a.value.split(""|"")[1]             event_hashes.append(h)         <MASK>             h = a.value.split(""|"")[1]             sample_hashes.append(h)     return event_hashes, sample_hashes","elif a . type == ""malware-sample"" :","if a . type in ( ""sample"" , ""sample"" ) :",11.633270842295033,11.633270842295033,0.0
"def _validate(self, event):     if self.type is None:         return     new = self.value     if not isinstance(new, self.type) and new is not None:         <MASK>             self.value = event.old         types = repr(self.type) if isinstance(self.type, tuple) else self.type.__name__         raise ValueError(             ""LiteralInput expected %s type but value %s ""             ""is of type %s."" % (types, new, type(new).__name__)         )",if event :,if new is not None :,9.652434877402245,9.652434877402245,0.0
"def update_dict(a, b):     for key, value in b.items():         <MASK>             continue         if key not in a:             a[key] = value         elif isinstance(a[key], dict) and isinstance(value, dict):             update_dict(a[key], value)         elif isinstance(a[key], list):             a[key].append(value)         else:             a[key] = [a[key], value]",if value is None :,if not value :,16.37226966703825,16.37226966703825,0.0
"def on_pre_save(self, view):     extOrClause = ""|"".join(s.get(""format_on_save_extensions""))     extRegex = ""\\.("" + extOrClause + "")$""     if s.get(""format_on_save"") and re.search(extRegex, view.file_name()):         # only auto-format on save if there are no ""lint errors""         # here are some named regions from sublimelint see https://github.com/lunixbochs/sublimelint/tree/st3         lints_regions = [""lint-keyword-underline"", ""lint-keyword-outline""]         for linter in lints_regions:             <MASK>                 return         view.run_command(""js_format"")",if len ( view . get_regions ( linter ) ) :,"if linter . name == ""lint"" :",4.396165418527572,4.396165418527572,0.0
"def readMemory(self, va, size):     for mva, mmaxva, mmap, mbytes in self._map_defs:         if mva <= va < mmaxva:             mva, msize, mperms, mfname = mmap             <MASK>                 raise envi.SegmentationViolation(va)             offset = va - mva             return mbytes[offset : offset + size]     raise envi.SegmentationViolation(va)",if not mperms & MM_READ :,if msize < size :,6.9717291216921975,6.9717291216921975,0.0
"def assertFilepathsEqual(self, p1, p2):     if sys.platform == ""win32"":         <MASK>             p1 = [normcase(normpath(x)) for x in p1]             p2 = [normcase(normpath(x)) for x in p2]         else:             assert isinstance(p1, (str, unicode))             p1 = normcase(normpath(p1))             p2 = normcase(normpath(p2))     self.assertEqual(p1, p2)","if isinstance ( p1 , ( list , tuple ) ) :","if sys . platform == ""darwin"" :",4.085507150363302,4.085507150363302,0.0
"def add_directory_csv_files(dir_path, paths=None):     if not paths:         paths = []     for p in listdir(dir_path):         path = join(dir_path, p)         if isdir(path):             # call recursively for each dir             paths = add_directory_csv_files(path, paths)         <MASK>             # add every file to the list             paths.append(path)     return paths","elif isfile ( path ) and path . endswith ( "".csv"" ) :",if not isdir ( path ) :,8.036914931946859,8.036914931946859,0.0
"def _verifySubs(self):     for inst in self.subs:         <MASK>             raise BlockError(_error.ArgType % (self.name,))         if isinstance(inst, (_Block, _Instantiator)):             if not inst.modctxt:                 raise BlockError(_error.InstanceError % (self.name, inst.callername))","if not isinstance ( inst , ( _Block , _Instantiator , Cosimulation ) ) :","if not isinstance ( inst , _Block ) :",32.8060571266766,32.8060571266766,0.0
"def __annotations_bytes(self):     if self.annotations:         a = []         for k, v in self.annotations.items():             if len(k) != 4:                 raise errors.ProtocolError(""annotation key must be of length 4"")             <MASK>                 k = k.encode(""ASCII"")             a.append(struct.pack(""!4sH"", k, len(v)))             a.append(v)         return b"""".join(a)     return b""""","if sys . version_info >= ( 3 , 0 ) :",if len ( v ) != 4 :,4.0905089639506285,4.0905089639506285,0.0
"def session(self, profile: str = ""default"", region: str = None) -> boto3.Session:     region = self._get_region(region, profile)     try:         session = self._cache_lookup(             self._session_cache,             [profile, region],             self._boto3.Session,             [],             {""region_name"": region, ""profile_name"": profile},         )     except ProfileNotFound:         <MASK>             raise         session = self._boto3.Session(region_name=region)         self._cache_set(self._session_cache, [profile, region], session)     return session","if profile != ""default"" :",if profile not in self . _session_cache :,8.29519350710986,8.29519350710986,0.0
"def spans_score(gold_spans, system_spans):     correct, gi, si = 0, 0, 0     while gi < len(gold_spans) and si < len(system_spans):         if system_spans[si].start < gold_spans[gi].start:             si += 1         <MASK>             gi += 1         else:             correct += gold_spans[gi].end == system_spans[si].end             si += 1             gi += 1     return Score(len(gold_spans), len(system_spans), correct)",elif gold_spans [ gi ] . start < system_spans [ si ] . start :,if si . end > gold_spans [ si . end :,20.499930055179675,20.499930055179675,0.0
"def to_api(tag, raw_value):     try:         api_tag, converter = _QL_TO_SC[tag] if tag else (""q"", None)     except KeyError:         <MASK>             raise self.error(                 ""Unsupported '%s' tag. Try: %s"" % (tag, "", "".join(SUPPORTED))             )         return None, None     else:         value = str(converter(raw_value) if converter else raw_value)         return api_tag, value",if tag not in SUPPORTED :,if api_tag not in SUPPORTED :,54.10822690539397,54.10822690539397,0.0
"def unpack(self, buf):     dpkt.Packet.unpack(self, buf)     buf = buf[self.__hdr_len__ :]     # single-byte IE     if self.type & 0x80:         self.len = 0         self.data = b""""     # multi-byte IE     else:         # special PER-encoded UUIE         <MASK>             self.len = struct.unpack("">H"", buf[:2])[0]             buf = buf[2:]         # normal TLV-like IE         else:             self.len = struct.unpack(""B"", buf[:1])[0]             buf = buf[1:]         self.data = buf[: self.len]",if self . type == USER_TO_USER :,if self . type & 0x80 :,21.28139770959968,21.28139770959968,0.0
"def on_bt_search_clicked(self, widget):     if self.current_provider is None:         return     query = self.en_query.get_text()     @self.obtain_podcasts_with     def load_data():         <MASK>             return self.current_provider.on_search(query)         elif self.current_provider.kind == directory.Provider.PROVIDER_URL:             return self.current_provider.on_url(query)         elif self.current_provider.kind == directory.Provider.PROVIDER_FILE:             return self.current_provider.on_file(query)",if self . current_provider . kind == directory . Provider . PROVIDER_SEARCH :,if self . current_provider . kind == directory . Provider . SEARCH :,80.04754550859003,80.04754550859003,0.0
"def _text(bitlist):     out = """"     for typ, text in bitlist:         if not typ:             out += text         <MASK>             out += ""\\fI%s\\fR"" % text         elif typ in [""strong"", ""code""]:             out += ""\\fB%s\\fR"" % text         else:             raise ValueError(""unexpected tag %r inside text"" % (typ,))     out = out.strip()     out = re.sub(re.compile(r""^\s+"", re.M), """", out)     return out","elif typ == ""em"" :","if typ in [ ""strong"" , ""code"" ] :",4.368583925857938,4.368583925857938,0.0
"def process(self, buckets):     with self.executor_factory(max_workers=3) as w:         futures = {}         results = []         for b in buckets:             futures[w.submit(self.process_bucket, b)] = b         for f in as_completed(futures):             <MASK>                 b = futures[f]                 self.log.error(                     ""error modifying bucket:%s\n%s"", b[""Name""], f.exception()                 )             results += filter(None, [f.result()])         return results",if f . exception ( ) :,if f . exception ( ) :,100.00000000000004,100.00000000000004,1.0
"def check_settings(self):     if self.settings_dict[""TIME_ZONE""] is not None:         <MASK>             raise ImproperlyConfigured(                 ""Connection '%s' cannot set TIME_ZONE because USE_TZ is ""                 ""False."" % self.alias             )         elif self.features.supports_timezones:             raise ImproperlyConfigured(                 ""Connection '%s' cannot set TIME_ZONE because its engine ""                 ""handles time zones conversions natively."" % self.alias             )",if not settings . USE_TZ :,if self . features . use_tz :,6.742555929751843,6.742555929751843,0.0
"def process_webhook_prop(namespace):     if not isinstance(namespace.webhook_properties, list):         return     result = {}     for each in namespace.webhook_properties:         if each:             <MASK>                 key, value = each.split(""="", 1)             else:                 key, value = each, """"             result[key] = value     namespace.webhook_properties = result","if ""="" in each :","if ""="" in each :",100.00000000000004,100.00000000000004,1.0
"def _expand_query_values(original_query_list):     query_list = []     for key, value in original_query_list:         <MASK>             query_list.append((key, value))         else:             key_fmt = key + ""[%s]""             value_list = _to_kv_list(value)             query_list.extend((key_fmt % k, v) for k, v in value_list)     return query_list","if isinstance ( value , basestring ) :","if isinstance ( value , dict ) :",59.4603557501361,59.4603557501361,0.0
"def tags():     """"""Return a dictionary of all tags in the form {hash: [tag_names, ...]}.""""""     tags = {}     for (n, c) in list_refs():         if n.startswith(""refs/tags/""):             name = n[10:]             <MASK>                 tags[c] = []             tags[c].append(name)  # more than one tag can point at 'c'     return tags",if not c in tags :,if name not in tags :,34.32945239845197,34.32945239845197,0.0
"def test_colorspiral(self):     """"""Set of 625 colours, with jitter, using get_colors().""""""     boxedge = 20     boxes_per_row = 25     rows = 0     for i, c in enumerate(get_colors(625)):         self.c.setFillColor(c)         x1 = boxedge * (i % boxes_per_row)         y1 = rows * boxedge         self.c.rect(x1, y1, boxedge, boxedge, fill=1, stroke=0)         <MASK>             rows += 1     self.finish()",if not ( i + 1 ) % boxes_per_row :,if i % boxes_per_row == 0 :,39.085161980674464,39.085161980674464,0.0
"def oldest_pending_update_in_days():     """"""Return the datestamp of the oldest pending update""""""     pendingupdatespath = os.path.join(         prefs.pref(""ManagedInstallDir""), ""UpdateNotificationTracking.plist""     )     try:         pending_updates = FoundationPlist.readPlist(pendingupdatespath)     except FoundationPlist.NSPropertyListSerializationException:         return 0     oldest_date = now = NSDate.date()     for category in pending_updates:         for name in pending_updates[category]:             this_date = pending_updates[category][name]             <MASK>                 oldest_date = this_date     return now.timeIntervalSinceDate_(oldest_date) / (24 * 60 * 60)",if this_date < oldest_date :,if this_date < oldest_date :,100.00000000000004,100.00000000000004,1.0
"def _try_read_gpg(path):     path = os.path.expanduser(path)     cmd = _gpg_cmd() + [path]     log.debug(""gpg cmd: %s"", cmd)     try:         p = subprocess.Popen(             cmd, env=os.environ, stdout=subprocess.PIPE, stderr=subprocess.PIPE         )     except OSError as e:         log.error(""cannot decode %s with command '%s' (%s)"", path, "" "".join(cmd), e)     else:         out, err = p.communicate()         <MASK>             log.error(err.decode(errors=""replace"").strip())             return None         return out.decode(errors=""replace"")",if p . returncode != 0 :,if err :,6.5479514338598115,0.0,0.0
"def sort_nested_dictionary_lists(d):     for k, v in d.items():         <MASK>             for i in range(0, len(v)):                 if isinstance(v[i], dict):                     v[i] = await sort_nested_dictionary_lists(v[i])                 d[k] = sorted(v)         if isinstance(v, dict):             d[k] = await sort_nested_dictionary_lists(v)     return d","if isinstance ( v , list ) :","if isinstance ( v , list ) :",100.00000000000004,100.00000000000004,1.0
"def _the_callback(widget, event_id):     point = widget.GetCenter()     index = widget.WIDGET_INDEX     if hasattr(callback, ""__call__""):         <MASK>             args = [point, index]         else:             args = [point]         if pass_widget:             args.append(widget)         try_callback(callback, *args)     return",if num > 1 :,"if hasattr ( callback , ""call"" ) :",4.990049701936832,4.990049701936832,0.0
"def _add_cs(master_cs, sub_cs, prefix, delimiter=""."", parent_hp=None):     new_parameters = []     for hp in sub_cs.get_hyperparameters():         new_parameter = copy.deepcopy(hp)         # Allow for an empty top-level parameter         <MASK>             new_parameter.name = prefix         elif not prefix == """":             new_parameter.name = ""{}{}{}"".format(prefix, SPLITTER, new_parameter.name)         new_parameters.append(new_parameter)     for hp in new_parameters:         _add_hp(master_cs, hp)","if new_parameter . name == """" :",if new_parameter . name == delimiter :,70.80735452207037,70.80735452207037,0.0
"def tearDown(self):     """"""Shutdown the server.""""""     try:         <MASK>             self.server.stop()         if self.sl_hdlr:             self.root_logger.removeHandler(self.sl_hdlr)             self.sl_hdlr.close()     finally:         BaseTest.tearDown(self)",if self . server :,if self . server :,100.00000000000004,100.00000000000004,1.0
"def app_uninstall_all(self, excludes=[], verbose=False):     """"""Uninstall all apps""""""     our_apps = [""com.github.uiautomator"", ""com.github.uiautomator.test""]     output, _ = self.shell([""pm"", ""list"", ""packages"", ""-3""])     pkgs = re.findall(r""package:([^\s]+)"", output)     pkgs = set(pkgs).difference(our_apps + excludes)     pkgs = list(pkgs)     for pkg_name in pkgs:         <MASK>             print(""uninstalling"", pkg_name, "" "", end="""", flush=True)         ok = self.app_uninstall(pkg_name)         if verbose:             print(""OK"" if ok else ""FAIL"")     return pkgs",if verbose :,if verbose :,100.00000000000004,0.0,1.0
"def httpapi(self, arg, opts):     sc = HttpAPIStatsCollector()     headers = [""#Item"", ""Value""]     table = []     for k, v in sc.get().getStats().items():         if isinstance(v, dict):             v = json.dumps(v)         row = []         row.append(""#%s"" % k)         <MASK>             row.append(formatDateTime(v))         else:             row.append(v)         table.append(row)     self.protocol.sendData(         tabulate(table, headers, tablefmt=""plain"", numalign=""left"").encode(""ascii"")     )","if k [ - 3 : ] == ""_at"" :","if isinstance ( v , datetime ) :",3.102160927976006,3.102160927976006,0.0
"def Get_Gene(self, id):     """"""Retreive the gene name (GN).""""""     entry = self.Get(id)     if not entry:         return None     GN = """"     for line in string.split(entry, ""\n""):         <MASK>             GN = string.strip(line[5:])             if GN[-1] == ""."":                 GN = GN[0:-1]             return GN         if line[0:2] == ""//"":             break     return GN","if line [ 0 : 5 ] == ""GN   "" :","if line [ 0 : 5 ] == ""#"" :",79.10665071754353,79.10665071754353,0.0
"def replace_dir_vars(path, d):     """"""Replace common directory paths with appropriate variable references (e.g. /etc becomes ${sysconfdir})""""""     dirvars = {}     # Sort by length so we get the variables we're interested in first     for var in sorted(list(d.keys()), key=len):         if var.endswith(""dir"") and var.lower() == var:             value = d.getVar(var)             <MASK>                 dirvars[value] = var     for dirpath in sorted(list(dirvars.keys()), reverse=True):         path = path.replace(dirpath, ""${%s}"" % dirvars[dirpath])     return path","if value . startswith ( ""/"" ) and not ""\n"" in value and value not in dirvars :",if value :,0.08017090575578772,0.0,0.0
"def _scrub_generated_timestamps(self, target_workdir):     """"""Remove the first line of comment from each file if it contains a timestamp.""""""     for root, _, filenames in safe_walk(target_workdir):         for filename in filenames:             source = os.path.join(root, filename)             with open(source, ""r"") as f:                 lines = f.readlines()             if len(lines) < 1:                 return             with open(source, ""w"") as f:                 <MASK>                     f.write(lines[0])                 for line in lines[1:]:                     f.write(line)",if not self . _COMMENT_WITH_TIMESTAMP_RE . match ( lines [ 0 ] ) :,"if lines [ 0 ] == ""\n"" :",11.08282954010985,11.08282954010985,0.0
"def get_all_active_plugins(self) -> List[BotPlugin]:     """"""This returns the list of plugins in the callback ordered defined from the config.""""""     all_plugins = []     for name in self.plugins_callback_order:         # None is a placeholder for any plugin not having a defined order         <MASK>             all_plugins += [                 plugin                 for name, plugin in self.plugins.items()                 if name not in self.plugins_callback_order and plugin.is_activated             ]         else:             plugin = self.plugins[name]             if plugin.is_activated:                 all_plugins.append(plugin)     return all_plugins",if name is None :,if name in self . plugins :,14.535768424205482,14.535768424205482,0.0
"def test_query_level(self):     ""Tests querying at a level other than max""     # level 2     l2 = set()     for p in self.tile_paths:         l2.add(p[0:2])     for path in iterate_base4(2):         <MASK>             self.assertTrue(self.tree.query_path(path))         else:             self.assertFalse(self.tree.query_path(path))     # level 1:     self.assertTrue(self.tree.query_path((0,)))     self.assertTrue(self.tree.query_path((1,)))     self.assertTrue(self.tree.query_path((2,)))     self.assertFalse(self.tree.query_path((3,)))",if path in l2 :,if l2 . count ( p ) > 1 :,5.522397783539471,5.522397783539471,0.0
"def program_exists(name):     paths = (os.getenv(""PATH"") or os.defpath).split(os.pathsep)     for p in paths:         fn = ""%s/%s"" % (p, name)         <MASK>             return not os.path.isdir(fn) and os.access(fn, os.X_OK)",if os . path . exists ( fn ) :,if os . path . isfile ( fn ) :,65.80370064762461,65.80370064762461,0.0
"def decoration_helper(self, patched, args, keywargs):     extra_args = []     with contextlib.ExitStack() as exit_stack:         for patching in patched.patchings:             arg = exit_stack.enter_context(patching)             <MASK>                 keywargs.update(arg)             elif patching.new is DEFAULT:                 extra_args.append(arg)         args += tuple(extra_args)         yield (args, keywargs)",if patching . attribute_name is not None :,if patching . new is None :,20.95871245288356,20.95871245288356,0.0
"def update_neighbor(neigh_ip_address, changes):     rets = []     for k, v in changes.items():         if k == neighbors.MULTI_EXIT_DISC:             rets.append(_update_med(neigh_ip_address, v))         <MASK>             rets.append(update_neighbor_enabled(neigh_ip_address, v))         if k == neighbors.CONNECT_MODE:             rets.append(_update_connect_mode(neigh_ip_address, v))     return all(rets)",if k == neighbors . ENABLED :,if k == neighbors . ENABLE :,70.71067811865478,70.71067811865478,0.0
"def calcUniqueStates(self):     # Here we show which colors can be relied on to map to an     # internal state.  The current position will be at the first     # character in the buffer styled that color, so this might not     # work in all cases.     self.uniqueStates = {}     for k in self.holdUniqueStates.keys():         v = self.holdUniqueStates[k]         <MASK>             self.uniqueStates[k] = v.keys()[0]             log.debug(""Map style [%s] to state [%s]"", k, v.keys()[0])         log.debug(""Style [%s] maps to states [%s]"", k, "", "".join(v.keys()))     self.holdUniqueStates = None",if len ( v . keys ( ) ) == 1 :,if v :,1.4157233641833757,0.0,0.0
"def init_logger():     configured_loggers = [log_config.get(""root"", {})] + [         logger for logger in log_config.get(""loggers"", {}).values()     ]     used_handlers = {         handler for log in configured_loggers for handler in log.get(""handlers"", [])     }     for handler_id, handler in list(log_config[""handlers""].items()):         <MASK>             del log_config[""handlers""][handler_id]         elif ""filename"" in handler.keys():             filename = handler[""filename""]             logfile_path = Path(filename).expanduser().resolve()             handler[""filename""] = str(logfile_path)     logging.config.dictConfig(log_config)",if handler_id not in used_handlers :,if handler_id in used_handlers :,66.90484408935988,66.90484408935988,0.0
"def _selected_machines(self, virtual_machines):     selected_machines = []     for machine in virtual_machines:         <MASK>             selected_machines.append(machine)         if self.tags and self._tags_match(machine.tags, self.tags):             selected_machines.append(machine)         if self.locations and machine.location in self.locations:             selected_machines.append(machine)     return selected_machines",if self . _args . host and self . _args . host == machine . name :,if machine . name in self . names :,7.150682764832732,7.150682764832732,0.0
"def init(self):     r = self.get_redis()     if r:         key = ""pocsuite_target""         info_msg = ""[PLUGIN] try fetch targets from redis...""         logger.info(info_msg)         targets = r.get(key)         count = 0         if targets:             for target in targets:                 <MASK>                     count += 1         info_msg = ""[PLUGIN] get {0} target(s) from redis"".format(count)         logger.info(info_msg)",if self . add_target ( target ) :,if target . id == self . id :,10.552670315936318,10.552670315936318,0.0
"def tearDown(self):     suffix = str(os.getgid())     cli = monitoring_v3.MetricServiceClient()     for md in cli.list_metric_descriptors(""projects/{}"".format(PROJECT)):         <MASK>             try:                 cli.delete_metric_descriptor(md.name)             except Exception:                 pass","if ""OpenCensus"" in md . name and suffix in md . name :",if suffix == md . name :,15.949263498192172,15.949263498192172,0.0
"def InitializeColours(self):     """"""Initializes the 16 custom colours in :class:`CustomPanel`.""""""     curr = self._colourData.GetColour()     self._colourSelection = -1     for i in range(16):         c = self._colourData.GetCustomColour(i)         if c.IsOk():             self._customColours[i] = self._colourData.GetCustomColour(i)         else:             self._customColours[i] = wx.WHITE         <MASK>             self._colourSelection = i",if c == curr :,if curr == self . _colourData :,11.99014838091355,11.99014838091355,0.0
"def __getitem__(self, index):     if self._check():         if isinstance(index, int):             if index < 0 or index >= len(self.features):                 raise IndexError(index)             if self.features[index] is None:                 feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index)                 <MASK>                     (feature,) = _unpack(""!H"", feature[:2])                     self.features[index] = FEATURE[feature]             return self.features[index]         elif isinstance(index, slice):             indices = index.indices(len(self.features))             return [self.__getitem__(i) for i in range(*indices)]",if feature :,if feature is not None :,17.965205598154213,17.965205598154213,0.0
"def _get_data_from_buffer(obj):     try:         view = memoryview(obj)     except TypeError:         # try to use legacy buffer protocol if 2.7, otherwise re-raise         <MASK>             view = memoryview(buffer(obj))             warnings.warn(                 ""using old buffer interface to unpack %s; ""                 ""this leads to unpacking errors if slicing is used and ""                 ""will be removed in a future version"" % type(obj),                 RuntimeWarning,                 stacklevel=3,             )         else:             raise     if view.itemsize != 1:         raise ValueError(""cannot unpack from multi-byte object"")     return view",if PY2 :,"if isinstance ( obj , bytes ) :",6.567274736060395,6.567274736060395,0.0
"def import_modules(modules, safe=True):     """"""Safely import a list of *modules*""""""     all = []     for mname in modules:         if mname.endswith("".*""):             to_load = expand_star(mname)         else:             to_load = [mname]         for module in to_load:             try:                 all.append(import_module(module))             except ImportError:                 <MASK>                     raise     return all",if not safe :,if safe :,45.13864405503391,0.0,0.0
"def pack(types, *args):     if len(types) != len(args):         raise Exception(""number of arguments does not match format string"")     port = StringIO()     for (type, value) in zip(types, args):         if type == ""V"":             write_vuint(port, value)         <MASK>             write_vint(port, value)         elif type == ""s"":             write_bvec(port, value)         else:             raise Exception('unknown xpack format string item ""' + type + '""')     return port.getvalue()","elif type == ""v"" :","if type == ""V"" :",41.11336169005198,41.11336169005198,0.0
"def create_local_app_folder(local_app_path):     if exists(local_app_path):         raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path)     for folder in subfolders(local_app_path):         <MASK>             os.mkdir(folder)             init_path = join(folder, ""__init__.py"")             if not exists(init_path):                 create_file(init_path)",if not exists ( folder ) :,if not exists ( folder ) :,100.00000000000004,100.00000000000004,1.0
"def _get_node_type_specific_fields(self, node_id: str, fields_key: str) -> Any:     fields = self.config[fields_key]     node_tags = self.provider.node_tags(node_id)     if TAG_RAY_USER_NODE_TYPE in node_tags:         node_type = node_tags[TAG_RAY_USER_NODE_TYPE]         <MASK>             raise ValueError(f""Unknown node type tag: {node_type}."")         node_specific_config = self.available_node_types[node_type]         if fields_key in node_specific_config:             fields = node_specific_config[fields_key]     return fields",if node_type not in self . available_node_types :,if node_type not in self . available_node_types :,100.00000000000004,100.00000000000004,1.0
"def _maybe_fix_sequence_in_union(     aliases: List[Alias], typecst: cst.SubscriptElement ) -> cst.SubscriptElement:     slc = typecst.slice     if isinstance(slc, cst.Index):         val = slc.value         <MASK>             return cst.ensure_type(                 typecst.deep_replace(val, _get_clean_type_from_subscript(aliases, val)),                 cst.SubscriptElement,             )     return typecst","if isinstance ( val , cst . Subscript ) :",if val is not None :,5.484411595600381,5.484411595600381,0.0
"def cancel_download(self, downloads):     # Make sure we're always dealing with a list     if isinstance(downloads, Download):         downloads = [downloads]     for download in downloads:         <MASK>             self.cancel_current_download()         else:             self.__paused = True             new_queue = queue.Queue()             while not self.__queue.empty():                 queued_download = self.__queue.get()                 if download == queued_download:                     download.cancel()                 else:                     new_queue.put(queued_download)             self.__queue = new_queue             self.__paused = False",if download == self . __current_download :,if download . is_current ( ) :,10.693319442988287,10.693319442988287,0.0
"def migrate_account_metadata(account_id):     from inbox.models.session import session_scope     from inbox.models import Account     with session_scope(versioned=False) as db_session:         account = db_session.query(Account).get(account_id)         if account.discriminator == ""easaccount"":             create_categories_for_easfoldersyncstatuses(account, db_session)         else:             create_categories_for_folders(account, db_session)         <MASK>             set_labels_for_imapuids(account, db_session)         db_session.commit()","if account . discriminator == ""gmailaccount"" :","if account . discriminator == ""imapuids"" :",70.71067811865478,70.71067811865478,0.0
"def __init__(self, fmt=None, *args):     if not isinstance(fmt, BaseException):         Error.__init__(self, fmt, *args)     else:         e = fmt         cls = e.__class__         fmt = ""%s.%s: %s"" % (cls.__module__, cls.__name__, e)         tb = sys.exc_info()[2]         <MASK>             fmt += ""\n""             fmt += """".join(traceback.format_tb(tb))         Error.__init__(self, fmt)",if tb :,if tb :,100.00000000000004,0.0,1.0
"def setLabel(self, label):     if label is None:         <MASK>             self.label.scene().removeItem(self.label)             self.label = None     else:         if self.label is None:             self.label = TextItem()             self.label.setParentItem(self)         self.label.setText(label)         self._updateLabel()",if self . label is not None :,if self . label is not None :,100.00000000000004,100.00000000000004,1.0
"def serve_until_stopped(self) -> None:     while True:         rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout)         <MASK>             self.handle_request()         if self.event is not None and self.event.is_set():             break",if rd :,if ex is None :,12.703318703865365,12.703318703865365,0.0
"def generateCompressedFile(inputfile, outputfile, formatstring):     try:         <MASK>             in_file = open(inputfile, ""rb"")             in_data = in_file.read()             out_file = open(inputfile + "".xz"", ""wb"")             out_file.write(xz.compress(in_data))             in_file.close()             out_file.close()         else:             tarout = tarfile.open(outputfile, formatstring)             tarout.add(inputfile, arcname=os.path.basename(inputfile))             tarout.close()     except Exception as e:         print(e)         return False     return True","if formatstring == ""w:xz"" :","if formatstring == ""xz"" :",53.849523560640876,53.849523560640876,0.0
"def _datastore_get_handler(signal, sender, keys, **kwargs):     txn = current_transaction()     if txn:         for key in keys:             <MASK>                 raise PreventedReadError(                     ""Attempted to read key (%s:%s) inside a transaction ""                     ""where it was marked protected"" % (key.kind(), key.id_or_name())                 )         txn._fetched_keys.update(set(keys))",if key in txn . _protected_keys :,"if key . kind ( ) != ""protected"" :",8.516593018819643,8.516593018819643,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             self.set_access_token(d.getPrefixedString())             continue         if tt == 16:             self.set_expiration_time(d.getVarInt64())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 10 :,if tt == 8 :,53.7284965911771,53.7284965911771,0.0
"def write_vuint(port, x):     if x < 0:         raise Exception(""vuints must not be negative"")     elif x == 0:         port.write(""\0"")     else:         while x:             seven_bits = x & 0x7F             x >>= 7             <MASK>                 port.write(chr(0x80 | seven_bits))             else:                 port.write(chr(seven_bits))",if x :,if seven_bits != 0 :,6.567274736060395,6.567274736060395,0.0
"def _expand_srcs(self):     """"""Expand src to [(src, full_path)]""""""     result = []     for src in self.srcs:         full_path = self._source_file_path(src)         <MASK>             # Assume generated             full_path = self._target_file_path(src)         result.append((src, full_path))     return result",if not os . path . exists ( full_path ) :,if full_path :,9.121704909091086,9.121704909091086,0.0
"def pytest_collection_modifyitems(items):     for item in items:         if item.nodeid.startswith(""tests/ops""):             if ""stage"" not in item.keywords:                 item.add_marker(pytest.mark.stage(""unit""))             <MASK>                 item.add_marker(pytest.mark.init(rng_seed=123))","if ""init"" not in item . keywords :","if ""init"" not in item . keywords :",100.00000000000004,100.00000000000004,1.0
"def set_shape(self, shape):     """"""Sets a shape.""""""     if self._shape is not None:         logger.warning('Modifying the shape of Placeholder ""%s"".', self.name)     if not isinstance(shape, (list, tuple)):         shape = (shape,)     shape = tuple(x if x != ""None"" else None for x in shape)     for x in shape:         <MASK>             raise ParsingError(                 'All entries in ""shape"" must be integers, or in special '                 ""cases None. Shape is: {}"".format(shape)             )     self._shape = shape","if not isinstance ( x , ( int , type ( None ) ) ) :","if not isinstance ( shape , ( int , float ) ) :",37.78066027750078,37.78066027750078,0.0
"def _get_field_actual(cant_be_number, raw_string, field_names):     for line in raw_string.splitlines():         for field_name in field_names:             field_name = field_name.lower()             if "":"" in line:                 left, right = line.split("":"", 1)                 left = left.strip().lower()                 right = right.strip()                 <MASK>                     if cant_be_number:                         if not right.isdigit():                             return right                     else:                         return right     return None",if left == field_name and len ( right ) > 0 :,if not left . isdigit ( ) :,3.442409372343047,3.442409372343047,0.0
"def validate_attributes(self):     for attribute in self.get_all_attributes():         value = getattr(self, attribute.code, None)         if value is None:             <MASK>                 raise ValidationError(                     _(""%(attr)s attribute cannot be blank"") % {""attr"": attribute.code}                 )         else:             try:                 attribute.validate_value(value)             except ValidationError as e:                 raise ValidationError(                     _(""%(attr)s attribute %(err)s"") % {""attr"": attribute.code, ""err"": e}                 )",if attribute . required :,if not attribute . is_blank :,13.134549472120788,13.134549472120788,0.0
"def append(self, s):     buf = self.buf     if buf is None:         strbuf = self.strbuf         <MASK>             self.strbuf = strbuf + s             return         buf = self._create_buffer()     buf.append(s)     # use buf.__len__ rather than len(buf) FBO of not getting     # OverflowError on Python 2     sz = buf.__len__()     if not self.overflowed:         if sz >= self.overflow:             self._set_large_buffer()",if len ( strbuf ) + len ( s ) < STRBUF_LIMIT :,if len ( strbuf ) > 0 :,22.555664749005537,22.555664749005537,0.0
"def billing_invoice_show_validator(namespace):     from azure.cli.core.azclierror import (         RequiredArgumentMissingError,         MutuallyExclusiveArgumentError,     )     valid_combs = (         ""only --account-name, --name / --name / --name, --by-subscription is valid""     )     if namespace.account_name is not None:         <MASK>             raise MutuallyExclusiveArgumentError(valid_combs)         if namespace.name is None:             raise RequiredArgumentMissingError(""--name is also required"")     if namespace.by_subscription is not None:         if namespace.name is None:             raise RequiredArgumentMissingError(""--name is also required"")",if namespace . by_subscription is not None :,if namespace . by_subscription is not None :,100.00000000000004,100.00000000000004,1.0
"def Handle(self, args, context=None):     for client_id in args.client_ids:         cid = str(client_id)         data_store.REL_DB.RemoveClientLabels(cid, context.username, args.labels)         labels_to_remove = set(args.labels)         existing_labels = data_store.REL_DB.ReadClientLabels(cid)         for label in existing_labels:             labels_to_remove.discard(label.name)         <MASK>             idx = client_index.ClientIndex()             idx.RemoveClientLabels(cid, labels_to_remove)",if labels_to_remove :,if len ( labels_to_remove ) > 0 :,34.48444257953326,34.48444257953326,0.0
"def delete_snapshot(self, snapshot):     snap_name = self._get_snap_name(snapshot[""id""])     LOG.debug(""Deleting snapshot (%s)"", snapshot[""id""])     self.client_login()     try:         self.client.delete_snapshot(snap_name, self.backend_type)     except exception.DotHillRequestError as ex:         # if the volume wasn't found, ignore the error         <MASK>             return         LOG.exception(""Deleting snapshot %s failed"", snapshot[""id""])         raise exception.Invalid(ex)     finally:         self.client_logout()","if ""The volume was not found on this system."" in ex . args :",if ex . status == 404 :,4.264163893764324,4.264163893764324,0.0
"def jobs(self):     # How many jobs have we done?     total_processed = 0     for jobEntity in self.jobItems.query_entities():         # Process the items in the page         yield AzureJob.fromEntity(jobEntity)         total_processed += 1         <MASK>             # Produce some feedback for the user, because this can take             # a long time on, for example, Azure             logger.debug(""Processed %d total jobs"" % total_processed)     logger.debug(""Processed %d total jobs"" % total_processed)",if total_processed % 1000 == 0 :,if total_processed > self . total_jobs :,24.808415001701817,24.808415001701817,0.0
def run(self):     while not self.completed:         if self.block:             time.sleep(self.period)         else:             self._completed.wait(self.period)         self.counter += 1         try:             self.callback(self.counter)         except Exception:             self.stop()         if self.timeout is not None:             dt = time.time() - self._start_time             if dt > self.timeout:                 self.stop()         <MASK>             self.stop(),if self . counter == self . count :,if self . block :,15.719010513286515,15.719010513286515,0.0
"def get_instance(cls, pool_size=None):     if cls._instance is not None:         return cls._instance     # Lazy init     with cls._SINGLETON_LOCK:         <MASK>             cls._instance = cls(                 ARCTIC_ASYNC_NWORKERS if pool_size is None else pool_size             )     return cls._instance",if cls . _instance is None :,if pool_size is not None :,13.888095170058955,13.888095170058955,0.0
"def set_state(self, state):     if self._inhibit_play:         # PLAYING, PAUSED change the state for after buffering is finished,         # everything else aborts buffering         <MASK>             # abort             self.__set_inhibit_play(False)             self.bin.set_state(state)             return         self._wanted_state = state     else:         self.bin.set_state(state)","if state not in ( Gst . State . PLAYING , Gst . State . PAUSED ) :",if state == self . _wanted_state :,4.71728363692704,4.71728363692704,0.0
"def seen_add(options):     seen_name = options.add_value     if is_imdb_url(seen_name):         console(""IMDB url detected, try to parse ID"")         imdb_id = extract_id(seen_name)         <MASK>             seen_name = imdb_id         else:             console(""Could not parse IMDB ID"")     db.add(seen_name, ""cli_add"", {""cli_add"": seen_name})     console(""Added %s as seen. This will affect all tasks."" % seen_name)",if imdb_id :,if imdb_id :,100.00000000000004,100.00000000000004,1.0
"def test_204_invalid_content_length(self):     # 204 status with non-zero content length is malformed     with ExpectLog(gen_log, "".*Response with code 204 should not have body""):         response = self.fetch(""/?error=1"")         if not self.http1:             self.skipTest(""requires HTTP/1.x"")         <MASK>             self.skipTest(""curl client accepts invalid headers"")         self.assertEqual(response.code, 599)",if self . http_client . configured_class != SimpleAsyncHTTPClient :,if response . status_code != 204 :,7.40354787297858,7.40354787297858,0.0
"def set_related_perm(_mapper: Mapper, _connection: Connection, target: Slice) -> None:     src_class = target.cls_model     id_ = target.datasource_id     if id_:         ds = db.session.query(src_class).filter_by(id=int(id_)).first()         <MASK>             target.perm = ds.perm             target.schema_perm = ds.schema_perm",if ds :,if ds :,100.00000000000004,0.0,1.0
"def on_modified_async(self, view):     if self.is_command_line(view):         <MASK>             view.run_command(""text_pastry_selection_preview"")","if view . size ( ) > 6 and view . substr ( sublime . Region ( 0 , 6 ) ) . lower ( ) == ""search"" :",if self . is_command_line ( view ) :,1.0010702899112565,1.0010702899112565,0.0
"def _improve_answer_span(     doc_tokens, input_start, input_end, tokenizer, orig_answer_text ):     """"""Returns tokenized answer spans that better match the annotated answer.""""""     tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))     for new_start in range(input_start, input_end + 1):         for new_end in range(input_end, new_start - 1, -1):             text_span = "" "".join(doc_tokens[new_start : (new_end + 1)])             <MASK>                 return new_start, new_end     return input_start, input_end",if text_span == tok_answer_text :,if tok_answer_text == text_span :,48.76836638685217,48.76836638685217,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             self.set_url(d.getPrefixedString())             continue         if tt == 18:             self.set_app_version_id(d.getPrefixedString())             continue         if tt == 26:             self.set_method(d.getPrefixedString())             continue         if tt == 34:             self.set_queue(d.getPrefixedString())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 10 :,if tt == 17 :,53.7284965911771,53.7284965911771,0.0
"def _add_resource_group(obj):     if isinstance(obj, list):         for array_item in obj:             _add_resource_group(array_item)     elif isinstance(obj, dict):         try:             <MASK>                 if obj[""id""]:                     obj[""resourceGroup""] = _parse_id(obj[""id""])[""resource-group""]         except (KeyError, IndexError, TypeError):             pass         for item_key in obj:             if item_key != ""sourceVault"":                 _add_resource_group(obj[item_key])","if ""resourcegroup"" not in [ x . lower ( ) for x in obj . keys ( ) ] :","if obj [ ""id"" ] :",2.6251815872142057,2.6251815872142057,0.0
"def build(opt):     dpath = os.path.join(opt[""datapath""], DECODE)     version = DECODE_VERSION     if not build_data.built(dpath, version_string=version):         print(""[building data: "" + dpath + ""]"")         <MASK>             # An older version exists, so remove these outdated files.             build_data.remove_dir(dpath)         build_data.make_dir(dpath)         # Download the data.         for downloadable_file in RESOURCES:             downloadable_file.download_file(dpath)         # Mark the data as built.         build_data.mark_done(dpath, version_string=version)",if build_data . built ( dpath ) :,if version_string in VERSIONS :,5.630400552901077,5.630400552901077,0.0
"def toterminal(self, tw):     # the entries might have different styles     last_style = None     for i, entry in enumerate(self.reprentries):         <MASK>             tw.line("""")         entry.toterminal(tw)         if i < len(self.reprentries) - 1:             next_entry = self.reprentries[i + 1]             if (                 entry.style == ""long""                 or entry.style == ""short""                 and next_entry.style == ""long""             ):                 tw.sep(self.entrysep)     if self.extraline:         tw.line(self.extraline)","if entry . style == ""long"" :",if last_style :,5.171845311465849,5.171845311465849,0.0
"def reposition_division(f1):     lines = f1.splitlines()     if lines[2] == division:         lines.pop(2)     found = 0     for i, line in enumerate(lines):         <MASK>             found += 1             if found == 2:                 if division in ""\n"".join(lines):                     break  # already in the right place                 lines.insert(i + 1, """")                 lines.insert(i + 2, division)                 break     return ""\n"".join(lines)","if line . startswith ( '""""""' ) :","if line == ""\n"" :",8.591316733350183,8.591316733350183,0.0
def run_on_module(self):     try:         self.module_base.disable(self.opts.module_spec)     except dnf.exceptions.MarkingErrors as e:         if self.base.conf.strict:             <MASK>                 raise e             if (                 e.module_depsolv_errors                 and e.module_depsolv_errors[1]                 != libdnf.module.ModulePackageContainer.ModuleErrorType_ERROR_IN_DEFAULTS             ):                 raise e         logger.error(str(e)),if e . no_match_group_specs or e . error_group_specs :,if e . module_depsolv_errors :,6.947730141309741,6.947730141309741,0.0
"def test_len(self):     eq = self.assertEqual     eq(base64mime.base64_len(""hello""), len(base64mime.encode(""hello"", eol="""")))     for size in range(15):         if size == 0:             bsize = 0         elif size <= 3:             bsize = 4         elif size <= 6:             bsize = 8         <MASK>             bsize = 12         elif size <= 12:             bsize = 16         else:             bsize = 20         eq(base64mime.base64_len(""x"" * size), bsize)",elif size <= 9 :,if size <= 7 :,32.46679154750991,32.46679154750991,0.0
"def is_valid(self):     """"""Determines whether file is valid for this reader""""""     blocklist = self.open()     valid = True     for line in blocklist:         line = decode_bytes(line)         <MASK>             try:                 (start, end) = self.parse(line)                 if not re.match(r""^(\d{1,3}\.){4}$"", start + ""."") or not re.match(                     r""^(\d{1,3}\.){4}$"", end + "".""                 ):                     valid = False             except Exception:                 valid = False             break     blocklist.close()     return valid",if not self . is_ignored ( line ) :,if not line :,6.6019821735025035,6.6019821735025035,0.0
"def next(self):     while self.index < len(self.data):         uid = self._read_next_word()         dont_care = self._read_next_word()         entry = self._read_next_string()         total_size = int(4 + 4 + len(entry))         count = int(total_size / self.SIZE)         if count == 0:             mod = self.SIZE - total_size         else:             mod = self.SIZE - int(total_size - (count * self.SIZE))         <MASK>             remainder = self._read_next_block(mod)         yield (uid, entry)",if mod > 0 :,if dont_care == 0 :,12.22307556087252,12.22307556087252,0.0
"def _str_param_list(self, name):     out = []     if self[name]:         out += self._str_header(name)         for param in self[name]:             parts = []             if param.name:                 parts.append(param.name)             if param.type:                 parts.append(param.type)             out += ["" : "".join(parts)]             <MASK>                 out += self._str_indent(param.desc)         out += [""""]     return out","if param . desc and """" . join ( param . desc ) . strip ( ) :",if param . desc :,4.299920764667028,4.299920764667028,0.0
"def assert_backend(self, expected_translated, language=""cs""):     """"""Check that backend has correct data.""""""     translation = self.get_translation(language)     translation.commit_pending(""test"", None)     store = translation.component.file_format_cls(translation.get_filename(), None)     messages = set()     translated = 0     for unit in store.content_units:         id_hash = unit.id_hash         self.assertFalse(id_hash in messages, ""Duplicate string in in backend file!"")         <MASK>             translated += 1     self.assertEqual(         translated,         expected_translated,         ""Did not found expected number of translations ({} != {})."".format(             translated, expected_translated         ),     )",if unit . is_translated ( ) :,if unit . id_hash == expected_translated :,16.59038701421971,16.59038701421971,0.0
"def status(self, name, error=""No matching script logs found""):     with self.script_lock:         <MASK>             return self.script_running[1:]         elif self.script_last and self.script_last[1] == name:             return self.script_last[1:]         else:             raise ValueError(error)",if self . script_running and self . script_running [ 1 ] == name :,if self . script_running :,15.14389796999661,15.14389796999661,0.0
"def dict_no_value_from_proto_list(obj_list):     d = dict()     for item in obj_list:         possible_dict = json.loads(item.value_json)         <MASK>             # (tss) TODO: This is protecting against legacy 'wandb_version' field.             # Should investigate why the config payload even has 'wandb_version'.             logger.warning(""key '{}' has no 'value' attribute"".format(item.key))             continue         d[item.key] = possible_dict[""value""]     return d","if not isinstance ( possible_dict , dict ) or ""value"" not in possible_dict :",if possible_dict is None :,4.336364542223566,4.336364542223566,0.0
"def visit(self, node):     """"""dispatcher on node's class/bases name.""""""     cls = node.__class__     try:         visitmethod = self.cache[cls]     except KeyError:         for subclass in cls.__mro__:             visitmethod = getattr(self, subclass.__name__, None)             <MASK>                 break         else:             visitmethod = self.__object         self.cache[cls] = visitmethod     visitmethod(node)",if visitmethod is not None :,if visitmethod is None :,40.93653765389909,40.93653765389909,0.0
"def _get_adapter(     mcls,     reversed_mro: Tuple[type, ...],     collection: Dict[Any, Dict[type, Adapter]],     kwargs: Dict[str, Any], ) -> Optional[Adapter]:     registry_key = mcls.get_registry_key(kwargs)     adapters = collection.get(registry_key)     if adapters is None:         return None     result = None     seen: Set[Adapter] = set()     for base in reversed_mro:         for adaptee, adapter in adapters.items():             found = mcls._match_adapter(base, adaptee, adapter)             <MASK>                 result = found                 seen.add(found)     return result",if found and found not in seen :,if found in seen :,27.440581804701317,27.440581804701317,0.0
"def test_pt_BR_rg(self):     for _ in range(100):         to_test = self.fake.rg()         <MASK>             assert re.search(r""^\d{8}X"", to_test)         else:             assert re.search(r""^\d{9}$"", to_test)","if ""X"" in to_test :",if self . fake . is_hex :,6.27465531099474,6.27465531099474,0.0
"def get_user_extra_data_by_client_id(self, client_id, username):     extra_data = {}     current_client = self.clients.get(client_id, None)     if current_client:         for readable_field in current_client.get_readable_fields():             attribute = list(                 filter(                     lambda f: f[""Name""] == readable_field,                     self.users.get(username).attributes,                 )             )             <MASK>                 extra_data.update({attribute[0][""Name""]: attribute[0][""Value""]})     return extra_data",if len ( attribute ) > 0 :,if len ( attribute ) == 1 :,46.713797772819994,46.713797772819994,0.0
"def augment(self, resources):     super().augment(resources)     for r in resources:         md = r.get(""SAMLMetadataDocument"")         <MASK>             continue         root = sso_metadata(md)         r[""IDPSSODescriptor""] = root[""IDPSSODescriptor""]     return resources",if not md :,if not md :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, mode=0, decode=None):     self.regex = self.REGEX[mode]     self.decode = decode     if decode:         self.header = _(             ""### This log has been decoded with automatic search pattern\n""             ""### If some paths are not decoded you can manually decode them with:\n""         )         self.header += ""### 'backintime --quiet ""         <MASK>             self.header += '--profile ""%s"" ' % decode.config.profileName()         self.header += ""--decode <path>'\n\n""     else:         self.header = """"",if int ( decode . config . currentProfile ( ) ) > 1 :,if decode . config . profileName ( ) :,21.88390569648905,21.88390569648905,0.0
"def _get_dynamic_attr(self, attname, obj, default=None):     try:         attr = getattr(self, attname)     except AttributeError:         return default     if callable(attr):         # Check co_argcount rather than try/excepting the function and         # catching the TypeError, because something inside the function         # may raise the TypeError. This technique is more accurate.         try:             code = six.get_function_code(attr)         except AttributeError:             code = six.get_function_code(attr.__call__)         <MASK>  # one argument is 'self'             return attr(obj)         else:             return attr()     return attr",if code . co_argcount == 2 :,"if code in [ 'self' , 'obj' ] :",9.442944296079734,9.442944296079734,0.0
"def grep_full_py_identifiers(tokens):     global pykeywords     tokens = list(tokens)     i = 0     while i < len(tokens):         tokentype, token = tokens[i]         i += 1         <MASK>             continue         while (             i + 1 < len(tokens)             and tokens[i] == (""op"", ""."")             and tokens[i + 1][0] == ""id""         ):             token += ""."" + tokens[i + 1][1]             i += 2         if token == """":             continue         if token in pykeywords:             continue         if token[0] in "".0123456789"":             continue         yield token","if tokentype != ""id"" :","if tokentype == ""op"" :",19.130147081392227,19.130147081392227,0.0
"def _add_disk_config(self, context, images):     for image in images:         metadata = image[""metadata""]         <MASK>             raw_value = metadata[INTERNAL_DISK_CONFIG]             value = utils.bool_from_str(raw_value)             image[API_DISK_CONFIG] = disk_config_to_api(value)",if INTERNAL_DISK_CONFIG in metadata :,"if ""internal_disk_config"" in metadata :",16.59038701421971,16.59038701421971,0.0
"def test_edgeql_expr_valid_setop_07(self):     expected_error_msg = ""cannot be applied to operands""     # IF ELSE with every scalar as the condition     for val in get_test_values():         query = f""""""SELECT 1 IF {val} ELSE 2;""""""         <MASK>             await self.assert_query_result(query, [1])         else:             # every other combination must produce an error             with self.assertRaisesRegex(                 edgedb.QueryError, expected_error_msg, msg=query             ):                 async with self.con.transaction():                     await self.con.execute(query)","if val == ""<bool>True"" :","if val == ""1"" :",40.866465020165684,40.866465020165684,0.0
"def get_all_url_infos() -> Dict[str, UrlInfo]:     """"""Returns dict associating URL to UrlInfo.""""""     url_infos = {}     for path in _checksum_paths().values():         dataset_url_infos = load_url_infos(path)         for url, url_info in dataset_url_infos.items():             <MASK>                 raise AssertionError(                     ""URL {} is registered with 2+ distinct size/checksum tuples. ""                     ""{} vs {}"".format(url, url_info, url_infos[url])                 )         url_infos.update(dataset_url_infos)     return url_infos","if url_infos . get ( url , url_info ) != url_info :",if url_info . size != 2 :,11.476641793765525,11.476641793765525,0.0
"def global_fixes():     """"""Yield multiple (code, function) tuples.""""""     for function in list(globals().values()):         <MASK>             arguments = _get_parameters(function)             if arguments[:1] != [""source""]:                 continue             code = extract_code_from_function(function)             if code:                 yield (code, function)",if inspect . isfunction ( function ) :,"if arguments [ 0 ] != ""code"" :",4.456882760699063,4.456882760699063,0.0
"def createSocket(self):     skt = Port.createSocket(self)     if self.listenMultiple:         skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)         <MASK>             skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)     return skt","if hasattr ( socket , ""SO_REUSEPORT"" ) :",if self . listenMultiple :,3.1325998243558226,3.1325998243558226,0.0
"def _asStringList(self, sep=""""):     out = []     for item in self._toklist:         if out and sep:             out.append(sep)         <MASK>             out += item._asStringList()         else:             out.append(str(item))     return out","if isinstance ( item , ParseResults ) :",if item . _isString ( ) :,13.888095170058955,13.888095170058955,0.0
"def parse_c_comments(lexer, tok, ntok):     if tok != ""/"" or ntok != ""*"":         return False     quotes = lexer.quotes     lexer.quotes = """"     while True:         tok = lexer.get_token()         ntok = lexer.get_token()         <MASK>             lexer.quotes = quotes             break         else:             lexer.push_token(ntok)     return True","if tok == ""*"" and ntok == ""/"" :","if tok == ""\\"" :",26.356013563443188,26.356013563443188,0.0
"def doWorkForFindAll(self, v, target, partialMatch):     sibling = self     while sibling:         c1 = partialMatch and sibling.equalsTreePartial(target)         if c1:             v.append(sibling)         else:             c2 = not partialMatch and sibling.equalsTree(target)             <MASK>                 v.append(sibling)         ### regardless of match or not, check any children for matches         if sibling.getFirstChild():             sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch)         sibling = sibling.getNextSibling()",if c2 :,if c2 :,100.00000000000004,0.0,1.0
"def __view_beside(self, onsideof, **kwargs):     bounds = self.info[""bounds""]     min_dist, found = -1, None     for ui in UiObject(self.session, Selector(**kwargs)):         dist = onsideof(bounds, ui.info[""bounds""])         <MASK>             min_dist, found = dist, ui     return found",if dist >= 0 and ( min_dist < 0 or dist < min_dist ) :,if dist < min_dist :,11.039212850851191,11.039212850851191,0.0
"def __eq__(self, other):     if isinstance(other, numeric_range):         empty_self = not bool(self)         empty_other = not bool(other)         <MASK>             return empty_self and empty_other  # True if both empty         else:             return (                 self._start == other._start                 and self._step == other._step                 and self._get_by_index(-1) == other._get_by_index(-1)             )     else:         return False",if empty_self or empty_other :,if self is empty_other :,34.191776499651844,34.191776499651844,0.0
"def _buffered_generator(self, size):     buf = []     c_size = 0     push = buf.append     while 1:         try:             while c_size < size:                 c = next(self._gen)                 push(c)                 if c:                     c_size += 1         except StopIteration:             <MASK>                 return         yield concat(buf)         del buf[:]         c_size = 0",if not c_size :,if c_size == size :,25.848657697858535,25.848657697858535,0.0
"def connect(self):     with self._conn_lock:         <MASK>             raise Exception(                 ""Error, database not properly initialized "" ""before opening connection""             )         with self.exception_wrapper():             self.__local.conn = self._connect(self.database, **self.connect_kwargs)             self.__local.closed = False             self.initialize_connection(self.__local.conn)",if self . deferred :,if not self . database :,19.304869754804482,19.304869754804482,0.0
"def _merge_substs(self, subst, new_substs):     subst = subst.copy()     for new_subst in new_substs:         for name, var in new_subst.items():             <MASK>                 subst[name] = var             elif subst[name] is not var:                 subst[name].PasteVariable(var)     return subst",if name not in subst :,if var is None :,10.400597689005304,10.400597689005304,0.0
"def remove(self, tag):     """"""Removes a tag recursively from all containers.""""""     new_contents = []     self.content_size = 0     for element in self.contents:         if element.name != tag:             new_contents.append(element)             <MASK>                 element.remove(tag)             self.content_size += element.size()     self.contents = new_contents","if isinstance ( element , Container ) :",if element . tag == tag :,7.267884212102741,7.267884212102741,0.0
"def _create_object(self, obj_body):     props = obj_body[SYMBOL_PROPERTIES]     for prop_name, prop_value in props.items():         if isinstance(prop_value, dict) and prop_value:             # get the first key as the convert function             func_name = list(prop_value.keys())[0]             <MASK>                 func = getattr(self, func_name)                 props[prop_name] = func(prop_value[func_name])     if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping:         return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props)     else:         return props","if func_name . startswith ( ""_"" ) :",if func_name :,17.437038542312457,17.437038542312457,0.0
"def visit_try_stmt(self, o: ""mypy.nodes.TryStmt"") -> str:     a = [o.body]  # type: List[Any]     for i in range(len(o.vars)):         a.append(o.types[i])         <MASK>             a.append(o.vars[i])         a.append(o.handlers[i])     if o.else_body:         a.append((""Else"", o.else_body.body))     if o.finally_body:         a.append((""Finally"", o.finally_body.body))     return self.dump(a, o)",if o . vars [ i ] :,if o . handlers :,23.4500081062036,23.4500081062036,0.0
"def everythingIsUnicode(d):     """"""Takes a dictionary, recursively verifies that every value is unicode""""""     for k, v in d.iteritems():         if isinstance(v, dict) and k != ""headers"":             if not everythingIsUnicode(v):                 return False         elif isinstance(v, list):             for i in v:                 if isinstance(i, dict) and not everythingIsUnicode(i):                     return False                 <MASK>                     return False         elif isinstance(v, _bytes):             return False     return True","elif isinstance ( i , _bytes ) :","if isinstance ( v , _bytes ) :",51.33450480401705,51.33450480401705,0.0
"def msg_ser(inst, sformat, lev=0):     if sformat in [""urlencoded"", ""json""]:         if isinstance(inst, Message):             res = inst.serialize(sformat, lev)         else:             res = inst     elif sformat == ""dict"":         if isinstance(inst, Message):             res = inst.serialize(sformat, lev)         <MASK>             res = inst         elif isinstance(inst, str):  # Iff ID Token             res = inst         else:             raise MessageException(""Wrong type: %s"" % type(inst))     else:         raise PyoidcError(""Unknown sformat"", inst)     return res","elif isinstance ( inst , dict ) :","if isinstance ( inst , int ) :",41.11336169005198,41.11336169005198,0.0
"def start_container_if_stopped(self, container, attach_logs=False, quiet=False):     if not container.is_running:         <MASK>             log.info(""Starting %s"" % container.name)         if attach_logs:             container.attach_log_stream()         return self.start_container(container)",if not quiet :,if quiet :,45.13864405503391,0.0,0.0
"def layer_op(self, input_image, mask=None):     if not isinstance(input_image, dict):         self._set_full_border(input_image)         input_image = np.pad(input_image, self.full_border, mode=self.mode)         return input_image, mask     for name, image in input_image.items():         self._set_full_border(image)         <MASK>             tf.logging.warning(                 ""could not pad, dict name %s not in %s"", name, self.image_name             )             continue         input_image[name] = np.pad(image, self.full_border, mode=self.mode)     return input_image, mask",if name not in self . image_name :,if name not in self . image_name :,100.00000000000004,100.00000000000004,1.0
"def __Suffix_Noun_Step2b(self, token):     for suffix in self.__suffix_noun_step2b:         <MASK>             token = token[:-2]             self.suffix_noun_step2b_success = True             break     return token",if token . endswith ( suffix ) and len ( token ) >= 5 :,if suffix . startswith ( token [ - 2 ] ) :,6.637936228852375,6.637936228852375,0.0
"def replace_header_items(ps, replacments):     match = read_while(ps, header_item_or_end_re.match, lambda match: match is None)     while not ps.current_line.startswith(""*/""):         match = header_item_re.match(ps.current_line)         <MASK>             key = match.groupdict()[""key""]             if key in replacments:                 ps.current_line = match.expand(                     ""\g<key>\g<space>%s\n"" % replacments[key]                 )         ps.read_line()",if match is not None :,if match :,23.174952587773145,0.0,0.0
"def __projectBookmark(widget, location):     script = None     while widget is not None:         <MASK>             script = widget.scriptNode()             if isinstance(script, Gaffer.ScriptNode):                 break         widget = widget.parent()     if script is not None:         p = script.context().substitute(location)         if not os.path.exists(p):             try:                 os.makedirs(p)             except OSError:                 pass         return p     else:         return os.getcwd()","if hasattr ( widget , ""scriptNode"" ) :",if widget . context ( ) is None :,6.379653897348568,6.379653897348568,0.0
"def events_to_str(event_field, all_events):     result = []     for (flag, string) in all_events:         c_flag = flag         if event_field & c_flag:             result.append(string)             event_field = event_field & (~c_flag)         <MASK>             break     if event_field:         result.append(hex(event_field))     return ""|"".join(result)",if not event_field :,if not event_field :,100.00000000000004,100.00000000000004,1.0
"def get_s3_bucket_locations(buckets, self_log=False):     """"""return (bucket_name, prefix) for all s3 logging targets""""""     for b in buckets:         if b.get(""Logging""):             if self_log:                 if b[""Name""] != b[""Logging""][""TargetBucket""]:                     continue             yield (b[""Logging""][""TargetBucket""], b[""Logging""][""TargetPrefix""])         <MASK>             yield (b[""Name""], """")","if not self_log and b [ ""Name"" ] . startswith ( ""cf-templates-"" ) :","if b [ ""Name"" ] :",16.313279083724467,16.313279083724467,0.0
"def extract_file(tgz, tarinfo, dst_path, buffer_size=10 << 20, log_function=None):     """"""Extracts 'tarinfo' from 'tgz' and writes to 'dst_path'.""""""     src = tgz.extractfile(tarinfo)     if src is None:         return     dst = tf.compat.v1.gfile.GFile(dst_path, ""wb"")     while 1:         buf = src.read(buffer_size)         if not buf:             break         dst.write(buf)         <MASK>             log_function(len(buf))     dst.close()     src.close()",if log_function is not None :,if log_function :,38.80684294761701,38.80684294761701,0.0
"def make_index_fields(rec):     fields = {}     for k, v in rec.iteritems():         if k in (""lccn"", ""oclc"", ""isbn""):             fields[k] = v             continue         <MASK>             fields[""title""] = [read_short_title(v)]     return fields","if k == ""full_title"" :",if v :,3.361830360737634,0.0,0.0
"def disconnect_application(self):     if not self.is_app_running(self.APP_BACKDROP):         self.socket.send(commands.CloseCommand(destination_id=False))         start_time = time.time()         while not self.is_app_running(None):             try:                 self.socket.send_and_wait(commands.StatusCommand())             except cast_socket.ConnectionTerminatedException:                 break             current_time = time.time()             <MASK>                 raise TimeoutException()             time.sleep(self.WAIT_INTERVAL)     else:         logger.debug(""Closing not necessary. Backdrop is running ..."")",if current_time - start_time > self . timeout :,if current_time - start_time > self . WAIT_INTERVAL :,71.66258375282708,71.66258375282708,0.0
"def matches(self, cursor_offset, line, **kwargs):     cs = lineparts.current_string(cursor_offset, line)     if cs is None:         return None     matches = set()     username = cs.word.split(os.path.sep, 1)[0]     user_dir = os.path.expanduser(username)     for filename in self.safe_glob(os.path.expanduser(cs.word)):         if os.path.isdir(filename):             filename += os.path.sep         <MASK>             filename = username + filename[len(user_dir) :]         matches.add(filename)     return matches","if cs . word . startswith ( ""~"" ) :",if os . path . isdir ( filename ) :,9.042713792226897,9.042713792226897,0.0
"def eventFilter(self, obj, event):     if event.type() == QEvent.MouseButtonPress:         button = event.button()         <MASK>             self._app.browser.back()             return True         elif button == Qt.ForwardButton:             self._app.browser.forward()             return True     return False",if button == Qt . BackButton :,if button == Qt . BackButton :,100.00000000000004,100.00000000000004,1.0
"def reset_parameters(self):     for m in self.modules():         if isinstance(m, nn.Embedding):             continue         <MASK>             nn.init.constant_(m.weight, 0.1)             nn.init.constant_(m.bias, 0)         else:             for p in m.parameters():                 nn.init.normal_(p, 0, 0.1)","elif isinstance ( m , nn . LayerNorm ) :",if m . weight :,5.171845311465849,5.171845311465849,0.0
"def get_scalding_core(self):     lib_dir = os.path.join(self.scalding_home, ""lib"")     for j in os.listdir(lib_dir):         <MASK>             p = os.path.join(lib_dir, j)             logger.debug(""Found scalding-core: %s"", p)             return p     raise luigi.contrib.hadoop.HadoopJobError(""Could not find scalding-core."")","if j . startswith ( ""scalding-core-"" ) :",if os . path . isdir ( j ) :,11.044795567078939,11.044795567078939,0.0
"def save(self):     """"""Saves a new set of golden output frames to disk.""""""     for pixels, (relative_to_assets, filename) in zip(         self.iter_render(), self._iter_paths()     ):         full_directory_path = os.path.join(self._ASSETS_DIR, relative_to_assets)         <MASK>             os.makedirs(full_directory_path)         path = os.path.join(full_directory_path, filename)         _save_pixels(pixels, path)",if not os . path . exists ( full_directory_path ) :,if not os . path . isdir ( full_directory_path ) :,80.03203203845001,80.03203203845001,0.0
"def _fix_var_naming(operators, names, mod=""input""):     new_names = []     map = {}     for op in operators:         <MASK>             iter = op.inputs         else:             iter = op.outputs         for i in iter:             for name in names:                 if i.raw_name == name and name not in map:                     map[i.raw_name] = i.full_name         if len(map) == len(names):             break     for name in names:         new_names.append(map[name])     return new_names","if mod == ""input"" :","if mod == ""input"" :",100.00000000000004,100.00000000000004,1.0
"def Tokenize(s):     # type: (str) -> Iterator[Token]     for item in TOKEN_RE.findall(s):         # The type checker can't know the true type of item!         item = cast(TupleStr4, item)         if item[0]:             typ = ""number""             val = item[0]         <MASK>             typ = ""name""             val = item[1]         elif item[2]:             typ = item[2]             val = item[2]         elif item[3]:             typ = item[3]             val = item[3]         yield Token(typ, val)",elif item [ 1 ] :,if item [ 1 ] :,75.98356856515926,75.98356856515926,0.0
"def init_errorhandler():     # http error handling     for ex in default_exceptions:         if ex < 500:             app.register_error_handler(ex, error_http)         <MASK>             app.register_error_handler(ex, internal_error)     if services.ldap:         # Only way of catching the LDAPException upon logging in with LDAP server down         @app.errorhandler(services.ldap.LDAPException)         def handle_exception(e):             log.debug(""LDAP server not accessible while trying to login to opds feed"")             return error_http(FailedDependency())",elif ex == 500 :,if ex < 500 :,19.3576934939088,19.3576934939088,0.0
"def decode(self, ids):     ids = pad_decr(ids)     tokens = []     for int_id in ids:         <MASK>             tokens.append(self._vocab_list[int_id])         else:             tokens.append(self._oov_token)     return self._decode_token_separator.join(tokens)",if int_id < len ( self . _vocab_list ) :,if self . _vocab_list :,30.477217700631865,30.477217700631865,0.0
"def remove_contest(contest_id):     with SessionGen() as session:         contest = session.query(Contest).filter(Contest.id == contest_id).first()         if not contest:             print(""No contest with id %s found."" % contest_id)             return False         contest_name = contest.name         <MASK>             print(""Not removing contest `%s'."" % contest_name)             return False         session.delete(contest)         session.commit()         print(""Contest `%s' removed."" % contest_name)     return True",if not ask ( contest ) :,if not contest_name :,16.341219448835542,16.341219448835542,0.0
def get_hi_lineno(self):     lineno = Node.get_hi_lineno(self)     if self.expr1 is None:         pass     else:         lineno = self.expr1.get_hi_lineno()         if self.expr2 is None:             pass         else:             lineno = self.expr2.get_hi_lineno()             <MASK>                 pass             else:                 lineno = self.expr3.get_hi_lineno()     return lineno,if self . expr3 is None :,if self . expr3 is None :,100.00000000000004,100.00000000000004,1.0
"def _send_internal(self, bytes_):     # buffering     if self.pendings:         self.pendings += bytes_         bytes_ = self.pendings     try:         # reconnect if possible         self._reconnect()         # send message         self.socket.sendall(bytes_)         # send finished         self.pendings = None     except Exception:  # pylint: disable=broad-except         # close socket         self._close()         # clear buffer if it exceeds max bufer size         <MASK>             # TODO: add callback handler here             self.pendings = None         else:             self.pendings = bytes_",if self . pendings and ( len ( self . pendings ) > self . bufmax ) :,if bytes_ > self . max_bufer :,7.974423230253494,7.974423230253494,0.0
"def _unpack(self, fmt, byt):     d = unpack(self._header[""byteorder""] + fmt, byt)[0]     if fmt[-1] in self.MISSING_VALUES:         nmin, nmax = self.MISSING_VALUES[fmt[-1]]         if d < nmin or d > nmax:             <MASK>                 return StataMissingValue(nmax, d)             else:                 return None     return d",if self . _missing_values :,if d < nmax :,6.9717291216921975,6.9717291216921975,0.0
"def tuple_iter(self):     for x in range(         self.center.x - self.max_radius, self.center.x + self.max_radius + 1     ):         for y in range(             self.center.y - self.max_radius, self.center.y + self.max_radius + 1         ):             <MASK>                 yield (x, y)","if self . min_radius <= self . center . distance ( ( x , y ) ) <= self . max_radius :",if x > y :,0.15185184608553637,0.15185184608553637,0.0
"def _parse_gene(element):     for genename_element in element:         <MASK>             ann_key = ""gene_%s_%s"" % (                 genename_element.tag.replace(NS, """"),                 genename_element.attrib[""type""],             )             if genename_element.attrib[""type""] == ""primary"":                 self.ParsedSeqRecord.annotations[ann_key] = genename_element.text             else:                 append_to_annotations(ann_key, genename_element.text)","if ""type"" in genename_element . attrib :","if genename_element . tag in [ NS , NS , NS ] :",18.92240568795936,18.92240568795936,0.0
"def invalidateDependentSlices(self, iFirstCurve):     # only user defined curve can have slice dependency relationships     if self.isSystemCurveIndex(iFirstCurve):         return     nCurves = self.getNCurves()     for i in range(iFirstCurve, nCurves):         c = self.getSystemCurve(i)         if isinstance(c.getSymbol().getSymbolType(), SymbolType.PieSliceSymbolType):             c.invalidate()         <MASK>             # if first curve isn't a slice,             break             # there are no dependent slices",elif i == iFirstCurve :,if c . isSlice ( ) :,6.567274736060395,6.567274736060395,0.0
"def gen_app_versions(self):     for app_config in apps.get_app_configs():         name = app_config.verbose_name         app = app_config.module         version = self.get_app_version(app)         <MASK>             yield app.__name__, name, version",if version :,if version :,100.00000000000004,0.0,1.0
"def verify_relative_valid_path(root, path):     if len(path) < 1:         raise PackagerError(""Empty chown path"")     checkpath = root     parts = path.split(os.sep)     for part in parts:         if part in (""."", ""..""):             raise PackagerError("". and .. is not allowed in chown path"")         checkpath = os.path.join(checkpath, part)         relpath = checkpath[len(root) + 1 :]         <MASK>             raise PackagerError(f""chown path {relpath} does not exist"")         if os.path.islink(checkpath):             raise PackagerError(f""chown path {relpath} is a soft link"")",if not os . path . exists ( checkpath ) :,if not os . path . exists ( checkpath ) :,100.00000000000004,100.00000000000004,1.0
"def create_or_update_tag_at_scope(cmd, resource_id=None, tags=None, tag_name=None):     rcf = _resource_client_factory(cmd.cli_ctx)     if resource_id is not None:         <MASK>             raise IncorrectUsageError(""Tags could not be empty."")         Tags = cmd.get_models(""Tags"")         tag_obj = Tags(tags=tags)         return rcf.tags.create_or_update_at_scope(scope=resource_id, properties=tag_obj)     return rcf.tags.create_or_update(tag_name=tag_name)",if not tags :,if tags is None :,14.058533129758727,14.058533129758727,0.0
"def generate_auto_complete(self, base, iterable_var):     sugg = []     for entry in iterable_var:         compare_entry = entry         compare_base = base         if self.settings.get(IGNORE_CASE_SETTING):             compare_entry = compare_entry.lower()             compare_base = compare_base.lower()         <MASK>             if entry not in sugg:                 sugg.append(entry)     return sugg","if self . compare_entries ( compare_entry , compare_base ) :",if compare_base == compare_entry :,15.491846006709249,15.491846006709249,0.0
"def createFields(self):     yield String(self, ""dict_start"", 2)     while not self.eof:         addr = self.absolute_address + self.current_size         <MASK>             for field in parsePDFType(self):                 yield field         else:             break     yield String(self, ""dict_end"", 2)","if self . stream . readBytes ( addr , 2 ) != "">>"" :",if addr == self . current_address :,4.96274655104206,4.96274655104206,0.0
"def Visit_and_test(self, node):  # pylint: disable=invalid-name     # and_test ::= not_test ('and' not_test)*     for child in node.children:         self.Visit(child)         <MASK>             _AppendTokenSubtype(child, format_token.Subtype.BINARY_OPERATOR)","if isinstance ( child , pytree . Leaf ) and child . value == ""and"" :","if isinstance ( child , ast . Node ) :",17.96191510244705,17.96191510244705,0.0
"def getfiledata(directories):     columns = None     data = []     counter = 1     for directory in directories:         for f in os.listdir(directory):             if not os.path.isfile(os.path.join(directory, f)):                 continue             counter += 1             st = os.stat(os.path.join(directory, f))             <MASK>                 columns = [""rowid"", ""name"", ""directory""] + [                     x for x in dir(st) if x.startswith(""st_"")                 ]             data.append([counter, f, directory] + [getattr(st, x) for x in columns[3:]])     return columns, data",if columns is None :,"if not st . st_ . startswith ( ""st_"" ) :",3.1251907639724417,3.1251907639724417,0.0
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None):     for attr in attributes:         value = getattr(obj, attr, None)         if value is None:             continue         name = name_fmt % attr         <MASK>             value = formatter(attr, value)         info_add(name, value)",if formatter is not None :,if formatter is not None :,100.00000000000004,100.00000000000004,1.0
"def main(args):     ap = argparse.ArgumentParser()     ap.add_argument(""job_ids"", nargs=""+"", type=int, help=""ID of a running job"")     ns = ap.parse_args(args)     _stash = globals()[""_stash""]     """""":type : StaSh""""""     for job_id in ns.job_ids:         <MASK>             print(""killing job {} ..."".format(job_id))             worker = _stash.runtime.worker_registry.get_worker(job_id)             worker.kill()             time.sleep(1)         else:             print(""error: no such job with id: {}"".format(job_id))             break",if job_id in _stash . runtime . worker_registry :,if _stash . runtime . worker_registry . has_worker ( job_id ) :,45.80519369844352,45.80519369844352,0.0
"def _check_choice(self):     if self.type == ""choice"":         if self.choices is None:             raise OptionError(""must supply a list of choices for type 'choice'"", self)         <MASK>             raise OptionError(                 ""choices must be a list of strings ('%s' supplied)""                 % str(type(self.choices)).split(""'"")[1],                 self,             )     elif self.choices is not None:         raise OptionError(""must not supply choices for type %r"" % self.type, self)","elif type ( self . choices ) not in ( types . TupleType , types . ListType ) :",if type ( self . choices ) != list :,23.188918537888732,23.188918537888732,0.0
"def add_file(pipe, srcpath, tgtpath):     with open(srcpath, ""rb"") as handle:         <MASK>             write(pipe, enc(""M 100755 inline %s\n"" % tgtpath))         else:             write(pipe, enc(""M 100644 inline %s\n"" % tgtpath))         data = handle.read()         write(pipe, enc(""data %d\n"" % len(data)))         write(pipe, enc(data))         write(pipe, enc(""\n""))","if os . access ( srcpath , os . X_OK ) :",if not handle . is_file ( ) :,7.40354787297858,7.40354787297858,0.0
"def cdf(self, x):     if x == numpy.inf:         return 1.0     else:  # Inefficient sum.         <MASK>             raise RuntimeError(""Invalid value."")         c = 0.0         for i in xrange(x + 1):             c += self.probability(i)         return c",if x != int ( x ) :,if x < 0 :,10.62372743739878,10.62372743739878,0.0
"def convert_to_strings(self, out, seq_len):     results = []     for b, batch in enumerate(out):         utterances = []         for p, utt in enumerate(batch):             size = seq_len[b][p]             <MASK>                 transcript = """".join(                     map(lambda x: self.int_to_char[x.item()], utt[0:size])                 )             else:                 transcript = """"             utterances.append(transcript)         results.append(utterances)     return results",if size > 0 :,if size > 0 :,100.00000000000004,100.00000000000004,1.0
"def get_date_range(self):     if not hasattr(self, ""start"") or not hasattr(self, ""end""):         args = (self.today.year, self.today.month)         form = self.get_form()         <MASK>             args = (int(form.cleaned_data[""year""]), int(form.cleaned_data[""month""]))         self.start = self.get_start(*args)         self.end = self.get_end(*args)     return self.start, self.end",if form . is_valid ( ) :,if form . is_valid ( ) :,100.00000000000004,100.00000000000004,1.0
"def save_stats(self):     LOGGER.info(""Saving task-level statistics."")     has_headers = os.path.isfile(paths.TABLE_COUNT_PATH)     with open(paths.TABLE_COUNT_PATH, ""a"") as csvfile:         headers = [""start_time"", ""database_name"", ""number_tables""]         writer = csv.DictWriter(             csvfile, delimiter="","", lineterminator=""\n"", fieldnames=headers         )         <MASK>             writer.writeheader()         writer.writerow(             {                 ""start_time"": self.start_time,                 ""database_name"": self.database_name,                 ""number_tables"": self.count,             }         )",if not has_headers :,if has_headers :,57.89300674674101,57.89300674674101,0.0
"def _CheckCanaryCommand(self):     <MASK>  # fast path         return     with self._lock:         if OpenStackVirtualMachine.command_works:             return         logging.info(""Testing OpenStack CLI command is installed and working"")         cmd = os_utils.OpenStackCLICommand(self, ""image"", ""list"")         stdout, stderr, _ = cmd.Issue()         if stderr:             raise errors.Config.InvalidValue(                 ""OpenStack CLI test command failed. Please make sure the OpenStack ""                 ""CLI client is installed and properly configured""             )         OpenStackVirtualMachine.command_works = True",if OpenStackVirtualMachine . command_works :,if not stdout :,8.9730240870212,8.9730240870212,0.0
"def test_windows_hidden(self):     if not sys.platform == ""win32"":         self.skipTest(""sys.platform is not windows"")         return     # FILE_ATTRIBUTE_HIDDEN = 2 (0x2) from GetFileAttributes documentation.     hidden_mask = 2     with tempfile.NamedTemporaryFile() as f:         # Hide the file using         success = ctypes.windll.kernel32.SetFileAttributesW(f.name, hidden_mask)         <MASK>             self.skipTest(""unable to set file attributes"")         self.assertTrue(hidden.is_hidden(f.name))",if not success :,if not success :,100.00000000000004,100.00000000000004,1.0
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0):     if tr < 1:         tr = 1     x = time.time() + t     y = []     r = """"     if stderr:         pr = p.recv_err     else:         pr = p.recv     while time.time() < x or r:         r = pr()         if r is None:             break         <MASK>             y.append(r)         else:             time.sleep(max((x - time.time()) / tr, 0))     return b"""".join(y)",elif r :,if e :,27.516060407455225,0.0,0.0
"def _is_xml(accepts):     if accepts.startswith(b""application/""):         has_xml = accepts.find(b""xml"")         <MASK>             semicolon = accepts.find(b"";"")             if semicolon < 0 or has_xml < semicolon:                 return True     return False",if has_xml > 0 :,if has_xml > 0 :,100.00000000000004,100.00000000000004,1.0
"def times(self, value: int):     if value is None:         self._times = None     else:         try:             candidate = int(value)         except ValueError:             # pylint: disable:raise-missing-from             raise BarException(f""cannot set repeat times to: {value!r}"")         if candidate < 0:             raise BarException(                 f""cannot set repeat times to a value less than zero: {value}""             )         <MASK>             raise BarException(""cannot set repeat times on a start Repeat"")         self._times = candidate","if self . direction == ""start"" :",if candidate > self . _times :,10.229197414177778,10.229197414177778,0.0
"def __call__(self, *args, **kwargs):     if not NET_INITTED:         return self.raw(*args, **kwargs)     for stack in traceback.walk_stack(None):         if ""self"" in stack[0].f_locals:             layer = stack[0].f_locals[""self""]             <MASK>                 log.pytorch_layer_name = layer_names[layer]                 print(layer_names[layer])                 break     out = self.obj(self.raw, *args, **kwargs)     # if isinstance(out,Variable):     #     out=[out]     return out",if layer in layer_names :,if layer in layer_names :,100.00000000000004,100.00000000000004,1.0
"def do_begin(self, byte):     if byte.isspace():         return     if byte != ""<"":         <MASK>             self._leadingBodyData = byte             return ""bodydata""         self._parseError(""First char of document [{!r}] wasn't <"".format(byte))     return ""tagstart""",if self . beExtremelyLenient :,"if byte != "">"" :",6.567274736060395,6.567274736060395,0.0
"def pretty(self, n, comment=True):     if isinstance(n, (str, bytes, list, tuple, dict)):         r = repr(n)         <MASK>  # then it can be inside a comment!             r = r.replace(""*/"", r""\x2a/"")         return r     if not isinstance(n, six.integer_types):         return n     if isinstance(n, constants.Constant):         if comment:             return ""%s /* %s */"" % (n, self.pretty(int(n)))         else:             return ""%s (%s)"" % (n, self.pretty(int(n)))     elif abs(n) < 10:         return str(n)     else:         return hex(n)",if not comment :,if r :,24.840753130578644,0.0,0.0
"def test_training_script_with_max_history_set(tmpdir):     train_dialogue_model(         DEFAULT_DOMAIN_PATH,         DEFAULT_STORIES_FILE,         tmpdir.strpath,         interpreter=RegexInterpreter(),         policy_config=""data/test_config/max_hist_config.yml"",         kwargs={},     )     agent = Agent.load(tmpdir.strpath)     for policy in agent.policy_ensemble.policies:         if hasattr(policy.featurizer, ""max_history""):             <MASK>                 assert policy.featurizer.max_history == 2             else:                 assert policy.featurizer.max_history == 5",if type ( policy ) == FormPolicy :,if policy . featurizer . max_history == 1 :,8.516593018819643,8.516593018819643,0.0
"def cli_uninstall_distro():     distro_list = install_distro_list()     if distro_list is not None:         for index, _distro_dir in enumerate(distro_list):             log(str(index) + ""  --->>  "" + _distro_dir)         user_input = read_input_uninstall()         <MASK>             for index, _distro_dir in enumerate(distro_list):                 if index == user_input:                     config.uninstall_distro_dir_name = _distro_dir                     unin_distro()     else:         log(""No distro installed on "" + config.usb_disk)",if user_input is not False :,if user_input is not None :,70.71067811865478,70.71067811865478,0.0
"def set_random_avatar(user):     galleries = get_available_galleries(include_default=True)     if not galleries:         raise RuntimeError(""no avatar galleries are set"")     avatars_list = []     for gallery in galleries:         <MASK>             avatars_list = gallery[""images""]             break         else:             avatars_list += gallery[""images""]     random_avatar = random.choice(avatars_list)     store.store_new_avatar(user, Image.open(random_avatar.image))","if gallery [ ""name"" ] == DEFAULT_GALLERY :",if gallery . image is None :,6.168585410281235,6.168585410281235,0.0
"def make_query(self, key, filters):     meta = self.get_meta(key)     q = {meta.facet_key: self.normalize_key(meta.path)}     if filters:         if filters.get(""has_fulltext"") == ""true"":             q[""has_fulltext""] = ""true""         <MASK>             q[""publish_year""] = filters[""publish_year""]     return q","if filters . get ( ""publish_year"" ) :","if filters . get ( ""publish_year"" ) == ""true"" :",62.36362995619313,62.36362995619313,0.0
"def test_named_parameters_and_constraints(self):     likelihood = gpytorch.likelihoods.GaussianLikelihood()     model = ExactGPModel(None, None, likelihood)     for name, _param, constraint in model.named_parameters_and_constraints():         if name == ""likelihood.noise_covar.raw_noise"":             self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan)         <MASK>             self.assertIsNone(constraint)         elif name == ""covar_module.raw_outputscale"":             self.assertIsInstance(constraint, gpytorch.constraints.Positive)         elif name == ""covar_module.base_kernel.raw_lengthscale"":             self.assertIsInstance(constraint, gpytorch.constraints.Positive)","elif name == ""mean_module.constant"" :","if name == ""likelihood.noise_covar.raw_outputscale"" :",19.56475149792291,19.56475149792291,0.0
"def _test_pooling(input_shape, **kwargs):     _test_pooling_iteration(input_shape, **kwargs)     if is_gpu_available():         <MASK>             input_shape = [input_shape[ii] for ii in (0, 3, 1, 2)]             kwargs[""data_format""] = ""NCHW""             _test_pooling_iteration(input_shape, **kwargs)",if len ( input_shape ) == 4 :,if not is_gpu_available ( ) :,5.6775429106661015,5.6775429106661015,0.0
"def init(self):     r = self.get_redis()     if r:         key = ""pocsuite_target""         info_msg = ""[PLUGIN] try fetch targets from redis...""         logger.info(info_msg)         targets = r.get(key)         count = 0         <MASK>             for target in targets:                 if self.add_target(target):                     count += 1         info_msg = ""[PLUGIN] get {0} target(s) from redis"".format(count)         logger.info(info_msg)",if targets :,if not targets :,35.35533905932737,35.35533905932737,0.0
"def reload_json_api_settings(*args, **kwargs):     django_setting = kwargs[""setting""]     setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, """")     value = kwargs[""value""]     if setting in DEFAULTS.keys():         if value is not None:             setattr(json_api_settings, setting, value)         <MASK>             delattr(json_api_settings, setting)","elif hasattr ( json_api_settings , setting ) :",if settings . get ( setting ) :,14.16667529041554,14.16667529041554,0.0
"def update_metadata(self):     for attrname in dir(self):         if attrname.startswith(""__""):             continue         attrvalue = getattr(self, attrname, None)         if attrvalue == 0:             continue         if attrname == ""salt_version"":             attrname = ""version""         if hasattr(self.metadata, ""set_{0}"".format(attrname)):             getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue)         <MASK>             try:                 setattr(self.metadata, attrname, attrvalue)             except AttributeError:                 pass","elif hasattr ( self . metadata , attrname ) :",if attrvalue :,2.6682865255867765,0.0,0.0
"def test_02_looking_at_listdir_path_(name):     for dline in listdir.json():         <MASK>             assert dline[""type""] in (""DIRECTORY"", ""FILE""), listdir.text             assert dline[""uid""] == 0, listdir.text             assert dline[""gid""] == 0, listdir.text             assert dline[""name""] == name, listdir.text             break     else:         raise AssertionError(f""/{path}/{name} not found"")","if dline [ ""path"" ] == f""{path}/{name}"" :","if dline [ ""type"" ] == ""FILE"" :",25.404753321283366,25.404753321283366,0.0
"def DeletePlugin():     oid = request.form.get(""oid"", """")     if oid:         result = Mongo.coll[""Plugin""].find_one_and_delete(             {""_id"": ObjectId(oid)}, remove=True         )         <MASK>             result[""filename""] = result[""filename""] + "".py""         if os.path.exists(file_path + result[""filename""]):             os.remove(file_path + result[""filename""])             return ""success""     return ""fail""","if not result [ ""filename"" ] . find ( ""."" ) > - 1 :",if result :,0.19159732247644734,0.0,0.0
"def iterparent(self, node):     """"""Iterator wrapper to get allowed parent and child all at once.""""""     # We do not allow the marker inside a header as that     # would causes an enless loop of placing a new TOC     # inside previously generated TOC.     for child in node:         <MASK>             yield node, child             yield from self.iterparent(child)","if not self . header_rgx . match ( child . tag ) and child . tag not in [ ""pre"" , ""code"" ] :",if child . header is not None :,1.1050595168432016,1.1050595168432016,0.0
"def _get_matched_layout(command):     # don't use command.split_script here because a layout mismatch will likely     # result in a non-splitable script as per shlex     cmd = command.script.split("" "")     for source_layout in source_layouts:         is_all_match = True         for cmd_part in cmd:             if not all([ch in source_layout or ch in ""-_"" for ch in cmd_part]):                 is_all_match = False                 break         <MASK>             return source_layout",if is_all_match :,if not is_all_match :,70.71067811865478,70.71067811865478,0.0
"def _update_tileable_and_chunk_shape(self, tileable_graph, chunk_result, failed_ops):     for n in tileable_graph:         if n.op in failed_ops:             continue         tiled_n = get_tiled(n)         if has_unknown_shape(tiled_n):             <MASK>                 # some of the chunks has been fused                 continue             new_nsplits = self.get_tileable_nsplits(n, chunk_result=chunk_result)             for node in (n, tiled_n):                 node._update_shape(tuple(sum(nsplit) for nsplit in new_nsplits))             tiled_n._nsplits = new_nsplits",if any ( c . key not in chunk_result for c in tiled_n . chunks ) :,if n . op in failed_ops :,3.307962365281718,3.307962365281718,0.0
"def _get_items(self, name, target=1):     all_items = self.get_items(name)     items = [o for o in all_items if not o.disabled]     if len(items) < target:         if len(all_items) < target:             raise ItemNotFoundError(""insufficient items with name %r"" % name)         else:             raise AttributeError(""insufficient non-disabled items with name %s"" % name)     on = []     off = []     for o in items:         <MASK>             on.append(o)         else:             off.append(o)     return on, off",if o . selected :,if o . disabled :,42.72870063962342,42.72870063962342,0.0
def parse_flow_sequence_entry_mapping_value(self):     if self.check_token(ValueToken):         token = self.get_token()         <MASK>             self.states.append(self.parse_flow_sequence_entry_mapping_end)             return self.parse_flow_node()         else:             self.state = self.parse_flow_sequence_entry_mapping_end             return self.process_empty_scalar(token.end_mark)     else:         self.state = self.parse_flow_sequence_entry_mapping_end         token = self.peek_token()         return self.process_empty_scalar(token.start_mark),"if not self . check_token ( FlowEntryToken , FlowSequenceEndToken ) :",if token . end_mark == self . END_MARK :,7.474875887495341,7.474875887495341,0.0
"def serialize_config(self, session, key, tid, language):     cache_key = gen_cache_key(key, tid, language)     cache_obj = None     if cache_key not in self.cache:         <MASK>             cache_obj = db_admin_serialize_node(session, tid, language)         elif key == ""notification"":             cache_obj = db_get_notification(session, tid, language)         self.cache[cache_key] = cache_obj     return self.cache[cache_key]","if key == ""node"" :","if key == ""node"" :",100.00000000000004,100.00000000000004,1.0
"def get_lldp_neighbors(self):     commands = [""show lldp neighbors""]     output = self.device.run_commands(commands)[0][""lldpNeighbors""]     lldp = {}     for n in output:         <MASK>             lldp[n[""port""]] = []         lldp[n[""port""]].append(             {""hostname"": n[""neighborDevice""], ""port"": n[""neighborPort""]}         )     return lldp","if n [ ""port"" ] not in lldp . keys ( ) :","if n [ ""port"" ] not in lldp :",63.58420474065946,63.58420474065946,0.0
"def handle(self):     from poetry.utils.env import EnvManager     manager = EnvManager(self.poetry)     current_env = manager.get()     for venv in manager.list():         name = venv.path.name         if self.option(""full-path""):             name = str(venv.path)         <MASK>             self.line(""<info>{} (Activated)</info>"".format(name))             continue         self.line(name)",if venv == current_env :,if current_env . name == name :,21.36435031981171,21.36435031981171,0.0
"def resolve_env_secrets(config, environ):     """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ""""""     if isinstance(config, dict):         if list(config.keys()) == [""$env""]:             return environ.get(list(config.values())[0])         <MASK>             return open(list(config.values())[0]).read()         else:             return {                 key: resolve_env_secrets(value, environ)                 for key, value in config.items()             }     elif isinstance(config, list):         return [resolve_env_secrets(value, environ) for value in config]     else:         return config","elif list ( config . keys ( ) ) == [ ""$file"" ] :","if list ( config . keys ( ) ) == [ ""NAME"" ] :",71.89085812023326,71.89085812023326,0.0
"def _is_valid_16bit_as_path(cls, buf):     two_byte_as_size = struct.calcsize(""!H"")     while buf:         (type_, num_as) = struct.unpack_from(             cls._SEG_HDR_PACK_STR, six.binary_type(buf)         )         if type_ is not cls._AS_SET and type_ is not cls._AS_SEQUENCE:             return False         buf = buf[struct.calcsize(cls._SEG_HDR_PACK_STR) :]         <MASK>             return False         buf = buf[num_as * two_byte_as_size :]     return True",if len ( buf ) < num_as * two_byte_as_size :,if num_as > two_byte_as_size :,46.63449625549861,46.63449625549861,0.0
"def reparentChildren(self, newParent):     if newParent.childNodes:         newParent.childNodes[-1]._element.tail += self._element.text     else:         <MASK>             newParent._element.text = """"         if self._element.text is not None:             newParent._element.text += self._element.text     self._element.text = """"     base.Node.reparentChildren(self, newParent)",if not newParent . _element . text :,if self . _element . text is not None :,39.45881255591768,39.45881255591768,0.0
"def get_operation_ast(document_ast, operation_name=None):     operation = None     for definition in document_ast.definitions:         if isinstance(definition, ast.OperationDefinition):             if not operation_name:                 # If no operation name is provided, only return an Operation if it is the only one present in the                 # document. This means that if we've encountered a second operation as we were iterating over the                 # definitions in the document, there are more than one Operation defined, and we should return None.                 if operation:                     return None                 operation = definition             <MASK>                 return definition     return operation",elif definition . name and definition . name . value == operation_name :,if operation . name == operation_name :,31.909595403088403,31.909595403088403,0.0
"def reprSmart(vw, item):     ptype = type(item)     if ptype is int:         <MASK>             return str(item)         elif vw.isValidPointer(item):             return vw.reprPointer(item)         else:             return hex(item)     elif ptype in (list, tuple):         return reprComplex(vw, item)  # recurse     elif ptype is dict:         return ""{%s}"" % "","".join(             [""%s:%s"" % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()]         )     else:         return repr(item)",if - 1024 < item < 1024 :,if vw . isValidPointer ( item ) :,7.267884212102741,7.267884212102741,0.0
"def cleanDataCmd(cmd):     newcmd = ""AbracadabrA ** <?php ""     if cmd[:6] != ""php://"":         <MASK>             cmds = cmd.split(""&"")             for c in cmds:                 if len(c) > 0:                     newcmd += ""system('%s');"" % c         else:             b64cmd = base64.b64encode(cmd)             newcmd += ""system(base64_decode('%s'));"" % b64cmd     else:         newcmd += cmd[6:]     newcmd += ""?> **""     return newcmd",if reverseConn not in cmd :,"if cmd [ 0 ] == ""php://"" :",3.737437943747671,3.737437943747671,0.0
"def render_tasks(self) -> List:     results = []     for task in self.tasks.values():         job_entry = self.jobs.get(task.job_id)         <MASK>             if not self.should_render_job(job_entry):                 continue         files = self.get_file_counts([task])         entry = (             task.job_id,             task.task_id,             task.state,             task.type.name,             task.target,             files,             task.pool,             task.end_time,         )         results.append(entry)     return results",if job_entry :,if job_entry :,100.00000000000004,100.00000000000004,1.0
"def __call__(self, environ, start_response):     for key in ""REQUEST_URL"", ""REQUEST_URI"", ""UNENCODED_URL"":         if key not in environ:             continue         request_uri = unquote(environ[key])         script_name = unquote(environ.get(""SCRIPT_NAME"", """"))         <MASK>             environ[""PATH_INFO""] = request_uri[len(script_name) :].split(""?"", 1)[0]             break     return self.app(environ, start_response)",if request_uri . startswith ( script_name ) :,if script_name :,11.141275535087015,11.141275535087015,0.0
"def _add_role_information(self, function_dict, role_id):     # Make it easier to build rules based on policies attached to execution roles     function_dict[""role_arn""] = role_id     role_name = role_id.split(""/"")[-1]     function_dict[         ""execution_role""     ] = await self.facade.awslambda.get_role_with_managed_policies(role_name)     if function_dict.get(""execution_role""):         statements = []         for policy in function_dict[""execution_role""].get(""policies""):             <MASK>                 statements += policy[""Document""][""Statement""]         function_dict[""execution_role""][""policy_statements""] = statements","if ""Document"" in policy and ""Statement"" in policy [ ""Document"" ] :","if policy [ ""Document"" ] :",24.909923021496894,24.909923021496894,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 8:             self.set_ts(d.getVarInt64())             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt == 0 :,100.00000000000004,100.00000000000004,1.0
"def format_counts(results, json_output=False, human_readable=False):     if json_output:         for result in results:             yield json.dumps(result)     else:         for result in results:             space_consumed = result.get(""spaceConsumed"")             <MASK>                 space_consumed = _sizeof_fmt(int(result.get(""spaceConsumed"")))             yield ""%12s %12s %18s %s"" % (                 result.get(""directoryCount""),                 result.get(""fileCount""),                 space_consumed,                 result.get(""path""),             )",if human_readable :,if space_consumed > 0 :,8.643019616048525,8.643019616048525,0.0
"def parse_edges(self, pcb):     edges = []     drawings = list(pcb.GetDrawings())     bbox = None     for m in pcb.GetModules():         for g in m.GraphicalItems():             drawings.append(g)     for d in drawings:         if d.GetLayer() == pcbnew.Edge_Cuts:             parsed_drawing = self.parse_drawing(d)             if parsed_drawing:                 edges.append(parsed_drawing)                 <MASK>                     bbox = d.GetBoundingBox()                 else:                     bbox.Merge(d.GetBoundingBox())     if bbox:         bbox.Normalize()     return edges, bbox",if bbox is None :,if bbox is None :,100.00000000000004,100.00000000000004,1.0
"def __getitem__(self, k) -> ""SimMemView"":     if isinstance(k, slice):         if k.step is not None:             raise ValueError(""Slices with strides are not supported"")         <MASK>             raise ValueError(""Must specify start index"")         elif k.stop is not None:             raise ValueError(""Slices with stop index are not supported"")         else:             addr = k.start     elif self._type is not None and self._type._can_refine_int:         return self._type._refine(self, k)     else:         addr = k     return self._deeper(addr=addr)",elif k . start is None :,if k . start is None :,80.91067115702207,80.91067115702207,0.0
"def _parse(self, stream, context):     obj = []     try:         if self.subcon.conflags & self.FLAG_COPY_CONTEXT:             while True:                 subobj = self.subcon._parse(stream, context.__copy__())                 obj.append(subobj)                 <MASK>                     break         else:             while True:                 subobj = self.subcon._parse(stream, context)                 obj.append(subobj)                 if self.predicate(subobj, context):                     break     except ConstructError as ex:         raise ArrayError(""missing terminator"", ex)     return obj","if self . predicate ( subobj , context ) :","if self . predicate ( subobj , context ) :",100.00000000000004,100.00000000000004,1.0
"def before_run(self, run_context):     if ""featurizer"" in self.model_portion and (         self.need_to_refresh or self.refresh_base_model     ):         <MASK>             self.refresh_base_model = True         self.init_fn(             None, run_context.session, self.model_portion, self.refresh_base_model         )         self.need_to_refresh = False         self.refresh_base_model = False","if self . model_portion == ""whole_featurizer"" :","if ""featurizer"" in self . model_portion :",33.58575137215657,33.58575137215657,0.0
"def run(self):     while True:         task = self.requestQueue.get()         if task is None:             # The ""None"" value is used as a sentinel by             # ThreadPool.cleanup().  This indicates that there             # are no more tasks, so we should quit.             break         try:             <MASK>                 raise SCons.Errors.BuildError(task.targets[0], errstr=interrupt_msg)             task.execute()         except:             task.exception_set()             ok = False         else:             ok = True         self.resultsQueue.put((task, ok))",if self . interrupted ( ) :,"if task . targets [ 0 ] != ""None"" :",4.065425428798724,4.065425428798724,0.0
"def get_overdue_evergreen_documents(*, db_session) -> List[Optional[Document]]:     """"""Returns all documents that have need had a recent evergreen notification.""""""     documents = (         db_session.query(Document).filter(Document.evergreen == True)     ).all()  # noqa     overdue_documents = []     now = datetime.utcnow()     for d in documents:         next_reminder = d.evergreen_last_reminder_at + timedelta(             days=d.evergreen_reminder_interval         )         <MASK>             overdue_documents.append(d)     return overdue_documents",if now > next_reminder :,if next_reminder < now :,29.071536848410968,29.071536848410968,0.0
"def create_local_app_folder(local_app_path):     if exists(local_app_path):         raise ValueError(""There is already a '%s' folder! Aborting!"" % local_app_path)     for folder in subfolders(local_app_path):         if not exists(folder):             os.mkdir(folder)             init_path = join(folder, ""__init__.py"")             <MASK>                 create_file(init_path)",if not exists ( init_path ) :,if not os . path . isfile ( init_path ) :,42.803206067505954,42.803206067505954,0.0
"def generate():     for leaf in u.leaves:         <MASK>             val = leaf.get_int_value()             if val in (0, 1):                 yield val             else:                 raise _NoBoolVector         elif isinstance(leaf, Symbol):             if leaf == SymbolTrue:                 yield 1             elif leaf == SymbolFalse:                 yield 0             else:                 raise _NoBoolVector         else:             raise _NoBoolVector","if isinstance ( leaf , Integer ) :",if leaf . is_int ( ) :,11.99014838091355,11.99014838091355,0.0
"def replace(self, old, new):     v_m = self.var_map     size = v_m[self.size]     if not (size.is_const() or size.is_ident()):         size.replace(old, new)     else:         <MASK>             v_m[new.value()] = new             self.size = new.value()         else:             v_m[old] = new",if new . is_ident ( ) :,if new . is_const ( ) :,59.694917920196445,59.694917920196445,0.0
"def method_for_doctype(doctype):     method = ""xhtml""     if doctype:         if doctype.startswith(""html""):             method = ""html""         <MASK>             method = ""xhtml""         elif doctype.startswith(""svg""):             method = ""xml""         else:             method = ""xhtml""     return method","elif doctype . startswith ( ""xhtml"" ) :","if doctype . startswith ( ""xml"" ) :",52.53819788848316,52.53819788848316,0.0
"def delete(self, trans, **kwd):     idnum = kwd[self.tagged_item_id]     item = self._get_item_from_id(trans, idnum, check_writable=True)     if item is not None:         ex_obj = self.get_item_extended_metadata_obj(trans, item)         <MASK>             self.unset_item_extended_metadata_obj(trans, item)             self.delete_extended_metadata(trans, ex_obj)",if ex_obj is not None :,if ex_obj is not None :,100.00000000000004,100.00000000000004,1.0
"def check_testv(self, testv):     test_good = True     f = open(self.home, ""rb+"")     for (offset, length, operator, specimen) in testv:         data = self._read_share_data(f, offset, length)         <MASK>             test_good = False             break     f.close()     return test_good","if not testv_compare ( data , operator , specimen ) :",if not data :,4.004304603105518,4.004304603105518,0.0
"def get_history_user(self, instance):     """"""Get the modifying user from instance or middleware.""""""     try:         return instance._history_user     except AttributeError:         request = None         try:             <MASK>                 request = self.thread.request         except AttributeError:             pass     return self.get_user(instance=instance, request=request)",if self . thread . request . user . is_authenticated :,if self . thread :,14.276239697197271,14.276239697197271,0.0
"def _check(self, name, size=None, *extra):     func = getattr(imageop, name)     for height in VALUES:         for width in VALUES:             strlen = abs(width * height)             <MASK>                 strlen *= size             if strlen < MAX_LEN:                 data = ""A"" * strlen             else:                 data = AAAAA             if size:                 arguments = (data, size, width, height) + extra             else:                 arguments = (data, width, height) + extra             try:                 func(*arguments)             except (ValueError, imageop.error):                 pass",if size :,if size :,100.00000000000004,0.0,1.0
"def __setattr__(self, name, value):     if name == ""path"":         if value and value != """":             if value[0] != ""/"":                 raise ValueError(                     'The page path should always start with a slash (""/"").'                 )     elif name == ""load_time"":         <MASK>             raise ValueError(                 ""Page load time must be specified in integer milliseconds.""             )     object.__setattr__(self, name, value)","if value and not isinstance ( value , int ) :",if value and value [ 0 ] != 1000 :,15.851165692617148,15.851165692617148,0.0
"def __repr__(self):     if self._in_repr:         return ""<recursion>""     try:         self._in_repr = True         if self.is_computed():             status = ""computed, ""             <MASK>                 if self.value() is self:                     status += ""= self""                 else:                     status += ""= "" + repr(self.value())             else:                 status += ""error = "" + repr(self.error())         else:             status = ""isn't computed""         return ""%s (%s)"" % (type(self), status)     finally:         self._in_repr = False",if self . error ( ) is None :,if self . error ( ) is None :,100.00000000000004,100.00000000000004,1.0
"def _exclude_node(self, name):     if ""exclude_nodes"" in self.node_filters:         <MASK>             self.loggit.info('Excluding node ""{0}"" due to node_filters'.format(name))             return True     return False","if name in self . node_filters [ ""exclude_nodes"" ] :",if name in self . node_filters :,40.84937416616266,40.84937416616266,0.0
"def enumerate_projects():     """"""List projects in _DEFAULT_APP_DIR.""""""     src_path = os.path.join(_DEFAULT_APP_DIR, ""src"")     projects = {}     for project in os.listdir(src_path):         projects[project] = []         project_path = os.path.join(src_path, project)         for file in os.listdir(project_path):             <MASK>                 projects[project].append(file[:-8])     return projects","if file . endswith ( "".gwt.xml"" ) :","if file [ : - 8 ] == ""src"" :",7.768562846380176,7.768562846380176,0.0
"def zip_readline_read_test(self, f, compression):     self.make_test_archive(f, compression)     # Read the ZIP archive     with zipfile.ZipFile(f, ""r"") as zipfp, zipfp.open(TESTFN) as zipopen:         data = b""""         while True:             read = zipopen.readline()             <MASK>                 break             data += read             read = zipopen.read(100)             if not read:                 break             data += read     self.assertEqual(data, self.data)",if not read :,if not read :,100.00000000000004,100.00000000000004,1.0
"def f(view, s):     if mode == modes.NORMAL:         return sublime.Region(0)     elif mode == modes.VISUAL:         <MASK>             return sublime.Region(s.a + 1, 0)         else:             return sublime.Region(s.a, 0)     elif mode == modes.INTERNAL_NORMAL:         return sublime.Region(view.full_line(s.b).b, 0)     elif mode == modes.VISUAL_LINE:         if s.a < s.b:             return sublime.Region(0, s.b)         else:             return sublime.Region(0, s.a)     return s",if s . a < s . b :,if s . a < s . b :,100.00000000000004,100.00000000000004,1.0
def response(self):     try:         response = requests.get(str(self))         rjson = response.json()         <MASK>             raise Exception(response.text)         return rjson     except Exception as e:         raise ResponseFanartError(str(e)),"if not isinstance ( rjson , dict ) :",if not rjson :,10.88482843823664,10.88482843823664,0.0
"def __get_type(self, cexpr):     """"""Returns one of the following types: 'R' - read value, 'W' - write value, 'A' - function argument""""""     child = cexpr     for p in reversed(self.parents):         assert p, ""Failed to get type at "" + helper.to_hex(self.__function_address)         if p.cexpr.op == idaapi.cot_call:             return ""Arg""         if not p.is_expr():             return ""R""         if p.cexpr.op == idaapi.cot_asg:             <MASK>                 return ""W""             return ""R""         child = p.cexpr",if p . cexpr . x == child :,if child . is_expr ( ) :,6.033504141761816,6.033504141761816,0.0
"def _extract_lemma(self, parse: Parse) -> str:     special_feats = [x for x in self.SPECIAL_FEATURES if x in parse.tag]     if len(special_feats) == 0:         return parse.normal_form     # here we process surnames and patronyms since PyMorphy lemmatizes them incorrectly     for other in parse.lexeme:         tag = other.tag         <MASK>             continue         if (             tag.case == ""nomn""             and tag.gender == parse.tag.gender             and tag.number == ""sing""         ):             return other.word     return parse.normal_form",if any ( x not in tag for x in special_feats ) :,if tag is None :,1.9026155630072006,1.9026155630072006,0.0
"def evaluateWord(self, argument):     wildcard_count = argument[0].count(""*"")     if wildcard_count > 0:         if wildcard_count == 1 and argument[0].startswith(""*""):             return self.GetWordWildcard(argument[0][1:], method=""endswith"")         <MASK>             return self.GetWordWildcard(argument[0][:-1], method=""startswith"")         else:             _regex = argument[0].replace(""*"", "".+"")             matched = False             for w in self.words:                 matched = bool(re.search(_regex, w))                 if matched:                     break             return matched     return self.GetWord(argument[0])","if wildcard_count == 1 and argument [ 0 ] . endswith ( ""*"" ) :",if wildcard_count == 2 :,15.777684932819515,15.777684932819515,0.0
def getAllEntries(self):     entries = []     for bucket in self.buckets:         last = None         for entry in bucket.entries:             if last is not None:                 last.size = entry.virtualOffset - last.virtualOffset             last = entry             entries.append(entry)         <MASK>             entries[-1].size = bucket.endOffset - entries[-1].virtualOffset     return entries,if len ( entries ) != 0 :,if bucket . endOffset > last . virtualOffset :,5.669791110976001,5.669791110976001,0.0
def clean(self):     if self._ctx:         <MASK>             libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx)         else:             libcrypto.EVP_CIPHER_CTX_reset(self._ctx)         libcrypto.EVP_CIPHER_CTX_free(self._ctx),"if hasattr ( libcrypto , ""EVP_CIPHER_CTX_cleanup"" ) :",if self . _ctx . is_valid ( ) :,6.386992091194858,6.386992091194858,0.0
"def _addTab(self, name, label, idx=None):     label = getLanguageString(label)     tab = Tab(self, name, label)     tab.idx = self._makeTab(tab, idx)     if idx != None:         # Update index list when inserting tabs at arbitrary positions         newIdxList = {}         for tIdx, t in list(self._tabs_by_idx.items()):             <MASK>                 t.idx += 1             newIdxList[t.idx] = t         self._tabs_by_idx = newIdxList     self._tabs_by_idx[tab.idx] = tab     self._tabs_by_name[tab.name] = tab     return tab",if int ( tIdx ) >= idx :,if t . idx == idx :,19.493995755254467,19.493995755254467,0.0
"def set(self, _key, _new_login=True):     with self.lock:         user = self.users.get(current_user.id, None)         <MASK>             self.users[current_user.id] = dict(session_count=1, key=_key)         else:             if _new_login:                 user[""session_count""] += 1             user[""key""] = _key",if user is None :,if user is None :,100.00000000000004,100.00000000000004,1.0
"def stop(self):     # Try to shut the connection down, but if we get any sort of     # errors, go ahead and ignore them.. as we're shutting down anyway     try:         self.rpcserver.stop()         <MASK>             self.backend_rpcserver.stop()         if self.cluster_rpcserver:             self.cluster_rpcserver.stop()     except Exception:         pass     if self.coordination:         try:             coordination.COORDINATOR.stop()         except Exception:             pass     super(Service, self).stop(graceful=True)",if self . backend_rpcserver :,if self . backend_rpcserver :,100.00000000000004,100.00000000000004,1.0
"def __genmenuOnlyAllocated(menu):     for submenu in menu.Submenus:         __genmenuOnlyAllocated(submenu)     if menu.OnlyUnallocated == True:         tmp[""cache""].addMenuEntries(menu.AppDirs)         menuentries = []         for rule in menu.Rules:             menuentries = rule.do(                 tmp[""cache""].getMenuEntries(menu.AppDirs), rule.Type, 2             )         for menuentry in menuentries:             <MASK>                 menuentry.Parents.append(menu)                 #   menuentry.Add = False                 #   menuentry.Allocated = True                 menu.MenuEntries.append(menuentry)",if menuentry . Add == True :,if menuentry . Add == True :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, **options):     self.func_name_highlighting = get_bool_opt(options, ""func_name_highlighting"", True)     self.disabled_modules = get_list_opt(options, ""disabled_modules"", [])     self._functions = set()     if self.func_name_highlighting:         from pygments.lexers._lua_builtins import MODULES         for mod, func in iteritems(MODULES):             <MASK>                 self._functions.update(func)     RegexLexer.__init__(self, **options)",if mod not in self . disabled_modules :,if mod in self . disabled_modules :,71.89393375176813,71.89393375176813,0.0
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0):     if tr < 1:         tr = 1     x = time.time() + t     y = []     r = """"     if stderr:         pr = p.recv_err     else:         pr = p.recv     while time.time() < x or r:         r = pr()         <MASK>             break         elif r:             y.append(r)         else:             time.sleep(max((x - time.time()) / tr, 0))     return """".join(y)",if r is None :,if e :,17.799177396293473,0.0,0.0
"def get_menu_items(node):     aList = []     for child in node.children:         for tag in (""@menu"", ""@item""):             if child.h.startswith(tag):                 name = child.h[len(tag) + 1 :].strip()                 <MASK>                     aList.append((""%s %s"" % (tag, name), get_menu_items(child), None))                 else:                     b = g.splitLines("""".join(child.b))                     aList.append((tag, name, b[0] if b else """"))                 break     return aList","if tag == ""@menu"" :",if child . b is None :,5.868924818816531,5.868924818816531,0.0
"def import_suffix_generator(a_block, datatype=False):     if datatype is False:         for name, suffix in iteritems(a_block.component_map(Suffix)):             <MASK>                 yield name, suffix     else:         for name, suffix in iteritems(a_block.component_map(Suffix)):             if (suffix.import_enabled() is True) and (                 suffix.get_datatype() is datatype             ):                 yield name, suffix",if suffix . import_enabled ( ) is True :,if suffix . import_enabled ( ) :,71.19674182275,71.19674182275,0.0
"def verify_relative_valid_path(root, path):     if len(path) < 1:         raise PackagerError(""Empty chown path"")     checkpath = root     parts = path.split(os.sep)     for part in parts:         if part in (""."", ""..""):             raise PackagerError("". and .. is not allowed in chown path"")         checkpath = os.path.join(checkpath, part)         relpath = checkpath[len(root) + 1 :]         if not os.path.exists(checkpath):             raise PackagerError(f""chown path {relpath} does not exist"")         <MASK>             raise PackagerError(f""chown path {relpath} is a soft link"")",if os . path . islink ( checkpath ) :,if not os . path . isdir ( checkpath ) :,48.326978309062206,48.326978309062206,0.0
"def load_syntax(syntax):     context = _create_scheme() or {}     partition_scanner = PartitionScanner(syntax.get(""partitions"", []))     scanners = {}     for part_name, part_scanner in list(syntax.get(""scanner"", {}).items()):         scanners[part_name] = Scanner(part_scanner)     formats = []     for fname, fstyle in list(syntax.get(""formats"", {}).items()):         <MASK>             if fstyle.startswith(""%("") and fstyle.endswith("")s""):                 key = fstyle[2:-2]                 fstyle = context[key]             else:                 fstyle = fstyle % context         formats.append((fname, fstyle))     return partition_scanner, scanners, formats","if isinstance ( fstyle , basestring ) :",if fstyle :,7.49553326588684,0.0,0.0
"def should_keep_alive(commit_msg):     result = False     ci = get_current_ci() or """"     for line in commit_msg.splitlines():         parts = line.strip(""# "").split("":"", 1)         (key, val) = parts if len(parts) > 1 else (parts[0], """")         <MASK>             ci_names = val.replace("","", "" "").lower().split() if val else []             if len(ci_names) == 0 or ci.lower() in ci_names:                 result = True     return result","if key == ""CI_KEEP_ALIVE"" :","if key == ""alive"" :",36.06452879987793,36.06452879987793,0.0
"def get_note_title_file(note):     mo = note_title_re.match(note.get(""content"", """"))     if mo:         fn = mo.groups()[0]         fn = fn.replace("" "", ""_"")         fn = fn.replace(""/"", ""_"")         if not fn:             return """"         <MASK>             fn = unicode(fn, ""utf-8"")         else:             fn = unicode(fn)         if note_markdown(note):             fn += "".mkdn""         else:             fn += "".txt""         return fn     else:         return """"","if isinstance ( fn , str ) :",if note_html ( note ) :,13.134549472120788,13.134549472120788,0.0
"def post(self, orgname, teamname):     if _syncing_setup_allowed(orgname):         try:             team = model.team.get_organization_team(orgname, teamname)         except model.InvalidTeamException:             raise NotFound()         config = request.get_json()         # Ensure that the specified config points to a valid group.         status, err = authentication.check_group_lookup_args(config)         <MASK>             raise InvalidRequest(""Could not sync to group: %s"" % err)         # Set the team's syncing config.         model.team.set_team_syncing(team, authentication.federated_service, config)         return team_view(orgname, team)     raise Unauthorized()",if not status :,"if status != ""OK"" :",7.267884212102741,7.267884212102741,0.0
"def _marshalData(self):     if self._cache == None:         d = self._data         s = """"         s = time.strftime(""%H:%M:%S"", (0, 0, 0) + d + (0, 0, -1))         f = d[2] - int(d[2])         <MASK>             s += (""%g"" % f)[1:]         s += ""Z""         self._cache = s     return self._cache",if f != 0 :,if f :,23.174952587773145,0.0,0.0
"def _get_level(levels, level_ref):     if level_ref in levels:         return levels.index(level_ref)     if isinstance(level_ref, six.integer_types):         if level_ref < 0:             level_ref += len(levels)         <MASK>             raise PatsyError(""specified level %r is out of range"" % (level_ref,))         return level_ref     raise PatsyError(""specified level %r not found"" % (level_ref,))",if not ( 0 <= level_ref < len ( levels ) ) :,if level_ref > levels . count ( ) :,12.522972688689503,12.522972688689503,0.0
"def iterfieldselect(source, field, where, complement, missing):     it = iter(source)     hdr = next(it)     yield tuple(hdr)     indices = asindices(hdr, field)     getv = operator.itemgetter(*indices)     for row in it:         try:             v = getv(row)         except IndexError:             v = missing         <MASK>  # XOR             yield tuple(row)",if bool ( where ( v ) ) != complement :,if not complement :,4.784824825520546,4.784824825520546,0.0
"def _test_wait_read_invalid_switch(self, sleep):     sock1, sock2 = socket.socketpair()     try:         p = gevent.spawn(             util.wrap_errors(                 AssertionError, socket.wait_read             ),  # pylint:disable=no-member             sock1.fileno(),         )         gevent.get_hub().loop.run_callback(switch_None, p)         <MASK>             gevent.sleep(sleep)         result = p.get()         assert isinstance(result, AssertionError), result         assert ""Invalid switch"" in str(result), repr(str(result))     finally:         sock1.close()         sock2.close()",if sleep is not None :,if sleep :,23.174952587773145,0.0,0.0
"def train(config, args):     gan = setup_gan(config, inputs, args)     test_batches = []     for i in range(args.steps):         gan.step()         <MASK>             correct_prediction = 0             total = 0             for (x, y) in gan.inputs.testdata():                 prediction = gan.generator(x)                 correct_prediction += (                     torch.argmax(prediction, 1) == torch.argmax(y, 1)                 ).sum()                 total += y.shape[0]             accuracy = (float(correct_prediction) / total) * 100             print(""accuracy: "", accuracy)     return sum_metrics",if i % args . sample_every == 0 and i > 0 :,if args . testdata :,2.815135668143185,2.815135668143185,0.0
"def process_response(self, request, response, spider):     if not response.body:         return response     for fmt, func in six.iteritems(self._formats):         new_response = func(response)         <MASK>             logger.debug(                 ""Decompressed response with format: %(responsefmt)s"",                 {""responsefmt"": fmt},                 extra={""spider"": spider},             )             return new_response     return response",if new_response :,if new_response :,100.00000000000004,100.00000000000004,1.0
"def detect_ssl_option(self):     for option in self.ssl_options():         <MASK>             for other_option in self.ssl_options():                 if option != other_option:                     if scan_argv(self.argv, other_option) is not None:                         raise ConfigurationError(                             ""Cannot give both %s and %s"" % (option, other_option)                         )             return option","if scan_argv ( self . argv , option ) is not None :",if option != other_option :,3.255629778647324,3.255629778647324,0.0
"def load(cls, storefile, template_store):     # Did we get file or filename?     if not hasattr(storefile, ""read""):         storefile = open(storefile, ""rb"")     # Adjust store to have translations     store = cls.convertfile(storefile, template_store)     for unit in store.units:         <MASK>             continue         # HTML does this properly on loading, others need it         if cls.needs_target_sync:             unit.target = unit.source             unit.rich_target = unit.rich_source     return store",if unit . isheader ( ) :,if unit . source is None :,26.269098944241588,26.269098944241588,0.0
"def _pre_get_table(self, _ctx, table_name):     vsctl_table = self._get_table(table_name)     schema_helper = self.schema_helper     schema_helper.register_table(vsctl_table.table_name)     for row_id in vsctl_table.row_ids:         <MASK>             schema_helper.register_table(row_id.table)         if row_id.name_column:             schema_helper.register_columns(row_id.table, [row_id.name_column])         if row_id.uuid_column:             schema_helper.register_columns(row_id.table, [row_id.uuid_column])     return vsctl_table",if row_id . table :,if row_id . table_name :,61.04735835807847,61.04735835807847,0.0
"def __init__(self, pin=None, pull_up=False):     super(InputDevice, self).__init__(pin)     try:         self.pin.function = ""input""         pull = ""up"" if pull_up else ""down""         <MASK>             self.pin.pull = pull     except:         self.close()         raise     self._active_state = False if pull_up else True     self._inactive_state = True if pull_up else False",if self . pin . pull != pull :,if pull :,6.108851178104657,0.0,0.0
"def _increment_operations_count(self, operation, executed):     with self._lock:         <MASK>             self._executed_operations += 1             self._executed[operation.job_type] += 1         else:             self._skipped[operation.job_type] += 1",if executed :,if operation . job_type in self . _executed_operations :,3.737437943747671,3.737437943747671,0.0
"def emit(self, type, info=None):     # Overload emit() to send events to the proxy object at the other end     ev = super().emit(type, info)     if self._has_proxy is True and self._session.status > 0:         # implicit: and self._disposed is False:         <MASK>             self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])         elif type in self.__event_types_at_proxy:             self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])",if type in self . __proxy_properties__ :,if type in self . __event_types_at_proxy :,47.587330964125215,47.587330964125215,0.0
"def validate_pull_secret(namespace):     if namespace.pull_secret is None:         # TODO: add aka.ms link here         warning = (             ""No --pull-secret provided: cluster will not include samples or operators from ""             + ""Red Hat or from certified partners.""         )         logger.warning(warning)     else:         try:             <MASK>                 raise Exception()         except:             raise InvalidArgumentValueError(""Invalid --pull-secret."")","if not isinstance ( json . loads ( namespace . pull_secret ) , dict ) :",if namespace . pull_secret not in namespace . pull_secret :,23.14024593353986,23.14024593353986,0.0
"def pack(types, *args):     if len(types) != len(args):         raise Exception(""number of arguments does not match format string"")     port = StringIO()     for (type, value) in zip(types, args):         if type == ""V"":             write_vuint(port, value)         elif type == ""v"":             write_vint(port, value)         <MASK>             write_bvec(port, value)         else:             raise Exception('unknown xpack format string item ""' + type + '""')     return port.getvalue()","elif type == ""s"" :","if type == ""b"" :",41.11336169005198,41.11336169005198,0.0
"def data(self):     if self._data is not None:         return self._data     else:         <MASK>             with open(self.path, ""rb"") as jsonfile:                 data = jsonfile.read().decode(""utf8"")                 data = json.loads(data)                 self._data = data                 return self._data         else:             return dict()",if os . path . exists ( self . path ) :,if self . path :,11.141275535087015,11.141275535087015,0.0
"def interact(self):     self.output.write(""\n"")     while True:         try:             request = self.getline(""help> "")             <MASK>                 break         except (KeyboardInterrupt, EOFError):             break         request = strip(request)         # Make sure significant trailing quotation marks of literals don't         # get deleted while cleaning input         if (             len(request) > 2             and request[0] == request[-1] in (""'"", '""')             and request[0] not in request[1:-1]         ):             request = request[1:-1]         if lower(request) in (""q"", ""quit""):             break         self.help(request)",if not request :,"if request == """" :",8.643019616048525,8.643019616048525,0.0
"def api_attachment_metadata(self):     resp = []     for part in self.parts:         <MASK>             continue         k = {             ""content_type"": part.block.content_type,             ""size"": part.block.size,             ""filename"": part.block.filename,             ""id"": part.block.public_id,         }         content_id = part.content_id         if content_id:             if content_id[0] == ""<"" and content_id[-1] == "">"":                 content_id = content_id[1:-1]             k[""content_id""] = content_id         resp.append(k)     return resp",if not part . is_attachment :,if not part . block :,38.49815007763549,38.49815007763549,0.0
"def _notin_text(term, text, verbose=False):     index = text.find(term)     head = text[:index]     tail = text[index + len(term) :]     correct_text = head + tail     diff = _diff_text(correct_text, text, verbose)     newdiff = [u(""%s is contained here:"") % py.io.saferepr(term, maxsize=42)]     for line in diff:         if line.startswith(u(""Skipping"")):             continue         <MASK>             continue         if line.startswith(u(""+ "")):             newdiff.append(u(""  "") + line[2:])         else:             newdiff.append(line)     return newdiff","if line . startswith ( u ( ""- "" ) ) :",if not line :,2.215745752614824,2.215745752614824,0.0
"def get_api(user, url):     global API_CACHE     if API_CACHE is None or API_CACHE.get(url) is None:         API_CACHE_LOCK.acquire()         try:             <MASK>                 API_CACHE = {}             if API_CACHE.get(url) is None:                 API_CACHE[url] = ImpalaDaemonApi(url)         finally:             API_CACHE_LOCK.release()     api = API_CACHE[url]     api.set_user(user)     return api",if API_CACHE is None :,if API_CACHE is None :,100.00000000000004,100.00000000000004,1.0
"def __str__(self, prefix="""", printElemNumber=0):     res = """"     if self.has_index_name_:         res += prefix + (""index_name: %s\n"" % self.DebugFormatString(self.index_name_))     cnt = 0     for e in self.prefix_value_:         elm = """"         <MASK>             elm = ""(%d)"" % cnt         res += prefix + (""prefix_value%s: %s\n"" % (elm, self.DebugFormatString(e)))         cnt += 1     if self.has_value_prefix_:         res += prefix + (             ""value_prefix: %s\n"" % self.DebugFormatBool(self.value_prefix_)         )     return res",if printElemNumber :,if printElemNumber :,100.00000000000004,0.0,1.0
"def add_group(x, nl, in_group, mw):     if len(x) == 0:         return x     if len(x) > 1 and not in_group:         <MASK>             return [""[[""] + x + [""]]""]         mw.warn(             ""Equation will multiplex and may produce inaccurate results (see manual)""         )     return [""[""] + x + [""]""]","if supports_group ( x , nl ) :",if nl :,3.848335094984576,0.0,0.0
"def unfulfilled_items(self):     unfulfilled_items = 0     for order_item in self.items.all():         <MASK>             aggr = order_item.deliver_item.aggregate(delivered=Sum(""quantity""))             unfulfilled_items += order_item.quantity - (aggr[""delivered""] or 0)     return unfulfilled_items",if not order_item . canceled :,if order_item . deliver_item :,33.03164318013809,33.03164318013809,0.0
"def _get_pattern(self, pattern_id):     """"""Get pattern item by id.""""""     for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3):         <MASK>             data = self.tagged_blocks.get_data(key)             for pattern in data:                 if pattern.pattern_id == pattern_id:                     return pattern     return None",if key in self . tagged_blocks :,if key in self . tagged_blocks :,100.00000000000004,100.00000000000004,1.0
"def query_lister(domain, query="""", max_items=None, attr_names=None):     more_results = True     num_results = 0     next_token = None     while more_results:         rs = domain.connection.query_with_attributes(             domain, query, attr_names, next_token=next_token         )         for item in rs:             if max_items:                 <MASK>                     raise StopIteration             yield item             num_results += 1         next_token = rs.next_token         more_results = next_token != None",if num_results == max_items :,if num_results > max_items :,53.417359568998464,53.417359568998464,0.0
"def find_deprecated_settings(source):  # pragma: no cover     from celery.utils import deprecated     for name, opt in flatten(NAMESPACES):         <MASK>             deprecated.warn(                 description=""The {0!r} setting"".format(name),                 deprecation=opt.deprecate_by,                 removal=opt.remove_by,                 alternative=""Use the {0.alt} instead"".format(opt),             )     return source","if ( opt . deprecate_by or opt . remove_by ) and getattr ( source , name , None ) :",if opt . deprecated :,0.568366089080667,0.568366089080667,0.0
"def tearDown(self):     """"""Shutdown the server.""""""     try:         <MASK>             self.server.stop(2.0)         if self.sl_hdlr:             self.root_logger.removeHandler(self.sl_hdlr)             self.sl_hdlr.close()     finally:         BaseTest.tearDown(self)",if self . server :,if self . server :,100.00000000000004,100.00000000000004,1.0
"def broadcast_events(self, events):     LOGGER.debug(""Broadcasting events: %s"", events)     with self._subscribers_cv:         # Copy the subscribers         subscribers = {conn: sub.copy() for conn, sub in self._subscribers.items()}     if subscribers:         for connection_id, subscriber in subscribers.items():             <MASK>                 subscriber_events = [                     event for event in events if subscriber.is_subscribed(event)                 ]                 event_list = EventList(events=subscriber_events)                 self._send(connection_id, event_list.SerializeToString())",if subscriber . is_listening ( ) :,if subscriber . is_subscribed ( ) :,59.694917920196445,59.694917920196445,0.0
"def _get_info(self, path):     info = OrderedDict()     if not self._is_mac() or self._has_xcode_tools():         stdout = None         try:             stdout, stderr = Popen(                 [self._find_binary(), ""info"", os.path.realpath(path)],                 stdout=PIPE,                 stderr=PIPE,             ).communicate()         except OSError:             pass         else:             if stdout:                 for line in stdout.splitlines():                     line = u(line).split("": "", 1)                     <MASK>                         info[line[0]] = line[1]     return info",if len ( line ) == 2 :,if len ( line ) == 2 :,100.00000000000004,100.00000000000004,1.0
"def test_call_extern_c_fn(self):     global memcmp     memcmp = cffi_support.ExternCFunction(         ""memcmp"",         (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""),     )     @udf(BooleanVal(FunctionContext, StringVal, StringVal))     def fn(context, a, b):         if a.is_null != b.is_null:             return False         <MASK>             return True         if len(a) != b.len:             return False         if a.ptr == b.ptr:             return True         return memcmp(a.ptr, b.ptr, a.len) == 0",if a is None :,if len ( a ) != b . len :,4.9323515694897075,4.9323515694897075,0.0
"def _flatten(*args):     ahs = set()     if len(args) > 0:         for item in args:             if type(item) is ActionHandle:                 ahs.add(item)             elif type(item) in (list, tuple, dict, set):                 for ah in item:                     <MASK>  # pragma:nocover                         raise ActionManagerError(""Bad argument type %s"" % str(ah))                     ahs.add(ah)             else:  # pragma:nocover                 raise ActionManagerError(""Bad argument type %s"" % str(item))     return ahs",if type ( ah ) is not ActionHandle :,if ah not in ahs :,6.962210312500384,6.962210312500384,0.0
"def startElement(self, name, attrs, connection):     if name == ""Parameter"":         <MASK>             self[self._current_param.name] = self._current_param         self._current_param = Parameter(self)         return self._current_param",if self . _current_param :,if self . _current_param . name in self :,53.3167536340577,53.3167536340577,0.0
"def _find_class_in_descendants(self, search_key):     for cls in self.primitive_classes:         cls_key = (cls.__name__, cls.__module__)         self.class_cache[cls_key] = cls         <MASK>             return cls",if cls_key == search_key :,if search_key in self . class_cache :,16.59038701421971,16.59038701421971,0.0
"def doWorkForFindAll(self, v, target, partialMatch):     sibling = self     while sibling:         c1 = partialMatch and sibling.equalsTreePartial(target)         if c1:             v.append(sibling)         else:             c2 = not partialMatch and sibling.equalsTree(target)             if c2:                 v.append(sibling)         ### regardless of match or not, check any children for matches         <MASK>             sibling.getFirstChild().doWorkForFindAll(v, target, partialMatch)         sibling = sibling.getNextSibling()",if sibling . getFirstChild ( ) :,if c1 :,9.138402379955025,0.0,0.0
"def forward(self, inputs: paddle.Tensor):     outputs = []     blocks = self.block(inputs)     route = None     for i, block in enumerate(blocks):         if i > 0:             block = paddle.concat([route, block], axis=1)         route, tip = self.yolo_blocks[i](block)         block_out = self.block_outputs[i](tip)         outputs.append(block_out)         <MASK>             route = self.route_blocks_2[i](route)             route = self.upsample(route)     return outputs",if i < 2 :,if i > 0 :,23.643540225079384,23.643540225079384,0.0
"def _filter_paths(basename, path, is_dir, exclude):     """""".gitignore style file filtering.""""""     for item in exclude:         # Items ending in '/' apply only to directories.         <MASK>             continue         # Items starting with '/' apply to the whole path.         # In any other cases just the basename is used.         match = path if item.startswith(""/"") else basename         if fnmatch.fnmatch(match, item.strip(""/"")):             return True     return False","if item . endswith ( ""/"" ) and not is_dir :",if is_dir :,9.569649651041097,9.569649651041097,0.0
"def reposition_division(f1):     lines = f1.splitlines()     if lines[2] == division:         lines.pop(2)     found = 0     for i, line in enumerate(lines):         if line.startswith('""""""'):             found += 1             if found == 2:                 <MASK>                     break  # already in the right place                 lines.insert(i + 1, """")                 lines.insert(i + 2, division)                 break     return ""\n"".join(lines)","if division in ""\n"" . join ( lines ) :",if found == 1 :,3.005799339448764,3.005799339448764,0.0
"def buildImage(opt):     dpath = os.path.join(opt[""datapath""], ""COCO-IMG-2015"")     version = ""1""     if not build_data.built(dpath, version_string=version):         print(""[building image data: "" + dpath + ""]"")         <MASK>             # An older version exists, so remove these outdated files.             build_data.remove_dir(dpath)         build_data.make_dir(dpath)         # Download the data.         for downloadable_file in RESOURCES[:1]:             downloadable_file.download_file(dpath)         # Mark the data as built.         build_data.mark_done(dpath, version_string=version)",if build_data . built ( dpath ) :,"if version < ""1"" :",5.08764122072739,5.08764122072739,0.0
"def colorformat(text):     if text[0:1] == ""#"":         col = text[1:]         <MASK>             return col         elif len(col) == 3:             return col[0] * 2 + col[1] * 2 + col[2] * 2     elif text == """":         return """"     assert False, ""wrong color format %r"" % text",if len ( col ) == 6 :,if len ( col ) == 2 :,75.06238537503395,75.06238537503395,0.0
"def tree_print(tree):     for key in tree:         print(key, end="" "")  # end=' ' prevents a newline character         tree_element = tree[key]  # multiple lookups is expensive, even amortized O(1)!         for subElem in tree_element:             print("" -> "", subElem, end="" "")             <MASK>  # OP wants indenting after digits                 print(""\n "")  # newline and a space to match indenting         print()  # forces a newline",if type ( subElem ) != str :,"if subElem . startswith ( "" "" ) :",7.129384882260374,7.129384882260374,0.0
"def is_dse_cluster(path):     try:         with open(os.path.join(path, ""CURRENT""), ""r"") as f:             name = f.readline().strip()             cluster_path = os.path.join(path, name)             filename = os.path.join(cluster_path, ""cluster.conf"")             with open(filename, ""r"") as f:                 data = yaml.load(f)             <MASK>                 return True     except IOError:         return False","if ""dse_dir"" in data :",if data :,8.525588607164655,0.0,0.0
"def delete_old_target_output_files(classpath_prefix):     """"""Delete existing output files or symlinks for target.""""""     directory, basename = os.path.split(classpath_prefix)     pattern = re.compile(         r""^{basename}(([0-9]+)(\.jar)?|classpath\.txt)$"".format(             basename=re.escape(basename)         )     )     files = [filename for filename in os.listdir(directory) if pattern.match(filename)]     for rel_path in files:         path = os.path.join(directory, rel_path)         <MASK>             safe_delete(path)",if os . path . islink ( path ) or os . path . isfile ( path ) :,if os . path . isfile ( path ) :,40.656965974059936,40.656965974059936,0.0
"def test_files(self):     # get names of files to test     dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)     names = []     for d in self.test_directories:         test_dir = os.path.join(dist_dir, d)         for n in os.listdir(test_dir):             if n.endswith("".py"") and not n.startswith(""bad""):                 names.append(os.path.join(test_dir, n))     for filename in names:         <MASK>             print(""Testing %s"" % filename)         source = read_pyfile(filename)         self.check_roundtrip(source)",if test_support . verbose :,if os . path . isfile ( filename ) :,5.522397783539471,5.522397783539471,0.0
"def __str__(self):     if self.HasError():         return self.ErrorAsStr()     else:         # Format is: {action} ""{target}"" ({filename}:{lineno})         string = self._action         if self._target is not None:             string += ' ""{target}""'.format(target=self._target)         <MASK>             path = self._filename             if self._lineno is not None:                 path += "":{lineno}"".format(lineno=self._lineno)             string += "" ({path})"".format(path=path)         return string",if self . _filename is not None :,if self . _filename is not None :,100.00000000000004,100.00000000000004,1.0
"def extra_action_out(self, input_dict, state_batches, model, action_dist):     with self._no_grad_context():         <MASK>             stats_dict = extra_action_out_fn(                 self, input_dict, state_batches, model, action_dist             )         else:             stats_dict = parent_cls.extra_action_out(                 self, input_dict, state_batches, model, action_dist             )         return self._convert_to_non_torch_type(stats_dict)",if extra_action_out_fn :,if self . _no_grad_context :,6.27465531099474,6.27465531099474,0.0
"def _retract_bindings(fstruct, inv_bindings, fs_class, visited):     # Visit each node only once:     if id(fstruct) in visited:         return     visited.add(id(fstruct))     if _is_mapping(fstruct):         items = fstruct.items()     elif _is_sequence(fstruct):         items = enumerate(fstruct)     else:         raise ValueError(""Expected mapping or sequence"")     for (fname, fval) in items:         if isinstance(fval, fs_class):             <MASK>                 fstruct[fname] = inv_bindings[id(fval)]             _retract_bindings(fval, inv_bindings, fs_class, visited)",if id ( fval ) in inv_bindings :,if id ( fval ) in inv_bindings :,100.00000000000004,100.00000000000004,1.0
"def warehouses(self) -> tuple:     from ..repositories import WarehouseBaseRepo     repos = dict()     for dep in chain(self.dependencies, [self]):         <MASK>             continue         if not isinstance(dep.repo, WarehouseBaseRepo):             continue         for repo in dep.repo.repos:             if repo.from_config:                 continue             repos[repo.name] = repo     return tuple(repos.values())",if dep . repo is None :,if not dep . repo :,29.05925408079185,29.05925408079185,0.0
"def detype(self):     if self._detyped is not None:         return self._detyped     ctx = {}     for key, val in self._d.items():         if not isinstance(key, str):             key = str(key)         detyper = self.get_detyper(key)         <MASK>             # cannot be detyped             continue         deval = detyper(val)         if deval is None:             # cannot be detyped             continue         ctx[key] = deval     self._detyped = ctx     return ctx",if detyper is None :,if detyper is None :,100.00000000000004,100.00000000000004,1.0
"def populate_obj(self, obj, name):     field = getattr(obj, name, None)     if field is not None:         # If field should be deleted, clean it up         <MASK>             field.delete()             return         if isinstance(self.data, FileStorage) and not is_empty(self.data.stream):             if not field.grid_id:                 func = field.put             else:                 func = field.replace             func(                 self.data.stream,                 filename=self.data.filename,                 content_type=self.data.content_type,             )",if self . _should_delete :,if field . grid_id :,8.051153633013374,8.051153633013374,0.0
"def _load(container):     if isinstance(container, str):         # If container is a filename.         <MASK>             with open(container, ""rb"") as f:                 return pickle.load(f)         # If container is a pickle string.         else:             return pickle.loads(container)     # If container is an open file     elif isinstance(container, IOBase):         return pickle.load(container)     # What else could it be?     else:         l.error(""Cannot unpickle container of type %s"", type(container))         return None",if all ( c in string . printable for c in container ) and os . path . exists ( container ) :,"if isinstance ( container , ( str , unicode ) ) :",4.405063030221482,4.405063030221482,0.0
"def append_row(self, row):     self.allocate_future_payments(row)     self.set_invoice_details(row)     self.set_party_details(row)     self.set_ageing(row)     if self.filters.get(""group_by_party""):         self.update_sub_total_row(row, row.party)         <MASK>             self.append_subtotal_row(self.previous_party)         self.previous_party = row.party     self.data.append(row)",if self . previous_party and ( self . previous_party != row . party ) :,if self . previous_party :,13.127910466261701,13.127910466261701,0.0
"def gg1():     while 1:         tt = 3         while tt > 0:             trace.append(tt)             val = yield             <MASK>                 tt = 10  # <= uncomment this line                 trace.append(""breaking early..."")                 break             tt -= 1         trace.append(""try!"")",if val is not None :,if val == 0 :,17.965205598154213,17.965205598154213,0.0
"def migrate_common_facts(facts):     """"""Migrate facts from various roles into common""""""     params = {""node"": (""portal_net""), ""master"": (""portal_net"")}     if ""common"" not in facts:         facts[""common""] = {}     # pylint: disable=consider-iterating-dictionary     for role in params.keys():         if role in facts:             for param in params[role]:                 <MASK>                     facts[""common""][param] = facts[role].pop(param)     return facts",if param in facts [ role ] :,"if ""common"" in facts [ role ] :",53.7284965911771,53.7284965911771,0.0
"def get_measurements(self, pipeline, object_name, category):     if self.get_categories(pipeline, object_name) == [category]:         results = []         <MASK>             if object_name == ""Image"":                 results += [""Correlation"", ""Slope""]             else:                 results += [""Correlation""]         if self.do_overlap:             results += [""Overlap"", ""K""]         if self.do_manders:             results += [""Manders""]         if self.do_rwc:             results += [""RWC""]         if self.do_costes:             results += [""Costes""]         return results     return []",if self . do_corr_and_slope :,"if object_name == ""Slope"" :",4.996872151825361,4.996872151825361,0.0
"def access_modes(self):     """"""access_modes property""""""     if self._access_modes is None:         self._access_modes = self.get_access_modes()         <MASK>             self._access_modes = list(self._access_modes)     return self._access_modes","if not isinstance ( self . _access_modes , list ) :",if self . _access_modes is not None :,38.90211335140371,38.90211335140371,0.0
"def unwrap_envelope(self, data, many):     if many:         <MASK>             if isinstance(data, InstrumentedList) or isinstance(data, list):                 self.context[""total""] = len(data)                 return data             else:                 self.context[""total""] = data[""total""]         else:             self.context[""total""] = 0             data = {""items"": []}         return data[""items""]     return data","if data [ ""items"" ] :",if data :,11.898417391331403,0.0,0.0
"def to_string(self, fmt=""{:.4f}""):     result_str = """"     for key in self.measures:         result = self.m_dict[key][0]()         result_str += (             "","".join(fmt.format(x) for x in result)             <MASK>             else fmt.format(result)         )         result_str += "",""     return result_str[:-1]  # trim the last comma","if isinstance ( result , tuple )","if key == ""m_dict"" :",4.196114906296549,4.196114906296549,0.0
"def on_torrent_created(self, result):     if not result:         return     self.dialog_widget.btn_create.setEnabled(True)     self.dialog_widget.edit_channel_create_torrent_progress_label.setText(         ""Created torrent""     )     if ""torrent"" in result:         self.create_torrent_notification.emit({""msg"": ""Torrent successfully created""})         <MASK>             self.add_torrent_to_channel(result[""torrent""])         self.close_dialog()",if self . dialog_widget . add_to_channel_checkbox . isChecked ( ) :,"if ""torrent"" in result :",1.4064939156282428,1.4064939156282428,0.0
"def save(self):     for var_name in self.default_config:         <MASK>             if var_name in self.file_config:                 del self.file_config[var_name]         else:             self.file_config[var_name] = getattr(self, var_name)     with open(self.config_path, ""w"") as f:         f.write(json.dumps(self.file_config, indent=2))","if getattr ( self , var_name , None ) == self . default_config [ var_name ] :",if var_name not in self . config_path :,6.7543700425927256,6.7543700425927256,0.0
"def get_class_parameters(kwarg):     ret = {""attrs"": []}     for key in (""rsc"", ""fsc"", ""usc""):         <MASK>             ret[""attrs""].append(                 [                     ""TCA_HFSC_%s"" % key.upper(),                     {                         ""m1"": get_rate(kwarg[key].get(""m1"", 0)),                         ""d"": get_time(kwarg[key].get(""d"", 0)),                         ""m2"": get_rate(kwarg[key].get(""m2"", 0)),                     },                 ]             )     return ret",if key in kwarg :,"if key in ( ""tca_HFSC"" , ""tca_hfsc"" , ""tca_hfsc_d"" , ""tca_hfsc_d2"" ) :",4.492398578415928,4.492398578415928,0.0
"def forward(self, x):     f_x = x     if self.exp:         f_x = self.exp_swish(self.exp_bn(self.exp(f_x)))     f_x = self.dwise_swish(self.dwise_bn(self.dwise(f_x)))     f_x = self.se(f_x)     f_x = self.lin_proj_bn(self.lin_proj(f_x))     if self.has_skip:         <MASK>             f_x = drop_connect(f_x, effnet_cfg.EN.DC_RATIO)         f_x = x + f_x     return f_x",if self . training and effnet_cfg . EN . DC_RATIO > 0.0 :,if self . has_skip :,6.656592803413299,6.656592803413299,0.0
"def cli_uninstall_distro():     distro_list = install_distro_list()     if distro_list is not None:         for index, _distro_dir in enumerate(distro_list):             log(str(index) + ""  --->>  "" + _distro_dir)         user_input = read_input_uninstall()         if user_input is not False:             for index, _distro_dir in enumerate(distro_list):                 <MASK>                     config.uninstall_distro_dir_name = _distro_dir                     unin_distro()     else:         log(""No distro installed on "" + config.usb_disk)",if index == user_input :,"if user_input == ""uninstall"" :",21.36435031981171,21.36435031981171,0.0
"def IMPORTFROM(self, node):     if node.module == ""__future__"":         <MASK>             self.report(messages.LateFutureImport, node, [n.name for n in node.names])     else:         self.futuresAllowed = False     for alias in node.names:         if alias.name == ""*"":             self.scope.importStarred = True             self.report(messages.ImportStarUsed, node, node.module)             continue         name = alias.asname or alias.name         importation = Importation(name, node)         if node.module == ""__future__"":             importation.used = (self.scope, node)         self.addBinding(node, importation)",if not self . futuresAllowed :,"if node . name == ""*"" :",5.522397783539471,5.522397783539471,0.0
"def _split_and_load(batch, ctx_list):     """"""Split data to 1 batch each device.""""""     new_batch = []     for _, data in enumerate(batch):         <MASK>             new_data = [x.as_in_context(ctx) for x, ctx in zip(data, ctx_list)]         else:             new_data = [data.as_in_context(ctx_list[0])]         new_batch.append(new_data)     return new_batch","if isinstance ( data , ( list , tuple ) ) :",if len ( data ) == 1 :,8.591316733350183,8.591316733350183,0.0
"def wait_success(self, timeout=60 * 10):     for i in range(timeout // 10):         time.sleep(10)         status = self.query_job()         print(""job {} status is {}"".format(self.job_id, status))         <MASK>             return True         if status and status in [             StatusSet.CANCELED,             StatusSet.TIMEOUT,             StatusSet.FAILED,         ]:             return False     return False",if status and status == StatusSet . SUCCESS :,if status == StatusSet . SUCCESS :,70.37259479962373,70.37259479962373,0.0
"def copy_tree(self, src_dir, dst_dir, skip_variables=False):     for src_root, _, files in os.walk(src_dir):         <MASK>             rel_root = os.path.relpath(src_root, src_dir)         else:             rel_root = """"         if skip_variables and rel_root.startswith(""variables""):             continue         dst_root = os.path.join(dst_dir, rel_root)         if not os.path.exists(dst_root):             os.makedirs(dst_root)         for f in files:             shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))",if src_root != src_dir :,if os . path . isdir ( src_root ) :,14.323145079400492,14.323145079400492,0.0
"def _make_padded_shapes(self, dataset, decoders):     padded_shapes = dataset.output_shapes     for i, hparams_i in enumerate(self._hparams.datasets):         <MASK>             continue         if not hparams_i[""pad_to_max_seq_length""]:             continue         text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes(             dataset, hparams_i, decoders[i], self.text_name(i), self.text_id_name(i)         )         padded_shapes.update(text_and_id_shapes)     return padded_shapes","if not _is_text_data ( hparams_i [ ""data_type"" ] ) :",if not self . _padded_text_and_id_shapes :,9.550578445401381,9.550578445401381,0.0
"def format_errors(messages):     errors = {}     for k, v in messages.items():         key = camelize(k, uppercase_first_letter=False)         <MASK>             errors[key] = format_errors(v)         elif isinstance(v, list):             errors[key] = v[0]     return errors","if isinstance ( v , dict ) :","if isinstance ( v , ( str , unicode ) ) :",36.462858619364674,36.462858619364674,0.0
"def generic_visit(self, node, parents=None):     parents = (parents or []) + [node]     for field, value in iter_fields(node):         <MASK>             for item in value:                 if isinstance(item, AST):                     self.visit(item, parents)         elif isinstance(value, AST):             self.visit(value, parents)","if isinstance ( value , list ) :","if field == ""id"" :",6.567274736060395,6.567274736060395,0.0
"def get_override_css(self):     """"""handls allow_css_overrides setting.""""""     if self.settings.get(""allow_css_overrides""):         filename = self.view.file_name()         filetypes = self.settings.get(""markdown_filetypes"")         if filename and filetypes:             for filetype in filetypes:                 <MASK>                     css_filename = filename.rpartition(filetype)[0] + "".css""                     if os.path.isfile(css_filename):                         return u""<style>%s</style>"" % load_utf8(css_filename)     return """"",if filename . endswith ( filetype ) :,"if filetype . startswith ( ""css"" ) :",11.044795567078939,11.044795567078939,0.0
"def clean(self):     super().clean()     # If the Cluster is assigned to a Site, all Devices must be assigned to that Site.     if self.cluster.site is not None:         for device in self.cleaned_data.get(""devices"", []):             <MASK>                 raise ValidationError(                     {                         ""devices"": ""{} belongs to a different site ({}) than the cluster ({})"".format(                             device, device.site, self.cluster.site                         )                     }                 )",if device . site != self . cluster . site :,if device . site != self . cluster . site :,100.00000000000004,100.00000000000004,1.0
"def _setProcessPriority(process, nice_val, disable_gc):     org_nice_val = Computer._process_original_nice_value     try:         process.nice(nice_val)         Computer.in_high_priority_mode = nice_val != org_nice_val         <MASK>             gc.disable()         else:             gc.enable()         return True     except psutil.AccessDenied:         print2err(             ""WARNING: Could not set process {} priority ""             ""to {}"".format(process.pid, nice_val)         )         return False",if disable_gc :,if disable_gc :,100.00000000000004,100.00000000000004,1.0
"def _setResultsName(self, name, listAllMatches=False):     if __diag__.warn_multiple_tokens_in_named_alternation:         <MASK>             warnings.warn(                 ""{}: setting results name {!r} on {} expression ""                 ""may only return a single token for an And alternative, ""                 ""in future will return the full list of tokens"".format(                     ""warn_multiple_tokens_in_named_alternation"",                     name,                     type(self).__name__,                 ),                 stacklevel=3,             )     return super()._setResultsName(name, listAllMatches)","if any ( isinstance ( e , And ) for e in self . exprs ) :","if name in [ ""And"" , ""And"" , ""And"" ] :",3.4197980307804725,3.4197980307804725,0.0
"def make_sources(project: RootDependency) -> str:     content = []     if project.readme:         content.append(project.readme.path.name)         if project.readme.markup != ""rst"":             content.append(project.readme.to_rst().path.name)     path = project.package.path     for fname in (""setup.cfg"", ""setup.py""):         <MASK>             content.append(fname)     for package in chain(project.package.packages, project.package.data):         for fpath in package:             fpath = fpath.relative_to(project.package.path)             content.append(""/"".join(fpath.parts))     return ""\n"".join(content)",if ( path / fname ) . exists ( ) :,if path . endswith ( fname ) :,12.347293198886947,12.347293198886947,0.0
"def findControlPointsInMesh(glyph, va, subsegments):     controlPointIndices = np.zeros((len(va), 1))     index = 0     for i, c in enumerate(subsegments):         segmentCount = len(glyph.contours[i].segments) - 1         for j, s in enumerate(c):             if j < segmentCount:                 <MASK>                     controlPointIndices[index] = 1             index += s[1]     return controlPointIndices","if glyph . contours [ i ] . segments [ j ] . type == ""line"" :",if s [ 0 ] == va [ j ] :,9.426482755071667,9.426482755071667,0.0
"def MergeFrom(self, other):     if self.message_class is not None:         if other.Parse(self.message_class):             self.message.MergeFrom(other.message)     elif other.message_class is not None:         <MASK>             self.message = other.message_class()             self.message_class = other.message_class         self.message.MergeFrom(other.message)     else:         self.message += other.message",if not self . Parse ( other . message_class ) :,if self . message_class is not None :,24.86846702926914,24.86846702926914,0.0
"def remove_old_snapshot(install_dir):     logging.info(""Removing any old files in {}"".format(install_dir))     for file in glob.glob(""{}/*"".format(install_dir)):         try:             <MASK>                 os.unlink(file)             elif os.path.isdir(file):                 shutil.rmtree(file)         except Exception as error:             logging.error(""Error: {}"".format(error))             sys.exit(1)",if os . path . isfile ( file ) :,if os . path . isfile ( file ) :,100.00000000000004,100.00000000000004,1.0
"def writexml(     self,     stream,     indent="""",     addindent="""",     newl="""",     strip=0,     nsprefixes={},     namespace="""", ):     w = _streamWriteWrapper(stream)     if self.raw:         val = self.nodeValue         if not isinstance(val, str):             val = str(self.nodeValue)     else:         v = self.nodeValue         if not isinstance(v, str):             v = str(v)         <MASK>             v = "" "".join(v.split())         val = escape(v)     w(val)",if strip :,if strip :,100.00000000000004,0.0,1.0
"def validate_attributes(self):     for attribute in self.get_all_attributes():         value = getattr(self, attribute.code, None)         <MASK>             if attribute.required:                 raise ValidationError(                     _(""%(attr)s attribute cannot be blank"") % {""attr"": attribute.code}                 )         else:             try:                 attribute.validate_value(value)             except ValidationError as e:                 raise ValidationError(                     _(""%(attr)s attribute %(err)s"") % {""attr"": attribute.code, ""err"": e}                 )",if value is None :,if not value :,16.37226966703825,16.37226966703825,0.0
"def PyJsHoisted_BinaryExpression_(node, parent, this, arguments, var=var):     var = Scope(         {u""node"": node, u""this"": this, u""arguments"": arguments, u""parent"": parent}, var     )     var.registers([u""node"", u""parent""])     if PyJsStrictEq(var.get(u""node"").get(u""operator""), Js(u""in"")):         <MASK>             return var.get(u""true"")         if var.get(u""t"").callprop(u""isFor"", var.get(u""parent"")):             return var.get(u""true"")     return Js(False)","if var . get ( u""t"" ) . callprop ( u""isVariableDeclarator"" , var . get ( u""parent"" ) ) :","if var . get ( u""t"" ) :",18.283114748268417,18.283114748268417,0.0
"def distinct(expr, *on):     fields = frozenset(expr.fields)     _on = []     append = _on.append     for n in on:         if isinstance(n, Field):             if n._child.isidentical(expr):                 n = n._name             else:                 raise ValueError(""{0} is not a field of {1}"".format(n, expr))         <MASK>             raise TypeError(""on must be a name or field, not: {0}"".format(n))         elif n not in fields:             raise ValueError(""{0} is not a field of {1}"".format(n, expr))         append(n)     return Distinct(expr, tuple(_on))","if not isinstance ( n , _strtypes ) :",if n not in fields :,5.893383794376546,5.893383794376546,0.0
"def encode(self, msg):     """"""Encodes the message to the stream encoding.""""""     stream = self.stream     rv = msg + ""\n""     if (PY2 and is_unicode(rv)) or not (         PY2 or is_unicode(rv) or _is_text_stream(stream)     ):         enc = self.encoding         <MASK>             enc = getattr(stream, ""encoding"", None) or ""utf-8""         rv = rv.encode(enc, ""replace"")     return rv",if enc is None :,"if enc == ""utf-8"" :",12.22307556087252,12.22307556087252,0.0
"def color_convert(self, to_color_space, preserve_alpha=True):     if to_color_space == self.color_space and preserve_alpha:         return self     else:         pixels = pixels_as_float(self.pixels)         converted = convert_color(             pixels, self.color_space, to_color_space, preserve_alpha         )         <MASK>             return None         return Image(converted, to_color_space)",if converted is None :,if converted is None :,100.00000000000004,100.00000000000004,1.0
"def seek(self, pos):     if self.closed:         raise IOError(""Cannot seek on a closed file"")     for n, idx in enumerate(self._indexes[::-1]):         if idx.offset <= pos:             <MASK>                 self._idxiter = iter(self._indexes[-(n + 1) :])                 self._nextidx()             break     else:         raise Exception(""Cannot seek to pos"")     self._curfile.seek(pos - self._curidx.offset)",if idx != self . _curidx :,if idx . offset >= pos :,12.256200970377108,12.256200970377108,0.0
"def load_from_json(self, node_data: dict, import_version: float):     if import_version <= 0.08:         self.image_pointer = unpack_pointer_property_name(             bpy.data.images, node_data, ""image_name""         )         <MASK>             proposed_name = node_data.get(""image_name"")             self.info(f""image data not found in current {proposed_name}"")",if not self . image_pointer :,if node_data :,7.715486568024961,7.715486568024961,0.0
"def __init__(self, execution_context, aggregate_operators):     super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context)     self._local_aggregators = []     self._results = None     self._result_index = 0     for operator in aggregate_operators:         if operator == ""Average"":             self._local_aggregators.append(_AverageAggregator())         <MASK>             self._local_aggregators.append(_CountAggregator())         elif operator == ""Max"":             self._local_aggregators.append(_MaxAggregator())         elif operator == ""Min"":             self._local_aggregators.append(_MinAggregator())         elif operator == ""Sum"":             self._local_aggregators.append(_SumAggregator())","elif operator == ""Count"" :","if operator == ""Count"" :",84.08964152537145,84.08964152537145,0.0
"def attrgetter(item):     items = [None] * len(attribute)     for i, attribute_part in enumerate(attribute):         item_i = item         for part in attribute_part:             item_i = environment.getitem(item_i, part)         <MASK>             item_i = postprocess(item_i)         items[i] = item_i     return items",if postprocess is not None :,if item_i is not None :,36.55552228545123,36.55552228545123,0.0
"def work(self):     while True:         timeout = self.timeout         <MASK>             timeout = self.idle_timeout         log.debug(""Wait for {}"".format(timeout))         fetch.wait(timeout)         if shutting_down.is_set():             log.info(""Stop fetch worker"")             break         self.fetch()",if idle . is_set ( ) :,if timeout is None :,6.316906128202129,6.316906128202129,0.0
"def testCoreInterfaceIntInputData():     result_testing = False     for _ in range(10):         hsyncnet_instance = hsyncnet(             [[1], [2], [3], [20], [21], [22]], 2, initial_type.EQUIPARTITION, ccore=True         )         analyser = hsyncnet_instance.process()         <MASK>             result_testing = True             break     assert result_testing",if len ( analyser . allocate_clusters ( 0.1 ) ) == 2 :,if analyser . status == 0 :,6.359178452771488,6.359178452771488,0.0
"def _gen():     buf = []     iterable = dataset()     try:         while len(buf) < buffer_size:             buf.append(next(iterable))         while 1:             i = random.randint(0, buffer_size - 1)             n = next(iterable)             yield buf[i]             buf[i] = n     except StopIteration:         <MASK>             random.shuffle(buf)             for i in buf:                 yield i",if len ( buf ) :,if n == 0 :,9.652434877402245,9.652434877402245,0.0
"def debug_tree(tree):     l = []     for elt in tree:         if isinstance(elt, (int, long)):             l.append(_names.get(elt, elt))         <MASK>             l.append(elt)         else:             l.append(debug_tree(elt))     return l","elif isinstance ( elt , str ) :","if isinstance ( elt , ( int , long ) ) :",25.211936184349828,25.211936184349828,0.0
"def reverse_code(apps: StateApps, schema_editor: DatabaseSchemaEditor) -> None:     PreregistrationUser = apps.get_model(""zerver"", ""PreregistrationUser"")     for user in PreregistrationUser.objects.all():         <MASK>  # PreregistrationUser.INVITE_AS['REALM_ADMIN']             user.invited_as_admin = True         else:  # PreregistrationUser.INVITE_AS['MEMBER']             user.invited_as_admin = False         user.save(update_fields=[""invited_as_admin""])",if user . invited_as == 2 :,if user . is_admin :,18.094495256969623,18.094495256969623,0.0
"def _fastqc_data_section(self, section_name):     out = []     in_section = False     data_file = os.path.join(self._dir, ""fastqc_data.txt"")     if os.path.exists(data_file):         with open(data_file) as in_handle:             for line in in_handle:                 if line.startswith("">>%s"" % section_name):                     in_section = True                 elif in_section:                     <MASK>                         break                     out.append(line.rstrip(""\r\n""))     return out","if line . startswith ( "">>END"" ) :",if not line :,2.845073863275343,2.845073863275343,0.0
"def determine_block_hints(self, text):     hints = """"     if text:         if text[0] in "" \n\x85\u2028\u2029"":             hints += str(self.best_indent)         <MASK>             hints += ""-""         elif len(text) == 1 or text[-2] in ""\n\x85\u2028\u2029"":             hints += ""+""     return hints","if text [ - 1 ] not in ""\n\x85\u2028\u2029"" :",if len ( text ) == 1 :,2.2196021319769197,2.2196021319769197,0.0
"def database_app(request):     if request.param == ""postgres_app"":         if not which(""initdb""):             pytest.skip(""initdb must be on PATH for postgresql fixture"")         if not psycopg2:             pytest.skip(""psycopg2 must be installed for postgresql fixture"")     if request.param == ""sqlite_rabbitmq_app"":         <MASK>             pytest.skip(                 ""rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset""             )     return request.getfixturevalue(request.param)","if not os . environ . get ( ""GALAXY_TEST_AMQP_INTERNAL_CONNECTION"" ) :","if not which ( ""rabbitmq_test_amqp_INTERNAL_CONNECTION"" ) :",34.52485371671628,34.52485371671628,0.0
"def do_rollout(agent, env, num_steps, render=False):     total_rew = 0     ob = env.reset()     for t in range(num_steps):         a = agent.act(ob)         (ob, reward, done, _info) = env.step(a)         total_rew += reward         <MASK>             env.render()         if done:             break     return total_rew, t + 1",if render and t % 3 == 0 :,if render :,6.108851178104657,0.0,0.0
"def _handle_subrepos(self, ctx, dirty_trees):     substate = util.parse_hgsubstate(ctx["".hgsubstate""].data().splitlines())     sub = util.OrderedDict()     if "".hgsub"" in ctx:         sub = util.parse_hgsub(ctx["".hgsub""].data().splitlines())     for path, sha in substate.iteritems():         # Ignore non-Git repositories keeping state in .hgsubstate.         <MASK>             continue         d = os.path.dirname(path)         dirty_trees.add(d)         tree = self._dirs.setdefault(d, dulobjs.Tree())         tree.add(os.path.basename(path), dulobjs.S_IFGITLINK, sha)","if path in sub and not sub [ path ] . startswith ( ""[git]"" ) :",if not os . path . isdir ( path ) :,4.677209851690883,4.677209851690883,0.0
"def get_property_file_image_choices(self, pipeline):     columns = pipeline.get_measurement_columns()     image_names = []     for column in columns:         object_name, feature, coltype = column[:3]         choice = feature[(len(C_FILE_NAME) + 1) :]         <MASK>             image_names.append(choice)     return image_names","if object_name == ""Image"" and ( feature . startswith ( C_FILE_NAME ) ) :","if coltype == ""image"" :",3.7489386947613252,3.7489386947613252,0.0
"def check_all_decorator_order():     """"""Check that in all test files, the slow decorator is always last.""""""     errors = []     for fname in os.listdir(PATH_TO_TESTS):         <MASK>             filename = os.path.join(PATH_TO_TESTS, fname)             new_errors = check_decorator_order(filename)             errors += [f""- {filename}, line {i}"" for i in new_errors]     if len(errors) > 0:         msg = ""\n"".join(errors)         raise ValueError(             f""The parameterized decorator (and its variants) should always be first, but this is not the case in the following files:\n{msg}""         )","if fname . endswith ( "".py"" ) :",if os . path . isfile ( fname ) :,10.386397294360817,10.386397294360817,0.0
"def on_edit_button_clicked(self, event=None, a=None, col=None):     tree, tree_id = self.treeView.get_selection().get_selected()     watchdir_id = str(self.store.get_value(tree_id, 0))     if watchdir_id:         if col and col.get_title() == _(""Active""):             <MASK>                 client.autoadd.disable_watchdir(watchdir_id)             else:                 client.autoadd.enable_watchdir(watchdir_id)         else:             self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)","if self . watchdirs [ watchdir_id ] [ ""enabled"" ] :",if self . opts_dialog . is_active ( ) :,11.202079346313559,11.202079346313559,0.0
"def get_conv_output_size(input_size, kernel_size, stride, padding, dilation):     ndim = len(input_size)     output_size = []     for i in range(ndim):         size = (             input_size[i] + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1         ) // stride[i] + 1         <MASK>             output_size.append(1)         else:             output_size.append(size)     return output_size",if kernel_size [ i ] == - 1 :,if size == 0 :,7.509307647752128,7.509307647752128,0.0
"def from_location(cls, location, basename, metadata=None, **kw):     project_name, version, py_version, platform = [None] * 4     basename, ext = os.path.splitext(basename)     if ext.lower() in ("".egg"", "".egg-info""):         match = EGG_NAME(basename)         <MASK>             project_name, version, py_version, platform = match.group(                 ""name"", ""ver"", ""pyver"", ""plat""             )     return cls(         location,         metadata,         project_name=project_name,         version=version,         py_version=py_version,         platform=platform,         **kw     )",if match :,if match :,100.00000000000004,0.0,1.0
"def __new__(metacls, typename, bases, namespace):     annotations = namespace.get(""__annotations__"", {})     for t in annotations.values():         <MASK>             for ut in t.__args__:                 _assert_tensorizer_type(ut)         else:             _assert_tensorizer_type(t)     return super().__new__(metacls, typename, bases, namespace)","if getattr ( t , ""__origin__"" , """" ) is Union :",if t . __args__ :,5.105809900606254,5.105809900606254,0.0
"def decode_content(self):     """"""Return the best possible representation of the response body.""""""     ct = self.headers.get(""content-type"")     if ct:         ct, options = parse_options_header(ct)         charset = options.get(""charset"")         <MASK>             return self.json(charset)         elif ct.startswith(""text/""):             return self.text(charset)         elif ct == FORM_URL_ENCODED:             return parse_qsl(self.content.decode(charset), keep_blank_values=True)     return self.content",if ct in JSON_CONTENT_TYPES :,"if ct . startswith ( ""json/"" ) :",8.29519350710986,8.29519350710986,0.0
"def get_full_path(path):     if ""://"" not in path:         path = os.path.join(self.AUTO_COLL_TEMPL, path, """")         <MASK>             path = os.path.join(abs_path, path)     return path",if abs_path :,if os . path . isdir ( path ) :,5.522397783539471,5.522397783539471,0.0
"def __getitem__(self, name_or_path):     if isinstance(name_or_path, integer_types):         return list.__getitem__(self, name_or_path)     elif isinstance(name_or_path, tuple):         try:             val = self             for fid in name_or_path:                 <MASK>                     raise KeyError  # path contains base value                 val = val[fid]             return val         except (KeyError, IndexError):             raise KeyError(name_or_path)     else:         raise TypeError(self._INDEX_ERROR % name_or_path)","if not isinstance ( val , FeatStruct ) :",if fid not in val :,6.962210312500384,6.962210312500384,0.0
"def scan(scope):     for s in scope.children:         if s.start_pos <= position <= s.end_pos:             if isinstance(s, (tree.Scope, tree.Flow)):                 return scan(s) or s             <MASK>                 return scan(s)     return None","elif s . type in ( ""suite"" , ""decorated"" ) :","if isinstance ( s , tree . Flow ) :",6.699007141691558,6.699007141691558,0.0
"def _get_key(self):     if not self.key:         self._channel.send(u""pake"", self.msg1)         pake_msg = self._channel.get(u""pake"")         self.key = self.sp.finish(pake_msg)         self.verifier = self.derive_key(u""wormhole:verifier"")         <MASK>             return         confkey = self.derive_key(u""wormhole:confirmation"")         nonce = os.urandom(CONFMSG_NONCE_LENGTH)         confmsg = make_confmsg(confkey, nonce)         self._channel.send(u""_confirm"", confmsg)",if not self . _send_confirm :,if not pake_msg :,11.708995388048026,11.708995388048026,0.0
"def executeScript(self, script):     if len(script) > 0:         commands = []         for l in script:             extracted = self.extract_command(l)             <MASK>                 commands.append(extracted)         for command in commands:             cmd, argv = command             self.dispatch_command(cmd, argv)",if extracted :,if extracted :,100.00000000000004,0.0,1.0
"def create_path(n, fullname, meta):     if meta:         meta.create_path(fullname)     else:         # These fallbacks are important -- meta could be null if, for         # example, save created a ""fake"" item, i.e. a new strip/graft         # path element, etc.  You can find cases like that by         # searching for ""Metadata()"".         unlink(fullname)         if stat.S_ISDIR(n.mode):             mkdirp(fullname)         <MASK>             os.symlink(n.readlink(), fullname)",elif stat . S_ISLNK ( n . mode ) :,if n . exists ( ) :,9.614956805006122,9.614956805006122,0.0
def get_cycle(self):     if self.has_cycle():         cross_node = self.path[-1]         <MASK>             return self.path[self.path.index(cross_node) :]         else:             return self.path     return [],if self . path . count ( cross_node ) > 1 :,if cross_node :,7.468220329575271,7.468220329575271,0.0
"def _select_block(str_in, start_tag, end_tag):     """"""Select first block delimited by start_tag and end_tag""""""     start_pos = str_in.find(start_tag)     if start_pos < 0:         raise ValueError(""start_tag not found"")     depth = 0     for pos in range(start_pos, len(str_in)):         <MASK>             depth += 1         elif str_in[pos] == end_tag:             depth -= 1         if depth == 0:             break     sel = str_in[start_pos + 1 : pos]     return sel",if str_in [ pos ] == start_tag :,if str_in [ pos ] == start_tag :,100.00000000000004,100.00000000000004,1.0
"def device(self):     """"""Device on which the data array of this variable reside.""""""     # lazy initialization for performance     if self._device is None:         <MASK>             self._device = backend.CpuDevice()         else:             self._device = backend.get_device_from_array(self._data[0])     return self._device",if self . _data [ 0 ] is None :,if self . _data is None :,47.52203774792177,47.52203774792177,0.0
"def function_out(*args, **kwargs):     try:         return function_in(*args, **kwargs)     except dbus.exceptions.DBusException as e:         if e.get_dbus_name() == DBUS_UNKNOWN_METHOD:             raise ItemNotFoundException(""Item does not exist!"")         <MASK>             raise ItemNotFoundException(e.get_dbus_message())         if e.get_dbus_name() in (DBUS_NO_REPLY, DBUS_NOT_SUPPORTED):             raise SecretServiceNotAvailableException(e.get_dbus_message())         raise",if e . get_dbus_name ( ) == DBUS_NO_SUCH_OBJECT :,if e . get_dbus_name ( ) == DBUS_NOT_FOUND :,69.97150369717171,69.97150369717171,0.0
"def run(self):     """"""Continual loop evaluating when_statements""""""     while len(self.library) > 0:         for name, expression in self.library.items():             <MASK>                 del self.library[name]             else:                 expression.evaluate()         sleep(0.01)     return",if expression . remove_me == True :,if name in self . library :,5.630400552901077,5.630400552901077,0.0
"def tamper(payload, **kwargs):     junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`""     retval = """"     for i, char in enumerate(payload, start=1):         amount = random.randint(10, 15)         <MASK>             retval += "">""             for _ in range(amount):                 retval += random.choice(junk_chars)         elif char == ""<"":             retval += ""<""             for _ in range(amount):                 retval += random.choice(junk_chars)         elif char == "" "":             for _ in range(amount):                 retval += random.choice(junk_chars)         else:             retval += char     return retval","if char == "">"" :","if char == "">"" :",100.00000000000004,100.00000000000004,1.0
"def _source_target_path(source, source_path, source_location):     target_path_attr = source.target_path or source.resdef.target_path     if source.preserve_path:         <MASK>             log.warning(                 ""target-path '%s' specified with preserve-path - ignoring"",                 target_path_attr,             )         return os.path.relpath(os.path.dirname(source_path), source_location)     else:         return target_path_attr or source.resdef.target_path or """"",if target_path_attr :,if target_path_attr :,100.00000000000004,100.00000000000004,1.0
"def _load_user_from_header(self, header):     if self._header_callback:         user = self._header_callback(header)         <MASK>             app = current_app._get_current_object()             user_loaded_from_header.send(app, user=user)             return user     return None",if user is not None :,if user :,23.174952587773145,0.0,0.0
"def setup(cls):     ""Check dependencies and warn about firewalling""     pathCheck(""brctl"", moduleName=""bridge-utils"")     # Disable Linux bridge firewalling so that traffic can flow!     for table in ""arp"", ""ip"", ""ip6"":         cmd = ""sysctl net.bridge.bridge-nf-call-%stables"" % table         out = quietRun(cmd).strip()         <MASK>             warn(""Warning: Linux bridge may not work with"", out, ""\n"")","if out . endswith ( ""1"" ) :",if out :,6.108851178104657,0.0,0.0
"def _browse_your_music(web_client, variant):     if not web_client.logged_in:         return []     if variant in (""tracks"", ""albums""):         items = flatten(             [                 page.get(""items"", [])                 for page in web_client.get_all(                     f""me/{variant}"",                     params={""market"": ""from_token"", ""limit"": 50},                 )                 if page             ]         )         <MASK>             return list(translator.web_to_track_refs(items))         else:             return list(translator.web_to_album_refs(items))     else:         return []","if variant == ""tracks"" :","if variant in ( ""tracks"" , ""albums"" ) :",15.133218633429316,15.133218633429316,0.0
"def reset_styling(self):     for edge in self.fsm_graph.edges_iter():         style_attr = self.fsm_graph.style_attributes.get(""edge"", {}).get(""default"")         edge.attr.update(style_attr)     for node in self.fsm_graph.nodes_iter():         <MASK>             style_attr = self.fsm_graph.style_attributes.get(""node"", {}).get(""inactive"")             node.attr.update(style_attr)     for sub_graph in self.fsm_graph.subgraphs_iter():         style_attr = self.fsm_graph.style_attributes.get(""graph"", {}).get(""default"")         sub_graph.graph_attr.update(style_attr)","if ""point"" not in node . attr [ ""shape"" ] :","if node . state == ""inactive"" :",6.699007141691558,6.699007141691558,0.0
"def set_message_type_visibility(self, message_type: MessageType):     try:         rows = {             i             for i, msg in enumerate(self.proto_analyzer.messages)             <MASK>         }         if message_type.show:             self.ui.tblViewProtocol.show_rows(rows)         else:             self.ui.tblViewProtocol.hide_rows(rows)     except Exception as e:         logger.exception(e)",if msg . message_type == message_type,if msg . visibility == message_type . visibility :,42.40125351805035,42.40125351805035,0.0
"def POP(cpu, *regs):     for reg in regs:         val = cpu.stack_pop(cpu.address_bit_size // 8)         <MASK>             cpu._set_mode_by_val(val)             val = val & ~0x1         reg.write(val)","if reg . reg in ( ""PC"" , ""R15"" ) :",if cpu . mode_by_val :,3.221515452693472,3.221515452693472,0.0
"def processMovie(self, atom):     for field in atom:         if ""track"" in field:             self.processTrack(field[""track""])         <MASK>             self.processMovieHeader(field[""movie_hdr""])","if ""movie_hdr"" in field :","if ""movie_hdr"" in field :",100.00000000000004,100.00000000000004,1.0
"def check_update_function(url, folder, update_setter, version_setter, auto):     remote_version = urllib.urlopen(url).read()     if remote_version.isdigit():         local_version = get_local_timestamp(folder)         <MASK>             if auto:                 update_setter.set_value(True)             version_setter.set_value(remote_version)             return True         else:             return False     else:         return False",if remote_version > local_version :,if local_version != remote_version :,33.031643180138055,33.031643180138055,0.0
"def init(self, view, items=None):     selections = []     if view.sel():         for region in view.sel():             selections.append(view.substr(region))     values = []     for idx, index in enumerate(map(int, items)):         if idx >= len(selections):             break         i = index - 1         <MASK>             values.append(selections[i])         else:             values.append(None)     # fill up     for idx, value in enumerate(selections):         if len(values) + 1 < idx:             values.append(value)     self.stack = values",if i >= 0 and i < len ( selections ) :,if i >= 0 :,24.764986882297123,24.764986882297123,0.0
"def find_int_identifiers(directory):     results = find_rules(directory, has_int_identifier)     print(""Number of rules with integer identifiers: %d"" % len(results))     for result in results:         rule_path = result[0]         product_yaml_path = result[1]         product_yaml = None         <MASK>             product_yaml = yaml.open_raw(product_yaml_path)         fix_file(rule_path, product_yaml, fix_int_identifier)",if product_yaml_path is not None :,if product_yaml_path :,54.77927682341229,54.77927682341229,0.0
"def condition(self):     if self.__condition is None:         if len(self.flat_conditions) == 1:             # Avoid an extra indirection in the common case of only one condition.             self.__condition = self.flat_conditions[0]         <MASK>             # Possible, if unlikely, due to filter predicate rewriting             self.__condition = lambda _: True         else:             self.__condition = lambda x: all(cond(x) for cond in self.flat_conditions)     return self.__condition",elif len ( self . flat_conditions ) == 0 :,if self . filter_predicate :,6.628576403773604,6.628576403773604,0.0
"def get_scene_exceptions_by_season(self, season=-1):     scene_exceptions = []     for scene_exception in self.scene_exceptions:         <MASK>             continue         scene_name, scene_season = scene_exception.split(""|"")         if season == scene_season:             scene_exceptions.append(scene_name)     return scene_exceptions",if not len ( scene_exception ) == 2 :,"if scene_exception == """" :",17.39350277271197,17.39350277271197,0.0
"def init(self, view, items=None):     selections = []     if view.sel():         for region in view.sel():             selections.append(view.substr(region))     values = []     for idx, index in enumerate(map(int, items)):         <MASK>             break         i = index - 1         if i >= 0 and i < len(selections):             values.append(selections[i])         else:             values.append(None)     # fill up     for idx, value in enumerate(selections):         if len(values) + 1 < idx:             values.append(value)     self.stack = values",if idx >= len ( selections ) :,if idx == 0 :,11.708995388048026,11.708995388048026,0.0
"def to_tool_path(self, path_or_uri_like, **kwds):     if ""://"" not in path_or_uri_like:         path = path_or_uri_like     else:         uri_like = path_or_uri_like         <MASK>             raise Exception(""Invalid URI passed to get_tool_source"")         scheme, rest = uri_like.split("":"", 2)         if scheme not in self.resolver_classes:             raise Exception(                 ""Unknown tool scheme [{}] for URI [{}]"".format(scheme, uri_like)             )         path = self.resolver_classes[scheme]().get_tool_source_path(uri_like)     return path","if "":"" not in path_or_uri_like :","if ""://"" not in uri_like :",33.166755101991235,33.166755101991235,0.0
def mainWindow():     global MW     if not MW:         for i in qApp.topLevelWidgets():             <MASK>                 MW = i                 return MW         return None     else:         return MW,"if i . objectName ( ) == ""MainWindow"" :",if i . isWindow ( ) :,15.749996500436227,15.749996500436227,0.0
"def async_get_service(hass, config, discovery_info=None):     # pylint: disable=unused-argument     """"""Get the demo notification service.""""""     for account, account_dict in hass.data[DATA_ALEXAMEDIA][""accounts""].items():         for key, _ in account_dict[""devices""][""media_player""].items():             <MASK>                 _LOGGER.debug(                     ""%s: Media player %s not loaded yet; delaying load"",                     hide_email(account),                     hide_serial(key),                 )                 return False     return AlexaNotificationService(hass)","if key not in account_dict [ ""entities"" ] [ ""media_player"" ] :","if not account_dict [ ""devices"" ] [ ""media_player"" ] [ ""loaded"" ] :",55.00077190915321,55.00077190915321,0.0
"def _migrate_bool(self, name: str, true_value: str, false_value: str) -> None:     if name not in self._settings:         return     values = self._settings[name]     if not isinstance(values, dict):         return     for scope, val in values.items():         <MASK>             new_value = true_value if val else false_value             self._settings[name][scope] = new_value             self.changed.emit()","if isinstance ( val , bool ) :",if scope in self . _settings :,6.567274736060395,6.567274736060395,0.0
"def send(self, data, flags=0):     self._checkClosed()     if self._sslobj:         <MASK>             raise ValueError(                 ""non-zero flags not allowed in calls to send() on %s"" % self.__class__             )         return self._sslobj.write(data)     else:         return socket.send(self, data, flags)",if flags != 0 :,if flags == 0 :,37.99178428257963,37.99178428257963,0.0
"def rec_deps(services, container_by_name, cnt, init_service):     deps = cnt[""_deps""]     for dep in deps.copy():         dep_cnts = services.get(dep)         if not dep_cnts:             continue         dep_cnt = container_by_name.get(dep_cnts[0])         <MASK>             # TODO: avoid creating loops, A->B->A             if init_service and init_service in dep_cnt[""_deps""]:                 continue             new_deps = rec_deps(services, container_by_name, dep_cnt, init_service)             deps.update(new_deps)     return deps",if dep_cnt :,if not dep_cnt :,53.7284965911771,53.7284965911771,0.0
"def as_dict(path="""", version=""latest"", section=""meta-data""):     result = {}     dirs = dir(path, version, section)     if not dirs:         return None     for item in dirs:         if item.endswith(""/""):             records = as_dict(path + item, version, section)             if records:                 result[item[:-1]] = records         <MASK>             idx, name = is_dict.match(item).groups()             records = as_dict(path + idx + ""/"", version, section)             if records:                 result[name] = records         else:             result[item] = valueconv(get(path + item, version, section))     return result",elif is_dict . match ( item ) :,if name :,2.6682865255867765,0.0,0.0
"def PrintColGroup(col_names, schema):     """"""Print HTML colgroup element, used for JavaScript sorting.""""""     print(""  <colgroup>"")     for i, col in enumerate(col_names):         if col.endswith(""_HREF""):             continue         # CSS class is used for sorting         <MASK>             css_class = ""number""         else:             css_class = ""case-insensitive""         # NOTE: id is a comment only; not used         print('    <col id=""{}"" type=""{}"" />'.format(col, css_class))     print(""  </colgroup>"")",if schema . IsNumeric ( col ) :,if i == 0 :,6.916271812933183,6.916271812933183,0.0
"def check_region(self, region):     for other in self.regions:         <MASK>             continue         if (other.start < region.start < other.end) or (             other.start < region.end < other.end         ):             raise Exception(""%r overlaps with %r"" % (region, other))",if other is region :,if not other . start :,10.682175159905853,10.682175159905853,0.0
"def _write_value(self, rng, value, scalar):     if rng.api and value:         # it is assumed by this stage that value is a list of lists         <MASK>             value = value[0][0]         else:             rng = rng.resize(len(value), len(value[0]))         rng.raw_value = value",if scalar :,if scalar :,100.00000000000004,0.0,1.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             length = d.getVarInt32()             tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)             d.skip(length)             self.mutable_cost().TryMerge(tmp)             continue         if tt == 24:             self.add_version(d.getVarInt64())             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt == 32 :,53.7284965911771,53.7284965911771,0.0
"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None):     # This part of function creates faces in SV format     # It ignores  boundless super face     sv_faces = []     for i, face in enumerate(dcel_mesh.faces):         <MASK>             ""Face ({}) has inner components! Sverchok cant show polygons with holes."".format(                 i             )         if not face.outer or del_flag in face.flags:             continue         if only_select and not face.select:             continue         sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges])     return sv_faces",if face . inners and face . outer :,if i != 0 :,5.854497694024015,5.854497694024015,0.0
"def _get_x_for_y(self, xValue, x, y):     # print(""searching ""+x+"" with the value ""+str(xValue)+"" and want to give back ""+y)     x_value = str(xValue)     for anime in self.xmlMap.findall(""anime""):         try:             <MASK>                 return int(anime.get(y, 0))         except ValueError as e:             continue     return 0","if anime . get ( x , False ) == x_value :","if anime . get ( x , 0 ) == x_value :",80.03203203845001,80.03203203845001,0.0
"def dir_copy(src_dir, dest_dir, merge_if_exists=True):     try:         if not os.path.exists(dest_dir):             shutil.copytree(src_dir, dest_dir)         <MASK>             merge_dir(src_dir, dest_dir)     except OSError as e:         # If source is not a directory, copy with shutil.copy         if e.errno == errno.ENOTDIR:             shutil.copy(src_dir, dest_dir)         else:             logging.error(""Could not copy %s to %s"", src_dir, dest_dir)",elif merge_if_exists :,if merge_if_exists :,80.91067115702207,80.91067115702207,0.0
"def mapping(self):     m = {}     if getGdriveCredentialsFile() is not None:         m[""gdrive""] = """"     unknown = 0     for f in self.scan:         bits = f.split(""#"", 2)         if len(bits) == 1:             label = os.path.basename(f)         else:             label = bits[1]         <MASK>             label = ""L"" + str(unknown)             unknown += 1         m[label] = bits[0]     return m","if not label or len ( label ) == 0 or label == """" :",if unknown < 0 :,1.044177559991939,1.044177559991939,0.0
"def get_tag_values(self, event):     http = event.interfaces.get(""sentry.interfaces.Http"")     if not http:         return []     if not http.headers:         return []     headers = http.headers     # XXX: transitional support for workers     if isinstance(headers, dict):         headers = headers.items()     output = []     for key, value in headers:         if key != ""User-Agent"":             continue         ua = Parse(value)         <MASK>             continue         result = self.get_tag_from_ua(ua)         if result:             output.append(result)     return output",if not ua :,if ua is None :,14.058533129758727,14.058533129758727,0.0
"def __iter__(self):     it = DiskHashMerger.__iter__(self)     direct_upstreams = self.direct_upstreams     for k, groups in it:         t = list([[] for _ in range(self.size)])         for i, g in enumerate(groups):             if g:                 <MASK>                     t[i] = g                 else:                     g.sort(key=itemgetter(0))                     g1 = []                     for _, vs in g:                         g1.extend(vs)                     t[i] = g1         yield k, tuple(t)",if i in direct_upstreams :,if k in direct_upstreams :,64.34588841607616,64.34588841607616,0.0
"def process_question(qtxt):     question = """"     skip = False     for letter in qtxt:         <MASK>             skip = True         if letter == "">"":             skip = False         if skip:             continue         if letter.isalnum() or letter == "" "":             if letter == "" "":                 letter = ""_""             question += letter.lower()     return question","if letter == ""<"" :","if letter == ""<"" :",100.00000000000004,100.00000000000004,1.0
"def _module_repr_from_spec(spec):     """"""Return the repr to use for the module.""""""     # We mostly replicate _module_repr() using the spec attributes.     name = ""?"" if spec.name is None else spec.name     if spec.origin is None:         if spec.loader is None:             return ""<module {!r}>"".format(name)         else:             return ""<module {!r} ({!r})>"".format(name, spec.loader)     else:         <MASK>             return ""<module {!r} from {!r}>"".format(name, spec.origin)         else:             return ""<module {!r} ({})>"".format(spec.name, spec.origin)",if spec . has_location :,if spec . loader is not None :,22.089591134157878,22.089591134157878,0.0
"def test_row(self, row):     for idx, test in self.patterns.items():         try:             value = row[idx]         except IndexError:             value = """"         result = test(value)         if self.any_match:             if result:                 return not self.inverse  # True         else:             <MASK>                 return self.inverse  # False     if self.any_match:         return self.inverse  # False     else:         return not self.inverse  # True",if not result :,if self . any_match :,7.809849842300637,7.809849842300637,0.0
"def frequent_thread_switches():     """"""Make concurrency bugs more likely to manifest.""""""     interval = None     if not sys.platform.startswith(""java""):         <MASK>             interval = sys.getswitchinterval()             sys.setswitchinterval(1e-6)         else:             interval = sys.getcheckinterval()             sys.setcheckinterval(1)     try:         yield     finally:         if not sys.platform.startswith(""java""):             if hasattr(sys, ""setswitchinterval""):                 sys.setswitchinterval(interval)             else:                 sys.setcheckinterval(interval)","if hasattr ( sys , ""getswitchinterval"" ) :","if hasattr ( sys , ""setswitchinterval"" ) :",65.80370064762461,65.80370064762461,0.0
"def record_expected_exportable_production(self, ticks):     """"""Record the amount of production that should be transferred to other islands.""""""     for (quota_holder, resource_id), amount in self._low_priority_requests.items():         <MASK>             self._settlement_manager_id[quota_holder] = WorldObject.get_object_by_id(                 int(quota_holder[1:].split("","")[0])             ).settlement_manager.worldid         self.trade_storage[self._settlement_manager_id[quota_holder]][resource_id] += (             ticks * amount         )",if quota_holder not in self . _settlement_manager_id :,"if quota_holder [ 0 ] == ""exportable"" :",17.532201649672796,17.532201649672796,0.0
"def _method_events_callback(self, values):     try:         previous_echoed = (             values[""child_result_list""][-1].decode().split(""\n"")[-2].strip()         )         <MASK>             return ""echo foo2\n""         elif previous_echoed.endswith(""foo2""):             return ""echo foo3\n""         elif previous_echoed.endswith(""foo3""):             return ""exit\n""         else:             raise Exception(""Unexpected output {0!r}"".format(previous_echoed))     except IndexError:         return ""echo foo1\n""","if previous_echoed . endswith ( ""foo1"" ) :","if previous_echoed . endswith ( ""foo1"" ) :",100.00000000000004,100.00000000000004,1.0
"def describe_cluster_snapshots(self, cluster_identifier=None, snapshot_identifier=None):     if cluster_identifier:         cluster_snapshots = []         for snapshot in self.snapshots.values():             <MASK>                 cluster_snapshots.append(snapshot)         if cluster_snapshots:             return cluster_snapshots     if snapshot_identifier:         if snapshot_identifier in self.snapshots:             return [self.snapshots[snapshot_identifier]]         raise ClusterSnapshotNotFoundError(snapshot_identifier)     return self.snapshots.values()",if snapshot . cluster . cluster_identifier == cluster_identifier :,if snapshot . cluster_identifier == cluster_identifier :,82.19198246700309,82.19198246700309,0.0
def get_snippet_edit_handler(model):     if model not in SNIPPET_EDIT_HANDLERS:         <MASK>             # use the edit handler specified on the page class             edit_handler = model.edit_handler         else:             panels = extract_panel_definitions_from_model_class(model)             edit_handler = ObjectList(panels)         SNIPPET_EDIT_HANDLERS[model] = edit_handler.bind_to(model=model)     return SNIPPET_EDIT_HANDLERS[model],"if hasattr ( model , ""edit_handler"" ) :",if model . edit_handler is not None :,15.20797122409784,15.20797122409784,0.0
"def start():     if os.environ.get(""RUN_MAIN"") != ""true"":         try:             exit_code = restart_with_reloader()             <MASK>                 os.kill(os.getpid(), -exit_code)             else:                 sys.exit(exit_code)         except KeyboardInterrupt:             pass",if exit_code < 0 :,if exit_code != 0 :,41.11336169005198,41.11336169005198,0.0
"def discover(self, *objlist):     ret = []     for l in self.splitlines():         if len(l) < 5:             continue         <MASK>             continue         try:             int(l[2])             int(l[3])         except:             continue         #           ret.append(improve(l[0]))         ret.append(l[0])     ret.sort()     for item in objlist:         ret.append(item)     return ret","if l [ 0 ] == ""Filename"" :",if l [ 0 ] == '0' :,60.10525952194528,60.10525952194528,0.0
"def ipfs_publish(self, lib):     with tempfile.NamedTemporaryFile() as tmp:         self.ipfs_added_albums(lib, tmp.name)         try:             <MASK>                 cmd = ""ipfs add --nocopy -q "".split()             else:                 cmd = ""ipfs add -q "".split()             cmd.append(tmp.name)             output = util.command_output(cmd)         except (OSError, subprocess.CalledProcessError) as err:             msg = ""Failed to publish library. Error: {0}"".format(err)             self._log.error(msg)             return False         self._log.info(""hash of library: {0}"", output)","if self . config [ ""nocopy"" ] :",if lib . is_open :,5.630400552901077,5.630400552901077,0.0
"def spends(self):     # Return spends indexed by hashX     spends = defaultdict(list)     utxos = self.mempool_utxos()     for tx_hash, tx in self.txs.items():         for n, input in enumerate(tx.inputs):             if input.is_generation():                 continue             prevout = (input.prev_hash, input.prev_idx)             <MASK>                 hashX, value = utxos.pop(prevout)             else:                 hashX, value = self.db_utxos[prevout]             spends[hashX].append(prevout)     return spends",if prevout in utxos :,if n == 0 :,9.652434877402245,9.652434877402245,0.0
"def terminate(self):     if self.returncode is None:         try:             os.kill(self.pid, TERM_SIGNAL)         except OSError as exc:             if getattr(exc, ""errno"", None) != errno.ESRCH:                 <MASK>                     raise",if self . wait ( timeout = 0.1 ) is None :,if exc . errno == errno . ESRCH :,4.85851417160653,4.85851417160653,0.0
"def _getVolumeScalar(self):     if self._volumeScalar is not None:         return self._volumeScalar     # use default     elif self._value in dynamicStrToScalar:         return dynamicStrToScalar[self._value]     else:         thisDynamic = self._value         # ignore leading s like in sf         <MASK>             thisDynamic = thisDynamic[1:]         # ignore closing z like in fz         if thisDynamic[-1] == ""z"":             thisDynamic = thisDynamic[:-1]         if thisDynamic in dynamicStrToScalar:             return dynamicStrToScalar[thisDynamic]         else:             return dynamicStrToScalar[None]","if ""s"" in thisDynamic :","if thisDynamic [ 0 ] == ""s"" :",16.59038701421971,16.59038701421971,0.0
"def init_values(self):     config = self._raw_config     for valname, value in self.overrides.iteritems():         if ""."" in valname:             realvalname, key = valname.split(""."", 1)             config.setdefault(realvalname, {})[key] = value         else:             config[valname] = value     for name in config:         <MASK>             self.__dict__[name] = config[name]     del self._raw_config",if name in self . values :,if name in config :,28.641904579795423,28.641904579795423,0.0
"def modified(self):     paths = set()     dictionary_list = []     for op_list in self._operations:         if not isinstance(op_list, list):             op_list = (op_list,)         for item in chain(*op_list):             if item is None:                 continue             dictionary = item.dictionary             <MASK>                 continue             paths.add(dictionary.path)             dictionary_list.append(dictionary)     return dictionary_list",if dictionary . path in paths :,if not paths . has ( item . path ) :,9.425159511373677,9.425159511373677,0.0
"def __getitem__(self, key, _get_mode=False):     if not _get_mode:         <MASK>             return self._list[key]         elif isinstance(key, slice):             return self.__class__(self._list[key])     ikey = key.lower()     for k, v in self._list:         if k.lower() == ikey:             return v     # micro optimization: if we are in get mode we will catch that     # exception one stack level down so we can raise a standard     # key error instead of our special one.     if _get_mode:         raise KeyError()     raise BadRequestKeyError(key)","if isinstance ( key , ( int , long ) ) :","if isinstance ( key , ( int , long ) ) :",100.00000000000004,100.00000000000004,1.0
"def _get_items(self, name, target=1):     all_items = self.get_items(name)     items = [o for o in all_items if not o.disabled]     if len(items) < target:         <MASK>             raise ItemNotFoundError(""insufficient items with name %r"" % name)         else:             raise AttributeError(""insufficient non-disabled items with name %s"" % name)     on = []     off = []     for o in items:         if o.selected:             on.append(o)         else:             off.append(o)     return on, off",if len ( all_items ) < target :,if o . selected :,4.673289785800722,4.673289785800722,0.0
"def get_genome_dir(gid, galaxy_dir, data):     """"""Return standard location of genome directories.""""""     if galaxy_dir:         refs = genome.get_refs(gid, None, galaxy_dir, data)         seq_file = tz.get_in([""fasta"", ""base""], refs)         <MASK>             return os.path.dirname(os.path.dirname(seq_file))     else:         gdirs = glob.glob(os.path.join(_get_data_dir(), ""genomes"", ""*"", gid))         if len(gdirs) == 1 and os.path.exists(gdirs[0]):             return gdirs[0]",if seq_file and os . path . exists ( seq_file ) :,if seq_file :,7.834966465489322,7.834966465489322,0.0
"def _PrintFuncs(self, names):     # type: (List[str]) -> int     status = 0     for name in names:         <MASK>             print(name)             # TODO: Could print LST for -f, or render LST.  Bash does this.  'trap'             # could use that too.         else:             status = 1     return status",if name in self . funcs :,if name in self . _Funcs :,54.10822690539397,54.10822690539397,0.0
"def package_files(self):     seen_package_directories = ()     directories = self.distribution.package_dir or {}     empty_directory_exists = """" in directories     packages = self.distribution.packages or []     for package in packages:         if package in directories:             package_directory = directories[package]         elif empty_directory_exists:             package_directory = os.path.join(directories[""""], package)         else:             package_directory = package         <MASK>             seen_package_directories += (package_directory + ""."",)             yield package_directory",if not package_directory . startswith ( seen_package_directories ) :,if package_directory not in seen_package_directories :,35.758619990303956,35.758619990303956,0.0
"def apply_conf_file(fn, conf_filename):     for env in LSF_CONF_ENV:         conf_file = get_conf_file(conf_filename, env)         <MASK>             with open(conf_file) as conf_handle:                 value = fn(conf_handle)             if value:                 return value     return None",if conf_file :,if conf_file :,100.00000000000004,100.00000000000004,1.0
"def on_text(self, text):     if text != self.chosen_text:         self.fail_test('Expected ""{}"", received ""{}""'.format(self.chosen_text, text))     else:         self.checks_passed += 1         <MASK>             self.pass_test()         else:             self._select_next_text()",if self . checks_passed >= self . number_of_checks :,if self . checks_passed == 0 :,30.48779293003838,30.48779293003838,0.0
"def test_field_attr_existence(self):     for name, item in ast.__dict__.items():         if self._is_ast_node(name, item):             <MASK>                 # Index(value) just returns value now.                 # The argument is required.                 continue             x = item()             if isinstance(x, ast.AST):                 self.assertEqual(type(x._fields), tuple)","if name == ""Index"" :","if isinstance ( item , ast . Index ) :",5.522397783539471,5.522397783539471,0.0
"def apply(self, response):     updated_headers = self.update_headers(response)     if updated_headers:         response.headers.update(updated_headers)         warning_header_value = self.warning(response)         <MASK>             response.headers.update({""Warning"": warning_header_value})     return response",if warning_header_value is not None :,if warning_header_value :,54.77927682341229,54.77927682341229,0.0
"def validate(self):     self.assertEqual(len(self.inputs), len(self.outputs))     for batch_in, batch_out in zip(self.inputs, self.outputs):         self.assertEqual(len(batch_in), len(batch_out))         if self.use_parallel_executor and not self.use_double_buffer:             self.validate_unordered_batch(batch_in, batch_out)         else:             for in_data, out_data in zip(batch_in, batch_out):                 self.assertEqual(in_data.shape, out_data.shape)                 <MASK>                     self.assertTrue((in_data == out_data).all())",if not self . use_parallel_executor :,if self . use_parallel_executor and not self . use_double_buffer :,41.12175645551035,41.12175645551035,0.0
def finalize(self):     if self._started:         <MASK>             self._queue.put(None)             self._queue.join()             self._consumer.join()         self._started = False     self._finalized = True,if not self . _finalized :,if self . _finalized :,67.31821382417488,67.31821382417488,0.0
"def _get_ilo_version(self):     try:         self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>')     except ResponseError as e:         <MASK>             if e.code == 405:                 return 3             if e.code == 501:                 return 1         raise     return 2","if hasattr ( e , ""code"" ) :",if e . code == 402 :,6.082317172853824,6.082317172853824,0.0
"def _check_data(self, source, expected_bytes, expected_duration):     received_bytes = 0     received_seconds = 0.0     bytes_to_read = 1024     while True:         data = source.get_audio_data(bytes_to_read)         <MASK>             break         received_bytes += data.length         received_seconds += data.duration         self.assertEqual(data.length, len(data.data))     self.assertAlmostEqual(expected_duration, received_seconds, places=1)     self.assertAlmostEqual(expected_bytes, received_bytes, delta=5)",if data is None :,if not data :,16.37226966703825,16.37226966703825,0.0
"def __randomize_interval_task(self):     for job in self.aps_scheduler.get_jobs():         <MASK>             self.aps_scheduler.modify_job(                 job.id,                 next_run_time=datetime.now()                 + timedelta(                     seconds=randrange(                         job.trigger.interval.total_seconds() * 0.75,                         job.trigger.interval.total_seconds(),                     )                 ),             )","if isinstance ( job . trigger , IntervalTrigger ) :",if job . trigger . interval . total_seconds ( ) > next_run_time :,9.313775329024091,9.313775329024091,0.0
"def find_approximant(x):     c = 1e-4     it = sympy.ntheory.continued_fraction_convergents(         sympy.ntheory.continued_fraction_iterator(x)     )     for i in it:         p, q = i.as_numer_denom()         tol = c / q ** 2         <MASK>             return i         if tol < machine_epsilon:             break     return x",if abs ( i - x ) <= tol :,if p < machine_epsilon :,4.880869806051147,4.880869806051147,0.0
"def fix_newlines(lines):     """"""Convert newlines to unix.""""""     for i, line in enumerate(lines):         if line.endswith(""\r\n""):             lines[i] = line[:-2] + ""\n""         <MASK>             lines[i] = line[:-1] + ""\n""","elif line . endswith ( ""\r"" ) :","if line . endswith ( ""\r"" ) :",89.31539818068698,89.31539818068698,0.0
"def payment_control_render(self, request: HttpRequest, payment: OrderPayment):     template = get_template(""pretixplugins/paypal/control.html"")     sale_id = None     for trans in payment.info_data.get(""transactions"", []):         for res in trans.get(""related_resources"", []):             <MASK>                 sale_id = res[""sale""][""id""]     ctx = {         ""request"": request,         ""event"": self.event,         ""settings"": self.settings,         ""payment_info"": payment.info_data,         ""order"": payment.order,         ""sale_id"": sale_id,     }     return template.render(ctx)","if ""sale"" in res and ""id"" in res [ ""sale"" ] :","if res [ ""sale"" ] :",24.909923021496894,24.909923021496894,0.0
"def for_name(self, name):     try:         name_resources = self._resources[name]     except KeyError:         raise LookupError(name)     else:         for res in name_resources:             try:                 inst = res.inst()             except Exception as e:                 <MASK>                     log.exception(""error initializing %s"", res)                 else:                     log.error(""error initializing %s: %s"", res, e)             else:                 yield inst",if log . getEffectiveLevel ( ) <= logging . DEBUG :,"if e . code == ""ENOENT"" :",4.85851417160653,4.85851417160653,0.0
"def describe(self, done=False):     description = ShellCommand.describe(self, done)     if done:         <MASK>             description = [""compile""]         description.append(""%d projects"" % self.getStatistic(""projects"", 0))         description.append(""%d files"" % self.getStatistic(""files"", 0))         warnings = self.getStatistic(""warnings"", 0)         if warnings > 0:             description.append(""%d warnings"" % warnings)         errors = self.getStatistic(""errors"", 0)         if errors > 0:             description.append(""%d errors"" % errors)     return description",if not description :,if not description :,100.00000000000004,100.00000000000004,1.0
"def parse_list(tl):     ls = []     nm = []     while True:         term, nmt, tl = parse_term(tl)         ls.append(term)         <MASK>             nm.append(nmt)         if tl[0] != "","":             break         tl = tl[1:]     return ls, nm, tl",if nmt is not None :,if nmt :,23.174952587773145,0.0,0.0
"def infer_dataset_impl(path):     if IndexedRawTextDataset.exists(path):         return ""raw""     elif IndexedDataset.exists(path):         with open(index_file_path(path), ""rb"") as f:             magic = f.read(8)             <MASK>                 return ""cached""             elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:                 return ""mmap""             else:                 return None     elif FastaDataset.exists(path):         return ""fasta""     else:         return None",if magic == IndexedDataset . _HDR_MAGIC :,if magic == MMapIndexedDataset . Index . _HDR_MAGIC :,57.83569866465144,57.83569866465144,0.0
"def _get(self):     fut = item = None     with self._mutex:         # Critical section never blocks.         <MASK>             fut = Future()             fut.add_done_callback(                 lambda f: self._get_complete() if not f.cancelled() else None             )             self._getters.append(fut)         else:             item = self._get_item()             self._get_complete()     return item, fut",if not self . _queue or self . _getters :,if not item :,4.784824825520546,4.784824825520546,0.0
"def validate(self):     dates = []     for d in self.get(""leave_block_list_dates""):         # date is not repeated         <MASK>             frappe.msgprint(                 _(""Date is repeated"") + "":"" + d.block_date, raise_exception=1             )         dates.append(d.block_date)",if d . block_date in dates :,if d . block_date not in dates :,65.80370064762461,65.80370064762461,0.0
"def on_choose_watch_dir_clicked(self):     if self.window().watchfolder_enabled_checkbox.isChecked():         previous_watch_dir = self.window().watchfolder_location_input.text() or """"         watch_dir = QFileDialog.getExistingDirectory(             self.window(),             ""Please select the watch folder"",             previous_watch_dir,             QFileDialog.ShowDirsOnly,         )         <MASK>             return         self.window().watchfolder_location_input.setText(watch_dir)",if not watch_dir :,if watch_dir :,57.89300674674101,57.89300674674101,0.0
"def log_generator(self, limit=6000, **kwargs):     # Generator for show_log_panel     skip = 0     while True:         logs = self.log(limit=limit, skip=skip, **kwargs)         if not logs:             break         for entry in logs:             yield entry         <MASK>             break         skip = skip + limit",if len ( logs ) < limit :,if skip > limit :,12.975849993980741,12.975849993980741,0.0
"def _setUpClass(cls):     global solver     import pyomo.environ     from pyomo.solvers.tests.io.writer_test_cases import testCases     for test_case in testCases:         <MASK>             solver[(test_case.name, test_case.io)] = True","if ( ( test_case . name , test_case . io ) in solver ) and ( test_case . available ) :",if test_case . io :,3.840750732462118,3.840750732462118,0.0
"def _get_file_data(self, normpath, normrev):     data = self.client.cat(normpath, normrev)     if has_expanded_svn_keywords(data):         # Find out if this file has any keyword expansion set.         # If it does, collapse these keywords. This is because SVN         # will return the file expanded to us, which would break patching.         keywords = self.client.propget(""svn:keywords"", normpath, normrev, recurse=True)         <MASK>             data = collapse_svn_keywords(data, force_bytes(keywords[normpath]))     return data",if normpath in keywords :,if keywords :,32.34325178227722,0.0,0.0
"def add_controller_list(path):     if not os.path.exists(os.path.join(path, ""__init__.py"")):         bb.fatal(""Controllers directory %s exists but is missing __init__.py"" % path)     files = sorted(         [f for f in os.listdir(path) if f.endswith("".py"") and not f.startswith(""_"")]     )     for f in files:         module = ""oeqa.controllers."" + f[:-3]         <MASK>             controllerslist.append(module)         else:             bb.warn(                 ""Duplicate controller module found for %s, only one added. Layers should create unique controller module names""                 % module             )",if module not in controllerslist :,if module in controllerslist :,40.93653765389909,40.93653765389909,0.0
"def on_session2(event):     new_xmpp.get_roster()     new_xmpp.send_presence()     logging.info(roster[0])     data = roster[0][""roster""][""items""]     logging.info(data)     for jid, item in data.items():         <MASK>             new_xmpp.send_presence(ptype=""subscribe"", pto=jid)         new_xmpp.update_roster(jid, name=item[""name""], groups=item[""groups""])     new_xmpp.disconnect()","if item [ ""subscription"" ] != ""none"" :","if ""name"" in item :",4.167496780656209,4.167496780656209,0.0
"def _parse_class_simplified(symbol):     results = {}     name = symbol.name + ""(""     name += "", "".join([analyzer.expand_attribute(base) for base in symbol.bases])     name += "")""     for sym in symbol.body:         <MASK>             result = _parse_function_simplified(sym, symbol.name)             results.update(result)         elif isinstance(sym, ast.ClassDef):             result = _parse_class_simplified(sym)             results.update(result)     lineno = symbol.lineno     for decorator in symbol.decorator_list:         lineno += 1     results[lineno] = (name, ""c"")     return results","if isinstance ( sym , ast . FunctionDef ) :","if isinstance ( sym , ast . FunctionDef ) :",100.00000000000004,100.00000000000004,1.0
"def check_args(args):     """"""Checks that the args are coherent.""""""     check_args_has_attributes(args)     if args.v:         non_version_attrs = [v for k, v in args.__dict__.items() if k != ""v""]         print(""non_version_attrs"", non_version_attrs)         <MASK>             fail(""Cannot show the version number with another command."")         return     if args.i is None:         fail(""Cannot draw ER diagram of no database."")     if args.o is None:         fail(""Cannot draw ER diagram with no output file."")",if len ( [ v for v in non_version_attrs if v is not None ] ) != 0 :,"if args . i != ""v"" :",2.6022564613866095,2.6022564613866095,0.0
"def handle(self, *args, **options):     if not settings.ST_BASE_DIR.endswith(""spirit""):         raise CommandError(             ""settings.ST_BASE_DIR is not the spirit root folder, are you overriding it?""         )     for root, dirs, files in os.walk(settings.ST_BASE_DIR):         <MASK>             continue         with utils.pushd(root):             call_command(                 ""makemessages"", stdout=self.stdout, stderr=self.stderr, **options             )     self.stdout.write(""ok"")","if ""locale"" not in dirs :",if not dirs or not files :,8.051153633013374,8.051153633013374,0.0
"def scan(scope):     for s in scope.children:         if s.start_pos <= position <= s.end_pos:             <MASK>                 return scan(s) or s             elif s.type in (""suite"", ""decorated""):                 return scan(s)     return None","if isinstance ( s , ( tree . Scope , tree . Flow ) ) :","if s . type in ( ""suite"" , ""decorated"" ) :",6.725321874176006,6.725321874176006,0.0
def run_sync(self):     count = 0     while count < self.args.num_messages:         batch = self.receiver.fetch_next(max_batch_size=self.args.num_messages - count)         <MASK>             for msg in batch:                 msg.complete()         count += len(batch),if self . args . peeklock :,if batch :,9.138402379955025,0.0,0.0
"def __getitem__(self, item):     if self._datas is not None:         ret = []         for data in self._datas:             <MASK>                 ret.append(data[self._offset])             else:                 ret.append(data.iloc[self._offset])         self._offset += 1         return ret     else:         return self._get_data(item)","if isinstance ( data , np . ndarray ) :",if data . is_array :,6.050259138270144,6.050259138270144,0.0
"def removedir(self, path):     # type: (Text) -> None     _path = self.validatepath(path)     if _path == ""/"":         raise errors.RemoveRootError()     with ftp_errors(self, path):         try:             self.ftp.rmd(_encode(_path, self.ftp.encoding))         except error_perm as error:             code, _ = _parse_ftp_error(error)             if code == ""550"":                 <MASK>                     raise errors.DirectoryExpected(path)                 if not self.isempty(path):                     raise errors.DirectoryNotEmpty(path)             raise  # pragma: no cover",if self . isfile ( path ) :,"if code == ""550"" :",6.567274736060395,6.567274736060395,0.0
"def replaces_in_file(file, replacement_list):     rs = [(re.compile(regexp), repl) for (regexp, repl) in replacement_list]     file_tmp = file + ""."" + str(os.getpid()) + "".tmp""     with open(file, ""r"") as f:         with open(file_tmp, ""w"") as f_tmp:             for line in f:                 for r, replace in rs:                     match = r.search(line)                     <MASK>                         line = replace + ""\n""                 f_tmp.write(line)     shutil.move(file_tmp, file)",if match :,if match :,100.00000000000004,0.0,1.0
"def _get_path_check_mem(self, i, size):     if size > 0:         <MASK>             p = self._get_path(i, -1)         else:             p = self._get_path(i, size)             if p.startswith(""/dev/shm""):                 env.meminfo.add(size)     else:         p = self._get_path(i, size)     return p",if env . meminfo . rss + size > env . meminfo . mem_limit_soft :,"if p . startswith ( ""/dev/shm"" ) :",2.56249125689746,2.56249125689746,0.0
"def find_widget_by_id(self, id, parent=None):     """"""Recursively searches for widget with specified ID""""""     if parent == None:         if id in self:             return self[id]  # Do things fast if possible         parent = self[""editor""]     for c in parent.get_children():         <MASK>             if c.get_id() == id:                 return c         if isinstance(c, Gtk.Container):             r = self.find_widget_by_id(id, c)             if not r is None:                 return r     return None","if hasattr ( c , ""get_id"" ) :",if c . get_id ( ) == id :,16.108992769687397,16.108992769687397,0.0
"def _deserialize(cls, io):     flags = VideoFlags()     flags.byte = U8.read(io)     if flags.bit.type == VIDEO_FRAME_TYPE_COMMAND_FRAME:         data = VideoCommandFrame.deserialize(io)     else:         <MASK>             data = AVCVideoData.deserialize(io)         else:             data = io.read()     return cls(flags.bit.type, flags.bit.codec, data)",if flags . bit . codec == VIDEO_CODEC_ID_AVC :,if flags . bit . type == AVC_DATA :,28.56349697533047,28.56349697533047,0.0
"def asciiLogData(data, maxlen=64, replace=False):     ellipses = "" ...""     try:         <MASK>             dd = data[:maxlen] + ellipses         else:             dd = data         return dd.decode(""utf8"", errors=""replace"" if replace else ""strict"")     except:         return ""0x"" + binLogData(data, maxlen)",if len ( data ) > maxlen - len ( ellipses ) :,if len ( data ) > maxlen :,46.53786298485943,46.53786298485943,0.0
"def _check_units(self, new_unit_system):     # If no unit system has been specified for me yet, adopt the incoming     # system     if self.unit_system is None:         self.unit_system = new_unit_system     else:         # Otherwise, make sure they match         <MASK>             raise ValueError(                 ""Unit system mismatch %d v. %d"" % (self.unit_system, new_unit_system)             )",if self . unit_system != new_unit_system :,if new_unit_system != self . unit_system :,79.71755824799465,79.71755824799465,0.0
"def command(filenames, dirnames, fix):     for filename in gather_files(dirnames, filenames):         visitor = process_file(filename)         <MASK>             print(""%s: %s"" % (filename, visitor.get_stats()))             if fix:                 print(""Fixing: %s"" % filename)                 fix_file(filename)",if visitor . needs_fix ( ) :,if visitor :,8.525588607164655,0.0,0.0
"def assign_attributes_to_variants(variant_attributes):     for value in variant_attributes:         pk = value[""pk""]         defaults = value[""fields""]         defaults[""variant_id""] = defaults.pop(""variant"")         defaults[""assignment_id""] = defaults.pop(""assignment"")         assigned_values = defaults.pop(""values"")         assoc, created = AssignedVariantAttribute.objects.update_or_create(             pk=pk, defaults=defaults         )         <MASK>             assoc.values.set(AttributeValue.objects.filter(pk__in=assigned_values))",if created :,if created :,100.00000000000004,0.0,1.0
"def _info(self, userlist):     for strng in userlist:         group_matched = False         for env in self.base.comps.environments_by_pattern(strng):             self.output.display_groups_in_environment(env)             group_matched = True         for group in self.base.comps.groups_by_pattern(strng):             self.output.display_pkgs_in_groups(group)             group_matched = True         <MASK>             logger.error(_(""Warning: Group %s does not exist.""), strng)     return 0, []",if not group_matched :,if not group_matched :,100.00000000000004,100.00000000000004,1.0
"def parse_implements_interfaces(parser):     types = []     if parser.token.value == ""implements"":         advance(parser)         while True:             types.append(parse_named_type(parser))             <MASK>                 break     return types","if not peek ( parser , TokenKind . NAME ) :","if parser . token . value == ""implements"" :",4.789232204309912,4.789232204309912,0.0
"def generate():     for leaf in u.leaves:         if isinstance(leaf, Integer):             val = leaf.get_int_value()             if val in (0, 1):                 yield val             else:                 raise _NoBoolVector         elif isinstance(leaf, Symbol):             if leaf == SymbolTrue:                 yield 1             <MASK>                 yield 0             else:                 raise _NoBoolVector         else:             raise _NoBoolVector",elif leaf == SymbolFalse :,if leaf == SymbolFalse :,75.98356856515926,75.98356856515926,0.0
"def update_gstin(context):     dirty = False     for key, value in iteritems(frappe.form_dict):         if key != ""party"":             address_name = frappe.get_value(""Address"", key)             <MASK>                 address = frappe.get_doc(""Address"", address_name)                 address.gstin = value.upper()                 address.save(ignore_permissions=True)                 dirty = True     if dirty:         frappe.db.commit()         context.updated = True",if address_name :,if address_name :,100.00000000000004,100.00000000000004,1.0
"def everythingIsUnicode(d):     """"""Takes a dictionary, recursively verifies that every value is unicode""""""     for k, v in d.iteritems():         <MASK>             if not everythingIsUnicode(v):                 return False         elif isinstance(v, list):             for i in v:                 if isinstance(i, dict) and not everythingIsUnicode(i):                     return False                 elif isinstance(i, _bytes):                     return False         elif isinstance(v, _bytes):             return False     return True","if isinstance ( v , dict ) and k != ""headers"" :","if isinstance ( v , unicode ) :",23.441874056753782,23.441874056753782,0.0
"def check_graph(graph):  # pragma: no cover     for c in graph:         <MASK>             raise RuntimeError(""cannot have fuse"")         for inp in c.inputs:             if isinstance(inp.op, Fuse):                 raise RuntimeError(""cannot have fuse"")","if isinstance ( c . op , Fuse ) :","if isinstance ( c . op , Fuse ) :",100.00000000000004,100.00000000000004,1.0
"def __getattr__(self, key):     try:         value = self.__parent.contents[key]     except KeyError:         pass     else:         <MASK>             if isinstance(value, _ModuleMarker):                 return value.mod_ns             else:                 assert isinstance(value, _MultipleClassMarker)                 return value.attempt_get(self.__parent.path, key)     raise AttributeError(         ""Module %r has no mapped classes ""         ""registered under the name %r"" % (self.__parent.name, key)     )",if value is not None :,if value is not None :,100.00000000000004,100.00000000000004,1.0
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None):     assert nw_id != self.nw_id_unknown     ret = []     for port in self.get_ports(dpid):         nw_id_ = port.network_id         if port.port_no == in_port:             continue         <MASK>             ret.append(port.port_no)         elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external:             ret.append(port.port_no)     return ret",if nw_id_ == nw_id :,if nw_id_ == self . nw_id_external :,50.389204852596336,50.389204852596336,0.0
"def _parse(self, contents):     entries = []     for line in contents.splitlines():         if not len(line.strip()):             entries.append((""blank"", [line]))             continue         (head, tail) = chop_comment(line.strip(), ""#"")         <MASK>             entries.append((""all_comment"", [line]))             continue         entries.append((""option"", [head.split(None), tail]))     return entries",if not len ( head ) :,"if head == ""all_comment"" :",5.522397783539471,5.522397783539471,0.0
"def _get_documented_completions(self, table, startswith=None):     names = []     for key, command in table.items():         if getattr(command, ""_UNDOCUMENTED"", False):             # Don't tab complete undocumented commands/params             continue         <MASK>             continue         if getattr(command, ""positional_arg"", False):             continue         names.append(key)     return names",if startswith is not None and not key . startswith ( startswith ) :,if startswith :,1.6102756877232731,0.0,0.0
"def _convert_example(example, use_bfloat16):     """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16.""""""     for key in list(example.keys()):         val = example[key]         <MASK>             val = tf.sparse.to_dense(val)         if val.dtype == tf.int64:             val = tf.cast(val, tf.int32)         if use_bfloat16 and val.dtype == tf.float32:             val = tf.cast(val, tf.bfloat16)         example[key] = val",if tf . keras . backend . is_sparse ( val ) :,if use_bfloat16 and val . dtype == tf . float32 :,7.768562846380172,7.768562846380172,0.0
"def _get_lang_zone(self, lang):     if lang not in self._lang_zone_from_lang:         <MASK>             self._lang_zone_from_lang[lang] = MultiLangZone(self.mgr, lang)         else:             self._lang_zone_from_lang[lang] = LangZone(self.mgr, lang)     return self._lang_zone_from_lang[lang]",if self . mgr . is_multilang ( lang ) :,if lang in self . _lang_zone_from_lang :,7.474875887495341,7.474875887495341,0.0
"def dispatch(self, request, *args, **kwargs):     try:         return super(Handler, self).dispatch(request, *args, **kwargs)     except Http404 as e:         <MASK>             try:                 request.original_path_info = request.path_info                 request.path_info = settings.FEINCMS_CMS_404_PAGE                 response = super(Handler, self).dispatch(request, *args, **kwargs)                 response.status_code = 404                 return response             except Http404:                 raise e         else:             raise",if settings . FEINCMS_CMS_404_PAGE :,if e . status_code == 404 :,5.6775429106661015,5.6775429106661015,0.0
"def _maybe_update_dropout(self, step):     for i in range(len(self.dropout_steps)):         <MASK>             self.model.update_dropout(self.dropout[i])             logger.info(""Updated dropout to %f from step %d"" % (self.dropout[i], step))",if step > 1 and step == self . dropout_steps [ i ] + 1 :,if self . dropout [ i ] == step :,13.949506535206227,13.949506535206227,0.0
"def bulk_move(*args, **kwargs):     for arg in args:         <MASK>             raise PopupException(_(""Source path and destination path cannot be same""))         request.fs.rename(             urllib.unquote(arg[""src_path""]), urllib.unquote(arg[""dest_path""])         )","if arg [ ""src_path"" ] == arg [ ""dest_path"" ] :","if arg [ ""src_path"" ] != arg [ ""dest_path"" ] :",85.78928092681438,85.78928092681438,0.0
"def asisWrite(self, root):     at, c = self, self.c     try:         c.endEditing()         c.init_error_dialogs()         fileName = at.initWriteIvars(root, root.atAsisFileNodeName())         <MASK>             at.addToOrphanList(root)             return         at.openOutputStream()         for p in root.self_and_subtree(copy=False):             at.writeAsisNode(p)         contents = at.closeOutputStream()         at.replaceFile(contents, at.encoding, fileName, root)     except Exception:         at.writeException(fileName, root)","if not at . precheck ( fileName , root ) :",if root . self_and_subtree ( copy = False ) :,7.474875887495341,7.474875887495341,0.0
"def next_event(it):     """"""read an event from an eventstream""""""     while True:         try:             line = next(it)         except StopIteration:             return         <MASK>             return json.loads(line.split("":"", 1)[1])","if line . startswith ( ""data:"" ) :",if line is None :,7.121297464907233,7.121297464907233,0.0
"def process_formdata(self, valuelist):     if valuelist:         if valuelist[0] == ""__None"":             self.data = None         else:             <MASK>                 self.data = None                 return             try:                 obj = self.queryset.get(pk=valuelist[0])                 self.data = obj             except DoesNotExist:                 self.data = None",if self . queryset is None :,"if valuelist [ 0 ] == ""__None"" :",4.065425428798724,4.065425428798724,0.0
"def _setResultsName(self, name, listAllMatches=False):     if __diag__.warn_multiple_tokens_in_named_alternation:         <MASK>             warnings.warn(                 ""{}: setting results name {!r} on {} expression ""                 ""will return a list of all parsed tokens in an And alternative, ""                 ""in prior versions only the first token was returned"".format(                     ""warn_multiple_tokens_in_named_alternation"",                     name,                     type(self).__name__,                 ),                 stacklevel=3,             )     return super()._setResultsName(name, listAllMatches)","if any ( isinstance ( e , And ) for e in self . exprs ) :",if name in self . _tokens :,7.582874853312503,7.582874853312503,0.0
"def add(request):     form_type = ""servers""     if request.method == ""POST"":         form = BookMarkForm(request.POST)         if form.is_valid():             form_type = form.save()             messages.add_message(request, messages.INFO, ""Bookmark created"")         else:             messages.add_message(request, messages.INFO, form.errors)         <MASK>             url = reverse(""servers"")         else:             url = reverse(""metrics"")         return redirect(url)     else:         return redirect(reverse(""servers""))","if form_type == ""server"" :","if form_type == ""metrics"" :",70.71067811865478,70.71067811865478,0.0
"def __init__(self, post_id, artist, page, tzInfo=None, dateFormat=None):     self.imageUrls = list()     self.imageResizedUrls = list()     self.imageId = int(post_id)     self._tzInfo = tzInfo     self.dateFormat = dateFormat     if page is not None:         post_json = demjson.decode(page)         <MASK>             artist_id = post_json[""data""][""item""][""user""][""id""]             self.artist = SketchArtist(artist_id, page, tzInfo, dateFormat)         else:             self.artist = artist         self.parse_post(post_json[""data""][""item""])",if artist is None :,"if post_json [ ""data"" ] [ ""user"" ] :",3.1251907639724417,3.1251907639724417,0.0
"def _create_batch_iterator(     self,     mark_as_delete: Callable[[Any], None],     to_key: Callable[[Any], Any],     to_value: Callable[[Any], Any],     batch: Iterable[EventT], ) -> Iterable[Tuple[Any, Any]]:     for event in batch:         key = to_key(event.key)         # to delete keys in the table we set the raw value to None         <MASK>             mark_as_delete(key)             continue         yield key, to_value(event.value)",if event . message . value is None :,if mark_as_delete :,5.868924818816531,5.868924818816531,0.0
"def test_lc_numeric_nl_langinfo(self):     # Test nl_langinfo against known values     tested = False     for loc in candidate_locales:         try:             setlocale(LC_NUMERIC, loc)             setlocale(LC_CTYPE, loc)         except Error:             continue         for li, lc in ((RADIXCHAR, ""decimal_point""), (THOUSEP, ""thousands_sep"")):             <MASK>                 tested = True     if not tested:         self.skipTest(""no suitable locales"")","if self . numeric_tester ( ""nl_langinfo"" , nl_langinfo ( li ) , lc , loc ) :",if li == lc :,0.48375840065150605,0.48375840065150605,0.0
"def _level_up_logging(self):     for handler in self.log.handlers:         <MASK>             if handler.level != logging.DEBUG:                 handler.setLevel(logging.DEBUG)                 self.log.debug(""Leveled up log file verbosity"")","if issubclass ( handler . __class__ , logging . FileHandler ) :",if handler . level != logging . INFO :,7.208393585152543,7.208393585152543,0.0
def _show_axes_changed(self):     marker = self.marker     if (self._vtk_control is not None) and (marker is not None):         <MASK>             marker.interactor = None             marker.enabled = False         else:             marker.interactor = self.interactor             marker.enabled = True         self.render(),if not self . show_axes :,if self . _vtk_control is not None :,9.864703138979419,9.864703138979419,0.0
"def handle_keypress(self, rawKey, modifiers, key, *args):     if self.recordKeyboard and self.__delayPassed():         <MASK>             self.insideKeys = True             self.targetParent.start_key_sequence()         modifierCount = len(modifiers)         if (             modifierCount > 1             or (modifierCount == 1 and Key.SHIFT not in modifiers)             or (Key.SHIFT in modifiers and len(rawKey) > 1)         ):             self.targetParent.append_hotkey(rawKey, modifiers)         elif key not in MODIFIERS:             self.targetParent.append_key(key)",if not self . insideKeys :,if key in MODIFIERS :,10.400597689005304,10.400597689005304,0.0
"def transform(self, data):     with timer(""transform %s"" % self.name, logging.DEBUG):         if self.operator in {""lat"", ""latitude""}:             return self.series(data).apply(GeoIP.get_latitude)         <MASK>             return self.series(data).apply(GeoIP.get_longitude)         elif self.operator in {""acc"", ""accuracy""}:             return self.series(data).apply(GeoIP.get_accuracy)         raise NameError(""Unknown GeoIP operator [lat, lon, acc]: %s"" % self.operator)","elif self . operator in { ""lon"" , ""longitude"" } :","if self . operator in {""lon"", ""longitude"" :",79.65485887268619,79.65485887268619,0.0
"def _get_sidebar_selected(self):     sidebar_selected = None     if self.businessline_id:         sidebar_selected = ""bl_%s"" % self.businessline_id         <MASK>             sidebar_selected += ""_s_%s"" % self.service_id             if self.environment_id:                 sidebar_selected += ""_env_%s"" % self.environment_id     return sidebar_selected",if self . service_id :,if self . service_id :,100.00000000000004,100.00000000000004,1.0
"def _run_response_middleware(self, request, response, request_name=None):     named_middleware = self.named_response_middleware.get(request_name, deque())     applicable_middleware = self.response_middleware + named_middleware     if applicable_middleware:         for middleware in applicable_middleware:             _response = middleware(request, response)             <MASK>                 _response = await _response             if _response:                 response = _response                 break     return response",if isawaitable ( _response ) :,if _response :,17.946048174042403,17.946048174042403,0.0
"def populate_obj(self, obj, name):     field = getattr(obj, name, None)     if field is not None:         # If field should be deleted, clean it up         if self._should_delete:             field.delete()             return         <MASK>             if not field.grid_id:                 func = field.put             else:                 func = field.replace             func(                 self.data.stream,                 filename=self.data.filename,                 content_type=self.data.content_type,             )","if isinstance ( self . data , FileStorage ) and not is_empty ( self . data . stream ) :",if self . data . stream :,7.845605491344882,7.845605491344882,0.0
"def _import_hash(self, operator):     # Import required modules into local namespace so that pipelines     # may be evaluated directly     for key in sorted(operator.import_hash.keys()):         module_list = "", "".join(sorted(operator.import_hash[key]))         <MASK>             exec(""from {} import {}"".format(key[4:], module_list))         else:             exec(""from {} import {}"".format(key, module_list))         for var in operator.import_hash[key]:             self.operators_context[var] = eval(var)","if key . startswith ( ""tpot."" ) :","if key [ 0 ] == ""module"" :",9.425159511373677,9.425159511373677,0.0
"def remove_files(folder, file_extensions):     for f in os.listdir(folder):         f_path = os.path.join(folder, f)         <MASK>             extension = os.path.splitext(f_path)[1]             if extension in file_extensions:                 os.remove(f_path)",if os . path . isfile ( f_path ) :,if os . path . isfile ( f_path ) :,100.00000000000004,100.00000000000004,1.0
"def clearBuffer(self):     if self.shouldLose == -1:         return     if self.producer:         self.producer.resumeProducing()     if self.buffer:         <MASK>             self.logFile.write(""loopback receiving %s\n"" % repr(self.buffer))         buffer = self.buffer         self.buffer = b""""         self.target.dataReceived(buffer)     if self.shouldLose == 1:         self.shouldLose = -1         self.target.connectionLost(failure.Failure(main.CONNECTION_DONE))",if self . logFile :,if self . logFile :,100.00000000000004,100.00000000000004,1.0
"def write(self, data):     if mock_target._mirror_on_stderr:         if self._write_line:             sys.stderr.write(fn + "": "")         if bytes:             sys.stderr.write(data.decode(""utf8""))         else:             sys.stderr.write(data)         <MASK>             self._write_line = True         else:             self._write_line = False     super(Buffer, self).write(data)","if ( data [ - 1 ] ) == ""\n"" :",if data :,0.7268566109861576,0.0,0.0
def stop(self):     self.queue_com.state_lock.acquire()     try:         <MASK>             self.queue_com.state = STOPPED             self.remove()             return True         return False     finally:         self.queue_com.state_lock.release(),if self . queue_com . state == RUNNING and self . stop_task ( ) :,if self . queue_com . state == STOPPED :,42.434788372432536,42.434788372432536,0.0
"def _handle_special_args(self, pyobjects):     if len(pyobjects) == len(self.arguments.args):         if self.arguments.vararg:             pyobjects.append(rope.base.builtins.get_list())         <MASK>             pyobjects.append(rope.base.builtins.get_dict())",if self . arguments . kwarg :,if self . arguments . args :,64.34588841607616,64.34588841607616,0.0
"def go_to_last_edit_location(self):     if self.last_edit_cursor_pos is not None:         filename, position = self.last_edit_cursor_pos         if not osp.isfile(filename):             self.last_edit_cursor_pos = None             return         else:             self.load(filename)             editor = self.get_current_editor()             <MASK>                 editor.set_cursor_position(position)",if position < editor . document ( ) . characterCount ( ) :,if editor :,1.4157233641833757,0.0,0.0
"def _create_sentence_objects(self):     """"""Returns a list of Sentence objects from the raw text.""""""     sentence_objects = []     sent_tokenizer = SentenceTokenizer(locale=self.language.code)     seq = Sequence(self.raw)     seq = sent_tokenizer.transform(seq)     for start_index, end_index in zip(seq.idx[:-1], seq.idx[1:]):         # Sentences share the same models as their parent blob         sent = seq.text[start_index:end_index].strip()         <MASK>             continue         s = Sentence(sent, start_index=start_index, end_index=end_index)         s.detected_languages = self.detected_languages         sentence_objects.append(s)     return sentence_objects",if not sent :,if sent is None :,14.058533129758727,14.058533129758727,0.0
"def to_json_schema(self, parent=None):     schema = {}     if not parent:         schema[""title""] = self.title         <MASK>             schema[""description""] = self.description         if self.has_default:             schema[""default""] = self.default         schema[""_required_""] = self.required     if self.null:         schema[""type""] = [""string"", ""null""]     else:         schema[""type""] = ""string""     if self.enum is not None:         schema[""enum""] = self.enum     return schema",if self . description :,if self . has_description :,30.739407647563215,30.739407647563215,0.0
def rmdir(dirname):     if dirname[-1] == os.sep:         dirname = dirname[:-1]     if os.path.islink(dirname):         return  # do not clear link - we can get out of dir     for f in os.listdir(dirname):         <MASK>             continue         path = dirname + os.sep + f         if os.path.isdir(path):             rmdir(path)         else:             os.unlink(path)     os.rmdir(dirname),"if f in ( ""."" , "".."" ) :",if not os . path . isdir ( f ) :,7.805069386252457,7.805069386252457,0.0
"def convert_whole_dir(path=Path(""marian_ckpt/"")):     for subdir in tqdm(list(path.ls())):         dest_dir = f""marian_converted/{subdir.name}""         <MASK>             continue         convert(source_dir, dest_dir)","if ( dest_dir / ""pytorch_model.bin"" ) . exists ( ) :","if subdir . name == ""marian_ckpt"" :",2.9576737109722444,2.9576737109722444,0.0
"def colorformat(text):     if text[0:1] == ""#"":         col = text[1:]         if len(col) == 6:             return col         <MASK>             return col[0] * 2 + col[1] * 2 + col[2] * 2     elif text == """":         return """"     assert False, ""wrong color format %r"" % text",elif len ( col ) == 3 :,if len ( col ) == 3 :,86.33400213704509,86.33400213704509,0.0
"def _init_rel_seek(self):     ""Sets the file object's position to the relative location set above.""     rs, fo = self._rel_seek, self._file_obj     if rs == 0.0:         fo.seek(0, os.SEEK_SET)     else:         fo.seek(0, os.SEEK_END)         size = fo.tell()         <MASK>             self._cur_pos = size         else:             target = int(size * rs)             fo.seek(target, os.SEEK_SET)             self._align_to_newline()             self._cur_pos = fo.tell()",if rs == 1.0 :,if rs == 1 :,53.7284965911771,53.7284965911771,0.0
"def parse_command_line(self, argv=None):     """"""Parse the command line""""""     if self.config:         parser = argparse.ArgumentParser(add_help=False)         self.settings[""config""].add_argument(parser)         opts, _ = parser.parse_known_args(argv)         if opts.config is not None:             self.set(""config"", opts.config)         self.params.update(self.import_from_module())     parser = self.parser()     opts = parser.parse_args(argv)     for k, v in opts.__dict__.items():         <MASK>             continue         self.set(k.lower(), v)",if v is None :,if k not in self . settings :,6.567274736060395,6.567274736060395,0.0
"def process(self, resources, event=None):     client = local_session(self.manager.session_factory).client(         ""shield"", region_name=""us-east-1""     )     protections = get_type_protections(client, self.manager.get_model())     protected_resources = {p[""ResourceArn""] for p in protections}     state = self.data.get(""state"", False)     results = []     for arn, r in zip(self.manager.get_arns(resources), resources):         r[""c7n:ShieldProtected""] = shielded = arn in protected_resources         <MASK>             results.append(r)         elif not shielded and not state:             results.append(r)     return results",if shielded and state :,if not shielded and state :,53.7284965911771,53.7284965911771,0.0
"def removeTrailingWs(self, aList):     i = 0     while i < len(aList):         <MASK>             j = i             i = self.skip_ws(aList, i)             assert j < i             if i >= len(aList) or aList[i] == ""\n"":                 # print ""removing trailing ws:"", `i-j`                 del aList[j:i]                 i = j         else:             i += 1",if self . is_ws ( aList [ i ] ) :,"if aList [ i ] == ""\n"" :",21.586404366478295,21.586404366478295,0.0
"def predict(request: Request):     form = await request.form()     files, entry = convert_input(form)     try:         <MASK>             return JSONResponse(ALL_FEATURES_PRESENT_ERROR, status_code=400)         try:             resp = model.predict(data_dict=[entry]).to_dict(""records"")[0]             return JSONResponse(resp)         except Exception as e:             logger.error(""Error: {}"".format(str(e)))             return JSONResponse(COULD_NOT_RUN_INFERENCE_ERROR, status_code=500)     finally:         for f in files:             os.remove(f.name)",if ( entry . keys ( ) & input_features ) != input_features :,if not form . is_valid ( ) :,4.96274655104206,4.96274655104206,0.0
"def reset(self):     logger.debug(""Arctic.reset()"")     with self._lock:         <MASK>             self.__conn.close()             self.__conn = None         for _, l in self._library_cache.items():             if hasattr(l, ""_reset"") and callable(l._reset):                 logger.debug(""Library reset() %s"" % l)                 l._reset()  # the existence of _reset() is not guaranteed/enforced, it also triggers re-auth",if self . __conn is not None :,if self . __conn :,54.77927682341229,54.77927682341229,0.0
"def read(self):     if op.isfile(self.fileName):         with textfile_open(self.fileName, ""rt"") as fid:             items = json.load(fid)             # TODO: catch JSON exception...             <MASK>                 items = dict()     else:         items = dict()     self._items.clear()     self._items.update(items)     self._haveReadData = True",if items is None :,if not items :,16.37226966703825,16.37226966703825,0.0
"def get_django_comment(text: str, i: int) -> str:     end = i + 4     unclosed_end = 0     while end <= len(text):         if text[end - 2 : end] == ""#}"":             return text[i:end]         <MASK>             unclosed_end = end         end += 1     raise TokenizationException(""Unclosed comment"", text[i:unclosed_end])","if not unclosed_end and text [ end ] == ""<"" :",if unclosed_end == end :,10.694820729788422,10.694820729788422,0.0
"def _wrap_forwarded(self, key, value):     if isinstance(value, SourceCode) and value.late_binding:         # get cached return value if present         value_ = self._late_binding_returnvalues.get(key, KeyError)         <MASK>             # evaluate the late-bound function             value_ = self._eval_late_binding(value)             schema = self.late_bind_schemas.get(key)             if schema is not None:                 value_ = schema.validate(value_)             # cache result of late bound func             self._late_binding_returnvalues[key] = value_         return value_     else:         return value",if value_ is KeyError :,if value_ is not None :,43.47208719449914,43.47208719449914,0.0
"def connect(*args, **ckwargs):     if ""give_content_type"" in kwargs:         <MASK>             kwargs[""give_content_type""](args[6][""content-type""])         else:             kwargs[""give_content_type""]("""")     if ""give_connect"" in kwargs:         kwargs[""give_connect""](*args, **ckwargs)     status = code_iter.next()     etag = etag_iter.next()     timestamp = timestamps_iter.next()     if status == -1:         raise HTTPException()     return FakeConn(status, etag, body=kwargs.get(""body"", """"), timestamp=timestamp)","if len ( args ) >= 7 and ""content_type"" in args [ 6 ] :","if args [ 6 ] [ ""content-type"" ] :",13.229147212652599,13.229147212652599,0.0
"def _reset(self):     self._handle_connect()     if self.rewarder_session:         <MASK>             env_id = random.choice(self._sample_env_ids)             logger.info(""Randomly sampled env_id={}"".format(env_id))         else:             env_id = None         self.rewarder_session.reset(env_id=env_id)     else:         logger.info(             ""No rewarder session exists, so cannot send a reset via the rewarder channel""         )     self._reset_mask()     return [None] * self.n",if self . _sample_env_ids :,if self . rewarder_session . is_active ( ) :,13.674406678232565,13.674406678232565,0.0
"def _create_architecture_list(architectures, current_arch):     if not architectures:         return [_Architecture(build_on=[current_arch])]     build_architectures: List[str] = []     architecture_list: List[_Architecture] = []     for item in architectures:         if isinstance(item, str):             build_architectures.append(item)         <MASK>             architecture_list.append(                 _Architecture(build_on=item.get(""build-on""), run_on=item.get(""run-on""))             )     if build_architectures:         architecture_list.append(_Architecture(build_on=build_architectures))     return architecture_list","if isinstance ( item , dict ) :","if item . get ( ""build-on"" ) :",10.552670315936318,10.552670315936318,0.0
"def inspect(self, pokemon):     # Make sure it was not caught!     for caught_pokemon in self.cache:         same_latitude = ""{0:.4f}"".format(pokemon[""latitude""]) == ""{0:.4f}"".format(             caught_pokemon[""latitude""]         )         same_longitude = ""{0:.4f}"".format(pokemon[""longitude""]) == ""{0:.4f}"".format(             caught_pokemon[""longitude""]         )         <MASK>             return     if len(self.cache) >= 200:         self.cache.pop(0)     self.cache.append(pokemon)",if same_latitude and same_longitude :,"if same_latitude != pokemon [ ""longitude"" ] :",21.401603033752977,21.401603033752977,0.0
"def parley(self):     for x in [0, 1]:         a = self.agents[x].act()         <MASK>             if ""[DONE]"" in a[""text""]:                 self.agents[x - 1].observe(                     {""id"": ""World"", ""text"": ""The other agent has ended the chat.""}                 )                 self.episodeDone = True             else:                 self.agents[x - 1].observe(a)",if a is not None :,"if a [ ""id"" ] == ""World"" :",6.837203339116283,6.837203339116283,0.0
"def _prepare_subset(     full_data: torch.Tensor,     full_targets: torch.Tensor,     num_samples: int,     digits: Sequence, ):     classes = {d: 0 for d in digits}     indexes = []     for idx, target in enumerate(full_targets):         label = target.item()         if classes.get(label, float(""inf"")) >= num_samples:             continue         indexes.append(idx)         classes[label] += 1         <MASK>             break     data = full_data[indexes]     targets = full_targets[indexes]     return data, targets",if all ( classes [ k ] >= num_samples for k in classes ) :,if idx == 0 :,1.4456752008489673,1.4456752008489673,0.0
"def get_work_root(self, flags):     _flags = flags.copy()     _flags[""is_toplevel""] = True     target = self._get_target(_flags)     if target:         _flags[""target""] = target.name         tool = self.get_tool(_flags)         <MASK>             return target.name + ""-"" + tool         else:             raise SyntaxError(                 ""Failed to determine work root. Could not resolve tool for target ""                 + target.name             )     else:         raise SyntaxError(""Failed to determine work root. Could not resolve target"")",if tool :,if tool :,100.00000000000004,0.0,1.0
"def run_command(self, data):     """"""Run editor commands.""""""     parts = data.split("" "")     cmd = parts[0].lower()     if cmd in self.operations.keys():         return self.run_operation(cmd)     args = "" "".join(parts[1:])     self.logger.debug(""Looking for command '{0}'"".format(cmd))     if cmd in self.modules.modules.keys():         self.logger.debug(""Trying to run command '{0}'"".format(cmd))         self.get_editor().store_action_state(cmd)         <MASK>             return False     else:         self.set_status(""Command '{0}' not found."".format(cmd))         return False     return True","if not self . run_module ( cmd , args ) :","if args [ 0 ] == ""run"" :",4.41902110634,4.41902110634,0.0
"def get_main_chain_layers(self):     """"""Return a list of layer IDs in the main chain.""""""     main_chain = self.get_main_chain()     ret = []     for u in main_chain:         for v, layer_id in self.adj_list[u]:             <MASK>                 ret.append(layer_id)     return ret",if v in main_chain and u in main_chain :,if v == self . main_chain :,23.142716255858215,23.142716255858215,0.0
"def hash(self, context):     with context:         <MASK>             return IECore.MurmurHash()         h = GafferDispatch.TaskNode.hash(self, context)         h.append(self[""fileName""].hash())         h.append(self[""in""].hash())         h.append(self.__parameterHandler.hash())         return h","if not self [ ""fileName"" ] . getValue ( ) or self [ ""in"" ] . source ( ) == self [ ""in"" ] :",if self . __parameterHandler is None :,0.5235532762795567,0.5235532762795567,0.0
"def consume_buf():     ty = state[""ty""] - 1     for i in xrange(state[""buf""].shape[1] // N):         tx = x // N + i         src = state[""buf""][:, i * N : (i + 1) * N, :]         <MASK>             with self.tile_request(tx, ty, readonly=False) as dst:                 mypaintlib.tile_convert_rgba8_to_rgba16(src, dst, self.EOTF)     if state[""progress""]:         try:             state[""progress""].completed(ty - ty0)         except Exception:             logger.exception(""Progress.completed() failed"")             state[""progress""] = None","if src [ : , : , 3 ] . any ( ) :","if state [ ""buf"" ] [ i ] == 0 :",4.016138436407654,4.016138436407654,0.0
"def check_permissions(self, obj):     request = self.context.get(""request"")     for Perm in permissions:         perm = Perm()         if not perm.has_permission(request, self):             return False         <MASK>             return False     return True","if not perm . has_object_permission ( request , self , obj ) :",if not obj . is_admin :,4.719073083867901,4.719073083867901,0.0
"def _post_order(op):     if isinstance(op, tvm.tir.Allocate):         lift_stmt[-1].append(op)         return op.body     if isinstance(op, tvm.tir.AttrStmt):         if op.attr_key == ""storage_scope"":             lift_stmt[-1].append(op)             return op.body         <MASK>             return _merge_block(lift_stmt.pop() + [op], op.body)         return op     if isinstance(op, tvm.tir.For):         return _merge_block(lift_stmt.pop() + [op], op.body)     raise RuntimeError(""not reached"")","if op . attr_key == ""virtual_thread"" :","if op . attr_key == ""storage_scope"" :",65.91844162499147,65.91844162499147,0.0
"def task_done(self):     with self._cond:         if not self._unfinished_tasks.acquire(False):             raise ValueError(""task_done() called too many times"")         <MASK>             self._cond.notify_all()",if self . _unfinished_tasks . _semlock . _is_zero ( ) :,if self . _finished_tasks . get ( ) :,23.76501850070969,23.76501850070969,0.0
"def get_json(self):     if not hasattr(self, ""_json""):         self._json = None         <MASK>             self._json = json.loads(self.request.body)     return self._json","if self . request . headers . get ( ""Content-Type"" , """" ) . startswith ( ""application/json"" ) :","if self . request . method == ""GET"" :",11.10310128232865,11.10310128232865,0.0
"def userfullname():     """"""Get the user's full name.""""""     global _userfullname     <MASK>         uid = os.getuid()         entry = pwd_from_uid(uid)         if entry:             _userfullname = entry[4].split("","")[0] or entry[0]         if not _userfullname:             _userfullname = ""user%d"" % uid     return _userfullname",if not _userfullname :,if _userfullname :,49.76093899250716,49.76093899250716,0.0
"def test_scatter(self):     for rank in range(self.world_size):         tensor = []         <MASK>             tensor = [torch.tensor(i) for i in range(self.world_size)]         result = comm.get().scatter(tensor, rank, size=())         self.assertTrue(torch.is_tensor(result))         self.assertEqual(result.item(), self.rank)",if self . rank == rank :,if rank == self . rank :,39.28146509005134,39.28146509005134,0.0
"def decompile(decompiler):     for pos, next_pos, opname, arg in decompiler.instructions:         if pos in decompiler.targets:             decompiler.process_target(pos)         method = getattr(decompiler, opname, None)         if method is None:             throw(DecompileError(""Unsupported operation: %s"" % opname))         decompiler.pos = pos         decompiler.next_pos = next_pos         x = method(*arg)         <MASK>             decompiler.stack.append(x)",if x is not None :,if x is not None :,100.00000000000004,100.00000000000004,1.0
"def print_scenario_ran(self, scenario):     if scenario.passed:         self.wrt(""OK"")     elif scenario.failed:         reason = self.scenarios_and_its_fails[scenario]         <MASK>             self.wrt(""FAILED"")         else:             self.wrt(""ERROR"")     self.wrt(""\n"")","if isinstance ( reason . exception , AssertionError ) :",if reason :,3.848335094984576,0.0,0.0
"def detect_ssl_option(self):     for option in self.ssl_options():         if scan_argv(self.argv, option) is not None:             for other_option in self.ssl_options():                 if option != other_option:                     <MASK>                         raise ConfigurationError(                             ""Cannot give both %s and %s"" % (option, other_option)                         )             return option","if scan_argv ( self . argv , other_option ) is not None :",if option != other_option :,7.582874853312503,7.582874853312503,0.0
"def print_po_snippet(en_loc_old_lists, context):     for m, localized, old in zip(*en_loc_old_lists):         if m == """":             continue         <MASK>             localized = old         print(             ""#: {file}:{line}\n""             'msgid ""{context}{en_month}""\n'             'msgstr ""{localized_month}""\n'.format(                 context=context,                 file=filename,                 line=print_po_snippet.line,                 en_month=m,                 localized_month=localized,             )         )         print_po_snippet.line += 1",if m == localized :,"if localized == """" :",16.515821590069034,16.515821590069034,0.0
"def set_status(self, dict_new):     for i, value in dict_new.items():         self.dict_bili[i] = value         <MASK>             self.dict_bili[""pcheaders""][""cookie""] = value             self.dict_bili[""appheaders""][""cookie""] = value","if i == ""cookie"" :","if i in self . dict_bili [ ""status"" ] :",7.141816289329644,7.141816289329644,0.0
"def makeSomeFiles(pathobj, dirdict):     pathdict = {}     for (key, value) in dirdict.items():         child = pathobj.child(key)         <MASK>             pathdict[key] = child             child.setContent(value)         elif isinstance(value, dict):             child.createDirectory()             pathdict[key] = makeSomeFiles(child, value)         else:             raise ValueError(""only strings and dicts allowed as values"")     return pathdict","if isinstance ( value , bytes ) :","if isinstance ( value , ( str , unicode ) ) :",36.462858619364674,36.462858619364674,0.0
"def _truncate_to_length(generator, len_map=None):     for example in generator:         example = list(example)         if len_map is not None:             for key, max_len in len_map.items():                 example_len = example[key].shape                 <MASK>                     example[key] = np.resize(example[key], max_len)         yield tuple(example)",if example_len > max_len :,if example_len > max_len :,100.00000000000004,100.00000000000004,1.0
"def check(self, **kw):     if not kw:         return exists(self.strpath)     if len(kw) == 1:         if ""dir"" in kw:             return not kw[""dir""] ^ isdir(self.strpath)         <MASK>             return not kw[""file""] ^ isfile(self.strpath)     return super(LocalPath, self).check(**kw)","if ""file"" in kw :","if ""file"" in kw :",100.00000000000004,100.00000000000004,1.0
"def next_instruction_is_function_or_class(lines):     """"""Is the first non-empty, non-commented line of the cell either a function or a class?""""""     parser = StringParser(""python"")     for i, line in enumerate(lines):         if parser.is_quoted():             parser.read_line(line)             continue         parser.read_line(line)         if not line.strip():  # empty line             if i > 0 and not lines[i - 1].strip():                 return False             continue         if line.startswith(""def "") or line.startswith(""class ""):             return True         <MASK>             continue         return False     return False","if line . startswith ( ( ""#"" , ""@"" , "" "" , "")"" ) ) :","if line . startswith ( ""function "" ) :",14.262067559546713,14.262067559546713,0.0
"def askCheckReadFile(self, localFile, remoteFile):     if not kb.bruteMode:         message = ""do you want confirmation that the remote file '%s' "" % remoteFile         message += ""has been successfully downloaded from the back-end ""         message += ""DBMS file system? [Y/n] ""         <MASK>             return self._checkFileLength(localFile, remoteFile, True)     return None","if readInput ( message , default = ""Y"" , boolean = True ) :",if message :,0.5208155200691346,0.0,0.0
"def process_tag(hive_name, company, company_key, tag, default_arch):     with winreg.OpenKeyEx(company_key, tag) as tag_key:         version = load_version_data(hive_name, company, tag, tag_key)         if version is not None:  # if failed to get version bail             major, minor, _ = version             arch = load_arch_data(hive_name, company, tag, tag_key, default_arch)             <MASK>                 exe_data = load_exe(hive_name, company, company_key, tag)                 if exe_data is not None:                     exe, args = exe_data                     return company, major, minor, arch, exe, args",if arch is not None :,if arch is not None :,100.00000000000004,100.00000000000004,1.0
"def _get_matching_bracket(self, s, pos):     if s[pos] != ""{"":         return None     end = len(s)     depth = 1     pos += 1     while pos != end:         c = s[pos]         if c == ""{"":             depth += 1         <MASK>             depth -= 1         if depth == 0:             break         pos += 1     if pos < end and s[pos] == ""}"":         return pos     return None","elif c == ""}"" :","if c == ""}"" :",84.08964152537145,84.08964152537145,0.0
"def pred(field, value, item):     for suffix, p in _BUILTIN_PREDS.iteritems():         if field.endswith(suffix):             f = field[: field.index(suffix)]             <MASK>                 return False             return p(getattr(item, f), value)     if not hasattr(item, field) or getattr(item, field) is None:         return False     if isinstance(value, type(lambda x: x)):         return value(getattr(item, field))     return getattr(item, field) == value","if not hasattr ( item , f ) or getattr ( item , f ) is None :",if f is not None :,2.8913504016979332,2.8913504016979332,0.0
"def init_weights(self):     """"""Initialize model weights.""""""     for _, m in self.multi_deconv_layers.named_modules():         if isinstance(m, nn.ConvTranspose2d):             normal_init(m, std=0.001)         elif isinstance(m, nn.BatchNorm2d):             constant_init(m, 1)     for m in self.multi_final_layers.modules():         <MASK>             normal_init(m, std=0.001, bias=0)","if isinstance ( m , nn . Conv2d ) :","if isinstance ( m , nn . ConvNorm2d ) :",70.71067811865478,70.71067811865478,0.0
"def test_byteswap(self):     if self.typecode == ""u"":         example = ""\U00100100""     else:         example = self.example     a = array.array(self.typecode, example)     self.assertRaises(TypeError, a.byteswap, 42)     if a.itemsize in (1, 2, 4, 8):         b = array.array(self.typecode, example)         b.byteswap()         <MASK>             self.assertEqual(a, b)         else:             self.assertNotEqual(a, b)         b.byteswap()         self.assertEqual(a, b)",if a . itemsize == 1 :,"if a . itemsize == ( 2 , 4 , 8 ) :",36.362270465000705,36.362270465000705,0.0
"def _remove_blocks_from_variables(variables):     new_variables = []     for name, variable in variables:         <MASK>             new_variables.extend(variable.locals)             new_variables.append((name, variable.result))         else:             new_variables.append((name, variable))     return new_variables",if variable . is_block ( ) :,if variable . result :,19.199242796476852,19.199242796476852,0.0
def scope(self):     <MASK>         self.lazy_init_lock_.acquire()         try:             if self.scope_ is None:                 self.scope_ = Scope()         finally:             self.lazy_init_lock_.release()     return self.scope_,if self . scope_ is None :,if self . scope_ is None :,100.00000000000004,100.00000000000004,1.0
"def translate():     assert Lex.next() is AttributeList     reader.read()  # Discard attribute list from reader.     attrs = {}     d = AttributeList.match.groupdict()     for k, v in d.items():         if v is not None:             if k == ""attrlist"":                 v = subs_attrs(v)                 <MASK>                     parse_attributes(v, attrs)             else:                 AttributeList.attrs[k] = v     AttributeList.subs(attrs)     AttributeList.attrs.update(attrs)",if v :,"if k == ""attrs"" :",6.567274736060395,6.567274736060395,0.0
"def parse(self, response):     try:         content = response.content.decode(""utf-8"", ""ignore"")         content = json.loads(content, strict=False)     except:         self.logger.error(""Fail to parse the response in json format"")         return     for item in content[""data""]:         <MASK>             img_url = self._decode_url(item[""objURL""])         elif ""hoverURL"" in item:             img_url = item[""hoverURL""]         else:             continue         yield dict(file_url=img_url)","if ""objURL"" in item :","if ""objURL"" in item :",100.00000000000004,100.00000000000004,1.0
"def canonicalize_instruction_name(instr):     name = instr.insn_name().upper()     # XXX bypass a capstone bug that incorrectly labels some insns as mov     if name == ""MOV"":         if instr.mnemonic.startswith(""lsr""):             return ""LSR""         elif instr.mnemonic.startswith(""lsl""):             return ""LSL""         <MASK>             return ""ASR""     return OP_NAME_MAP.get(name, name)","elif instr . mnemonic . startswith ( ""asr"" ) :","if instr . mnemonic . startswith ( ""asr"" ) :",90.36020036098445,90.36020036098445,0.0
"def _clean_regions(items, region):     """"""Intersect region with target file if it exists""""""     variant_regions = bedutils.population_variant_regions(items, merged=True)     with utils.tmpfile() as tx_out_file:         target = subset_variant_regions(variant_regions, region, tx_out_file, items)         <MASK>             if isinstance(target, six.string_types) and os.path.isfile(target):                 target = _load_regions(target)             else:                 target = [target]             return target",if target :,"if not isinstance ( target , list ) :",6.27465531099474,6.27465531099474,0.0
def reader_leaves(self):     self.mutex.acquire()     try:         self.active_readers -= 1         <MASK>             self.active_writers += 1             self.waiting_writers -= 1             self.can_write.release()     finally:         self.mutex.release(),if self . active_readers == 0 and self . waiting_writers != 0 :,if self . can_read . acquire ( ) :,8.016891111916946,8.016891111916946,0.0
"def _bpe_to_words(sentence, delimiter=""@@""):     """"""Convert a sequence of bpe words into sentence.""""""     words = []     word = """"     delimiter_len = len(delimiter)     for subwords in sentence:         <MASK>             word += subwords[:-delimiter_len]         else:             word += subwords             words.append(word)             word = """"     return words",if len ( subwords ) >= delimiter_len and subwords [ - delimiter_len : ] == delimiter :,if subwords [ - delimiter_len ] :,13.72093791341434,13.72093791341434,0.0
"def _make_var_names(exog):     if hasattr(exog, ""name""):         var_names = exog.name     elif hasattr(exog, ""columns""):         var_names = exog.columns     else:         raise ValueError(""exog is not a Series or DataFrame or is unnamed."")     try:         var_names = "" "".join(var_names)     except TypeError:  # cannot have names that are numbers, pandas default         from statsmodels.base.data import _make_exog_names         <MASK>             var_names = ""x1""         else:             var_names = "" "".join(_make_exog_names(exog))     return var_names",if exog . ndim == 1 :,"if isinstance ( exog , numbers . Number ) :",5.934202609760488,5.934202609760488,0.0
"def __start_element_handler(self, name, attrs):     if name == ""mime-type"":         <MASK>             for extension in self.extensions:                 self[extension] = self.type         self.type = attrs[""type""].lower()         self.extensions = []     elif name == ""glob"":         pattern = attrs[""pattern""]         if pattern.startswith(""*.""):             self.extensions.append(pattern[1:].lower())",if self . type :,"if name == ""type"" :",7.267884212102741,7.267884212102741,0.0
"def nodes(self, id=None, name=None):     for node_dict in self.node_ls(id=id, name=name):         node_id = node_dict[""ID""]         node = DockerNode(self, node_id, inspect=node_dict)         <MASK>             continue         yield node",if self . _node_prefix and not node . name . startswith ( self . _node_prefix ) :,if not node :,0.3286933378355881,0.3286933378355881,0.0
"def fix_repeating_arguments(self):     """"""Fix elements that should accumulate/increment values.""""""     either = [list(child.children) for child in transform(self).children]     for case in either:         for e in [child for child in case if case.count(child) > 1]:             if type(e) is Argument or type(e) is Option and e.argcount:                 if e.value is None:                     e.value = []                 elif type(e.value) is not list:                     e.value = e.value.split()             <MASK>                 e.value = 0     return self",if type ( e ) is Command or type ( e ) is Option and e . argcount == 0 :,if e . argcount == 0 :,15.108632043619428,15.108632043619428,0.0
"def vi_search(self, rng):     for i in rng:         line_history = self._history.history[i]         pos = line_history.get_line_text().find(self._vi_search_text)         <MASK>             self._history.history_cursor = i             self.l_buffer.line_buffer = list(line_history.line_buffer)             self.l_buffer.point = pos             self.vi_undo_restart()             return True     self._bell()     return False",if pos >= 0 :,if pos is not None :,17.965205598154213,17.965205598154213,0.0
"def visitIf(self, node, scope):     for test, body in node.tests:         <MASK>             if type(test.value) in self._const_types:                 if not test.value:                     continue         self.visit(test, scope)         self.visit(body, scope)     if node.else_:         self.visit(node.else_, scope)","if isinstance ( test , ast . Const ) :","if not isinstance ( body , ast . If ) :",21.200626759025184,21.200626759025184,0.0
"def collect(self):     for nickname in self.squid_hosts.keys():         squid_host = self.squid_hosts[nickname]         fulldata = self._getData(squid_host[""host""], squid_host[""port""])         <MASK>             fulldata = fulldata.splitlines()             for data in fulldata:                 matches = self.stat_pattern.match(data)                 if matches:                     self.publish_counter(                         ""%s.%s"" % (nickname, matches.group(1)), float(matches.group(2))                     )",if fulldata is not None :,if fulldata :,23.174952587773145,0.0,0.0
"def convert(x, base, exponents):     out = []     for e in exponents:         d = int(x / (base ** e))         x -= d * (base ** e)         out.append(digits[d])         <MASK>             break     return out",if x == 0 and e < 0 :,if d > base :,4.673289785800722,4.673289785800722,0.0
"def print_doc(manager, options):     plugin_name = options.doc     plugin = plugins.get(plugin_name, None)     if plugin:         <MASK>             console(""Plugin %s does not have documentation"" % plugin_name)         else:             console("""")             console(trim(plugin.instance.__doc__))             console("""")     else:         console(""Could not find plugin %s"" % plugin_name)",if not plugin . instance . __doc__ :,if plugin . instance is None :,13.597602315271134,13.597602315271134,0.0
"def _set_attrs(self, attrs):     for attr in self.ATTRS:         if attr in attrs:             setattr(self, attr, attrs[attr])             del attrs[attr]         else:             <MASK>                 setattr(self, attr, NO_DEFAULT)             else:                 setattr(self, attr, None)     if attrs:         attrs = sorted(attrs.keys())         raise OptionError(""invalid keyword arguments: %s"" % "", "".join(attrs), self)","if attr == ""default"" :",if not attrs [ attr ] :,7.492442692259767,7.492442692259767,0.0
"def _get_set_scope(     ir_set: irast.Set, scope_tree: irast.ScopeTreeNode ) -> irast.ScopeTreeNode:     if ir_set.path_scope_id:         new_scope = scope_tree.root.find_by_unique_id(ir_set.path_scope_id)         <MASK>             raise errors.InternalServerError(                 f""dangling scope pointer to node with uid""                 f"":{ir_set.path_scope_id} in {ir_set!r}""             )     else:         new_scope = scope_tree     return new_scope",if new_scope is None :,if new_scope is None :,100.00000000000004,100.00000000000004,1.0
"def test_leave_one_out(self):     correct = 0     k = 3     model = kNN.train(xs, ys, k)     predictions = [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]     for i in range(len(predictions)):         model = kNN.train(xs[:i] + xs[i + 1 :], ys[:i] + ys[i + 1 :], k)         prediction = kNN.classify(model, xs[i])         self.assertEqual(prediction, predictions[i])         <MASK>             correct += 1     self.assertEqual(correct, 13)",if prediction == ys [ i ] :,if model . is_primary ( ) :,5.669791110976001,5.669791110976001,0.0
"def import_files(self, files):     """"""Import a list of MORE (.csv) files.""""""     c = self.c     if files:         changed = False         self.tab_width = c.getTabWidth(c.p)         for fileName in files:             g.setGlobalOpenDir(fileName)             p = self.import_file(fileName)             <MASK>                 p.contract()                 p.setDirty()                 c.setChanged(True)                 changed = True         if changed:             c.redraw(p)",if p :,if p . exists ( ) :,14.535768424205482,14.535768424205482,0.0
"def getPageTemplate(payload, place):     retVal = (kb.originalPage, kb.errorIsNone)     if payload and place:         <MASK>             page, _, _ = Request.queryPage(payload, place, content=True, raise404=False)             kb.pageTemplates[(payload, place)] = (page, kb.lastParserStatus is None)         retVal = kb.pageTemplates[(payload, place)]     return retVal","if ( payload , place ) not in kb . pageTemplates :",if kb . lastParserStatus is None :,7.64649370538093,7.64649370538093,0.0
"def _skip_trivial(constraint_data):     if skip_trivial_constraints:         if isinstance(constraint_data, LinearCanonicalRepn):             if constraint_data.variables is None:                 return True         else:             <MASK>                 return True     return False",if constraint_data . body . polynomial_degree ( ) == 0 :,if constraint_data . variables is not None :,22.563491294442894,22.563491294442894,0.0
"def get_unique_attribute(self, name: str):     feat = None     for f in self.features:         if self._return_feature(f) and hasattr(f, name):             <MASK>                 raise RuntimeError(""The attribute was not unique."")             feat = f     if feat is None:         raise RuntimeError(""The attribute did not exist"")     return getattr(feat, name)",if feat is not None :,if name not in self . unique_features :,5.522397783539471,5.522397783539471,0.0
"def hideEvent(self, event):     """"""Reimplement Qt method""""""     if not self.light:         for plugin in self.widgetlist:             <MASK>                 plugin.visibility_changed(True)     QMainWindow.hideEvent(self, event)",if plugin . isAncestorOf ( self . last_focused_widget ) :,if plugin . visibility_changed ( ) :,13.927628237847681,13.927628237847681,0.0
"def move_stdout_to_stderr(self):     to_remove = []     to_add = []     for consumer_level, consumer in self.consumers:         <MASK>             to_remove.append((consumer_level, consumer))             to_add.append((consumer_level, sys.stderr))     for item in to_remove:         self.consumers.remove(item)     self.consumers.extend(to_add)",if consumer == sys . stdout :,if consumer . is_stdout :,17.026116978186884,17.026116978186884,0.0
"def create(exported_python_target):     if exported_python_target not in created:         self.context.log.info(             ""Creating setup.py project for {}"".format(exported_python_target)         )         subject = self.derived_by_original.get(             exported_python_target, exported_python_target         )         setup_dir, dependencies = self.create_setup_py(subject, dist_dir)         created[exported_python_target] = setup_dir         <MASK>             for dep in dependencies:                 if is_exported_python_target(dep):                     create(dep)",if self . _recursive :,if dependencies :,12.753667906901528,0.0,0.0
"def __add__(self, other):     other = ArithmeticExpression.try_unpack_const(other)     if not self.symbolic and type(other) is int:         return SpOffset(self._bits, self._to_signed(self.offset + other))     else:         <MASK>             return SpOffset(self._bits, self.offset + other)         else:             return SpOffset(                 self._bits,                 ArithmeticExpression(                     ArithmeticExpression.Add,                     (                         self.offset,                         other,                     ),                 ),             )",if self . symbolic :,"if isinstance ( other , int ) :",6.567274736060395,6.567274736060395,0.0
"def check_connection(conn):     tables = [         r[0]         for r in conn.execute(             ""select name from sqlite_master where type='table'""         ).fetchall()     ]     for table in tables:         try:             conn.execute(                 f""PRAGMA table_info({escape_sqlite(table)});"",             )         except sqlite3.OperationalError as e:             <MASK>                 raise SpatialiteConnectionProblem(e)             else:                 raise ConnectionProblem(e)","if e . args [ 0 ] == ""no such module: VirtualSpatialIndex"" :",if e . errno == errno . EEXIST :,10.208145602370278,10.208145602370278,0.0
"def _get_github_client(self) -> ""Github"":     from github import Github     if self.access_token_secret is not None:         # If access token secret specified, load it         access_token = Secret(self.access_token_secret).get()     else:         # Otherwise, fallback to loading from local secret or environment variable         access_token = prefect.context.get(""secrets"", {}).get(""GITHUB_ACCESS_TOKEN"")         <MASK>             access_token = os.getenv(""GITHUB_ACCESS_TOKEN"")     return Github(access_token)",if access_token is None :,if access_token is None :,100.00000000000004,100.00000000000004,1.0
"def make_tab(lists):     if hasattr(lists, ""tolist""):         lists = lists.tolist()     ut = []     for rad in lists:         <MASK>             ut.append(""\t"".join([""%s"" % x for x in rad]))         else:             ut.append(""%s"" % rad)     return ""\n"".join(ut)","if type ( rad ) in [ list , tuple ] :","if isinstance ( rad , list ) :",9.1627840649916,9.1627840649916,0.0
"def _ensure_ffi_initialized(cls):     with cls._init_lock:         <MASK>             cls.lib = build_conditional_library(lib, CONDITIONAL_NAMES)             cls._lib_loaded = True             # initialize the SSL library             cls.lib.SSL_library_init()             # adds all ciphers/digests for EVP             cls.lib.OpenSSL_add_all_algorithms()             # loads error strings for libcrypto and libssl functions             cls.lib.SSL_load_error_strings()             cls._register_osrandom_engine()",if not cls . _lib_loaded :,if not cls . lib_loaded :,61.01950432112583,61.01950432112583,0.0
def writer_leaves(self):     self.mutex.acquire()     try:         self.active_writers -= 1         if self.waiting_writers != 0:             self.active_writers += 1             self.waiting_writers -= 1             self.can_write.release()         <MASK>             t = self.waiting_readers             self.waiting_readers = 0             self.active_readers += t             while t > 0:                 self.can_read.release()                 t -= 1     finally:         self.mutex.release(),elif self . waiting_readers != 0 :,if self . active_readers != 0 :,58.14307369682194,58.14307369682194,0.0
"def _spans(self, operands):     spans = {}     k = 0     j = 0     for mode in (self.FLOAT, self.MPMATH):         for i, operand in enumerate(operands[k:]):             if operand[0] > mode:                 break             j = i + k + 1         <MASK>  # only init state? then ignore.             j = 0         spans[mode] = slice(k, j)         k = j     spans[self.SYMBOLIC] = slice(k, len(operands))     return spans",if k == 0 and j == 1 :,if i == len ( operands ) :,9.080027618567454,9.080027618567454,0.0
"def _report_error(self, completion_routine, response=None, message=None):     if response:         # Only include the text in case of error.         <MASK>             status = location.Status(response.status_code, response.text)         else:             status = location.Status(response.status_code)     else:         status = location.Status(500, message)     if response is None or not response.ok:         if completion_routine:             return completion_routine(status)         raise IOError(response.text)     else:         if completion_routine:             completion_routine(status)     return location.Status(200, response.content)",if not response . ok :,if response . ok :,57.89300674674101,57.89300674674101,0.0
"def readinto(self, buf):     if self.current_frame:         n = self.current_frame.readinto(buf)         if n == 0 and len(buf) != 0:             self.current_frame = None             n = len(buf)             buf[:] = self.file_read(n)             return n         <MASK>             raise UnpicklingError(""pickle exhausted before end of frame"")         return n     else:         n = len(buf)         buf[:] = self.file_read(n)         return n",if n < len ( buf ) :,if n == 0 :,12.872632311973014,12.872632311973014,0.0
"def __getitem__(self, name, set=set, getattr=getattr, id=id):     visited = set()     mydict = self.basedict     while 1:         value = mydict[name]         <MASK>             return value         myid = id(mydict)         assert myid not in visited         visited.add(myid)         mydict = mydict.Parent         if mydict is None:             return",if value is not None :,if value is not None :,100.00000000000004,100.00000000000004,1.0
"def _handle_Mul(self, expr):     arg0, arg1 = expr.args     expr_0 = self._expr(arg0)     if expr_0 is None:         return None     expr_1 = self._expr(arg1)     if expr_1 is None:         return None     try:         <MASK>             # self.tyenv is not used             mask = (1 << expr.result_size(self.tyenv)) - 1             return (expr_0 * expr_1) & mask         else:             return expr_0 * expr_1     except TypeError as e:         self.l.warning(e)         return None","if isinstance ( expr_0 , int ) and isinstance ( expr_1 , int ) :",if expr . result_size ( ) == 1 :,3.073880349132242,3.073880349132242,0.0
"def end_request(self, request_id):     """"""Removes the information associated with given request_id.""""""     with self._lock:         del self._request_wsgi_environ[request_id]         del self._request_id_to_server_configuration[request_id]         <MASK>             del self._request_id_to_instance[request_id]",if request_id in self . _request_id_to_instance :,if self . _request_id_to_instance :,66.16975066206076,66.16975066206076,0.0
def generate():     <MASK>         decoder = zlib.decompressobj(16 + zlib.MAX_WBITS)     while True:         chunk = self.raw.read(chunk_size)         if not chunk:             break         if self._gzipped:             chunk = decoder.decompress(chunk)         yield chunk,if self . _gzipped :,if self . _gzipped :,100.00000000000004,100.00000000000004,1.0
"def handle(self):     from poetry.utils.env import EnvManager     manager = EnvManager(self.poetry)     current_env = manager.get()     for venv in manager.list():         name = venv.path.name         <MASK>             name = str(venv.path)         if venv == current_env:             self.line(""<info>{} (Activated)</info>"".format(name))             continue         self.line(name)","if self . option ( ""full-path"" ) :",if name == current_env :,5.11459870708889,5.11459870708889,0.0
"def addAggregators(sheet, cols, aggrnames):     ""Add each aggregator in list of *aggrnames* to each of *cols*.""     for aggrname in aggrnames:         aggrs = vd.aggregators.get(aggrname)         aggrs = aggrs if isinstance(aggrs, list) else [aggrs]         for aggr in aggrs:             for c in cols:                 <MASK>                     c.aggregators = []                 if aggr and aggr not in c.aggregators:                     c.aggregators += [aggr]","if not hasattr ( c , ""aggregators"" ) :",if aggr not in aggrs :,4.642454187453896,4.642454187453896,0.0
"def on_pre_output_coercion(     directive_args: Dict[str, Any],     next_directive: Callable,     value: Any,     ctx: Optional[Any],     info: ""ResolveInfo"", ):     value = await next_directive(value, ctx, info)     if value is None:         return value     try:         py_enum = _ENUM_MAP[directive_args[""name""]]         <MASK>             return [None if item is None else py_enum(item).name for item in value]         return py_enum(value).name     except Exception:         pass     return value","if isinstance ( value , list ) :",if py_enum is not None :,6.567274736060395,6.567274736060395,0.0
def cut(sentence):     sentence = strdecode(sentence)     blocks = re_han.split(sentence)     for blk in blocks:         <MASK>             for word in __cut(blk):                 if word not in Force_Split_Words:                     yield word                 else:                     for c in word:                         yield c         else:             tmp = re_skip.split(blk)             for x in tmp:                 if x:                     yield x,if re_han . match ( blk ) :,if blk in re_Split_Words :,10.729256185679601,10.729256185679601,0.0
"def refresh_archive_action(self):     archive_name = self.selected_archive_name()     if archive_name is not None:         params = BorgInfoArchiveThread.prepare(self.profile(), archive_name)         <MASK>             thread = BorgInfoArchiveThread(params[""cmd""], params, parent=self.app)             thread.updated.connect(self._set_status)             thread.result.connect(self.refresh_archive_result)             self._toggle_all_buttons(False)             thread.start()","if params [ ""ok"" ] :",if self . _toggle_all_buttons ( True ) :,3.673526562988939,3.673526562988939,0.0
"def get_resource_public_actions(resource_class):     resource_class_members = inspect.getmembers(resource_class)     resource_methods = {}     for name, member in resource_class_members:         if not name.startswith(""_""):             if not name[0].isupper():                 if not name.startswith(""wait_until""):                     <MASK>                         resource_methods[name] = member     return resource_methods",if is_resource_action ( member ) :,"if member . __name__ == ""actions"" :",4.246549372656572,4.246549372656572,0.0
"def _get_compressor(compress_type, compresslevel=None):     if compress_type == ZIP_DEFLATED:         <MASK>             return zlib.compressobj(compresslevel, zlib.DEFLATED, -15)         return zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -15)     elif compress_type == ZIP_BZIP2:         if compresslevel is not None:             return bz2.BZ2Compressor(compresslevel)         return bz2.BZ2Compressor()     # compresslevel is ignored for ZIP_LZMA     elif compress_type == ZIP_LZMA:         return LZMACompressor()     else:         return None",if compresslevel is not None :,if compresslevel is not None :,100.00000000000004,100.00000000000004,1.0
"def parse_header(plyfile, ext):     # Variables     line = []     properties = []     num_points = None     while b""end_header"" not in line and line != b"""":         line = plyfile.readline()         if b""element"" in line:             line = line.split()             num_points = int(line[2])         <MASK>             line = line.split()             properties.append((line[2].decode(), ext + ply_dtypes[line[1]]))     return num_points, properties","elif b""property"" in line :","if b""properties"" in line :",41.11336169005198,41.11336169005198,0.0
"def download_release_artifacts(self, version):     try:         os.mkdir(self.artifacts_dir)     except FileExistsError:         pass     for job_name in self.build_ids:         build_number = self.build_ids.get(job_name)         build_status = self._get_build_status(job_name, build_number)         <MASK>             self._download_job_artifact(job_name, build_number, version)         else:             print(""Build for {} is not fininished"".format(job_name))             print(""\tRun 'build' action to check status of {}"".format(job_name))","if build_status == ""built"" :",if build_status :,26.013004751144457,26.013004751144457,0.0
"def update_metadata(self):     for attrname in dir(self):         <MASK>             continue         attrvalue = getattr(self, attrname, None)         if attrvalue == 0:             continue         if attrname == ""salt_version"":             attrname = ""version""         if hasattr(self.metadata, ""set_{0}"".format(attrname)):             getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue)         elif hasattr(self.metadata, attrname):             try:                 setattr(self.metadata, attrname, attrvalue)             except AttributeError:                 pass","if attrname . startswith ( ""__"" ) :","if attrname == ""salt_version"" :",9.993744303650718,9.993744303650718,0.0
"def check_heuristic_in_sql():     heurs = set()     excluded = [""Equal assembly or pseudo-code"", ""All or most attributes""]     for heur in HEURISTICS:         name = heur[""name""]         if name in excluded:             continue         sql = heur[""sql""]         <MASK>             print((""SQL command not correctly associated to %s"" % repr(name)))             print(sql)             assert sql.find(name) != -1         heurs.add(name)     print(""Heuristics:"")     import pprint     pprint.pprint(heurs)",if sql . lower ( ) . find ( name . lower ( ) ) == - 1 :,if not sql :,0.38503887711545237,0.38503887711545237,0.0
def gettext(rv):     for child in rv.childNodes:         <MASK>             yield child.nodeValue         if child.nodeType == child.ELEMENT_NODE:             for item in gettext(child):                 yield item,if child . nodeType == child . TEXT_NODE :,if child . nodeType == child . TEXT_NODE :,100.00000000000004,100.00000000000004,1.0
"def update(self):     """"""Update properties over dbus.""""""     self._check_dbus()     _LOGGER.info(""Updating service information"")     self._services.clear()     try:         systemd_units = await self.sys_dbus.systemd.list_units()         for service_data in systemd_units[0]:             <MASK>                 continue             self._services.add(ServiceInfo.read_from(service_data))     except (HassioError, IndexError):         _LOGGER.warning(""Can't update host service information!"")","if not service_data [ 0 ] . endswith ( "".service"" ) or service_data [ 2 ] != ""loaded"" :",if service_data is None :,1.1988011889941206,1.1988011889941206,0.0
"def filtercomments(source):     """"""NOT USED: strips trailing comments and put them at the top.""""""     trailing_comments = []     comment = True     while comment:         <MASK>             comment = source[0, source.index(""*/"") + 2]         elif re.search(r""^\s*\/\/"", source):             comment = re.search(r""^\s*\/\/"", source).group(0)         else:             comment = None         if comment:             source = re.sub(r""^\s+"", """", source[len(comment) :])             trailing_comments.append(comment)     return ""\n"".join(trailing_comments) + source","if re . search ( r""^\s*\/\*"" , source ) :","if re . search ( r""^\s*/"" , source ) :",70.48590795061648,70.48590795061648,0.0
"def _getSourceStamp_sync(self, ssid):     if ssid in self.sourcestamps:         ssdict = self.sourcestamps[ssid].copy()         ssdict[""ssid""] = ssid         patchid = ssdict[""patchid""]         <MASK>             ssdict.update(self.patches[patchid])             ssdict[""patchid""] = patchid         else:             ssdict[""patch_body""] = None             ssdict[""patch_level""] = None             ssdict[""patch_subdir""] = None             ssdict[""patch_author""] = None             ssdict[""patch_comment""] = None         return ssdict     else:         return None",if patchid :,if patchid :,100.00000000000004,0.0,1.0
"def parseImpl(self, instring, loc, doActions=True):     try:         loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)     except (ParseException, IndexError):         if self.defaultValue is not self.__optionalNotMatched:             <MASK>                 tokens = ParseResults([self.defaultValue])                 tokens[self.expr.resultsName] = self.defaultValue             else:                 tokens = [self.defaultValue]         else:             tokens = []     return loc, tokens",if self . expr . resultsName :,if self . expr . resultsName in tokens :,61.04735835807847,61.04735835807847,0.0
"def _find_exceptions():     for _name, obj in iteritems(globals()):         try:             is_http_exception = issubclass(obj, HTTPException)         except TypeError:             is_http_exception = False         if not is_http_exception or obj.code is None:             continue         __all__.append(obj.__name__)         old_obj = default_exceptions.get(obj.code, None)         <MASK>             continue         default_exceptions[obj.code] = obj","if old_obj is not None and issubclass ( obj , old_obj ) :",if old_obj is not None :,28.22664073782293,28.22664073782293,0.0
"def generator(self, data):     for (proc_as, key_buf_ptr) in data:         key_buf = proc_as.read(key_buf_ptr, 24)         <MASK>             continue         key = """".join(""%02X"" % ord(k) for k in key_buf)         yield (             0,             [                 str(key),             ],         )",if not key_buf :,if not key_buf :,100.00000000000004,100.00000000000004,1.0
"def calculateEnableMargins(self):     self.cnc.resetEnableMargins()     for block in self.blocks:         <MASK>             CNC.vars[""xmin""] = min(CNC.vars[""xmin""], block.xmin)             CNC.vars[""ymin""] = min(CNC.vars[""ymin""], block.ymin)             CNC.vars[""zmin""] = min(CNC.vars[""zmin""], block.zmin)             CNC.vars[""xmax""] = max(CNC.vars[""xmax""], block.xmax)             CNC.vars[""ymax""] = max(CNC.vars[""ymax""], block.ymax)             CNC.vars[""zmax""] = max(CNC.vars[""zmax""], block.zmax)",if block . enable :,if block . isEnable :,42.72870063962342,42.72870063962342,0.0
"def __init__(self, client, job_id, callback=None):     self.client = client     self.job_id = job_id     # If a job event has been received already then we must set an Event     # to wait for this job to finish.     # Otherwise we create a new stub for the job with the Event for when     # the job event arrives to use existing event.     with client._jobs_lock:         job = client._jobs.get(job_id)         self.event = None         <MASK>             self.event = job.get(""__ready"")         if self.event is None:             self.event = job[""__ready""] = Event()         job[""__callback""] = callback",if job :,if job is not None :,17.965205598154213,17.965205598154213,0.0
"def asset(*paths):     for path in paths:         fspath = www_root + ""/assets/"" + path         etag = """"         try:             if env.cache_static:                 etag = asset_etag(fspath)             else:                 os.stat(fspath)         except FileNotFoundError as e:             if path == paths[-1]:                 <MASK>                     tell_sentry(e, {})             else:                 continue         except Exception as e:             tell_sentry(e, {})         return asset_url + path + (etag and ""?etag="" + etag)","if not os . path . exists ( fspath + "".spt"" ) :",if e . errno == 404 :,2.673705182447105,2.673705182447105,0.0
"def set_conf():     """"""Collapse all object_trail config into cherrypy.request.config.""""""     base = cherrypy.config.copy()     # Note that we merge the config from each node     # even if that node was None.     for name, obj, conf, segleft in object_trail:         base.update(conf)         <MASK>             base[""tools.staticdir.section""] = ""/"" + ""/"".join(                 fullpath[0 : fullpath_len - segleft]             )     return base","if ""tools.staticdir.dir"" in conf :",if segleft > 0 :,3.8261660656802645,3.8261660656802645,0.0
"def __init__(self):     self.setLayers(None, None)     self.interface = None     self.event_callbacks = {}     self.__stack = None     self.lock = threading.Lock()     members = inspect.getmembers(self, predicate=inspect.ismethod)     for m in members:         <MASK>             fname = m[0]             fn = m[1]             self.event_callbacks[fn.event_callback] = getattr(self, fname)","if hasattr ( m [ 1 ] , ""event_callback"" ) :","if isinstance ( m , tuple ) :",7.205893226533905,7.205893226533905,0.0
def multi_dev_generator(self):     for data in self._data_loader():         <MASK>             self._tail_data += data         if len(self._tail_data) == self._base_number:             yield self._tail_data             self._tail_data = [],if len ( self . _tail_data ) < self . _base_number :,if len ( data ) == self . _base_number :,43.06885061313956,43.06885061313956,0.0
"def replace_field_to_value(layout, cb):     for i, lo in enumerate(layout.fields):         if isinstance(lo, Field) or issubclass(lo.__class__, Field):             layout.fields[i] = ShowField(                 cb, *lo.fields, attrs=lo.attrs, wrapper_class=lo.wrapper_class             )         elif isinstance(lo, basestring):             layout.fields[i] = ShowField(cb, lo)         <MASK>             replace_field_to_value(lo, cb)","elif hasattr ( lo , ""get_field_names"" ) :","if isinstance ( lo , ( int , float ) ) :",13.417722077013488,13.417722077013488,0.0
"def function_out(*args, **kwargs):     try:         return function_in(*args, **kwargs)     except dbus.exceptions.DBusException as e:         if e.get_dbus_name() == DBUS_UNKNOWN_METHOD:             raise ItemNotFoundException(""Item does not exist!"")         if e.get_dbus_name() == DBUS_NO_SUCH_OBJECT:             raise ItemNotFoundException(e.get_dbus_message())         <MASK>             raise SecretServiceNotAvailableException(e.get_dbus_message())         raise","if e . get_dbus_name ( ) in ( DBUS_NO_REPLY , DBUS_NOT_SUPPORTED ) :",if e . get_dbus_name ( ) == DBUS_NO_SUCH_SERVICE :,46.95133308628678,46.95133308628678,0.0
"def results_iter(self):     if self.connection.ops.oracle:         from django.db.models.fields import DateTimeField         fields = [DateTimeField()]     else:         needs_string_cast = self.connection.features.needs_datetime_string_cast     offset = len(self.query.extra_select)     for rows in self.execute_sql(MULTI):         for row in rows:             date = row[offset]             if self.connection.ops.oracle:                 date = self.resolve_columns(row, fields)[offset]             <MASK>                 date = typecast_timestamp(str(date))             yield date",elif needs_string_cast :,if needs_string_cast :,80.91067115702207,80.91067115702207,0.0
"def handle_label(self, path, **options):     verbosity = int(options.get(""verbosity"", 1))     result = finders.find(path, all=options[""all""])     path = smart_unicode(path)     if result:         if not isinstance(result, (list, tuple)):             result = [result]         output = u""\n  "".join(             (smart_unicode(os.path.realpath(path)) for path in result)         )         self.stdout.write(smart_str(u""Found '%s' here:\n  %s\n"" % (path, output)))     else:         <MASK>             self.stderr.write(smart_str(""No matching file found for '%s'.\n"" % path))",if verbosity >= 1 :,if verbosity > 0 :,34.98330125272253,34.98330125272253,0.0
"def name(self):     """"""Get the enumeration name of this storage class.""""""     if self._name_map is None:         self._name_map = {}         for key, value in list(StorageClass.__dict__.items()):             <MASK>                 self._name_map[value] = key     return self._name_map[self]","if isinstance ( value , StorageClass ) :",if value is not None :,7.654112967106117,7.654112967106117,0.0
"def index(self, value):     if self._growing:         if self._start <= value < self._stop:             q, r = divmod(value - self._start, self._step)             if r == self._zero:                 return int(q)     else:         <MASK>             q, r = divmod(self._start - value, -self._step)             if r == self._zero:                 return int(q)     raise ValueError(""{} is not in numeric range"".format(value))",if self . _start >= value > self . _stop :,if self . _stop <= value < self . _start :,49.4799546857679,49.4799546857679,0.0
"def extract_cookie(cookie_header, cookie_name):     inx = cookie_header.find(cookie_name)     if inx >= 0:         end_inx = cookie_header.find("";"", inx)         <MASK>             value = cookie_header[inx:end_inx]         else:             value = cookie_header[inx:]         return value     return """"",if end_inx > 0 :,if end_inx >= 0 :,59.4603557501361,59.4603557501361,0.0
"def get_size(self, shape_info):     # The size is the data, that have constant size.     state = np.random.RandomState().get_state()     size = 0     for elem in state:         if isinstance(elem, str):             size += len(elem)         <MASK>             size += elem.size * elem.itemsize         elif isinstance(elem, int):             size += np.dtype(""int"").itemsize         elif isinstance(elem, float):             size += np.dtype(""float"").itemsize         else:             raise NotImplementedError()     return size","elif isinstance ( elem , np . ndarray ) :","if isinstance ( elem , float ) :",32.01911827891038,32.01911827891038,0.0
"def createFields(self):     size = self.size / 8     if size > 2:         <MASK>             yield UInt8(self, ""cs"", ""10ms units, values from 0 to 199"")         yield Bits(self, ""2sec"", 5, ""seconds/2"")         yield Bits(self, ""min"", 6, ""minutes"")         yield Bits(self, ""hour"", 5, ""hours"")     yield Bits(self, ""day"", 5, ""(1-31)"")     yield Bits(self, ""month"", 4, ""(1-12)"")     yield Bits(self, ""year"", 7, ""(0 = 1980, 127 = 2107)"")",if size > 4 :,if size % 2 :,23.643540225079384,23.643540225079384,0.0
"def detect(get_page):     retval = False     for vector in WAF_ATTACK_VECTORS:         page, headers, code = get_page(get=vector)         retval = (             re.search(                 r""incap_ses|visid_incap"", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I             )             is not None         )         retval |= re.search(r""Incapsula"", headers.get(""X-CDN"", """"), re.I) is not None         <MASK>             break     return retval",if retval :,if code == 0 :,9.652434877402245,9.652434877402245,0.0
"def _get_order_information(self, node_id, timeout=1200, check_interval=5):     mask = {         ""billingItem"": """",         ""powerState"": """",         ""operatingSystem"": {""passwords"": """"},         ""provisionDate"": """",     }     for i in range(0, timeout, check_interval):         res = self.connection.request(             ""SoftLayer_Virtual_Guest"", ""getObject"", id=node_id, object_mask=mask         ).object         <MASK>             return res         time.sleep(check_interval)     raise SoftLayerException(""Timeout on getting node details"")","if res . get ( ""provisionDate"" , None ) :",if res :,3.136388772461349,0.0,0.0
"def _process_param_change(self, msg):     msg = super(Select, self)._process_param_change(msg)     labels, values = self.labels, self.values     if ""value"" in msg:         msg[""value""] = [             labels[indexOf(v, values)] for v in msg[""value""] if isIn(v, values)         ]     if ""options"" in msg:         msg[""options""] = labels         <MASK>             self.value = [v for v in self.value if isIn(v, values)]     return msg","if any ( not isIn ( v , values ) for v in self . value ) :","if ""value"" in msg :",1.9294673127036233,1.9294673127036233,0.0
"def get_object_from_name(self, name, check_symlinks=True):     if not name:         return None     name = name.rstrip(""\\"")     for a, o in self.objects.items():         if not o.name:             continue         if o.name.lower() == name.lower():             return o     if check_symlinks:         m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()]         <MASK>             name = m[0]         return self.get_object_from_name(name, False)",if m :,if len ( m ) == 1 :,6.27465531099474,6.27465531099474,0.0
"def run(self):     for k, v in iteritems(self.objs):         <MASK>             continue         if v[""_class""] == ""User"":             if v[""email""] == """":                 v[""email""] = None             if v[""ip""] == ""0.0.0.0"":                 v[""ip""] = None     return self.objs","if k . startswith ( ""_"" ) :","if k == ""user"" :",10.816059393812111,10.816059393812111,0.0
"def _providers(self, descriptor):     res = []     for _md in self.metadata.values():         for ent_id, ent_desc in _md.items():             if descriptor in ent_desc:                 <MASK>                     # print(""duplicated entity_id: %s"" % res)                     pass                 else:                     res.append(ent_id)     return res",if ent_id in res :,if ent_id in res :,100.00000000000004,100.00000000000004,1.0
"def test_add_participant(self):     async with self.chat_client:         await self._create_thread()         async with self.chat_thread_client:             share_history_time = datetime.utcnow()             share_history_time = share_history_time.replace(tzinfo=TZ_UTC)             new_participant = ChatThreadParticipant(                 user=self.new_user,                 display_name=""name"",                 share_history_time=share_history_time,             )             await self.chat_thread_client.add_participant(new_participant)         <MASK>             await self.chat_client.delete_chat_thread(self.thread_id)",if not self . is_playback ( ) :,if self . thread_id :,10.759051250985632,10.759051250985632,0.0
"def url(regex, view, kwargs=None, name=None, prefix=""""):     if isinstance(view, (list, tuple)):         # For include(...) processing.         urlconf_module, app_name, namespace = view         return RegexURLResolver(             regex, urlconf_module, kwargs, app_name=app_name, namespace=namespace         )     else:         if isinstance(view, basestring):             if not view:                 raise ImproperlyConfigured(                     ""Empty URL pattern view name not permitted (for pattern %r)"" % regex                 )             <MASK>                 view = prefix + ""."" + view         return RegexURLPattern(regex, view, kwargs, name)",if prefix :,if prefix :,100.00000000000004,0.0,1.0
"def tx():     # Sync receiver ready to avoid loss of first packets     while not sub_ready.ready():         pub.send(b""test BEGIN"")         eventlet.sleep(0.005)     for i in range(1, 101):         msg = ""test {0}"".format(i).encode()         <MASK>             pub.send(msg)         else:             pub.send(b""test LAST"")             sub_last.wait()         # XXX: putting a real delay of 1ms here fixes sporadic failures on Travis         # just yield eventlet.sleep(0) doesn't cut it         eventlet.sleep(0.001)     pub.send(b""done DONE"")",if i != 50 :,if msg :,12.753667906901528,0.0,0.0
"def remove_tmp_snapshot_file(self, files):     for filepath in files:         path = Path(filepath)         if path.is_dir() and path.exists():             shutil.rmtree(path)         <MASK>             path.unlink()",elif path . is_file ( ) and path . exists ( ) :,if os . path . isfile ( path ) :,8.27951003977077,8.27951003977077,0.0
"def f(view, s):     if mode == modes.INTERNAL_NORMAL:         if count == 1:             <MASK>                 eol = view.line(s.b).b                 return R(s.b, eol)             return s     return s",if view . line ( s . b ) . size ( ) > 0 :,if count == 2 :,1.8231094563196564,1.8231094563196564,0.0
"def get_ids(self, **kwargs):     id = []     if ""id"" in kwargs:         id = kwargs[""id""]         # Coerce ids to list         <MASK>             id = id.split("","")         # Ensure ids are integers         try:             id = list(map(int, id))         except Exception:             decorators.error(""Invalid id"")     return id","if not isinstance ( id , list ) :","if ""id"" in id :",6.495032985064742,6.495032985064742,0.0
"def param_value(self):     # This is part of the ""handle quoted extended parameters"" hack.     for token in self:         <MASK>             return token.stripped_value         if token.token_type == ""quoted-string"":             for token in token:                 if token.token_type == ""bare-quoted-string"":                     for token in token:                         if token.token_type == ""value"":                             return token.stripped_value     return """"","if token . token_type == ""value"" :","if token . token_type == ""value"" :",100.00000000000004,100.00000000000004,1.0
"def get_all_start_methods(self):     if sys.platform == ""win32"":         return [""spawn""]     else:         methods = [""spawn"", ""fork""] if sys.platform == ""darwin"" else [""fork"", ""spawn""]         <MASK>             methods.append(""forkserver"")         return methods",if reduction . HAVE_SEND_HANDLE :,"if sys . platform == ""win64"" :",5.522397783539471,5.522397783539471,0.0
"def _process_watch(self, watched_event):     logger.debug(""process_watch: %r"", watched_event)     with handle_exception(self._tree._error_listeners):         <MASK>             assert self._parent is None, ""unexpected CREATED on non-root""             self.on_created()         elif watched_event.type == EventType.DELETED:             self.on_deleted()         elif watched_event.type == EventType.CHANGED:             self._refresh_data()         elif watched_event.type == EventType.CHILD:             self._refresh_children()",if watched_event . type == EventType . CREATED :,if watched_event . type == EventType . CREATED :,100.00000000000004,100.00000000000004,1.0
"def assert_open(self, sock, *rest):     if isinstance(sock, fd_types):         self.__assert_fd_open(sock)     else:         fileno = sock.fileno()         assert isinstance(fileno, fd_types), fileno         sockname = sock.getsockname()         assert isinstance(sockname, tuple), sockname         <MASK>             self.__assert_fd_open(fileno)         else:             self._assert_sock_open(sock)     if rest:         self.assert_open(rest[0], *rest[1:])",if not WIN :,if fileno :,24.840753130578644,0.0,0.0
"def detype(self):     """"""De-types the instance, allowing it to be exported to the environment.""""""     style = self.style     if self._detyped is None:         self._detyped = "":"".join(             [                 key                 + ""=""                 + "";"".join(                     [                         LsColors.target_value                         <MASK>                         else ansi_color_name_to_escape_code(v, cmap=style)                         for v in val                     ]                 )                 for key, val in sorted(self._d.items())             ]         )     return self._detyped",if key in self . _targets,if val is None or val is None :,4.767707020457095,4.767707020457095,0.0
"def gather_metrics(dry_run=False):     today = datetime.date.today()     first = today.replace(day=1)     last_month = first - datetime.timedelta(days=1)     filename = ""form_types_{}.csv"".format(last_month.strftime(""%Y-%m""))     with connection.cursor() as cursor:         cursor.execute(REGISTRATION_METRICS_SQL)         <MASK>             for row in cursor.fetchall():                 logger.info(encode_row(row))         else:             write_raw_data(cursor=cursor, filename=filename)",if dry_run :,if dry_run :,100.00000000000004,100.00000000000004,1.0
"def cat(tensors, dim=0):     assert isinstance(tensors, list), ""input to cat must be a list""     if len(tensors) == 1:         return tensors[0]     from .autograd_cryptensor import AutogradCrypTensor     if any(isinstance(t, AutogradCrypTensor) for t in tensors):         <MASK>             tensors[0] = AutogradCrypTensor(tensors[0], requires_grad=False)         return tensors[0].cat(*tensors[1:], dim=dim)     else:         return get_default_backend().cat(tensors, dim=dim)","if not isinstance ( tensors [ 0 ] , AutogradCrypTensor ) :",if len ( tensors ) == 1 :,8.591316733350183,8.591316733350183,0.0
"def is_installed(self, dlc_title="""") -> bool:     installed = False     if dlc_title:         dlc_version = self.get_dlc_info(""version"", dlc_title)         installed = True if dlc_version else False         # Start: Code for compatibility with minigalaxy 1.0         if not installed:             status = self.legacy_get_dlc_status(dlc_title)             installed = True if status in [""installed"", ""updatable""] else False         # End: Code for compatibility with minigalaxy 1.0     else:         <MASK>             installed = True     return installed",if self . install_dir and os . path . exists ( self . install_dir ) :,"if status in [ ""installed"" , ""updatable"" ] :",2.1440371673287992,2.1440371673287992,0.0
"def on_copy(self):     source_objects = self.__getSelection()     for source in source_objects:         <MASK>             new_obj = model.Phrase("""", """")         else:             new_obj = model.Script("""", """")         new_obj.copy(source)         self.cutCopiedItems.append(new_obj)","if isinstance ( source , model . Phrase ) :",if source . is_phrase :,6.050259138270144,6.050259138270144,0.0
"def FetchFn(type_name):     """"""Fetches all hunt results of a given type.""""""     offset = 0     while True:         results = data_store.REL_DB.ReadHuntResults(             hunt_id, offset=offset, count=self._RESULTS_PAGE_SIZE, with_type=type_name         )         <MASK>             break         for r in results:             msg = r.AsLegacyGrrMessage()             msg.source_urn = source_urn             yield msg         offset += self._RESULTS_PAGE_SIZE",if not results :,if not results :,100.00000000000004,100.00000000000004,1.0
"def get_blob_type_declaration_sql(self, column):     length = column.get(""length"")     if length:         <MASK>             return ""TINYBLOB""         if length <= self.LENGTH_LIMIT_BLOB:             return ""BLOB""         if length <= self.LENGTH_LIMIT_MEDIUMBLOB:             return ""MEDIUMBLOB""     return ""LONGBLOB""",if length <= self . LENGTH_LIMIT_TINYBLOB :,if length <= self . LENGTH_LIMIT_TINYBLOB :,100.00000000000004,100.00000000000004,1.0
"def decode(cls, data):     while data:         (             length,             atype,         ) = unpack(cls.Header.PACK, data[: cls.Header.LEN])         <MASK>             raise AttributesError(""Buffer underrun %d < %d"" % (len(data), length))         payload = data[cls.Header.LEN : length]         yield atype, payload         data = data[int((length + 3) / 4) * 4 :]",if len ( data ) < length :,if length < 4 :,8.290829875388036,8.290829875388036,0.0
"def test_join_diffs(db, series_of_diffs, expected):     diffs = []     for changes in series_of_diffs:         tracker = DBDiffTracker()         for key, val in changes.items():             <MASK>                 del tracker[key]             else:                 tracker[key] = val         diffs.append(tracker.diff())     DBDiff.join(diffs).apply_to(db)     assert db == expected",if val is None :,if val == expected :,17.965205598154213,17.965205598154213,0.0
"def ant_map(m):     tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0]))     players = {}     for row in m:         tmp += ""m ""         for col in row:             if col == LAND:                 tmp += "".""             elif col == BARRIER:                 tmp += ""%""             elif col == FOOD:                 tmp += ""*""             <MASK>                 tmp += ""?""             else:                 players[col] = True                 tmp += chr(col + 97)         tmp += ""\n""     tmp = (""players %s\n"" % len(players)) + tmp     return tmp",elif col == UNSEEN :,if col == BARRIER :,32.46679154750991,32.46679154750991,0.0
"def _report_error(self, completion_routine, response=None, message=None):     if response:         # Only include the text in case of error.         if not response.ok:             status = location.Status(response.status_code, response.text)         else:             status = location.Status(response.status_code)     else:         status = location.Status(500, message)     if response is None or not response.ok:         <MASK>             return completion_routine(status)         raise IOError(response.text)     else:         if completion_routine:             completion_routine(status)     return location.Status(200, response.content)",if completion_routine :,if completion_routine :,100.00000000000004,100.00000000000004,1.0
"def _generate_examples(self, src_path=None, tgt_path=None, replace_unk=None):     """"""Yields examples.""""""     with tf.io.gfile.GFile(src_path) as f_d, tf.io.gfile.GFile(tgt_path) as f_s:         for i, (doc_text, sum_text) in enumerate(zip(f_d, f_s)):             <MASK>                 yield i, {                     _DOCUMENT: doc_text.strip().replace(""<unk>"", ""UNK""),                     _SUMMARY: sum_text.strip().replace(""<unk>"", ""UNK""),                 }             else:                 yield i, {_DOCUMENT: doc_text.strip(), _SUMMARY: sum_text.strip()}",if replace_unk :,if replace_unk :,100.00000000000004,100.00000000000004,1.0
"def escape(text, newline=False):     """"""Escape special html characters.""""""     if isinstance(text, str):         if ""&"" in text:             text = text.replace(""&"", ""&amp;"")         if "">"" in text:             text = text.replace("">"", ""&gt;"")         if ""<"" in text:             text = text.replace(""<"", ""&lt;"")         if '""' in text:             text = text.replace('""', ""&quot;"")         if ""'"" in text:             text = text.replace(""'"", ""&quot;"")         <MASK>             if ""\n"" in text:                 text = text.replace(""\n"", ""<br>"")     return text",if newline :,if newline :,100.00000000000004,0.0,1.0
"def _handle_url_click(self, event):     url = _extract_click_text(self.info_text, event, ""url"")     if url is not None:         <MASK>             import webbrowser             webbrowser.open(url)         elif os.path.sep in url:             os.makedirs(url, exist_ok=True)             open_path_in_system_file_manager(url)         else:             self._start_show_package_info(url)","if url . startswith ( ""http:"" ) or url . startswith ( ""https:"" ) :",if os . path . isfile ( url ) :,3.820942032434038,3.820942032434038,0.0
"def SConsignFile(self, name="".sconsign"", dbm_module=None):     if name is not None:         name = self.subst(name)         <MASK>             name = os.path.join(str(self.fs.SConstruct_dir), name)     if name:         name = os.path.normpath(name)         sconsign_dir = os.path.dirname(name)         if sconsign_dir and not os.path.exists(sconsign_dir):             self.Execute(SCons.Defaults.Mkdir(sconsign_dir))     SCons.SConsign.File(name, dbm_module)",if not os . path . isabs ( name ) :,if self . fs . SConstruct_dir :,5.3990167242108145,5.3990167242108145,0.0
"def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None:     super().on_train_start(trainer, pl_module)     submodule_dict = dict(pl_module.named_modules())     self._hook_handles = []     for name in self._get_submodule_names(pl_module):         <MASK>             rank_zero_warn(                 f""{name} is not a valid identifier for a submodule in {pl_module.__class__.__name__},""                 "" skipping this key.""             )             continue         handle = self._register_hook(name, submodule_dict[name])         self._hook_handles.append(handle)",if name not in submodule_dict :,if name not in submodule_dict :,100.00000000000004,100.00000000000004,1.0
"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]):     super().validate_configuration(configuration)     if configuration is None:         configuration = self.configuration     try:         assert ""value_set"" in configuration.kwargs, ""value_set is required""         assert isinstance(             configuration.kwargs[""value_set""], (list, set, dict)         ), ""value_set must be a list or a set""         <MASK>             assert (                 ""$PARAMETER"" in configuration.kwargs[""value_set""]             ), 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key.'     except AssertionError as e:         raise InvalidExpectationConfigurationError(str(e))     return True","if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :","if ""$PARAMETER"" in configuration . kwargs :",9.586514611843183,9.586514611843183,0.0
"def check_refcounts(expected, timeout=10):     start = time.time()     while True:         try:             _check_refcounts(expected)             break         except AssertionError as e:             <MASK>                 raise e             else:                 time.sleep(0.1)",if time . time ( ) - start > timeout :,if timeout :,4.377183140747567,0.0,0.0
"def pickline(file, key, casefold=1):     try:         f = open(file, ""r"")     except IOError:         return None     pat = re.escape(key) + "":""     prog = re.compile(pat, casefold and re.IGNORECASE)     while 1:         line = f.readline()         <MASK>             break         if prog.match(line):             text = line[len(key) + 1 :]             while 1:                 line = f.readline()                 if not line or not line[0].isspace():                     break                 text = text + line             return text.strip()     return None",if not line :,if not line or not line [ 0 ] . isupper ( ) :,11.359354890271161,11.359354890271161,0.0
def _is_perf_file(file_path):     f = get_file(file_path)     for line in f:         <MASK>             continue         r = event_regexp.search(line)         if r:             f.close()             return True         f.close()         return False,"if line [ 0 ] == ""#"" :",if not line :,3.6531471527995247,3.6531471527995247,0.0
"def link_pantsrefs(soups, precomputed):     """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">""""""     for (page, soup) in soups.items():         for a in soup.find_all(""a""):             <MASK>                 continue             pantsref = a[""pantsref""]             if pantsref not in precomputed.pantsref:                 raise TaskError(                     f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it'                 )             a[""href""] = rel_href(page, precomputed.pantsref[pantsref])","if not a . has_attr ( ""pantsref"" ) :","if not a [ ""pantsref"" ] :",19.889519501129588,19.889519501129588,0.0
"def __init__(self, querylist=None):     self.query_id = -1     if querylist is None:         self.querylist = []     else:         self.querylist = querylist         for query in self.querylist:             <MASK>                 self.query_id = query.query_id             else:                 if self.query_id != query.query_id:                     raise ValueError(""query in list must be same query_id"")",if self . query_id == - 1 :,if query . query_id is not None :,26.305014340253436,26.305014340253436,0.0
"def _draw_number(     screen, x_offset, y_offset, number, token=Token.Clock, transparent=False ):     ""Write number at position.""     fg = Char("" "", token)     bg = Char("" "", Token)     for y, row in enumerate(_numbers[number]):         screen_row = screen.data_buffer[y + y_offset]         for x, n in enumerate(row):             <MASK>                 screen_row[x + x_offset] = fg             elif not transparent:                 screen_row[x + x_offset] = bg","if n == ""#"" :",if n == 1 :,38.49815007763549,38.49815007763549,0.0
"def init(self):     self.sock.setblocking(True)     if self.parser is None:         # wrap the socket if needed         <MASK>             self.sock = ssl.wrap_socket(                 self.sock, server_side=True, **self.cfg.ssl_options             )         # initialize the parser         self.parser = http.RequestParser(self.cfg, self.sock)",if self . cfg . is_ssl :,if self . cfg . ssl_options :,50.197242487957936,50.197242487957936,0.0
"def intersect_face(pt):     # todo: rewrite! inefficient!     nonlocal vis_faces2D     for f, vs in vis_faces2D:         v0 = vs[0]         for v1, v2 in iter_pairs(vs[1:], False):             <MASK>                 return f     return None","if intersect_point_tri_2d ( pt , v0 , v1 , v2 ) :",if v0 == v2 :,1.5534791020152603,1.5534791020152603,0.0
"def IMPORTFROM(self, node):     if node.module == ""__future__"":         if not self.futuresAllowed:             self.report(messages.LateFutureImport, node, [n.name for n in node.names])     else:         self.futuresAllowed = False     for alias in node.names:         <MASK>             self.scope.importStarred = True             self.report(messages.ImportStarUsed, node, node.module)             continue         name = alias.asname or alias.name         importation = Importation(name, node)         if node.module == ""__future__"":             importation.used = (self.scope, node)         self.addBinding(node, importation)","if alias . name == ""*"" :",if alias . asname :,15.719010513286515,15.719010513286515,0.0
"def PyObject_Bytes(obj):     if type(obj) == bytes:         return obj     if hasattr(obj, ""__bytes__""):         res = obj.__bytes__()         <MASK>             raise TypeError(                 ""__bytes__ returned non-bytes (type %s)"" % type(res).__name__             )     return PyBytes_FromObject(obj)","if not isinstance ( res , bytes ) :",if res != bytes :,6.962210312500384,6.962210312500384,0.0
"def on_bt_search_clicked(self, widget):     if self.current_provider is None:         return     query = self.en_query.get_text()     @self.obtain_podcasts_with     def load_data():         if self.current_provider.kind == directory.Provider.PROVIDER_SEARCH:             return self.current_provider.on_search(query)         elif self.current_provider.kind == directory.Provider.PROVIDER_URL:             return self.current_provider.on_url(query)         <MASK>             return self.current_provider.on_file(query)",elif self . current_provider . kind == directory . Provider . PROVIDER_FILE :,if self . current_provider . kind == directory . Provider . PROVIDER_FILE :,93.91044157537529,93.91044157537529,0.0
"def remove(self, name):     for s in [self.__storage(self.__category), self.__storage(None)]:         for i, b in enumerate(s):             <MASK>                 del s[i]                 if b.persistent:                     self.__save()                 return     raise KeyError(name)",if b . name == name :,if name == b . name :,39.28146509005134,39.28146509005134,0.0
"def _wrapper(data, axis=None, keepdims=False):     if not keepdims:         return func(data, axis=axis)     else:         <MASK>             axis = axis if isinstance(axis, int) else axis[0]             out_shape = list(data.shape)             out_shape[axis] = 1         else:             out_shape = [1 for _ in range(len(data.shape))]         return func(data, axis=axis).reshape(out_shape)",if axis is not None :,if axis :,23.174952587773145,0.0,0.0
"def authn_info(self):     res = []     for astat in self.assertion.authn_statement:         context = astat.authn_context         try:             authn_instant = astat.authn_instant         except AttributeError:             authn_instant = """"         <MASK>             try:                 aclass = context.authn_context_class_ref.text             except AttributeError:                 aclass = """"             try:                 authn_auth = [a.text for a in context.authenticating_authority]             except AttributeError:                 authn_auth = []             res.append((aclass, authn_auth, authn_instant))     return res",if context :,if context :,100.00000000000004,0.0,1.0
"def _persist_metadata(self, dirname, filename):     metadata_path = ""{0}/{1}.json"".format(dirname, filename)     if self.media_metadata or self.comments or self.include_location:         if self.posts:             <MASK>                 self.merge_json({""GraphImages"": self.posts}, metadata_path)             else:                 self.save_json({""GraphImages"": self.posts}, metadata_path)         if self.stories:             if self.latest:                 self.merge_json({""GraphStories"": self.stories}, metadata_path)             else:                 self.save_json({""GraphStories"": self.stories}, metadata_path)",if self . latest :,if self . latest :,100.00000000000004,100.00000000000004,1.0
"def update_record_image_detail(input_image_record, updated_image_detail, session=None):     if not session:         session = db.Session     image_record = {}     image_record.update(input_image_record)     image_record.pop(""created_at"", None)     image_record.pop(""last_updated"", None)     if image_record[""image_type""] == ""docker"":         for tag_record in updated_image_detail:             <MASK>                 image_record[""image_detail""].append(tag_record)                 return update_record(image_record, session=session)     return image_record","if tag_record not in image_record [ ""image_detail"" ] :","if tag_record [ ""image_type"" ] == ""docker"" :",43.85068972747104,43.85068972747104,0.0
"def backup(self):     for ds in [(""activedirectory"", ""AD""), (""ldap"", ""LDAP""), (""nis"", ""NIS"")]:         <MASK>             try:                 ds_cache = self.middleware.call_sync(""cache.get"", f""{ds[1]}_cache"")                 with open(f""/var/db/system/.{ds[1]}_cache_backup"", ""wb"") as f:                     pickle.dump(ds_cache, f)             except KeyError:                 self.logger.debug(""No cache exists for directory service [%s]."", ds[0])","if ( self . middleware . call_sync ( f""{ds[0]}.config"" ) ) [ ""enable"" ] :",if ds [ 0 ] in self . cache :,5.020856901074332,5.020856901074332,0.0
"def parse_setup_cfg(self):     # type: () -> Dict[STRING_TYPE, Any]     if self.setup_cfg is not None and self.setup_cfg.exists():         contents = self.setup_cfg.read_text()         base_dir = self.setup_cfg.absolute().parent.as_posix()         try:             parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix())         except Exception:             if six.PY2:                 contents = self.setup_cfg.read_bytes()             parsed = parse_setup_cfg(contents, base_dir)         <MASK>             return {}         return parsed     return {}",if not parsed :,if parsed is None :,14.058533129758727,14.058533129758727,0.0
"def parts():     for l in lists.leaves:         head_name = l.get_head_name()         <MASK>             yield l.leaves         elif head_name != ""System`Missing"":             raise MessageException(""Catenate"", ""invrp"", l)","if head_name == ""System`List"" :","if head_name == ""System`Missing"" :",76.91605673134588,76.91605673134588,0.0
"def _get_callback_and_order(self, hook):     if callable(hook):         return hook, None     elif isinstance(hook, tuple) and len(hook) == 2:         callback, order = hook         # test that callback is a callable         <MASK>             raise ValueError(""Hook callback is not a callable"")         # test that number is an int         try:             int(order)         except ValueError:             raise ValueError(""Hook order is not a number"")         return callback, order     else:         raise ValueError(             ""Invalid hook definition, neither a callable nor a 2-tuple (callback, order): {!r}"".format(                 hook             )         )",if not callable ( callback ) :,if not callable ( callback ) :,100.00000000000004,100.00000000000004,1.0
"def _resize_masks(self, results):     """"""Resize masks with ``results['scale']``""""""     for key in results.get(""mask_fields"", []):         if results[key] is None:             continue         <MASK>             results[key] = results[key].rescale(results[""scale""])         else:             results[key] = results[key].resize(results[""img_shape""][:2])",if self . keep_ratio :,"if results [ key ] [ ""scale"" ] :",4.456882760699063,4.456882760699063,0.0
"def getDataMax(self):     result = -Double.MAX_VALUE     nCurves = self.chart.getNCurves()     for i in range(nCurves):         c = self.getSystemCurve(i)         if not c.isVisible():             continue         <MASK>             nPoints = c.getNPoints()             for j in range(nPoints):                 result = self.maxIgnoreNaNAndMaxValue(result, c.getPoint(j).getY())     if result == -Double.MAX_VALUE:         return Double.NaN     return result",if c . getYAxis ( ) == Y_AXIS :,if c . isPoint ( i ) :,14.827340167306767,14.827340167306767,0.0
"def _check_token(self):     if settings.app.sso_client_cache and self.server_auth_token:         doc = self.sso_client_cache_collection.find_one(             {                 ""user_id"": self.user.id,                 ""server_id"": self.server.id,                 ""device_id"": self.device_id,                 ""device_name"": self.device_name,                 ""auth_token"": self.server_auth_token,             }         )         <MASK>             self.has_token = True",if doc :,if doc :,100.00000000000004,0.0,1.0
"def parse_header(plyfile, ext):     # Variables     line = []     properties = []     num_points = None     while b""end_header"" not in line and line != b"""":         line = plyfile.readline()         <MASK>             line = line.split()             num_points = int(line[2])         elif b""property"" in line:             line = line.split()             properties.append((line[2].decode(), ext + ply_dtypes[line[1]]))     return num_points, properties","if b""element"" in line :","if b""number"" in line :",50.000000000000014,50.000000000000014,0.0
"def __codeanalysis_settings_changed(self, current_finfo):     if self.data:         run_pyflakes, run_pep8 = self.pyflakes_enabled, self.pep8_enabled         for finfo in self.data:             self.__update_editor_margins(finfo.editor)             finfo.cleanup_analysis_results()             <MASK>                 if current_finfo is not finfo:                     finfo.run_code_analysis(run_pyflakes, run_pep8)",if ( run_pyflakes or run_pep8 ) and current_finfo is not None :,if current_finfo is not finfo :,16.111348713975108,16.111348713975108,0.0
"def __modules(self):     raw_output = self.__module_avail_output().decode(""utf-8"")     for line in StringIO(raw_output):         line = line and line.strip()         <MASK>             continue         line_modules = line.split()         for module in line_modules:             if module.endswith(self.default_indicator):                 module = module[0 : -len(self.default_indicator)].strip()             module_parts = module.split(""/"")             module_version = None             if len(module_parts) == 2:                 module_version = module_parts[1]             module_name = module_parts[0]             yield module_name, module_version","if not line or line . startswith ( ""-"" ) :",if not line :,6.734410772670761,6.734410772670761,0.0
"def _set_trailing_size(self, size):     if self.is_free():         next_chunk = self.next_chunk()         <MASK>             self.state.memory.store(next_chunk.base, size, self.state.arch.bytes)",if next_chunk is not None :,if next_chunk . base != next_chunk . base :,18.798317647335093,18.798317647335093,0.0
"def _execute_for_all_tables(self, app, bind, operation, skip_tables=False):     app = self.get_app(app)     if bind == ""__all__"":         binds = [None] + list(app.config.get(""SQLALCHEMY_BINDS"") or ())     elif isinstance(bind, string_types) or bind is None:         binds = [bind]     else:         binds = bind     for bind in binds:         extra = {}         <MASK>             tables = self.get_tables_for_bind(bind)             extra[""tables""] = tables         op = getattr(self.Model.metadata, operation)         op(bind=self.get_engine(app, bind), **extra)",if not skip_tables :,if not skip_tables :,100.00000000000004,100.00000000000004,1.0
"def getFileName():     extension = "".json""     file = ""%s-stats"" % self.clusterName     counter = 0     while True:         suffix = str(counter).zfill(3) + extension         fullName = os.path.join(self.statsPath, file + suffix)         <MASK>             return fullName         counter += 1",if not os . path . exists ( fullName ) :,if fullName :,2.7574525891364066,0.0,0.0
def logic():     # direction     if goRight == ACTIVE:         dir.next = DirType.RIGHT         run.next = True     elif goLeft == ACTIVE:         dir.next = DirType.LEFT         run.next = True     # stop     if stop == ACTIVE:         run.next = False     # counter action     if run:         <MASK>             q.next[4:1] = q[3:]             q.next[0] = not q[3]         else:             q.next[3:] = q[4:1]             q.next[3] = not q[0],if dir == DirType . LEFT :,if dir . next == DirType . LEFT :,58.14307369682194,58.14307369682194,0.0
"def test_broadcast(self):     """"""Test example broadcast functionality.""""""     self.create_lang_connection(""1000000000"", ""en"")     self.create_lang_connection(""1000000001"", ""en"")     self.create_lang_connection(""1000000002"", ""en"")     self.create_lang_connection(""1000000003"", ""es"")     self.create_lang_connection(""1000000004"", ""es"")     app.lang_broadcast()     self.assertEqual(2, len(self.outbound))     for message in self.outbound:         if message.text == ""hello"":             self.assertEqual(3, len(message.connections))         <MASK>             self.assertEqual(2, len(message.connections))","elif message . text == ""hola"" :","if message . text == ""hello"" :",58.14307369682194,58.14307369682194,0.0
"def get_ovf_env(dirname):     env_names = (""ovf-env.xml"", ""ovf_env.xml"", ""OVF_ENV.XML"", ""OVF-ENV.XML"")     for fname in env_names:         full_fn = os.path.join(dirname, fname)         <MASK>             try:                 contents = util.load_file(full_fn)                 return (fname, contents)             except Exception:                 util.logexc(LOG, ""Failed loading ovf file %s"", full_fn)     return (None, False)",if os . path . isfile ( full_fn ) :,if os . path . isfile ( full_fn ) :,100.00000000000004,100.00000000000004,1.0
"def _calc_offsets_children(self, offset, is_last):     if self.elems:         elem_last = self.elems[-1]         for elem in self.elems:             offset = elem._calc_offsets(offset, (elem is elem_last))         offset += _BLOCK_SENTINEL_LENGTH     elif not self.props or self.id in _ELEMS_ID_ALWAYS_BLOCK_SENTINEL:         <MASK>             offset += _BLOCK_SENTINEL_LENGTH     return offset",if not is_last :,if not is_last :,100.00000000000004,100.00000000000004,1.0
"def publish_state(cls, payload, state):     try:         <MASK>             if state == action_constants.LIVEACTION_STATUS_REQUESTED:                 cls.process(payload)             else:                 worker.get_worker().process(payload)     except Exception:         traceback.print_exc()         print(payload)","if isinstance ( payload , LiveActionDB ) :",if state == action_constants . LIVEACTION_STATUS_REQUESTED :,3.377156414337854,3.377156414337854,0.0
"def log_predictive_density(self, x_test, y_test, Y_metadata=None):     if isinstance(x_test, list):         x_test, y_test, ind = util.multioutput.build_XY(x_test, y_test)         <MASK>             Y_metadata = {""output_index"": ind, ""trials"": np.ones(ind.shape)}     return super(MultioutputGP, self).log_predictive_density(x_test, y_test, Y_metadata)",if Y_metadata is None :,if Y_metadata is None :,100.00000000000004,100.00000000000004,1.0
"def minimalBases(classes):     """"""Reduce a list of base classes to its ordered minimum equivalent""""""     if not __python3:  # pragma: no cover         classes = [c for c in classes if c is not ClassType]     candidates = []     for m in classes:         for n in classes:             <MASK>                 break         else:             # m has no subclasses in 'classes'             if m in candidates:                 candidates.remove(m)  # ensure that we're later in the list             candidates.append(m)     return candidates","if issubclass ( n , m ) and m is not n :",if n == m :,3.57451796074295,3.57451796074295,0.0
"def apply(self, operations, rotations=None, **kwargs):     rotations = rotations or []     # apply the circuit operations     for i, operation in enumerate(operations):         <MASK>             raise DeviceError(                 ""Operation {} cannot be used after other Operations have already been applied ""                 ""on a {} device."".format(operation.name, self.short_name)             )     for operation in operations:         self._apply_operation(operation)     # store the pre-rotated state     self._pre_rotated_state = self._state     # apply the circuit rotations     for operation in rotations:         self._apply_operation(operation)","if i > 0 and isinstance ( operation , ( QubitStateVector , BasisState ) ) :",if i == self . max_size :,5.097103985522972,5.097103985522972,0.0
"def __str__(self):     txt = str(self._called)     if self.call_gas or self.call_value:         gas = f""gas: {self.call_gas}"" if self.call_gas else """"         value = f""value: {self.call_value}"" if self.call_value else """"         salt = f""salt: {self.call_salt}"" if self.call_salt else """"         <MASK>             options = [gas, value, salt]             txt += ""{"" + "","".join([o for o in options if o != """"]) + ""}""     return txt + ""("" + "","".join([str(a) for a in self._arguments]) + "")""",if gas or value or salt :,if self . call_salt :,14.535768424205482,14.535768424205482,0.0
"def pop(self):     """"""Pop a nonterminal.  (Internal)""""""     popdfa, popstate, popnode = self.stack.pop()     newnode = self.convert(self.grammar, popnode)     if newnode is not None:         <MASK>             dfa, state, node = self.stack[-1]             node.children.append(newnode)         else:             self.rootnode = newnode",if self . stack :,if state is not None :,9.652434877402245,9.652434877402245,0.0
"def pollpacket(self, wait):     self._stage0()     if len(self.buffer) < self.bufneed:         r, w, x = select.select([self.sock.fileno()], [], [], wait)         <MASK>             return None         try:             s = self.sock.recv(BUFSIZE)         except socket.error:             raise EOFError         if len(s) == 0:             raise EOFError         self.buffer += s         self._stage0()     return self._stage1()",if len ( r ) == 0 :,if r == 0 :,34.1077254951379,34.1077254951379,0.0
"def increaseToolReach(self):     if self.draggingFace is not None:         d = (1, -1)[self.draggingFace & 1]         <MASK>  # xxxxx y             d = -d         self.draggingY += d         x, y, z = self.editor.mainViewport.cameraPosition         pos = [x, y, z]         pos[self.draggingFace >> 1] += d         self.editor.mainViewport.cameraPosition = tuple(pos)     else:         self.cloneCameraDistance = self.editor._incrementReach(self.cloneCameraDistance)     return True",if self . draggingFace >> 1 != 1 :,if self . cloneCameraDistance == 0 :,16.0529461904344,16.0529461904344,0.0
"def selectionToChunks(self, remove=False, add=False):     box = self.selectionBox()     if box:         if box == self.level.bounds:             self.selectedChunks = set(self.level.allChunks)             return         selectedChunks = self.selectedChunks         boxedChunks = set(box.chunkPositions)         if boxedChunks.issubset(selectedChunks):             remove = True         <MASK>             selectedChunks.difference_update(boxedChunks)         else:             selectedChunks.update(boxedChunks)     self.selectionTool.selectNone()",if remove and not add :,if remove :,23.174952587773145,0.0,0.0
"def __init__(self, *args, **kwargs):     super(ProjectForm, self).__init__(*args, **kwargs)     if self.instance.id:         <MASK>             self.fields[""localfiletype""].widget.attrs[""disabled""] = True             self.fields[""localfiletype""].required = False         if (             self.instance.treestyle != ""auto""             and self.instance.translationproject_set.count()             and self.instance.treestyle == self.instance._detect_treestyle()         ):             self.fields[""treestyle""].widget.attrs[""disabled""] = True             self.fields[""treestyle""].required = False",if Store . objects . filter ( translation_project__project = self . instance ) . count ( ) :,"if self . instance . localfiletype != ""auto"" :",6.229699880447543,6.229699880447543,0.0
"def _infer_return_type(*args):     """"""Look at the type of all args and divine their implied return type.""""""     return_type = None     for arg in args:         if arg is None:             continue         if isinstance(arg, bytes):             <MASK>                 raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")             return_type = bytes         else:             if return_type is bytes:                 raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")             return_type = str     if return_type is None:         return str  # tempfile APIs return a str by default.     return return_type",if return_type is str :,if return_type is None :,64.34588841607616,64.34588841607616,0.0
"def deleteDuplicates(gadgets, callback=None):     toReturn = []     inst = set()     count = 0     added = False     len_gadgets = len(gadgets)     for i, gadget in enumerate(gadgets):         inst.add(gadget._gadget)         if len(inst) > count:             count = len(inst)             toReturn.append(gadget)             added = True         <MASK>             callback(gadget, added, float(i + 1) / (len_gadgets))             added = False     return toReturn",if callback :,if callback :,100.00000000000004,0.0,1.0
"def send_all(self, data: bytes):     with self._conflict_detector:         <MASK>             raise _core.ClosedResourceError(""this pipe is already closed"")         if not data:             await _core.checkpoint()             return         try:             written = await _core.write_overlapped(self._handle_holder.handle, data)         except BrokenPipeError as ex:             raise _core.BrokenResourceError from ex         # By my reading of MSDN, this assert is guaranteed to pass so long         # as the pipe isn't in nonblocking mode, but... let's just         # double-check.         assert written == len(data)",if self . _handle_holder . closed :,if self . _handle_holder . handle is None :,63.15552371794033,63.15552371794033,0.0
"def setup_parameter_node(self, param_node):     if param_node.bl_idname == ""SvNumberNode"":         <MASK>             value = self.sv_get()[0][0]             print(""V"", value)             if isinstance(value, int):                 param_node.selected_mode = ""int""                 param_node.int_ = value             elif isinstance(value, float):                 param_node.selected_mode = ""float""                 param_node.float_ = value",if self . use_prop or self . get_prop_name ( ) :,if self . sv_get ( ) :,13.185679291149079,13.185679291149079,0.0
"def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map):     active_inst_idx_list = []     for inst_idx, inst_position in inst_idx_to_position_map.items():         is_inst_complete = inst_beams[inst_idx].advance(word_prob[inst_position])         <MASK>             active_inst_idx_list += [inst_idx]     return active_inst_idx_list",if not is_inst_complete :,if is_inst_complete :,72.89545183625967,72.89545183625967,0.0
"def compare_member_req_resp_without_key(self, request, response):     for user_response in resp_json(response)[""data""]:         for user_request in request:             <MASK>                 assert user_request[""role""] == user_response[""role""]","if user_request [ ""user_id"" ] == user_response [ ""user_id"" ] :",if user_response :,1.330174052972738,1.330174052972738,0.0
"def __init__(self, dir):     self.module_names = set()     for name in os.listdir(dir):         if name.endswith("".py""):             self.module_names.add(name[:-3])         <MASK>             self.module_names.add(name)","elif ""."" not in name :","if name . endswith ( "".py"" ) :",9.425159511373677,9.425159511373677,0.0
"def _read_filter(self, data):     if data:         if self.expected_inner_sha256:             self.inner_sha.update(data)         <MASK>             self.inner_md5.update(data)     return data",if self . expected_inner_md5sum :,if self . expected_inner_md5 :,75.06238537503395,75.06238537503395,0.0
"def _p_basicstr_content(s, content=_basicstr_re):     res = []     while True:         res.append(s.expect_re(content).group(0))         <MASK>             break         if s.consume_re(_newline_esc_re):             pass         elif s.consume_re(_short_uni_re) or s.consume_re(_long_uni_re):             res.append(_chr(int(s.last().group(1), 16)))         else:             s.expect_re(_escapes_re)             res.append(_escapes[s.last().group(0)])     return """".join(res)","if not s . consume ( ""\\"" ) :",if s . consume_re ( _newline_esc_re ) :,13.380161378318954,13.380161378318954,0.0
"def process_response(self, request, response):     if (         response.status_code == 404         and request.path_info.endswith(""/"")         and not is_valid_path(request.path_info)         and is_valid_path(request.path_info[:-1])     ):         # Use request.path because we munged app/locale in path_info.         newurl = request.path[:-1]         <MASK>             with safe_query_string(request):                 newurl += ""?"" + request.META.get(""QUERY_STRING"", """")         return HttpResponsePermanentRedirect(newurl)     else:         return response",if request . GET :,"if request . META . get ( ""QUERY_STRING"" , """" ) in request . META :",7.2643397661757225,7.2643397661757225,0.0
"def convertDict(obj):     obj = dict(obj)     for k, v in obj.items():         del obj[k]         <MASK>             k = dumps(k)             # Keep track of which keys need to be decoded when loading.             if Types.KEYS not in obj:                 obj[Types.KEYS] = []             obj[Types.KEYS].append(k)         obj[k] = convertObjects(v)     return obj","if not ( isinstance ( k , str ) or isinstance ( k , unicode ) ) :","if isinstance ( v , dict ) :",4.952533158709239,4.952533158709239,0.0
"def __repr__(self):     if self._in_repr:         return ""<recursion>""     try:         self._in_repr = True         <MASK>             status = ""computed, ""             if self.error() is None:                 if self.value() is self:                     status += ""= self""                 else:                     status += ""= "" + repr(self.value())             else:                 status += ""error = "" + repr(self.error())         else:             status = ""isn't computed""         return ""%s (%s)"" % (type(self), status)     finally:         self._in_repr = False",if self . is_computed ( ) :,if self . computed :,20.300727612812874,20.300727612812874,0.0
"def allocate_network(ipv=""ipv4""):     global dtcd_uuid     global network_pool     global allocations     network = None     try:         cx = httplib.HTTPConnection(""localhost:7623"")         cx.request(""POST"", ""/v1/network/%s/"" % ipv, body=dtcd_uuid)         resp = cx.getresponse()         <MASK>             network = netaddr.IPNetwork(resp.read().decode(""utf-8""))         cx.close()     except Exception:         pass     if network is None:         network = network_pool[ipv].pop()         allocations[network] = True     return network",if resp . status == 200 :,if resp is not None :,12.872632311973014,12.872632311973014,0.0
"def change_args_to_dict(string):     if string is None:         return None     ans = []     strings = string.split(""\n"")     ind = 1     start = 0     while ind <= len(strings):         if ind < len(strings) and strings[ind].startswith("" ""):             ind += 1         else:             if start < ind:                 ans.append(""\n"".join(strings[start:ind]))             start = ind             ind += 1     d = {}     for line in ans:         <MASK>             lines = line.split("":"")             d[lines[0]] = lines[1].strip()     return d","if "":"" in line and len ( line ) > 0 :","if line . startswith ( "" "" ) :",4.44948101053922,4.44948101053922,0.0
"def kill_members(members, sig, hosts=nodes):     for member in sorted(members):         try:             <MASK>                 print(""killing %s"" % member)             proc = hosts[member][""proc""]             # Not sure if cygwin makes sense here...             if sys.platform in (""win32"", ""cygwin""):                 os.kill(proc.pid, signal.CTRL_C_EVENT)             else:                 os.kill(proc.pid, sig)         except OSError:             if ha_tools_debug:                 print(""%s already dead?"" % member)",if ha_tools_debug :,if ha_tools_debug :,100.00000000000004,100.00000000000004,1.0
"def check(self):     for path in self.paths:         response = self.http_request(             method=""GET"",             path=path,         )         <MASK>             continue         if any(             map(                 lambda x: x in response.text,                 [                     ""report.db.server.name"",                     ""report.db.server.sa.pass"",                     ""report.db.server.user.pass"",                 ],             )         ):             self.valid = path             return True  # target is vulnerable     return False  # target not vulnerable",if response is None :,if response is None :,100.00000000000004,100.00000000000004,1.0
"def get_to_download_runs_ids(session, headers):     last_date = 0     result = []     while 1:         r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers)         if r.ok:             run_logs = r.json()[""data""][""records""]             result.extend([i[""logs""][0][""stats""][""id""] for i in run_logs])             last_date = r.json()[""data""][""lastTimestamp""]             since_time = datetime.utcfromtimestamp(last_date / 1000)             print(f""pares keep ids data since {since_time}"")             time.sleep(1)  # spider rule             <MASK>                 break     return result",if not last_date :,if not r . status_code :,13.134549472120788,13.134549472120788,0.0
"def button_press_cb(self, tdw, event):     self._update_zone_and_cursors(tdw, event.x, event.y)     if self._zone in (_EditZone.CREATE_FRAME, _EditZone.REMOVE_FRAME):         button = event.button         <MASK>             self._click_info = (button, self._zone)             return False     return super(FrameEditMode, self).button_press_cb(tdw, event)",if button == 1 and event . type == Gdk . EventType . BUTTON_PRESS :,if button is not None :,2.058073185415509,2.058073185415509,0.0
"def first_timestep():     assignment = self.has_previous.assign(         value=tf_util.constant(value=True, dtype=""bool""), read_value=False     )     with tf.control_dependencies(control_inputs=(assignment,)):         <MASK>             current = x         else:             current = tf.expand_dims(input=x, axis=(self.axis + 1))         multiples = tuple(             self.length if dims == self.axis + 1 else 1             for dims in range(self.output_spec().rank + 1)         )         return tf.tile(input=current, multiples=multiples)",if self . concatenate :,if self . has_previous :,26.269098944241588,26.269098944241588,0.0
"def main() -> None:     onefuzz = Onefuzz()     jobs = onefuzz.jobs.list()     for job in jobs:         print(             ""job:"",             str(job.job_id)[:8],             "":"".join([job.config.project, job.config.name, job.config.build]),         )         for task in onefuzz.tasks.list(job_id=job.job_id):             <MASK>                 continue             print(                 ""    "",                 str(task.task_id)[:8],                 task.config.task.type,                 task.config.task.target_exe,             )","if task . state in [ ""stopped"" , ""stopping"" ] :",if not task . config . task :,5.4752948205155585,5.4752948205155585,0.0
"def update_stack(self, full_name, template_url, parameters, tags):     """"""Updates an existing stack in CloudFormation.""""""     try:         logger.info(""Attempting to update stack %s."", full_name)         self.conn.cloudformation.update_stack(             full_name,             template_url=template_url,             parameters=parameters,             tags=tags,             capabilities=[""CAPABILITY_IAM""],         )         return SUBMITTED     except BotoServerError as e:         <MASK>             logger.info(""Stack %s did not change, not updating."", full_name)             return SKIPPED         raise","if ""No updates are to be performed."" in e . message :",if e . status == 404 :,5.4752948205155585,5.4752948205155585,0.0
"def header_tag_files(env, files, legal_header, script_files=False):     """"""Apply the legal_header to the list of files""""""     try:         import apply_legal_header     except:         xbc.cdie(""XED ERROR: mfile.py could not find scripts directory"")     for g in files:         print(""G: "", g)         for f in mbuild.glob(g):             print(""F: "", f)             <MASK>                 apply_legal_header.apply_header_to_data_file(legal_header, f)             else:                 apply_legal_header.apply_header_to_source_file(legal_header, f)",if script_files :,if script_files :,100.00000000000004,100.00000000000004,1.0
"def cleanDataCmd(cmd):     newcmd = ""AbracadabrA ** <?php ""     if cmd[:6] != ""php://"":         if reverseConn not in cmd:             cmds = cmd.split(""&"")             for c in cmds:                 <MASK>                     newcmd += ""system('%s');"" % c         else:             b64cmd = base64.b64encode(cmd)             newcmd += ""system(base64_decode('%s'));"" % b64cmd     else:         newcmd += cmd[6:]     newcmd += ""?> **""     return newcmd",if len ( c ) > 0 :,"if c != ""php://"" :",4.9323515694897075,4.9323515694897075,0.0
"def test_form(self):     n_qubits = 6     random_operator = get_fermion_operator(random_interaction_operator(n_qubits))     chemist_operator = chemist_ordered(random_operator)     for term, _ in chemist_operator.terms.items():         <MASK>             pass         else:             self.assertTrue(term[0][1])             self.assertTrue(term[2][1])             self.assertFalse(term[1][1])             self.assertFalse(term[3][1])             self.assertTrue(term[0][0] > term[2][0])             self.assertTrue(term[1][0] > term[3][0])",if len ( term ) == 2 or not len ( term ) :,if term [ 0 ] == term [ 1 ] :,6.942047568179753,6.942047568179753,0.0
"def do(server, handler, config, modargs):     data = []     clients = server.get_clients(handler.default_filter)     if not clients:         return     for client in clients:         tags = config.tags(client.node())         <MASK>             tags.remove(*modargs.remove)         if modargs.add:             tags.add(*modargs.add)         data.append({""ID"": client.node(), ""TAGS"": tags})     config.save(project=modargs.write_project, user=modargs.write_user)     handler.display(Table(data))",if modargs . remove :,if modargs . remove :,100.00000000000004,100.00000000000004,1.0
"def validate(self):     if self.data.get(""state"") == ""enabled"":         <MASK>             raise PolicyValidationError(                 (                     ""redshift logging enablement requires `bucket` ""                     ""and `prefix` specification on %s"" % (self.manager.data,)                 )             )     return self","if ""bucket"" not in self . data :","if not self . data [ ""bucket"" ] :",25.965358893403383,25.965358893403383,0.0
"def renumber(self, x1, y1, x2, y2, dx, dy):     out = []     for part in re.split(""(\w+)"", self.formula):         m = re.match(""^([A-Z]+)([1-9][0-9]*)$"", part)         <MASK>             sx, sy = m.groups()             x = colname2num(sx)             y = int(sy)             if x1 <= x <= x2 and y1 <= y <= y2:                 part = cellname(x + dx, y + dy)         out.append(part)     return FormulaCell("""".join(out), self.fmt, self.alignment)",if m is not None :,if m :,23.174952587773145,0.0,0.0
"def update_sysconfig_file(fn, adjustments, allow_empty=False):     if not adjustments:         return     (exists, contents) = read_sysconfig_file(fn)     updated_am = 0     for (k, v) in adjustments.items():         if v is None:             continue         v = str(v)         if len(v) == 0 and not allow_empty:             continue         contents[k] = v         updated_am += 1     if updated_am:         lines = [             str(contents),         ]         <MASK>             lines.insert(0, util.make_header())         util.write_file(fn, ""\n"".join(lines) + ""\n"", 0o644)",if not exists :,if exists :,45.13864405503391,0.0,0.0
"def getElement(self, aboutUri, namespace, name):     for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, ""Description""):         if desc.getAttributeNS(RDF_NAMESPACE, ""about"") == aboutUri:             attr = desc.getAttributeNodeNS(namespace, name)             <MASK>                 yield attr             for element in desc.getElementsByTagNameNS(namespace, name):                 yield element",if attr != None :,if attr :,23.174952587773145,0.0,0.0
"def get_store_name_from_connection_string(connection_string):     if is_valid_connection_string(connection_string):         segments = dict(seg.split(""="", 1) for seg in connection_string.split("";""))         endpoint = segments.get(""Endpoint"")         <MASK>             return endpoint.split(""//"")[1].split(""."")[0]     return None",if endpoint :,if endpoint :,100.00000000000004,0.0,1.0
"def insertLoopTemplate(self, layout):     col = layout.column(align=True)     for socket in self.activeNode.outputs:         <MASK>             props = col.operator(                 ""an.insert_loop_for_iterator"",                 text=""Loop through {}"".format(repr(socket.getDisplayedName())),                 icon=""MOD_ARRAY"",             )             props.nodeIdentifier = self.activeNode.identifier             props.socketIndex = socket.getIndex()",if not socket . hide and isList ( socket . bl_idname ) :,if not self . activeNode . isLoop :,5.789419402078114,5.789419402078114,0.0
"def do_task(self, task):     self.running_task += 1     result = yield gen.Task(self.fetcher.fetch, task)     type, task, response = result.args     self.processor.on_task(task, response)     # do with message     while not self.processor.inqueue.empty():         _task, _response = self.processor.inqueue.get()         self.processor.on_task(_task, _response)     # do with results     while not self.processor.result_queue.empty():         _task, _result = self.processor.result_queue.get()         <MASK>             self.result_worker.on_result(_task, _result)     self.running_task -= 1",if self . result_worker :,"if type == ""result"" :",7.267884212102741,7.267884212102741,0.0
"def _parse_config_result(data):     command_list = "" ; "".join([x.strip() for x in data[0]])     config_result = data[1]     if isinstance(config_result, list):         result = """"         <MASK>             for key in config_result[0]:                 result += config_result[0][key]             config_result = result         else:             config_result = config_result[0]     return [command_list, config_result]","if isinstance ( config_result [ 0 ] , dict ) :",if len ( config_result [ 0 ] ) == 1 :,47.27526606311562,47.27526606311562,0.0
"def load_api_handler(self, mod_name):     for name, hdl in API_HANDLERS:         name = name.lower()         <MASK>             handler = self.mods.get(name)             if not handler:                 handler = hdl(self.emu)                 self.mods.update({name: handler})             return handler     return None",if mod_name and name == mod_name . lower ( ) :,if mod_name == self . mod_name :,28.752045144349633,28.752045144349633,0.0
def heal(self):     if not self.doctors:         return     proc_ids = self._get_process_ids()     for proc_id in proc_ids:         # get proc every time for latest state         proc = PipelineProcess.objects.get(id=proc_id)         if not proc.is_alive or proc.is_frozen:             continue         for dr in self.doctors:             <MASK>                 dr.cure(proc)                 break,if dr . confirm ( proc ) :,if dr . is_alive :,22.772101321113862,22.772101321113862,0.0
"def __new__(cls, *args, **kwargs):     if len(args) == 1:         if len(kwargs):             raise ValueError(                 ""You can either use {} with one positional argument or with keyword arguments, not both."".format(                     cls.__name__                 )             )         <MASK>             return super().__new__(cls)         if isinstance(args[0], cls):             return cls     return super().__new__(cls, *args, **kwargs)",if not args [ 0 ] :,if args [ 0 ] == cls :,33.03164318013809,33.03164318013809,0.0
"def __lt__(self, other):     # 0: clock 1: timestamp 3: process id     try:         A, B = self[0], other[0]         # uses logical clock value first         <MASK>  # use logical clock if available             if A == B:  # equal clocks use lower process id                 return self[2] < other[2]             return A < B         return self[1] < other[1]  # ... or use timestamp     except IndexError:         return NotImplemented",if A and B :,if A == B :,22.957488466614336,22.957488466614336,0.0
"def _get_client(rp_mapping, resource_provider):     for key, value in rp_mapping.items():         if str.lower(key) == str.lower(resource_provider):             <MASK>                 return GeneralPrivateEndpointClient(                     key,                     value[""api_version""],                     value[""support_list_or_not""],                     value[""resource_get_api_version""],                 )             return value()     raise CLIError(         ""Resource type must be one of {}"".format("", "".join(rp_mapping.keys()))     )","if isinstance ( value , dict ) :","if value [ ""api_version"" ] == str . lower ( resource_provider ) :",4.814971807094068,4.814971807094068,0.0
"def test_progressbar_format_pos(runner, pos, length):     with _create_progress(length, length_known=length != 0, pos=pos) as progress:         result = progress.format_pos()         <MASK>             assert result == f""{pos}/{length}""         else:             assert result == str(pos)",if progress . length_known :,"if result in [ 0 , 1 , 2 ] :",4.456882760699063,4.456882760699063,0.0
"def optimize(self, graph: Graph):     MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE     flag_changed = False     for v in traverse.listup_variables(graph):         if not Placeholder.check_resolved(v.size):             continue         height, width = TextureShape.get(v)         if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE:             continue         <MASK>             flag_changed = True             v.attributes.add(SplitTarget())     return graph, flag_changed",if not v . has_attribute ( SplitTarget ) :,if not flag_changed :,8.389861810900507,8.389861810900507,0.0
"def ant_map(m):     tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0]))     players = {}     for row in m:         tmp += ""m ""         for col in row:             <MASK>                 tmp += "".""             elif col == BARRIER:                 tmp += ""%""             elif col == FOOD:                 tmp += ""*""             elif col == UNSEEN:                 tmp += ""?""             else:                 players[col] = True                 tmp += chr(col + 97)         tmp += ""\n""     tmp = (""players %s\n"" % len(players)) + tmp     return tmp",if col == LAND :,if col == BARRIER :,53.7284965911771,53.7284965911771,0.0
"def reset(self):     logger.debug(""Arctic.reset()"")     with self._lock:         if self.__conn is not None:             self.__conn.close()             self.__conn = None         for _, l in self._library_cache.items():             <MASK>                 logger.debug(""Library reset() %s"" % l)                 l._reset()  # the existence of _reset() is not guaranteed/enforced, it also triggers re-auth","if hasattr ( l , ""_reset"" ) and callable ( l . _reset ) :",if l . _reset ( ) :,11.17024658770518,11.17024658770518,0.0
"def add_cand_to_check(cands):     for cand in cands:         x = cand.creator         <MASK>             continue         if x not in fan_out:             # `len(fan_out)` is in order to avoid comparing `x`             heapq.heappush(cand_funcs, (-x.rank, len(fan_out), x))         fan_out[x] += 1",if x is None :,if x is None :,100.00000000000004,100.00000000000004,1.0
"def on_task_modify(self, task, config):     for entry in task.entries:         <MASK>             size = entry[""torrent""].size / 1024 / 1024             log.debug(""%s size: %s MB"" % (entry[""title""], size))             entry[""content_size""] = size","if ""torrent"" in entry :","if entry [ ""title"" ] == ""content_size"" :",3.9297193407553004,3.9297193407553004,0.0
"def get_measurements(self, pipeline, object_name, category):     if self.get_categories(pipeline, object_name) == [category]:         results = []         if self.do_corr_and_slope:             if object_name == ""Image"":                 results += [""Correlation"", ""Slope""]             else:                 results += [""Correlation""]         if self.do_overlap:             results += [""Overlap"", ""K""]         if self.do_manders:             results += [""Manders""]         if self.do_rwc:             results += [""RWC""]         <MASK>             results += [""Costes""]         return results     return []",if self . do_costes :,if self . do_costes :,100.00000000000004,100.00000000000004,1.0
"def create_root(cls, site=None, title=""Root"", request=None, **kwargs):     if not site:         site = Site.objects.get_current()     root_nodes = cls.objects.root_nodes().filter(site=site)     if not root_nodes:         article = Article()         revision = ArticleRevision(title=title, **kwargs)         <MASK>             revision.set_from_request(request)         article.add_revision(revision, save=True)         article.save()         root = cls.objects.create(site=site, article=article)         article.add_object_relation(root)     else:         root = root_nodes[0]     return root",if request :,if request :,100.00000000000004,0.0,1.0
"def get(self, key):     filename = self._get_filename(key)     try:         with open(filename, ""rb"") as f:             pickle_time = pickle.load(f)             <MASK>                 return pickle.load(f)             else:                 os.remove(filename)                 return None     except (IOError, OSError, pickle.PickleError):         return None",if pickle_time == 0 or pickle_time >= time ( ) :,if pickle_time is not None :,11.867840505544883,11.867840505544883,0.0
"def build_message(self, options, target):     message = multipart.MIMEMultipart()     for name, value in list(options.items()):         <MASK>             self.add_body(message, value)         elif name == ""EMAIL_ATTACHMENT"":             self.add_attachment(message, value)         else:  # From, To, Subject, etc.             self.set_option(message, name, value, target)     return message","if name == ""EMAIL_BODY"" :","if name == ""BODY"" :",53.849523560640876,53.849523560640876,0.0
"def updateVar(name, data, mode=None):     if mode:         <MASK>             core.config.globalVariables[name].append(data)         elif mode == ""add"":             core.config.globalVariables[name].add(data)     else:         core.config.globalVariables[name] = data","if mode == ""append"" :","if mode == ""append"" :",100.00000000000004,100.00000000000004,1.0
"def insert_errors(     el,     errors,     form_id=None,     form_index=None,     error_class=""error"",     error_creator=default_error_creator, ):     el = _find_form(el, form_id=form_id, form_index=form_index)     for name, error in errors.items():         <MASK>             continue         for error_el, message in _find_elements_for_name(el, name, error):             assert isinstance(message, (basestring, type(None), ElementBase)), (                 ""Bad message: %r"" % message             )             _insert_error(error_el, message, error_class, error_creator)",if error is None :,if not error :,16.37226966703825,16.37226966703825,0.0
"def read(self, item, recursive=False, sort=False):     item = _normalize_path(item)     if item in self._store:         <MASK>             del self._store[item]             raise KeyError(item)         return PathResult(item, value=self._store[item])     else:         return self._read_dir(item, recursive=recursive, sort=sort)",if item in self . _expire_time and self . _expire_time [ item ] < datetime . now ( ) :,if not self . _store [ item ] :,5.699131651662238,5.699131651662238,0.0
"def _stash_splitter(states):     keep, split = [], []     if state_func is not None:         for s in states:             ns = state_func(s)             if isinstance(ns, SimState):                 split.append(ns)             <MASK>                 split.extend(ns)             else:                 split.append(s)     if stash_func is not None:         split = stash_func(states)     if to_stash is not stash:         keep = states     return keep, split","elif isinstance ( ns , ( list , tuple , set ) ) :",if ns is not None :,2.544354209531657,2.544354209531657,0.0
"def run(self):     while self.runflag:         <MASK>             with self.lock:                 tasks = list(self.queue)                 self.queue.clear()             while len(tasks) > 0:                 pathname, remotepath = tasks.pop(0)                 self.bcloud_app.upload_page.add_bg_task(pathname, remotepath)             self.last = time()         else:             sleep(1)",if time ( ) - self . last > 5 and self . qsize ( ) > 0 :,if self . last > self . last :,12.555965752455986,12.555965752455986,0.0
"def _append_patch(self, patch_dir, patch_files):     for patch in patch_files:         <MASK>             tmp = patch             patch = {}             for key in tmp.keys():                 patch[os.path.join(patch_dir, key)] = tmp[key]             self.patches.append(patch)         else:             self.patches.append(os.path.join(patch_dir, patch))",if type ( patch ) is dict :,if os . path . isdir ( patch ) :,17.747405280050266,17.747405280050266,0.0
"def __remote_port(self):     port = 22     if self.git_has_remote:         m = re.match(r""^(.*?)?@([^/:]*):?([0-9]+)?"", self.git_remote.url)         if m:             <MASK>                 port = m.group(3)     return int(port)",if m . group ( 3 ) :,if m . group ( 1 ) :,59.4603557501361,59.4603557501361,0.0
"def _create_or_get_helper(self, infer_mode: Optional[bool] = None, **kwargs) -> Helper:     # Prefer creating a new helper when at least one kwarg is specified.     prefer_new = len(kwargs) > 0     kwargs.update(infer_mode=infer_mode)     is_training = not infer_mode if infer_mode is not None else self.training     helper = self._train_helper if is_training else self._infer_helper     if prefer_new or helper is None:         helper = self.create_helper(**kwargs)         <MASK>             self._train_helper = helper         elif not is_training and self._infer_helper is None:             self._infer_helper = helper     return helper",if is_training and self . _train_helper is None :,if not is_training and self . _infer_helper is None :,66.06328636027618,66.06328636027618,0.0
"def flushChangeClassifications(self, schedulerid, less_than=None):     if less_than is not None:         classifications = self.classifications.setdefault(schedulerid, {})         for changeid in list(classifications):             <MASK>                 del classifications[changeid]     else:         self.classifications[schedulerid] = {}     return defer.succeed(None)",if changeid < less_than :,if changeid < less_than :,100.00000000000004,100.00000000000004,1.0
"def pid_from_name(name):     processes = []     for pid in os.listdir(""/proc""):         try:             pid = int(pid)             pname, cmdline = SunProcess._name_args(pid)             if name in pname:                 return pid             <MASK>                 return pid         except:             pass     raise ProcessException(""No process with such name: %s"" % name)","if name in cmdline . split ( "" "" , 1 ) [ 0 ] :",if cmdline :,0.5208155200691346,0.0,0.0
"def spew():     seenUID = False     start()     for part in query:         <MASK>             seenUID = True         if part.type == ""body"":             yield self.spew_body(part, id, msg, write, flush)         else:             f = getattr(self, ""spew_"" + part.type)             yield f(id, msg, write, flush)         if part is not query[-1]:             space()     if uid and not seenUID:         space()         yield self.spew_uid(id, msg, write, flush)     finish()     flush()","if part . type == ""uid"" :",if uid and not seenUID :,5.484411595600381,5.484411595600381,0.0
"def rx():     while True:         rx_i = rep.recv()         <MASK>             rep.send(b""done"")             break         rep.send(b""i"")","if rx_i == b""1000"" :",if rx_i == 0 :,48.59869096699083,48.59869096699083,0.0
"def test_search_incorrect_base_exception_1(self):     self.connection_1c.bind()     try:         result = self.connection_1c.search(             ""o=nonexistant"", ""(cn=*)"", search_scope=SUBTREE, attributes=[""cn"", ""sn""]         )         <MASK>             _, result = self.connection_1c.get_response(result)         self.fail(""exception not raised"")     except LDAPNoSuchObjectResult:         pass",if not self . connection_1c . strategy . sync :,if result is not None :,3.9297526283216277,3.9297526283216277,0.0
"def value_from_datadict(self, data, files, prefix):     count = int(data[""%s-count"" % prefix])     values_with_indexes = []     for i in range(0, count):         <MASK>             continue         values_with_indexes.append(             (                 int(data[""%s-%d-order"" % (prefix, i)]),                 self.child_block.value_from_datadict(                     data, files, ""%s-%d-value"" % (prefix, i)                 ),             )         )     values_with_indexes.sort()     return [v for (i, v) in values_with_indexes]","if data [ ""%s-%d-deleted"" % ( prefix , i ) ] :",if i >= count :,1.707863452144561,1.707863452144561,0.0
"def _ensure_header_written(self, datasize):     if not self._headerwritten:         if not self._nchannels:             raise Error(""# channels not specified"")         if not self._sampwidth:             raise Error(""sample width not specified"")         <MASK>             raise Error(""sampling rate not specified"")         self._write_header(datasize)",if not self . _framerate :,if not self . _samplingrate :,64.34588841607616,64.34588841607616,0.0
def wait_til_ready(cls):     while True:         now = time.time()         next_iteration = now // 1.0 + 1         <MASK>             break         else:             await cls._clock.run_til(next_iteration)         await asyncio.sleep(1.0),if cls . connector . ready :,if next_iteration > cls . _clock . max_til_time :,6.150343144231885,6.150343144231885,0.0
"def lookup_actions(self, resp):     actions = {}     for action, conditions in self.actions.items():         for condition, opts in conditions:             for key, val in condition:                 <MASK>                     if resp.match(key[:-1], val):                         break                 else:                     if not resp.match(key, val):                         break             else:                 actions[action] = opts     return actions","if key [ - 1 ] == ""!"" :",if key :,3.136388772461349,0.0,0.0
"def close(self, wait=True, abort=False):     """"""Close the socket connection.""""""     if not self.closed and not self.closing:         self.closing = True         self.server._trigger_event(""disconnect"", self.sid, run_async=False)         if not abort:             self.send(packet.Packet(packet.CLOSE))         self.closed = True         self.queue.put(None)         <MASK>             self.queue.join()",if wait :,if wait :,100.00000000000004,0.0,1.0
"def model_parse(self):     for name, submodel in self.model.named_modules():         for op_type in SUPPORTED_OP_TYPE:             <MASK>                 self.target_layer[name] = submodel                 self.already_pruned[name] = 0","if isinstance ( submodel , op_type ) :","if op_type == ""target"" :",17.747405280050266,17.747405280050266,0.0
"def pack_identifier(self):     """"""Return a combined identifier for the whole pack if this has more than one episode.""""""     # Currently only supports ep mode     if self.id_type == ""ep"":         <MASK>             return ""S%02dE%02d-E%02d"" % (                 self.season,                 self.episode,                 self.episode + self.episodes - 1,             )         else:             return self.identifier     else:         return self.identifier",if self . episodes > 1 :,"if self . id_type == ""ep"" :",13.545994273378144,13.545994273378144,0.0
"def on_data(res):     if terminate.is_set():         return     if args.strings and not args.no_content:         if type(res) == tuple:             f, v = res             if type(f) == unicode:                 f = f.encode(""utf-8"")             <MASK>                 v = v.encode(""utf-8"")             self.success(""{}: {}"".format(f, v))         elif not args.content_only:             self.success(res)     else:         self.success(res)",if type ( v ) == unicode :,if type ( v ) == unicode :,100.00000000000004,100.00000000000004,1.0
"def _enable_contours_changed(self, value):     """"""Turns on and off the contours.""""""     if self.module_manager is None:         return     if value:         self.actor.inputs = [self.contour]         <MASK>             self.actor.mapper.scalar_mode = ""use_cell_data""     else:         self.actor.inputs = [self.grid_plane]         self.actor.mapper.scalar_mode = ""default""     self.render()",if self . contour . filled_contours :,if self . grid_plane is None :,20.164945583740657,20.164945583740657,0.0
"def _apply_abs_paths(data, script_dir):     for flag_data in data.values():         <MASK>             continue         default = flag_data.get(""default"")         if (             not default             or not isinstance(default, six.string_types)             or os.path.sep not in default         ):             continue         abs_path = os.path.join(script_dir, default)         if os.path.exists(abs_path):             flag_data[""default""] = abs_path","if not isinstance ( flag_data , dict ) :","if not flag_data [ ""default"" ] :",18.36028134946796,18.36028134946796,0.0
"def button_release(self, mapper):     self.pressed = False     if self.waiting_task and self.active is None and not self.action:         # In HoldModifier, button released before timeout         mapper.cancel_task(self.waiting_task)         self.waiting_task = None         <MASK>             self.normalaction.button_press(mapper)             mapper.schedule(0.02, self.normalaction.button_release)     elif self.active:         # Released held button         self.active.button_release(mapper)         self.active = None",if self . normalaction :,if self . normalaction :,100.00000000000004,100.00000000000004,1.0
"def goToPrevMarkedHeadline(self, event=None):     """"""Select the next marked node.""""""     c = self     p = c.p     if not p:         return     p.moveToThreadBack()     wrapped = False     while 1:         if p and p.isMarked():             break         <MASK>             p.moveToThreadBack()         elif wrapped:             break         else:             wrapped = True             p = c.rootPosition()     if not p:         g.blue(""done"")     c.treeSelectHelper(p)  # Sets focus.",elif p :,if p . isLeft ( ) :,7.809849842300637,7.809849842300637,0.0
"def status(self, name, error=""No matching script logs found""):     with self.script_lock:         if self.script_running and self.script_running[1] == name:             return self.script_running[1:]         <MASK>             return self.script_last[1:]         else:             raise ValueError(error)",elif self . script_last and self . script_last [ 1 ] == name :,if self . script_last :,11.588199822065299,11.588199822065299,0.0
"def _stderr_supports_color():     try:         if hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty():             <MASK>                 curses.setupterm()                 if curses.tigetnum(""colors"") > 0:                     return True             elif colorama:                 if sys.stderr is getattr(                     colorama.initialise, ""wrapped_stderr"", object()                 ):                     return True     except Exception:         # Very broad exception handling because it's always better to         # fall back to non-colored logs than to break at startup.         pass     return False",if curses :,if sys . stderr . isatty ( ) :,5.669791110976001,5.669791110976001,0.0
"def main():     configFilename = ""twitterbot.ini""     if sys.argv[1:]:         configFilename = sys.argv[1]     try:         <MASK>             raise Exception()         load_config(configFilename)     except Exception as e:         print(""Error while loading ini file %s"" % (configFilename), file=sys.stderr)         print(e, file=sys.stderr)         print(__doc__, file=sys.stderr)         sys.exit(1)     bot = TwitterBot(configFilename)     return bot.run()",if not os . path . exists ( configFilename ) :,if not configFilename :,6.6019821735025035,6.6019821735025035,0.0
def safe_to_kill(request):     if os.path.exists(DRAIN_FILE):         with open(DRAIN_FILE) as f:             dt = datetime.datetime.fromtimestamp(float(f.read()))             delta = datetime.datetime.now() - dt             <MASK>                 return Response(status_int=200)             else:                 return Response(status_int=400)     else:         return Response(status_int=400),if delta . seconds > 2 :,if delta > 0 :,17.030578356760866,17.030578356760866,0.0
"def get_class_name(item):     class_name, module_name = None, None     for parent in reversed(item.listchain()):         <MASK>             class_name = parent.name         elif isinstance(parent, pytest.Module):             module_name = parent.module.__name__             break     # heuristic:     # - better to group gpu and task tests, since tests from those modules     #   are likely to share caching more     # - split up the rest by class name because slow tests tend to be in     #   the same module     if class_name and "".tasks."" not in module_name:         return ""{}.{}"".format(module_name, class_name)     else:         return module_name","if isinstance ( parent , pytest . Class ) :","if isinstance ( parent , pytest . Class ) :",100.00000000000004,100.00000000000004,1.0
"def getAllFitsLite():     fits = eos.db.getFitListLite()     shipMap = {f.shipID: None for f in fits}     for shipID in shipMap:         ship = eos.db.getItem(shipID)         <MASK>             shipMap[shipID] = (ship.name, ship.getShortName())     fitsToPurge = set()     for fit in fits:         try:             fit.shipName, fit.shipNameShort = shipMap[fit.shipID]         except (KeyError, TypeError):             fitsToPurge.add(fit)     for fit in fitsToPurge:         fits.remove(fit)     return fits",if ship is not None :,if ship :,23.174952587773145,0.0,0.0
"def _process(self, event_data):     self.machine.callbacks(self.machine.prepare_event, event_data)     _LOGGER.debug(         ""%sExecuted machine preparation callbacks before conditions."", self.machine.name     )     try:         for trans in self.transitions[event_data.state.name]:             event_data.transition = trans             <MASK>                 event_data.result = True                 break     except Exception as err:         event_data.error = err         raise     finally:         self.machine.callbacks(self.machine.finalize_event, event_data)         _LOGGER.debug(""%sExecuted machine finalize callbacks"", self.machine.name)     return event_data.result",if trans . execute ( event_data ) :,if not trans . is_active ( ) :,13.650604313545333,13.650604313545333,0.0
"def fetch_comments(self, force=False, limit=None):     comments = []     if (force is True) or (self.badges[""comments""] > 0):         query_params = {""filter"": ""commentCard,copyCommentCard""}         <MASK>             query_params[""limit""] = limit         comments = self.client.fetch_json(             ""/cards/"" + self.id + ""/actions"", query_params=query_params         )         return sorted(comments, key=lambda comment: comment[""date""])     return comments",if limit is not None :,if limit is not None :,100.00000000000004,100.00000000000004,1.0
"def get_changed(self):     if self._is_expression():         result = self._get_node_text(self.ast)         if result == self.source:             return None         return result     else:         collector = codeanalyze.ChangeCollector(self.source)         last_end = -1         for match in self.matches:             start, end = match.get_region()             if start < last_end:                 <MASK>                     continue             last_end = end             replacement = self._get_matched_text(match)             collector.add_change(start, end, replacement)         return collector.get_changed()",if not self . _is_expression ( ) :,if end > last_end :,4.880869806051147,4.880869806051147,0.0
"def _replace_home(x):     if xp.ON_WINDOWS:         home = (             builtins.__xonsh__.env[""HOMEDRIVE""] + builtins.__xonsh__.env[""HOMEPATH""][0]         )         <MASK>             x = x.replace(home, ""~"", 1)         if builtins.__xonsh__.env.get(""FORCE_POSIX_PATHS""):             x = x.replace(os.sep, os.altsep)         return x     else:         home = builtins.__xonsh__.env[""HOME""]         if x.startswith(home):             x = x.replace(home, ""~"", 1)         return x",if x . startswith ( home ) :,if x . startswith ( home ) :,100.00000000000004,100.00000000000004,1.0
"def project_review(plans):     for plan in plans:         print(""Inspecting {} plan"".format(plan))         branches = get_branches_from_plan(plan)         for branch in branches:             build_results = get_results_from_branch(branch)             for build in build_results:                 build_key = build.get(""buildResultKey"") or None                 print(""Inspecting build - {}"".format(build_key))                 <MASK>                     for status in STATUS_CLEANED_RESULTS:                         remove_build_result(build_key=build_key, status=status)",if build_key :,if build_key :,100.00000000000004,100.00000000000004,1.0
"def _check_for_batch_clashes(xs):     """"""Check that batch names do not overlap with sample names.""""""     names = set([x[""description""] for x in xs])     dups = set([])     for x in xs:         batches = tz.get_in((""metadata"", ""batch""), x)         if batches:             <MASK>                 batches = [batches]             for batch in batches:                 if batch in names:                     dups.add(batch)     if len(dups) > 0:         raise ValueError(             ""Batch names must be unique from sample descriptions.\n""             ""Clashing batch names: %s"" % sorted(list(dups))         )","if not isinstance ( batches , ( list , tuple ) ) :",if len ( batches ) > 0 :,7.433761660133445,7.433761660133445,0.0
"def _check_signal(self):     """"""Checks if a signal was received and issues a message.""""""     proc_signal = getattr(self.proc, ""signal"", None)     if proc_signal is None:         return     sig, core = proc_signal     sig_str = SIGNAL_MESSAGES.get(sig)     if sig_str:         if core:             sig_str += "" (core dumped)""         print(sig_str, file=sys.stderr)         <MASK>             self.errors += sig_str + ""\n""",if self . errors is not None :,if sig_str :,6.9717291216921975,6.9717291216921975,0.0
"def loadLabelFile(self, labelpath):     labeldict = {}     if not os.path.exists(labelpath):         f = open(labelpath, ""w"", encoding=""utf-8"")     else:         with open(labelpath, ""r"", encoding=""utf-8"") as f:             data = f.readlines()             for each in data:                 file, label = each.split(""\t"")                 <MASK>                     label = label.replace(""false"", ""False"")                     label = label.replace(""true"", ""True"")                     labeldict[file] = eval(label)                 else:                     labeldict[file] = []     return labeldict",if label :,if label :,100.00000000000004,0.0,1.0
"def exists_col_to_many(self, select_columns: List[str]) -> bool:     for column in select_columns:         <MASK>             root_relation = get_column_root_relation(column)             if self.is_relation_many_to_many(                 root_relation             ) or self.is_relation_one_to_many(root_relation):                 return True     return False",if is_column_dotted ( column ) :,if self . is_relation_many ( column ) :,27.09198854675628,27.09198854675628,0.0
"def check_sequence_matches(seq, template):     i = 0     for pattern in template:         <MASK>             pattern = {pattern}         got = set(seq[i : i + len(pattern)])         assert got == pattern         i += len(got)","if not isinstance ( pattern , set ) :",if pattern not in seq :,6.962210312500384,6.962210312500384,0.0
"def load_modules(     to_load, load, attr, modules_dict, excluded_aliases, loading_message=None ):     if loading_message:         print(loading_message)     for name in to_load:         module = load(name)         if module is None or not hasattr(module, attr):             continue         cls = getattr(module, attr)         if hasattr(cls, ""initialize"") and not cls.initialize():             continue         <MASK>             for alias in module.aliases():                 if alias not in excluded_aliases:                     modules_dict[alias] = module         else:             modules_dict[name] = module     if loading_message:         print()","if hasattr ( module , ""aliases"" ) :",if module . is_module :,5.630400552901077,5.630400552901077,0.0
"def result():     # ""global"" does not work here...     R, V = rays, virtual_rays     if V is not None:         <MASK>             V = normalize_rays(V, lattice)         if check:             R = PointCollection(V, lattice)             V = PointCollection(V, lattice)             d = lattice.dimension()             if len(V) != d - R.dim() or (R + V).dim() != d:                 raise ValueError(                     ""virtual rays must be linearly ""                     ""independent and with other rays span the ambient space.""                 )     return RationalPolyhedralFan(cones, R, lattice, is_complete, V)",if normalize :,if len ( R ) > 0 :,6.567274736060395,6.567274736060395,0.0
"def communicate(self, _input=None, _timeout=None) -> Tuple[bytes, bytes]:     if parse_args().print_commands:         <MASK>             print_stderr(                 color_line(""=> "", 14) + "" "".join(str(arg) for arg in self.args)             )     stdout, stderr = super().communicate(_input, _timeout)     self.stdout_text = stdout.decode(""utf-8"") if stdout else None     self.stderr_text = stderr.decode(""utf-8"") if stderr else None     return stdout, stderr",if self . args != get_sudo_refresh_command ( ) :,if self . args :,7.834966465489322,7.834966465489322,0.0
"def convert(data):     result = []     for d in data:         # noinspection PyCompatibility         <MASK>             result.append((d[0], None, d[1]))         elif isinstance(d, basestring):             result.append(d)     return result","if isinstance ( d , tuple ) and len ( d ) == 2 :","if isinstance ( d , ( int , float ) ) :",26.31143291567556,26.31143291567556,0.0
"def validate(self, value):     try:         value = [             datetime.datetime.strptime(range, ""%Y-%m-%d %H:%M:%S"")             for range in value.split("" to "")         ]         <MASK>             return True         else:             return False     except ValueError:         return False",if ( len ( value ) == 2 ) and ( value [ 0 ] <= value [ 1 ] ) :,"if isinstance ( value , list ) :",2.235173392777355,2.235173392777355,0.0
"def rmdir(dirname):     if dirname[-1] == os.sep:         dirname = dirname[:-1]     if os.path.islink(dirname):         return  # do not clear link - we can get out of dir     for f in os.listdir(dirname):         if f in (""."", ""..""):             continue         path = dirname + os.sep + f         <MASK>             rmdir(path)         else:             os.unlink(path)     os.rmdir(dirname)",if os . path . isdir ( path ) :,if os . path . isdir ( path ) :,100.00000000000004,100.00000000000004,1.0
"def onCompletion(self, text):     res = []     for l in text.split(""\n""):         if not l:             continue         l = l.split("":"")         <MASK>             continue         res.append([l[0].strip(), l[1].strip()])     self.panel.setSlides(res)",if len ( l ) != 2 :,if not l :,6.023021415818187,6.023021415818187,0.0
"def pytest_collection_modifyitems(items):     for item in items:         <MASK>             if ""stage"" not in item.keywords:                 item.add_marker(pytest.mark.stage(""unit""))             if ""init"" not in item.keywords:                 item.add_marker(pytest.mark.init(rng_seed=123))","if item . nodeid . startswith ( ""tests/infer"" ) :","if ""unit"" not in item . keywords :",7.40354787297858,7.40354787297858,0.0
"def build_message(self, options, target):     message = multipart.MIMEMultipart()     for name, value in list(options.items()):         if name == ""EMAIL_BODY"":             self.add_body(message, value)         <MASK>             self.add_attachment(message, value)         else:  # From, To, Subject, etc.             self.set_option(message, name, value, target)     return message","elif name == ""EMAIL_ATTACHMENT"" :","if name == ""EMAIL_ATTACHMENT"" :",88.01117367933934,88.01117367933934,0.0
def extend_with_zeroes(b):     try:         for x in b:             x = to_constant(x)             <MASK>                 yield (x)             else:                 yield (0)         for _ in range(32):             yield (0)     except Exception as e:         return,"if isinstance ( x , int ) :",if x != 0 :,7.654112967106117,7.654112967106117,0.0
"def _start_cluster(*, cleanup_atexit=True):     global _default_cluster     if _default_cluster is None:         cluster_addr = os.environ.get(""EDGEDB_TEST_CLUSTER_ADDR"")         <MASK>             conn_spec = json.loads(cluster_addr)             _default_cluster = edgedb_cluster.RunningCluster(**conn_spec)         else:             data_dir = os.environ.get(""EDGEDB_TEST_DATA_DIR"")             _default_cluster = _init_cluster(                 data_dir=data_dir, cleanup_atexit=cleanup_atexit             )     return _default_cluster",if cluster_addr :,if cluster_addr :,100.00000000000004,100.00000000000004,1.0
"def preprocess_raw_enwik9(input_filename, output_filename):     with open(input_filename, ""r"") as f1:         with open(output_filename, ""w"") as f2:             while True:                 line = f1.readline()                 <MASK>                     break                 line = list(enwik9_norm_transform([line]))[0]                 if line != "" "" and line != """":                     if line[0] == "" "":                         line = line[1:]                     f2.writelines(line + ""\n"")",if not line :,if not line :,100.00000000000004,100.00000000000004,1.0
"def is_entirely_italic(line):     style = subs.styles.get(line.style, SSAStyle.DEFAULT_STYLE)     for fragment, sty in parse_tags(line.text, style, subs.styles):         fragment = fragment.replace(r""\h"", "" "")         fragment = fragment.replace(r""\n"", ""\n"")         fragment = fragment.replace(r""\N"", ""\n"")         <MASK>             return False     return True",if not sty . italic and fragment and not fragment . isspace ( ) :,if sty == SSAStyle . DEFAULT_STYLE :,3.59927582376646,3.59927582376646,0.0
def __get_all_nodes(self):     nodes = []     next_level = [self.__tree.get_root()]     while len(next_level) != 0:         cur_level = next_level         nodes += next_level         next_level = []         for cur_node in cur_level:             children = cur_node.get_children()             <MASK>                 next_level += children     return nodes,if children is not None :,if children :,23.174952587773145,0.0,0.0
"def _openvpn_stdout(self):     while True:         line = self.process.stdout.readline()         if not line:             <MASK>                 return             time.sleep(0.05)             continue         yield         try:             self.server.output.push_output(line)         except:             logger.exception(                 ""Failed to push vpn output"",                 ""server"",                 server_id=self.server.id,             )         yield",if self . process . poll ( ) is not None or self . is_interrupted ( ) :,if not line :,0.38503887711545237,0.38503887711545237,0.0
"def payment_received_handler(event):     if isinstance(event.message.action, types.MessageActionPaymentSentMe):         payment: types.MessageActionPaymentSentMe = event.message.action         # do something after payment was received         if payment.payload.decode(""UTF-8"") == ""product A"":             await bot.send_message(                 event.message.from_id, ""Thank you for buying product A!""             )         <MASK>             await bot.send_message(                 event.message.from_id, ""Thank you for buying product B!""             )         raise events.StopPropagation","elif payment . payload . decode ( ""UTF-8"" ) == ""product B"" :",if event . message . action == types . MessageActionPaymentB :,5.165574782208687,5.165574782208687,0.0
"def spaces_after(token, prev, next, min=-1, max=-1, min_desc=None, max_desc=None):     if next is not None and token.end_mark.line == next.start_mark.line:         spaces = next.start_mark.pointer - token.end_mark.pointer         <MASK>             return LintProblem(                 token.start_mark.line + 1, next.start_mark.column, max_desc             )         elif min != -1 and spaces < min:             return LintProblem(                 token.start_mark.line + 1, next.start_mark.column + 1, min_desc             )",if max != - 1 and spaces > max :,if spaces > max :,21.297646969725616,21.297646969725616,0.0
"def seek_to_block(self, pos):     baseofs = 0     ofs = 0     for b in self.blocks:         <MASK>             self.current_block = b             break         baseofs += b.compressed_size         ofs += b.uncompressed_size     else:         self.current_block = None         self.current_stream = BytesIO(b"""")         return     self.current_block_start = ofs     self.stream.seek(self.basepos + baseofs)     buf = BytesIO(self.stream.read(self.current_block.compressed_size))     self.current_stream = self.current_block.decompress(buf)",if ofs + b . uncompressed_size > pos :,if pos == self . basepos :,5.367626065580593,5.367626065580593,0.0
"def rewrite_hunks(hunks):     # type: (List[Hunk]) -> Iterator[Hunk]     # Assumes `hunks` are sorted, and from the same file     deltas = (hunk.b_length - hunk.a_length for hunk in hunks)     offsets = accumulate(deltas, initial=0)     for hunk, offset in zip(hunks, offsets):         new_b = hunk.a_start + offset         if hunk_of_additions_only(hunk):             new_b += 1         <MASK>             new_b -= 1         yield hunk._replace(b_start=new_b)",elif hunk_of_removals_only ( hunk ) :,if hunk . b_start > new_b :,4.839576869824698,4.839576869824698,0.0
"def do_query(data, q):     ret = []     if not q:         return ret     qkey = q[0]     for key, value in iterate(data):         <MASK>             if key == qkey:                 ret.append(value)             elif is_iterable(value):                 ret.extend(do_query(value, q))         else:             if not is_iterable(value):                 continue             if key == qkey:                 ret.extend(do_query(value, q[1:]))             else:                 ret.extend(do_query(value, q))     return ret",if len ( q ) == 1 :,if not is_iterable ( value ) :,6.742555929751843,6.742555929751843,0.0
"def get_url(token, base_url):     """"""Parse an <url> token.""""""     if token.type == ""url"":         return _get_url_tuple(token.value, base_url)     elif token.type == ""function"":         if token.name == ""attr"":             return check_attr_function(token, ""url"")         <MASK>             # Ignore url modifiers             # See https://drafts.csswg.org/css-values-3/#urls             return _get_url_tuple(token.arguments[0].value, base_url)","elif token . name == ""url"" and len ( token . arguments ) in ( 1 , 2 ) :","if token . arguments [ 0 ] . name == ""modifiers"" :",20.41035753156609,20.41035753156609,0.0
"def read(self, count):     if self.closed:         return self.upstream.read(count)     try:         while len(self.upstream) < count:             <MASK>                 with self.buf_in:                     self.transport.downstream_recv(self.buf_in)             else:                 break         return self.upstream.read(count)     except:         logger.debug(traceback.format_exc())",if self . buf_in or self . _poll_read ( 10 ) :,if self . transport . is_open ( ) :,11.434735332025594,11.434735332025594,0.0
"def get_timestamp_for_block(     self, block_hash: HexBytes, max_tries: Optional[int] = 10 ) -> int:     counter = 0     block: AttributeDict = None     if block_hash in self._block_cache.keys():         block = self._block_cache.get(block_hash)     else:         while block is None:             <MASK>                 raise ValueError(f""Block hash {block_hash.hex()} does not exist."")             counter += 1             block = self._block_cache.get(block_hash)             await asyncio.sleep(0.5)     return block.get(""timestamp"")",if counter == max_tries :,if counter >= max_tries :,59.4603557501361,59.4603557501361,0.0
"def reader():     batch_out = []     for video_name in self.video_list:         video_idx = self.video_list.index(video_name)         video_feat = self.load_file(video_name)         batch_out.append((video_feat, video_idx))         <MASK>             yield batch_out             batch_out = []",if len ( batch_out ) == self . batch_size :,if video_idx > 0 :,2.75631563063758,2.75631563063758,0.0
"def cleanup():     gscript.message(_(""Erasing temporary files...""))     for temp_map, maptype in temp_maps:         <MASK>             gscript.run_command(                 ""g.remove"", flags=""f"", type=maptype, name=temp_map, quiet=True             )","if gscript . find_file ( temp_map , element = maptype ) [ ""name"" ] :",if maptype in temp_maps :,2.235173392777356,2.235173392777356,0.0
"def run(self):     while True:         try:             with DelayedKeyboardInterrupt():                 raw_inputs = self._parent_task_queue.get()                 <MASK>                     self._rq.put(raw_inputs, block=True)                     break                 if self._flow_type == BATCH:                     self._rq.put(raw_inputs, block=True)                 elif self._flow_type == REALTIME:                     try:                         self._rq.put(raw_inputs, block=False)                     except:                         pass         except KeyboardInterrupt:             continue",if self . _has_stop_signal ( raw_inputs ) :,if not raw_inputs :,7.659936211474486,7.659936211474486,0.0
"def handle_sent(self, elt):     sent = []     for child in elt:         if child.tag in (""mw"", ""hi"", ""corr"", ""trunc""):             sent += [self.handle_word(w) for w in child]         <MASK>             sent.append(self.handle_word(child))         elif child.tag not in self.tags_to_ignore:             raise ValueError(""Unexpected element %s"" % child.tag)     return BNCSentence(elt.attrib[""n""], sent)","elif child . tag in ( ""w"" , ""c"" ) :","if child . tag in ( ""mw"" , ""hi"" , ""corr"" , ""trunc"" ) :",30.926025762875717,30.926025762875717,0.0
"def bind_subscribers_to_graphql_type(self, graphql_type):     for field, subscriber in self._subscribers.items():         <MASK>             raise ValueError(""Field %s is not defined on type %s"" % (field, self.name))         graphql_type.fields[field].subscribe = subscriber",if field not in graphql_type . fields :,if not field . is_field :,6.731190445080104,6.731190445080104,0.0
"def _get_from_json(self, *, name, version):     url = urljoin(self.url, posixpath.join(name, str(version), ""json""))     async with aiohttp_session(auth=self.auth) as session:         async with session.get(url) as response:             <MASK>                 raise PackageNotFoundError(package=name, url=url)             response.raise_for_status()             response = await response.json()     dist = response[""info""][""requires_dist""] or []     if dist:         return dist     # If no requires_dist then package metadata can be broken.     # Let's check distribution files.     return await self._get_from_files(response[""urls""])",if response . status == 404 :,"if not response [ ""info"" ] [ ""requires_dist"" ] :",3.21858262703621,3.21858262703621,0.0
"def is_active(self):     if not self.pk:         log_level = get_setting(""LOG_MISSING_SWITCHES"")         if log_level:             logger.log(log_level, ""Switch %s not found"", self.name)         <MASK>             switch, _created = Switch.objects.get_or_create(                 name=self.name, defaults={""active"": get_setting(""SWITCH_DEFAULT"")}             )             cache = get_cache()             cache.set(self._cache_key(self.name), switch)         return get_setting(""SWITCH_DEFAULT"")     return self.active","if get_setting ( ""CREATE_MISSING_SWITCHES"" ) :",if not _created :,2.3238598963754593,2.3238598963754593,0.0
"def add_requirements(self, requirements):     if self._legacy:         self._legacy.add_requirements(requirements)     else:         run_requires = self._data.setdefault(""run_requires"", [])         always = None         for entry in run_requires:             <MASK>                 always = entry                 break         if always is None:             always = {""requires"": requirements}             run_requires.insert(0, always)         else:             rset = set(always[""requires""]) | set(requirements)             always[""requires""] = sorted(rset)","if ""environment"" not in entry and ""extra"" not in entry :","if entry . name == ""run_requires"" :",3.9438444449522216,3.9438444449522216,0.0
"def display_failures_for_single_test(result: TestResult) -> None:     """"""Display a failure for a single method / endpoint.""""""     display_subsection(result)     checks = _get_unique_failures(result.checks)     for idx, check in enumerate(checks, 1):         message: Optional[str]         if check.message:             message = f""{idx}. {check.message}""         else:             message = None         example = cast(Case, check.example)  # filtered in `_get_unique_failures`         display_example(example, check.name, message, result.seed)         # Display every time except the last check         <MASK>             click.echo(""\n"")",if idx != len ( checks ) :,"if check . name == ""test"" :",5.522397783539471,5.522397783539471,0.0
"def __call__(self, frame: FrameType, event: str, arg: Any) -> ""CallTracer"":     code = frame.f_code     if (         event not in SUPPORTED_EVENTS         or code.co_name == ""trace_types""         or self.should_trace         and not self.should_trace(code)     ):         return self     try:         if event == EVENT_CALL:             self.handle_call(frame)         <MASK>             self.handle_return(frame, arg)         else:             logger.error(""Cannot handle event %s"", event)     except Exception:         logger.exception(""Failed collecting trace"")     return self",elif event == EVENT_RETURN :,if event == EVENT_RETURN :,84.08964152537145,84.08964152537145,0.0
"def get_maps(test):     pages = set()     for addr in test[""pre""][""memory""].keys():         pages.add(addr >> 12)     for addr in test[""pos""][""memory""].keys():         pages.add(addr >> 12)     maps = []     for p in sorted(pages):         <MASK>             maps[-1] = (maps[-1][0], maps[-1][1] + 0x1000)         else:             maps.append((p << 12, 0x1000))     return maps",if len ( maps ) > 0 and maps [ - 1 ] [ 0 ] + maps [ - 1 ] [ 1 ] == p << 12 :,if p == 0 :,0.28037993823818674,0.28037993823818674,0.0
"def process_rotate_aes_key(self):     if hasattr(self.options, ""rotate_aes_key"") and isinstance(         self.options.rotate_aes_key, six.string_types     ):         <MASK>             self.options.rotate_aes_key = True         elif self.options.rotate_aes_key.lower() == ""false"":             self.options.rotate_aes_key = False","if self . options . rotate_aes_key . lower ( ) == ""true"" :","if self . options . rotate_aes_key . lower ( ) == ""true"" :",100.00000000000004,100.00000000000004,1.0
"def apply_figure(self, figure):     super(legend_text_legend, self).apply_figure(figure)     properties = self.properties.copy()     with suppress(KeyError):         del properties[""margin""]     with suppress(KeyError):         texts = figure._themeable[""legend_text_legend""]         for text in texts:             <MASK>  # textarea                 text = text._text             text.set(**properties)","if not hasattr ( text , ""_x"" ) :",if text . _text :,4.222794013898957,4.222794013898957,0.0
"def tearDown(self):     for i in range(len(self.tree) - 1, -1, -1):         s = os.path.join(self.root, self.tree[i])         <MASK>             os.rmdir(s)         else:             os.remove(s)     os.rmdir(self.root)","if not ""."" in s :",if os . path . isdir ( s ) :,5.934202609760488,5.934202609760488,0.0
"def _get_id(self, type, id):     fields = id.split("":"")     if len(fields) >= 3:         if type != fields[-2]:             logger.warning(                 ""Expected id of type %s but found type %s %s"", type, fields[-2], id             )         return fields[-1]     fields = id.split(""/"")     if len(fields) >= 3:         itype = fields[-2]         <MASK>             logger.warning(                 ""Expected id of type %s but found type %s %s"", type, itype, id             )         return fields[-1].split(""?"")[0]     return id",if type != itype :,"if itype != ""id"" :",13.888095170058955,13.888095170058955,0.0
"def candidates() -> Generator[""Symbol"", None, None]:     s = self     if Symbol.debug_lookup:         Symbol.debug_print(""searching in self:"")         print(s.to_string(Symbol.debug_indent + 1), end="""")     while True:         <MASK>             yield s         if recurseInAnon:             yield from s.children_recurse_anon         else:             yield from s._children         if s.siblingAbove is None:             break         s = s.siblingAbove         if Symbol.debug_lookup:             Symbol.debug_print(""searching in sibling:"")             print(s.to_string(Symbol.debug_indent + 1), end="""")",if matchSelf :,if s . children is not None :,6.567274736060395,6.567274736060395,0.0
"def records(account_id):     """"""Fetch locks data""""""     s = boto3.Session()     table = s.resource(""dynamodb"").Table(""Sphere11.Dev.ResourceLocks"")     results = table.scan()     for r in results[""Items""]:         if ""LockDate"" in r:             r[""LockDate""] = datetime.fromtimestamp(r[""LockDate""])         <MASK>             r[""RevisionDate""] = datetime.fromtimestamp(r[""RevisionDate""])     print(tabulate.tabulate(results[""Items""], headers=""keys"", tablefmt=""fancy_grid""))","if ""RevisionDate"" in r :","if ""RevisionDate"" in r :",100.00000000000004,100.00000000000004,1.0
"def _handle_errors(errors):     """"""Log out and possibly reraise errors during import.""""""     if not errors:         return     log_all = True  # pylint: disable=unused-variable     err_msg = ""T2T: skipped importing {num_missing} data_generators modules.""     print(err_msg.format(num_missing=len(errors)))     for module, err in errors:         err_str = str(err)         if log_all:             print(""Did not import module: %s; Cause: %s"" % (module, err_str))         <MASK>             print(""From module %s"" % module)             raise err","if not _is_import_err_msg ( err_str , module ) :",if not module . startswith ( err_str ) :,17.278662279611076,17.278662279611076,0.0
"def find_needle(self, tree, focused=None):     if isinstance(tree, list):         for el in tree:             res = self.find_needle(el, focused)             <MASK>                 return res     elif isinstance(tree, dict):         nodes = tree.get(""nodes"", []) + tree.get(""floating_nodes"", [])         if focused:             for node in nodes:                 if node[""id""] == focused[""id""]:                     return tree         elif tree[""focused""]:             return tree         return self.find_needle(nodes, focused)     return {}",if res :,if res :,100.00000000000004,0.0,1.0
"def available_datasets(self):     """"""Automatically determine datasets provided by this file""""""     res = self.resolution     coordinates = [""pixel_longitude"", ""pixel_latitude""]     for var_name, val in self.file_content.items():         if isinstance(val, netCDF4.Variable):             ds_info = {                 ""file_type"": self.filetype_info[""file_type""],                 ""resolution"": res,             }             <MASK>                 ds_info[""coordinates""] = coordinates             yield DatasetID(name=var_name, resolution=res), ds_info",if not self . is_geo :,if val . type == netCDF4 . Coordinates :,5.522397783539471,5.522397783539471,0.0
"def get_subkeys(self, key):     # TODO: once we revamp the registry emulation,     # make this better     parent_path = key.get_path()     subkeys = []     for k in self.keys:         test_path = k.get_path()         <MASK>             sub = test_path[len(parent_path) :]             if sub.startswith(""\\""):                 sub = sub[1:]             end_slash = sub.find(""\\"")             if end_slash >= 0:                 sub = sub[:end_slash]             if not sub:                 continue             subkeys.append(sub)     return subkeys",if test_path . lower ( ) . startswith ( parent_path . lower ( ) ) :,if test_path :,3.520477365831487,3.520477365831487,0.0
"def default(self, o):     try:         if type(o) == datetime.datetime:             return str(o)         else:             # remove unwanted attributes from the provider object during conversion to json             if hasattr(o, ""profile""):                 del o.profile             <MASK>                 del o.credentials             if hasattr(o, ""metadata_path""):                 del o.metadata_path             if hasattr(o, ""services_config""):                 del o.services_config             return vars(o)     except Exception as e:         return str(o)","if hasattr ( o , ""credentials"" ) :","if hasattr ( o , ""credentials"" ) :",100.00000000000004,100.00000000000004,1.0
"def submit(self, fn, *args, **kwargs):     with self._shutdown_lock:         <MASK>             raise RuntimeError(""cannot schedule new futures after shutdown"")         f = _base.Future()         w = _WorkItem(f, fn, args, kwargs)         self._work_queue.put(w)         self._adjust_thread_count()         return f",if self . _shutdown :,if self . _shutdown_lock is not None :,36.72056269893591,36.72056269893591,0.0
"def __viewerKeyPress(viewer, event):     view = viewer.view()     if not isinstance(view, GafferSceneUI.SceneView):         return False     if event == __editSourceKeyPress:         selectedPath = __sceneViewSelectedPath(view)         <MASK>             __editSourceNode(view.getContext(), view[""in""], selectedPath)         return True     elif event == __editTweaksKeyPress:         selectedPath = __sceneViewSelectedPath(view)         if selectedPath is not None:             __editTweaksNode(view.getContext(), view[""in""], selectedPath)         return True",if selectedPath is not None :,if selectedPath is not None :,100.00000000000004,100.00000000000004,1.0
"def _split_to_option_groups_and_paths(self, args):     opt_groups = []     current = []     for arg in args:         <MASK>             opts = self._arg_parser.parse_args(current)[0]             opt_groups.append(opts)             current = []         else:             current.append(arg)     if opt_groups:         return opt_groups, current     raise ValueError(""Nothing to split"")","if arg . replace ( ""-"" , """" ) == """" and len ( arg ) >= 3 :",if arg in self . _arg_parser :,2.354619017588452,2.354619017588452,0.0
"def _on_change(self):     changed = False     self.save()     for key, value in self.data.items():         if isinstance(value, bool):             <MASK>                 changed = True                 break         if isinstance(value, int):             if value != 1:                 changed = True                 break         elif value is None:             continue         elif len(value) != 0:             changed = True             break     self._reset_button.disabled = not changed",if value :,if key != 0 :,9.652434877402245,9.652434877402245,0.0
"def wait_for_child(pid, timeout=1.0):     deadline = mitogen.core.now() + timeout     while timeout < mitogen.core.now():         try:             target_pid, status = os.waitpid(pid, os.WNOHANG)             if target_pid == pid:                 return         except OSError:             e = sys.exc_info()[1]             <MASK>                 return         time.sleep(0.05)     assert False, ""wait_for_child() timed out""",if e . args [ 0 ] == errno . ECHILD :,if e . errno == errno . EINTR :,29.10042507378281,29.10042507378281,0.0
"def _get_os_version_lsb_release():     try:         output = subprocess.check_output(""lsb_release -sri"", shell=True)         lines = output.strip().split()         name, version = lines         <MASK>             version = """"         return name, version     except:         return _get_os_version_uname()","if version . lower ( ) == ""rolling"" :",if not version :,2.845073863275343,2.845073863275343,0.0
"def _check_snapshot_status_healthy(self, snapshot_uuid):     status = """"     try:         while True:             status, locked = self._get_snapshot_status(snapshot_uuid)             <MASK>                 break             eventlet.sleep(2)     except Exception:         with excutils.save_and_reraise_exception():             LOG.exception(""Failed to get snapshot status. [%s]"", snapshot_uuid)     LOG.debug(         ""Lun [%(snapshot)s], status [%(status)s]."",         {""snapshot"": snapshot_uuid, ""status"": status},     )     return status == ""Healthy""",if not locked :,if not locked :,100.00000000000004,100.00000000000004,1.0
"def CountButtons(self):     """"""Returns the number of visible buttons in the docked pane.""""""     n = 0     if self.HasCaption() or self.HasCaptionLeft():         if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame):             return 1         <MASK>             n += 1         if self.HasMaximizeButton():             n += 1         if self.HasMinimizeButton():             n += 1         if self.HasPinButton():             n += 1     return n",if self . HasCloseButton ( ) :,if self . HasPinButton ( ) :,41.11336169005196,41.11336169005196,0.0
"def _url_encode_impl(obj, charset, encode_keys, sort, key):     from .datastructures import iter_multi_items     iterable = iter_multi_items(obj)     if sort:         iterable = sorted(iterable, key=key)     for key, value in iterable:         <MASK>             continue         if not isinstance(key, bytes):             key = text_type(key).encode(charset)         if not isinstance(value, bytes):             value = text_type(value).encode(charset)         yield _fast_url_quote_plus(key) + ""="" + _fast_url_quote_plus(value)",if value is None :,if not encode_keys :,9.652434877402245,9.652434877402245,0.0
"def get_response(self, exc_fmt=None):     self.callback = None     if __debug__:         self.parent._log(3, ""%s:%s.ready.wait"" % (self.name, self.tag))     self.ready.wait()     if self.aborted is not None:         typ, val = self.aborted         <MASK>             exc_fmt = ""%s - %%s"" % typ         raise typ(exc_fmt % str(val))     return self.response",if exc_fmt is None :,if not exc_fmt :,29.05925408079185,29.05925408079185,0.0
"def extract_items(self):     responses = self.fetch()     items = []     for response in responses:         page_key = response.meta.get(""page_key"") or response.url         item = {""key"": page_key, ""items"": None, ""templates"": None}         extracted_items = [             dict(i) for i in self.spider.parse(response) if not isinstance(i, Request)         ]         <MASK>             item[""items""] = extracted_items             item[""templates""] = [                 i[""_template""] for i in extracted_items if i.get(""_template"")             ]             items.append(item)     return items",if extracted_items :,if len ( extracted_items ) > 0 :,17.747405280050266,17.747405280050266,0.0
"def fit_one(self, x):     for i, xi in x.items():         if self.with_centering:             self.median[i].update(xi)         <MASK>             self.iqr[i].update(xi)     return self",if self . with_scaling :,if self . with_centering :,64.34588841607616,64.34588841607616,0.0
"def find_word_bounds(self, text, index, allowed_chars):     right = left = index     done = False     while not done:         if left == 0:             done = True         <MASK>             left -= 1         else:             done = True     done = False     while not done:         if right == len(text):             done = True         elif not self.word_boundary_char(text[right]):             right += 1         else:             done = True     return left, right",elif not self . word_boundary_char ( text [ left - 1 ] ) :,if not self . word_boundary_char ( text [ left ] ) :,71.9548353625319,71.9548353625319,0.0
"def _validate_duplicate_detection_history_time_window(namespace):     if namespace.duplicate_detection_history_time_window:         if iso8601pattern.match(namespace.duplicate_detection_history_time_window):             pass         <MASK>             pass         else:             raise CLIError(                 ""--duplicate-detection-history-time-window Value Error : {0} value is not in ISO 8601 timespan / duration format. e.g. PT10M for duration of 10 min or 00:10:00 for duration of 10 min"".format(                     namespace.duplicate_detection_history_time_window                 )             )",elif timedeltapattern . match ( namespace . duplicate_detection_history_time_window ) :,if namespace . duration_format :,3.2449642932033895,3.2449642932033895,0.0
"def get_subkeys(self, key):     # TODO: once we revamp the registry emulation,     # make this better     parent_path = key.get_path()     subkeys = []     for k in self.keys:         test_path = k.get_path()         if test_path.lower().startswith(parent_path.lower()):             sub = test_path[len(parent_path) :]             if sub.startswith(""\\""):                 sub = sub[1:]             end_slash = sub.find(""\\"")             if end_slash >= 0:                 sub = sub[:end_slash]             <MASK>                 continue             subkeys.append(sub)     return subkeys",if not sub :,"if sub . startswith ( ""\\"" ) :",4.9323515694897075,4.9323515694897075,0.0
"def generator(self, data):     <MASK>         silent_vars = self._get_silent_vars()     for task in data:         for var, val in task.environment_variables():             if self._config.SILENT:                 if var in silent_vars:                     continue             yield (                 0,                 [                     int(task.UniqueProcessId),                     str(task.ImageFileName),                     Address(task.Peb.ProcessParameters.Environment),                     str(var),                     str(val),                 ],             )",if self . _config . SILENT :,if self . _config . SLENT :,70.71067811865478,70.71067811865478,0.0
"def start_requests(self):     if self.fail_before_yield:         1 / 0     for s in range(100):         qargs = {""total"": 10, ""seed"": s}         url = self.mockserver.url(""/follow?%s"") % urlencode(qargs, doseq=1)         yield Request(url, meta={""seed"": s})         <MASK>             2 / 0     assert self.seedsseen, ""All start requests consumed before any download happened""",if self . fail_yielding :,if self . seedsseen :,28.641904579795423,28.641904579795423,0.0
"def populateGridlines(self):     cTicks = self.getSystemCurve(self.ticksId)     cGridlines = self.getSystemCurve(self.gridlinesId)     cGridlines.clearPoints()     nTicks = cTicks.getNPoints()     for iTick in range(nTicks):         <MASK>             p = cTicks.getPoint(iTick)             cGridlines.addPoint(p.getX(), p.getY())",if self . hasGridlines and ( iTick % self . ticksPerGridline ) == 0 :,if iTick < nTicks :,1.5577298727187734,1.5577298727187734,0.0
"def handle_before_events(request, event_list):     if not event_list:         return """"     if not hasattr(event_list, ""__iter__""):         project = event_list.project         event_list = [event_list]     else:         projects = set(e.project for e in event_list)         <MASK>             project = projects.pop()         else:             project = None     for plugin in plugins.for_project(project):         safe_execute(plugin.before_events, request, event_list)     return """"",if len ( projects ) == 1 :,if projects :,5.370784274455332,0.0,0.0
"def handle_parse_result(self, ctx, opts, args):     if self.name in opts:         <MASK>             self._raise_exclusive_error()         if self.multiple and len(set(opts[self.name])) > 1:             self._raise_exclusive_error()     return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)",if self . mutually_exclusive . intersection ( opts ) :,if self . multiple :,10.536767850900915,10.536767850900915,0.0
"def current_word(cursor_offset, line):     """"""the object.attribute.attribute just before or under the cursor""""""     pos = cursor_offset     start = pos     end = pos     word = None     for m in current_word_re.finditer(line):         <MASK>             start = m.start(1)             end = m.end(1)             word = m.group(1)     if word is None:         return None     return LinePart(start, end, word)",if m . start ( 1 ) < pos and m . end ( 1 ) >= pos :,if not m . group ( 0 ) :,3.696711021882758,3.696711021882758,0.0
"def query_to_script_path(path, query):     if path != ""*"":         script = os.path.join(path, query.split("" "")[0])         <MASK>             raise IOError(""Script '{}' not found in script directory"".format(query))         return os.path.join(path, query).split("" "")     return query",if not os . path . exists ( script ) :,if script not in script :,4.988641679706251,4.988641679706251,0.0
"def expand(self, pbegin):     # TODO(b/151921205): we have to do an identity map for unmodified     # PCollections below because otherwise we get an error from beam.     identity_map = ""Identity"" >> beam.Map(lambda x: x)     if self._dataset_key.is_flattened_dataset_key():         <MASK>             return self._flat_pcollection | identity_map         else:             return list(                 self._pcollection_dict.values()             ) | ""FlattenAnalysisInputs"" >> beam.Flatten(pipeline=pbegin.pipeline)     else:         return self._pcollection_dict[self._dataset_key] | identity_map",if self . _flat_pcollection :,if self . _flat_pcollection :,100.00000000000004,100.00000000000004,1.0
"def processCoords(coords):     newcoords = deque()     for (x, y, z) in coords:         for _dir, offsets in faceDirections:             if _dir == FaceYIncreasing:                 continue             dx, dy, dz = offsets             p = (x + dx, y + dy, z + dz)             if p not in box:                 continue             nx, ny, nz = p             <MASK>                 level.setBlockAt(nx, ny, nz, waterID)                 newcoords.append(p)     return newcoords","if level . blockAt ( nx , ny , nz ) == 0 :",if p . is_leaf ( ) :,3.6603482467246367,3.6603482467246367,0.0
"def delete_byfilter(userId, remove=True, session=None, **dbfilter):     if not session:         session = db.Session     ret = False     results = session.query(ObjectStorageMetadata).filter_by(**dbfilter)     if results:         for result in results:             <MASK>                 session.delete(result)             else:                 result.update(                     {                         ""record_state_key"": ""to_delete"",                         ""record_state_val"": str(time.time()),                     }                 )             ret = True     return ret",if remove :,if remove :,100.00000000000004,0.0,1.0
"def fields(self, fields):     fields_xml = """"     for field in fields:         field_dict = DEFAULT_FIELD.copy()         field_dict.update(field)         <MASK>             field_dict[""required""] = ""true""         fields_xml += FIELD_XML_TEMPLATE % field_dict + ""\n""     self.xml = force_unicode(         force_unicode(self.xml).replace(             u""<!-- REPLACE FIELDS -->"", force_unicode(fields_xml)         )     )","if self . unique_key_field == field [ ""name"" ] :",if field . required :,1.3704649608359576,1.3704649608359576,0.0
"def get_all_users(self, access_token, timeout=None):     if timeout is None:         timeout = DEFAULT_TIMEOUT     headers = self.retrieve_header(access_token)     try:         response = await self.standard_request(             ""get"", ""/walkoff/api/users"", timeout=DEFAULT_TIMEOUT, headers=headers         )         <MASK>             resp = await response.json()             return resp, ""Success""         else:             return ""Invalid Credentials""     except asyncio.CancelledError:         return False, ""TimedOut""",if response . status == 200 :,if response is not None :,12.872632311973014,12.872632311973014,0.0
"def set_val():     idx = 0     for idx in range(0, len(model)):         row = model[idx]         if value and row[0] == value:             break         <MASK>             idx = -1     os_widget.set_active(idx)     if idx == -1:         os_widget.set_active(0)     if idx >= 0:         return row[1]     if self.show_all_os:         return None",if idx == len ( os_widget . get_model ( ) ) - 1 :,if idx >= 0 :,2.2115435625398026,2.2115435625398026,0.0
"def translate_module_name(module: str, relative: int) -> Tuple[str, int]:     for pkg in VENDOR_PACKAGES:         for alt in ""six.moves"", ""six"":             substr = ""{}.{}"".format(pkg, alt)             if module.endswith(""."" + substr) or (module == substr and relative):                 return alt, 0             <MASK>                 return alt + ""."" + module.partition(""."" + substr + ""."")[2], 0     return module, relative","if ""."" + substr + ""."" in module :",if substr in module :,9.121704909091086,9.121704909091086,0.0
"def escape(m):     all, tail = m.group(0, 1)     assert all.startswith(""\\"")     esc = simple_escapes.get(tail)     if esc is not None:         return esc     if tail.startswith(""x""):         hexes = tail[1:]         <MASK>             raise ValueError(""invalid hex string escape ('\\%s')"" % tail)         try:             i = int(hexes, 16)         except ValueError:             raise ValueError(""invalid hex string escape ('\\%s')"" % tail)     else:         try:             i = int(tail, 8)         except ValueError:             raise ValueError(""invalid octal string escape ('\\%s')"" % tail)     return chr(i)",if len ( hexes ) < 2 :,if hexes :,7.49553326588684,0.0,0.0
"def __get_k8s_container_name(self, job_wrapper):     # These must follow a specific regex for Kubernetes.     raw_id = job_wrapper.job_destination.id     if isinstance(raw_id, str):         cleaned_id = re.sub(""[^-a-z0-9]"", ""-"", raw_id)         <MASK>             cleaned_id = ""x%sx"" % cleaned_id         return cleaned_id     return ""job-container""","if cleaned_id . startswith ( ""-"" ) or cleaned_id . endswith ( ""-"" ) :",if cleaned_id :,1.9320789429739673,1.9320789429739673,0.0
"def _power_exact(y, xc, yc, xe):     yc, ye = y.int, y.exp     while yc % 10 == 0:         yc //= 10         ye += 1     if xc == 1:         xe *= yc         while xe % 10 == 0:             xe //= 10             ye += 1         if ye < 0:             return None         exponent = xe * 10 ** ye         <MASK>             xc = exponent         else:             xc = 0         return 5",if y and xe :,if xc == 1 :,9.652434877402245,9.652434877402245,0.0
"def lpush(key, *vals, **kwargs):     ttl = kwargs.get(""ttl"")     cap = kwargs.get(""cap"")     if not ttl and not cap:         _client.lpush(key, *vals)     else:         pipe = _client.pipeline()         pipe.lpush(key, *vals)         <MASK>             pipe.ltrim(key, 0, cap)         if ttl:             pipe.expire(key, ttl)         pipe.execute()",if cap :,if cap :,100.00000000000004,0.0,1.0
"def render_headers(self) -> bytes:     if not hasattr(self, ""_headers""):         parts = [             b""Content-Disposition: form-data; "",             format_form_param(""name"", self.name),         ]         if self.filename:             filename = format_form_param(""filename"", self.filename)             parts.extend([b""; "", filename])         <MASK>             content_type = self.content_type.encode()             parts.extend([b""\r\nContent-Type: "", content_type])         parts.append(b""\r\n\r\n"")         self._headers = b"""".join(parts)     return self._headers",if self . content_type is not None :,if self . content_type :,54.77927682341229,54.77927682341229,0.0
"def validate_custom_field_data(field_type: int, field_data: ProfileFieldData) -> None:     try:         <MASK>             # Choice type field must have at least have one choice             if len(field_data) < 1:                 raise JsonableError(_(""Field must have at least one choice.""))             validate_choice_field_data(field_data)         elif field_type == CustomProfileField.EXTERNAL_ACCOUNT:             validate_external_account_field_data(field_data)     except ValidationError as error:         raise JsonableError(error.message)",if field_type == CustomProfileField . CHOICE :,if field_type == CustomProfileField . CHAN :,78.25422900366438,78.25422900366438,0.0
"def get_data(self, path):     """"""Gross hack to contort loader to deal w/ load_*()'s bad API.""""""     if self.file and path == self.path:         <MASK>             file = self.file         else:             self.file = file = open(self.path, ""r"")         with file:             # Technically should be returning bytes, but             # SourceLoader.get_code() just passed what is returned to             # compile() which can handle str. And converting to bytes would             # require figuring out the encoding to decode to and             # tokenize.detect_encoding() only accepts bytes.             return file.read()     else:         return super().get_data(path)",if not self . file . closed :,if self . file . read ( ) :,33.03164318013809,33.03164318013809,0.0
"def handle_read(self):     """"""Called when there is data waiting to be read.""""""     try:         chunk = self.recv(self.ac_in_buffer_size)     except RetryError:         pass     except socket.error:         self.handle_error()     else:         self.tot_bytes_received += len(chunk)         <MASK>             self.transfer_finished = True             # self.close()  # <-- asyncore.recv() already do that...             return         if self._data_wrapper is not None:             chunk = self._data_wrapper(chunk)         try:             self.file_obj.write(chunk)         except OSError as err:             raise _FileReadWriteError(err)",if not chunk :,if self . transfer_finished :,7.809849842300637,7.809849842300637,0.0
"def _swig_extract_dependency_files(self, src):     dep = []     for line in open(src):         <MASK>             line = line.split("" "")[1].strip(""""""'""\r\n"""""")             if not (""<"" in line or line in dep):                 dep.append(line)     return [i for i in dep if os.path.exists(i)]","if line . startswith ( ""#include"" ) or line . startswith ( ""%include"" ) :",if line :,0.15615160224273406,0.0,0.0
"def buffer(self, lines, scroll_end=True, scroll_if_editing=False):     ""Add data to be displayed in the buffer.""     self.values.extend(lines)     if scroll_end:         if not self.editing:             self.start_display_at = len(self.values) - len(self._my_widgets)         <MASK>             self.start_display_at = len(self.values) - len(self._my_widgets)",elif scroll_if_editing :,if scroll_if_editing :,80.91067115702207,80.91067115702207,0.0
"def test_getline(self):     with tokenize.open(self.file_name) as fp:         for index, line in enumerate(fp):             <MASK>                 line += ""\n""             cached_line = linecache.getline(self.file_name, index + 1)             self.assertEqual(line, cached_line)","if not line . endswith ( ""\n"" ) :",if not line :,8.64715459852499,8.64715459852499,0.0
"def selectRow(self, rowNumber, highlight=None):     if rowNumber == ""h"":         rowNumber = 0     else:         rowNumber = int(rowNumber) + 1     if 1 > rowNumber >= len(self.cells) + 1:         raise Exception(""Invalid row number."")     else:         selected = self.cells[rowNumber][0].selected         for cell in self.cells[rowNumber]:             <MASK>                 if selected:                     cell.deselect()                 else:                     cell.select()             else:                 if highlight:                     cell.mouseEnter()                 else:                     cell.mouseLeave()",if highlight is None :,if highlight :,32.34325178227722,0.0,0.0
"def put(self, session):     with sess_lock:         self.parent.put(session)         # Do not store the session if skip paths         for sp in self.skip_paths:             if request.path.startswith(sp):                 return         <MASK>             try:                 del self._cache[session.sid]             except Exception:                 pass         self._cache[session.sid] = session     self._normalize()",if session . sid in self . _cache :,if session . sid in self . _cache :,100.00000000000004,100.00000000000004,1.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             length = d.getVarInt32()             tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)             d.skip(length)             self.add_status().TryMerge(tmp)             continue         <MASK>             self.add_doc_id(d.getPrefixedString())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 18 :,if tt == 1 :,53.7284965911771,53.7284965911771,0.0
"def extract(self, zip):     max_nb = maxNbFile(self)     for index, field in enumerate(zip.array(""file"")):         <MASK>             self.warning(                 ""ZIP archive contains many files, but only first %s files are processed""                 % max_nb             )             break         self.processFile(field)",if max_nb is not None and max_nb <= index :,if index >= max_nb :,10.59106218302618,10.59106218302618,0.0
"def get_norm(norm, out_channels):     if isinstance(norm, str):         <MASK>             return None         norm = {             ""BN"": BatchNorm2d,             ""GN"": lambda channels: nn.GroupNorm(32, channels),             ""nnSyncBN"": nn.SyncBatchNorm,  # keep for debugging             """": lambda x: x,         }[norm]     return norm(out_channels)",if len ( norm ) == 0 :,"if norm == ""nn"" :",12.256200970377108,12.256200970377108,0.0
"def execute(self):     if self._dirty or not self._qr:         model_class = self.model_class         query_meta = self.get_query_meta()         if self._tuples:             ResultWrapper = TuplesQueryResultWrapper         elif self._dicts:             ResultWrapper = DictQueryResultWrapper         elif self._naive or not self._joins or self.verify_naive():             ResultWrapper = NaiveQueryResultWrapper         <MASK>             ResultWrapper = AggregateQueryResultWrapper         else:             ResultWrapper = ModelQueryResultWrapper         self._qr = ResultWrapper(model_class, self._execute(), query_meta)         self._dirty = False         return self._qr     else:         return self._qr",elif self . _aggregate_rows :,if self . _aggregate :,38.49815007763549,38.49815007763549,0.0
"def emitIpToDomainsData(self, data, event):     self.emitRawRirData(data, event)     domains = data.get(""domains"")     if isinstance(domains, list):         for domain in domains:             if self.checkForStop():                 return None             domain = domain.strip()             <MASK>                 self.emitHostname(domain, event)",if domain :,if domain :,100.00000000000004,0.0,1.0
"def delete(self):     from weblate.trans.models import Change, Suggestion, Vote     fast_deletes = []     for item in self.fast_deletes:         <MASK>             fast_deletes.append(Vote.objects.filter(suggestion__in=item))             fast_deletes.append(Change.objects.filter(suggestion__in=item))         fast_deletes.append(item)     self.fast_deletes = fast_deletes     return super().delete()",if item . model is Suggestion :,"if isinstance ( item , Suggestion ) :",7.809849842300637,7.809849842300637,0.0
"def token(self):     if not self._token:         try:             cookie_token = self.state[""request""].headers.cookie[CSRF_TOKEN].value         except KeyError:             cookie_token = """"         <MASK>             self._token = cookie_token         else:             self._token = get_random_string(TOKEN_LENGTH)     return self._token",if len ( cookie_token ) == TOKEN_LENGTH :,if cookie_token :,9.121704909091086,9.121704909091086,0.0
"def get_logs(last_file=None, last_time=None):     try:         response = client.get_logs(last_file=last_file, last_time=last_time)         get_logs_streamer(             show_timestamp=not hide_time,             all_containers=all_containers,             all_info=all_info,         )(response)         return response     except (ApiException, HTTPError) as e:         <MASK>             handle_cli_error(                 e,                 message=""Could not get logs for run `{}`."".format(client.run_uuid),             )             sys.exit(1)",if not follow :,if e . status_code == 404 :,4.990049701936832,4.990049701936832,0.0
"def update(self, targets):     Section.update(self, targets)     outputNames = set()     for target in targets:         g = target.globals()         outputNames.update([k for k in g.keys() if k.startswith(""output:"")])     rows = []     outputNames = sorted(outputNames)     for outputName in outputNames:         row = self.__rows.get(outputName)         <MASK>             row = _OutputRow(outputName)             self.__rows[outputName] = row         row.update(targets)         row.setAlternate(len(rows) % 2)         rows.append(row)     self._mainColumn()[:] = rows",if row is None :,if not row :,16.37226966703825,16.37226966703825,0.0
"def getBranches(self):     returned = []     for git_branch_line in self._executeGitCommandAssertSuccess(""branch"").stdout:         if git_branch_line.startswith(""*""):             git_branch_line = git_branch_line[1:]         git_branch_line = git_branch_line.strip()         <MASK>             alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER)             returned.append(branch.LocalBranchAlias(self, alias_name, aliased))         else:             returned.append(branch.LocalBranch(self, git_branch_line))     return returned",if BRANCH_ALIAS_MARKER in git_branch_line :,if branch . LocalBranchAlias :,2.8383688870107915,2.8383688870107915,0.0
"def has_bad_headers(self):     headers = [self.sender, self.reply_to] + self.recipients     for header in headers:         if _has_newline(header):             return True     if self.subject:         if _has_newline(self.subject):             for linenum, line in enumerate(self.subject.split(""\r\n"")):                 if not line:                     return True                 <MASK>                     return True                 if _has_newline(line):                     return True                 if len(line.strip()) == 0:                     return True     return False","if linenum > 0 and line [ 0 ] not in ""\t "" :",if linenum == 0 :,3.6462189126393114,3.6462189126393114,0.0
"def resolve_references(self, note, reflist):     assert len(note[""ids""]) == 1     id = note[""ids""][0]     for ref in reflist:         <MASK>             continue         ref.delattr(""refname"")         ref[""refid""] = id         assert len(ref[""ids""]) == 1         note.add_backref(ref[""ids""][0])         ref.resolved = 1     note.resolved = 1",if ref . resolved :,if ref . refid == id :,22.089591134157878,22.089591134157878,0.0
"def pickPath(self, color):     self.path[color] = ()     currentPos = self.starts[color]     while True:         minDist = None         minGuide = None         for guide in self.guides[color]:             guideDist = dist(currentPos, guide)             if minDist == None or guideDist < minDist:                 minDist = guideDist                 minGuide = guide         <MASK>             return         if minGuide == None:             return         self.path[color] = self.path[color] + (minGuide,)         currentPos = minGuide         self.guides[color].remove(minGuide)","if dist ( currentPos , self . ends [ color ] ) == 1 :",if guideDist > minDist :,1.407567834071592,1.407567834071592,0.0
"def __hierarchyViewKeyPress(hierarchyView, event):     if event == __editSourceKeyPress:         selectedPath = __hierarchyViewSelectedPath(hierarchyView)         <MASK>             __editSourceNode(                 hierarchyView.getContext(), hierarchyView.scene(), selectedPath             )         return True     elif event == __editTweaksKeyPress:         selectedPath = __hierarchyViewSelectedPath(hierarchyView)         if selectedPath is not None:             __editTweaksNode(                 hierarchyView.getContext(), hierarchyView.scene(), selectedPath             )         return True",if selectedPath is not None :,if selectedPath is not None :,100.00000000000004,100.00000000000004,1.0
"def getSubsegments(self):     for num, localdata in self.lfh.LocalData:         for bucket, seginfo in localdata.SegmentInfo:             <MASK>                 continue             yield Win32Subsegment(self.trace, self.heap, seginfo.ActiveSubsegment)",if seginfo . ActiveSubsegment == 0 :,if num == 0 :,38.49815007763549,38.49815007763549,0.0
"def test_full_hd_bluray(self):     cur_test = ""full_hd_bluray""     cur_qual = common.Quality.FULLHDBLURAY     for name, tests in iteritems(self.test_cases):         for test in tests:             <MASK>                 self.assertEqual(cur_qual, common.Quality.name_quality(test))             else:                 self.assertNotEqual(cur_qual, common.Quality.name_quality(test))",if name == cur_test :,"if name == ""full_hd_bluray"" :",23.462350320527996,23.462350320527996,0.0
"def calc(self, arg):     op = arg[""op""]     if op == ""C"":         self.clear()         return str(self.current)     num = decimal.Decimal(arg[""num""])     if self.op:         if self.op == ""+"":             self.current += num         elif self.op == ""-"":             self.current -= num         elif self.op == ""*"":             self.current *= num         <MASK>             self.current /= num         self.op = op     else:         self.op = op         self.current = num     res = str(self.current)     if op == ""="":         self.clear()     return res","elif self . op == ""/"" :","if self . op == ""C"" :",58.14307369682194,58.14307369682194,0.0
"def strip_export_type(path):     matched = re.search(r""#([a-zA-Z0-9\-]+\\+[a-zA-Z0-9\-]+)?$"", path.encode(""utf-8""))     mime_type = None     if matched:         fragment = matched.group(0)         mime_type = matched.group(1)         <MASK>             mime_type = mime_type.replace(""+"", ""/"")         path = path[: -len(fragment)]     return (path, mime_type)",if mime_type is not None :,if fragment :,6.5479514338598115,0.0,0.0
"def _save_as_module(file, data, binary=False):     if not data:         return     with open(file, ""w"") as f:         f.write(""DATA="")         <MASK>             f.write('""')             f.write(base64.b64encode(data).decode(""ascii""))             f.write('""')         else:             f.write(str(data).replace(""\\\\"", ""\\""))         f.flush()",if binary :,if binary :,100.00000000000004,0.0,1.0
"def ProcessStringLiteral(self):     if self._lastToken == None or self._lastToken.type == self.OpenBrace:         text = super(JavaScriptBaseLexer, self).text         <MASK>             if len(self._scopeStrictModes) > 0:                 self._scopeStrictModes.pop()             self._useStrictCurrent = True             self._scopeStrictModes.append(self._useStrictCurrent)","if text == '""use strict""' or text == ""'use strict'"" :",if text :,0.30414179125340285,0.0,0.0
"def run(self, ttl=None):     self.zeroconf = zeroconf.Zeroconf()     zeroconf.ServiceBrowser(self.zeroconf, self.domain, MDNSHandler(self))     if ttl:         gobject.timeout_add(ttl * 1000, self.shutdown)     self.__running = True     self.__mainloop = gobject.MainLoop()     context = self.__mainloop.get_context()     while self.__running:         try:             <MASK>                 context.iteration(True)             else:                 time.sleep(0.1)         except KeyboardInterrupt:             break     self.zeroconf.close()     logger.debug(""MDNSListener.run() quit"")",if context . pending ( ) :,if context :,16.605579150202516,0.0,0.0
"def topology_change_notify(self, port_state):     notice = False     if port_state is PORT_STATE_FORWARD:         for port in self.ports.values():             if port.role is DESIGNATED_PORT:                 notice = True                 break     else:         notice = True     if notice:         self.send_event(EventTopologyChange(self.dp))         <MASK>             self._transmit_tc_bpdu()         else:             self._transmit_tcn_bpdu()",if self . is_root_bridge :,if port . role is DESIGNATED_PORT :,7.129384882260374,7.129384882260374,0.0
def close_open_fds(keep=None):  # noqa     keep = [maybe_fileno(f) for f in (keep or []) if maybe_fileno(f) is not None]     for fd in reversed(range(get_fdmax(default=2048))):         if fd not in keep:             try:                 os.close(fd)             except OSError as exc:                 <MASK>                     raise,if exc . errno != errno . EBADF :,if exc . errno == errno . EINVAL :,39.281465090051285,39.281465090051285,0.0
"def collect_attributes(options, node, master_list):     """"""Collect all attributes""""""     for ii in node.instructions:         if field_check(ii, ""attributes""):             s = getattr(ii, ""attributes"")             if isinstance(s, list):                 for x in s:                     if x not in master_list:                         master_list.append(x)             <MASK>                 master_list.append(s)     for nxt in node.next.values():         collect_attributes(options, nxt, master_list)",elif s != None and s not in master_list :,"if isinstance ( s , tuple ) :",3.515208856700362,3.515208856700362,0.0
"def remove_test_run_directories(expiry_time: int = 60 * 60) -> int:     removed = 0     directories = glob.glob(os.path.join(UUID_VAR_DIR, ""test-backend"", ""run_*""))     for test_run in directories:         <MASK>             try:                 shutil.rmtree(test_run)                 removed += 1             except FileNotFoundError:                 pass     return removed",if round ( time . time ( ) ) - os . path . getmtime ( test_run ) > expiry_time :,if expiry_time > 0 :,2.221829618439346,2.221829618439346,0.0
"def read_work_titles(fields):     found = []     if ""240"" in fields:         for line in fields[""240""]:             title = join_subfield_values(line, [""a"", ""m"", ""n"", ""p"", ""r""])             <MASK>                 found.append(title)     if ""130"" in fields:         for line in fields[""130""]:             title = "" "".join(get_lower_subfields(line))             if title not in found:                 found.append(title)     return {""work_titles"": found} if found else {}",if title not in found :,if title not in found :,100.00000000000004,100.00000000000004,1.0
"def _process_v1_msg(prot, msg):     header = None     body = msg[1]     if not isinstance(body, (binary_type, mmap, memoryview)):         raise ValidationError(body, ""Body must be a bytestream."")     if len(msg) > 2:         header = msg[2]         <MASK>             raise ValidationError(header, ""Header must be a dict."")         for k, v in header.items():             header[k] = msgpack.unpackb(v)     ctx = MessagePackMethodContext(prot, MessagePackMethodContext.SERVER)     ctx.in_string = [body]     ctx.transport.in_header = header     return ctx","if not isinstance ( header , dict ) :","if not isinstance ( header , dict ) :",100.00000000000004,100.00000000000004,1.0
"def find(self, node):     typename = type(node).__name__     method = getattr(self, ""find_{}"".format(typename), None)     if method is None:         fields = getattr(node, ""_fields"", None)         <MASK>             return         for field in fields:             value = getattr(node, field)             for result in self.find(value):                 yield result     else:         for result in method(node):             yield result",if fields is None :,if not fields :,16.37226966703825,16.37226966703825,0.0
"def _str_param_list(self, name):     out = []     if self[name]:         out += self._str_header(name)         for param in self[name]:             parts = []             <MASK>                 parts.append(param.name)             if param.type:                 parts.append(param.type)             out += ["" : "".join(parts)]             if param.desc and """".join(param.desc).strip():                 out += self._str_indent(param.desc)         out += [""""]     return out",if param . name :,if param . name :,100.00000000000004,100.00000000000004,1.0
"def _get_image(self, image_list, source):     if source.startswith(""wx""):         img = wx.ArtProvider_GetBitmap(source, wx.ART_OTHER, _SIZE)     else:         path = os.path.join(_BASE, source)         <MASK>             img = wx.Image(path, wx.BITMAP_TYPE_GIF).ConvertToBitmap()         else:             img = wx.Image(path, wx.BITMAP_TYPE_PNG).ConvertToBitmap()     return image_list.Add(img)","if source . endswith ( ""gif"" ) :","if source . startswith ( ""wx"" ) :",29.84745896009822,29.84745896009822,0.0
"def change_opacity_function(self, new_f):     self.opacity_function = new_f     dr = self.radius / self.num_levels     sectors = []     for submob in self.submobjects:         <MASK>             sectors.append(submob)     for (r, submob) in zip(np.arange(0, self.radius, dr), sectors):         if type(submob) != AnnularSector:             # it's the shadow, don't dim it             continue         alpha = self.opacity_function(r)         submob.set_fill(opacity=alpha)",if type ( submob ) == AnnularSector :,if submob . is_shadow :,6.495032985064742,6.495032985064742,0.0
"def _sqlite_post_configure_engine(url, engine, follower_ident):     from sqlalchemy import event     @event.listens_for(engine, ""connect"")     def connect(dbapi_connection, connection_record):         # use file DBs in all cases, memory acts kind of strangely         # as an attached         <MASK>             dbapi_connection.execute('ATTACH DATABASE ""test_schema.db"" AS test_schema')         else:             dbapi_connection.execute(                 'ATTACH DATABASE ""%s_test_schema.db"" AS test_schema' % follower_ident             )",if not follower_ident :,"if follower_ident == ""test"" :",17.747405280050266,17.747405280050266,0.0
"def apply_conf_file(fn, conf_filename):     for env in LSF_CONF_ENV:         conf_file = get_conf_file(conf_filename, env)         if conf_file:             with open(conf_file) as conf_handle:                 value = fn(conf_handle)             <MASK>                 return value     return None",if value :,if value is not None :,17.965205598154213,17.965205598154213,0.0
"def test_call_extern_c_fn(self):     global memcmp     memcmp = cffi_support.ExternCFunction(         ""memcmp"",         (""int memcmp ( const uint8_t * ptr1, "" ""const uint8_t * ptr2, size_t num )""),     )     @udf(BooleanVal(FunctionContext, StringVal, StringVal))     def fn(context, a, b):         if a.is_null != b.is_null:             return False         if a is None:             return True         <MASK>             return False         if a.ptr == b.ptr:             return True         return memcmp(a.ptr, b.ptr, a.len) == 0",if len ( a ) != b . len :,if b is None :,4.234348806659263,4.234348806659263,0.0
"def _get_initialized_app(app):     """"""Returns a reference to an initialized App instance.""""""     if app is None:         return firebase_admin.get_app()     if isinstance(app, firebase_admin.App):         initialized_app = firebase_admin.get_app(app.name)         <MASK>             raise ValueError(                 ""Illegal app argument. App instance not ""                 ""initialized via the firebase module.""             )         return app     raise ValueError(         ""Illegal app argument. Argument must be of type ""         ' firebase_admin.App, but given ""{0}"".'.format(type(app))     )",if app is not initialized_app :,if initialized_app is not None :,35.930411196308434,35.930411196308434,0.0
def compiled_query(self):     <MASK>         self.lazy_init_lock_.acquire()         try:             if self.compiled_query_ is None:                 self.compiled_query_ = CompiledQuery()         finally:             self.lazy_init_lock_.release()     return self.compiled_query_,if self . compiled_query_ is None :,if self . compiled_query_ is None :,100.00000000000004,100.00000000000004,1.0
"def clean_subevent(event, subevent):     if event.has_subevents:         <MASK>             raise ValidationError(_(""Subevent cannot be null for event series.""))         if event != subevent.event:             raise ValidationError(_(""The subevent does not belong to this event.""))     else:         if subevent:             raise ValidationError(_(""The subevent does not belong to this event.""))",if not subevent :,if subevent :,45.13864405503391,0.0,0.0
"def get_blob_type_declaration_sql(self, column):     length = column.get(""length"")     if length:         if length <= self.LENGTH_LIMIT_TINYBLOB:             return ""TINYBLOB""         if length <= self.LENGTH_LIMIT_BLOB:             return ""BLOB""         <MASK>             return ""MEDIUMBLOB""     return ""LONGBLOB""",if length <= self . LENGTH_LIMIT_MEDIUMBLOB :,if length <= self . LENGTH_LIMIT_MEDIUMBLOB :,100.00000000000004,100.00000000000004,1.0
"def decompress(self, data):     if not data:         return data     if not self._first_try:         return self._obj.decompress(data)     self._data += data     try:         decompressed = self._obj.decompress(data)         <MASK>             self._first_try = False             self._data = None         return decompressed     except zlib.error:         self._first_try = False         self._obj = zlib.decompressobj(-zlib.MAX_WBITS)         try:             return self.decompress(self._data)         finally:             self._data = None",if decompressed :,if decompressed is None :,23.643540225079384,23.643540225079384,0.0
"def _record_event(self, path, fsevent_handle, filename, events, error):     with self.lock:         self.events[path].append(events)         if events | pyuv.fs.UV_RENAME:             <MASK>                 self.watches.pop(path).close()",if not os . path . exists ( path ) :,if error :,2.4088567143060917,0.0,0.0
"def __init__(self, duration, batch_shape, event_shape, validate_args=None):     if duration is None:         <MASK>             # Infer duration from event_shape.             duration = event_shape[0]     elif duration != event_shape[0]:         if event_shape[0] != 1:             raise ValueError(                 ""duration, event_shape mismatch: {} vs {}"".format(duration, event_shape)             )         # Infer event_shape from duration.         event_shape = torch.Size((duration,) + event_shape[1:])     self._duration = duration     super().__init__(batch_shape, event_shape, validate_args)",if event_shape [ 0 ] != 1 :,if duration == 1 :,14.110009442520557,14.110009442520557,0.0
"def _CheckPrerequisites(self):     """"""Exits if any of the prerequisites is not met.""""""     if not FLAGS.kubectl:         raise Exception(             ""Please provide path to kubectl tool using --kubectl "" ""flag. Exiting.""         )     if not FLAGS.kubeconfig:         raise Exception(             ""Please provide path to kubeconfig using --kubeconfig "" ""flag. Exiting.""         )     if self.disk_specs and self.disk_specs[0].disk_type == disk.STANDARD:         <MASK>             raise Exception(                 ""Please provide a list of Ceph Monitors using "" ""--ceph_monitors flag.""             )",if not FLAGS . ceph_monitors :,if not FLAGS . ceph_monitors :,100.00000000000004,100.00000000000004,1.0
"def invalidateDependentSlices(self, iFirstCurve):     # only user defined curve can have slice dependency relationships     if self.isSystemCurveIndex(iFirstCurve):         return     nCurves = self.getNCurves()     for i in range(iFirstCurve, nCurves):         c = self.getSystemCurve(i)         <MASK>             c.invalidate()         elif i == iFirstCurve:             # if first curve isn't a slice,             break             # there are no dependent slices","if isinstance ( c . getSymbol ( ) . getSymbolType ( ) , SymbolType . PieSliceSymbolType ) :",if c is not None :,1.4456752008489673,1.4456752008489673,0.0
"def find_backwards(self, offset):     try:         for _, token_type, token_value in reversed(self.tokens[self.offset : offset]):             if token_type in (""comment"", ""linecomment""):                 try:                     prefix, comment = token_value.split(None, 1)                 except ValueError:                     continue                 <MASK>                     return [comment.rstrip()]         return []     finally:         self.offset = offset",if prefix in self . comment_tags :,"if prefix == ""linecomment"" :",10.786826322527466,10.786826322527466,0.0
"def parse_column_definitions(self, elem):     for column_elem in elem.findall(""column""):         name = column_elem.get(""name"", None)         assert name is not None, ""Required 'name' attribute missing from column def""         index = column_elem.get(""index"", None)         assert index is not None, ""Required 'index' attribute missing from column def""         index = int(index)         self.columns[name] = index         <MASK>             self.largest_index = index     assert ""value"" in self.columns, ""Required 'value' column missing from column def""     if ""name"" not in self.columns:         self.columns[""name""] = self.columns[""value""]",if index > self . largest_index :,"if ""largest_index"" in self . columns :",19.081654556856684,19.081654556856684,0.0
"def __find_smallest(self):     """"""Find the smallest uncovered value in the matrix.""""""     minval = sys.maxsize     for i in range(self.n):         for j in range(self.n):             if (not self.row_covered[i]) and (not self.col_covered[j]):                 <MASK>                     minval = self.C[i][j]     return minval",if minval > self . C [ i ] [ j ] :,if minval < minval :,4.773548444510098,4.773548444510098,0.0
"def includes_tools_for_display_in_tool_panel(self):     if self.includes_tools:         tool_dicts = self.metadata[""tools""]         for tool_dict in tool_dicts:             <MASK>                 return True     return False","if tool_dict . get ( ""add_to_tool_panel"" , True ) :","if tool_dict [ ""display_in_tool_panel"" ] == self . display_in_tool_panel :",23.85948537905441,23.85948537905441,0.0
"def commit(self, notify=False):     if self.editing:         text = self._text         if text:             try:                 value = self.type(text)             except ValueError:                 return             value = self.clamp_value(value)         else:             value = self.empty             if value is NotImplemented:                 return         self.value = value         self.insertion_point = None         <MASK>             self.change_text(unicode(value))         else:             self._text = unicode(value)         self.editing = False     else:         self.insertion_point = None",if notify :,if notify :,100.00000000000004,0.0,1.0
"def GeneratePageMetatadata(self, task):     address_space = self.session.GetParameter(""default_address_space"")     for vma in task.mm.mmap.walk_list(""vm_next""):         start = vma.vm_start         end = vma.vm_end         # Skip the entire region.         if end < self.plugin_args.start:             continue         # Done.         <MASK>             break         for vaddr in utils.xrange(start, end, 0x1000):             if self.plugin_args.start <= vaddr <= self.plugin_args.end:                 yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))",if start > self . plugin_args . end :,if start > self . plugin_args . end :,100.00000000000004,100.00000000000004,1.0
"def _check_for_duplicate_host_entries(self, task_entries):     non_host_statuses = (         models.HostQueueEntry.Status.PARSING,         models.HostQueueEntry.Status.ARCHIVING,     )     for task_entry in task_entries:         using_host = (             task_entry.host is not None and task_entry.status not in non_host_statuses         )         <MASK>             self._assert_host_has_no_agent(task_entry)",if using_host :,if using_host :,100.00000000000004,100.00000000000004,1.0
"def get_biggest_wall_time(jsons):     lowest_wall = None     for j in jsons:         <MASK>             lowest_wall = j[""wall_time""]         if lowest_wall < j[""wall_time""]:             lowest_wall = j[""wall_time""]     return lowest_wall",if lowest_wall is None :,if lowest_wall is None :,100.00000000000004,100.00000000000004,1.0
"def log_change_report(self, old_value, new_value, include_details=False):     from octoprint.util import map_boolean     with self._check_mutex:         self._logger.info(             ""Connectivity changed from {} to {}"".format(                 map_boolean(old_value, ""online"", ""offline""),                 map_boolean(new_value, ""online"", ""offline""),             )         )         <MASK>             self.log_details()",if include_details :,if include_details :,100.00000000000004,100.00000000000004,1.0
"def _include_block(self, value, context=None):     if hasattr(value, ""render_as_block""):         <MASK>             new_context = context.get_all()         else:             new_context = {}         return jinja2.Markup(value.render_as_block(context=new_context))     return jinja2.Markup(value)",if context :,if context :,100.00000000000004,0.0,1.0
"def __lt__(self, other):     # 0: clock 1: timestamp 3: process id     try:         A, B = self[0], other[0]         # uses logical clock value first         if A and B:  # use logical clock if available             <MASK>  # equal clocks use lower process id                 return self[2] < other[2]             return A < B         return self[1] < other[1]  # ... or use timestamp     except IndexError:         return NotImplemented",if A == B :,if A == B :,100.00000000000004,100.00000000000004,1.0
"def _get_port():     while True:         port = 20000 + random.randint(1, 9999)         for i in range(5):             sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)             result = sock.connect_ex((""127.0.0.1"", port))             <MASK>                 continue         else:             return port",if result == 0 :,if result == 0 :,100.00000000000004,100.00000000000004,1.0
"def fetch_all(self, api_client, fetchstatuslogger, q, targets):     self.fetchstatuslogger = fetchstatuslogger     if targets != None:         # Ensure targets is a tuple         if type(targets) != list and type(targets) != tuple:             targets = tuple(                 targets,             )         <MASK>             targets = tuple(targets)     for target in targets:         self._fetch_targets(api_client, q, target)",elif type ( targets ) != tuple :,if type ( targets ) != list :,61.04735835807847,61.04735835807847,0.0
"def migrate_node_facts(facts):     """"""Migrate facts from various roles into node""""""     params = {         ""common"": (""dns_ip""),     }     if ""node"" not in facts:         facts[""node""] = {}     # pylint: disable=consider-iterating-dictionary     for role in params.keys():         <MASK>             for param in params[role]:                 if param in facts[role]:                     facts[""node""][param] = facts[role].pop(param)     return facts",if role in facts :,"if ""common"" in params [ role ] :",5.934202609760488,5.934202609760488,0.0
"def build_dimension_param(self, dimension, params):     prefix = ""Dimensions.member""     i = 0     for dim_name in dimension:         dim_value = dimension[dim_name]         <MASK>             if isinstance(dim_value, six.string_types):                 dim_value = [dim_value]             for value in dim_value:                 params[""%s.%d.Name"" % (prefix, i + 1)] = dim_name                 params[""%s.%d.Value"" % (prefix, i + 1)] = value                 i += 1         else:             params[""%s.%d.Name"" % (prefix, i + 1)] = dim_name             i += 1",if dim_value :,"if isinstance ( dim_value , list ) :",17.747405280050266,17.747405280050266,0.0
"def add_if_unique(self, issuer, use, keys):     if use in self.issuer_keys[issuer] and self.issuer_keys[issuer][use]:         for typ, key in keys:             flag = 1             for _typ, _key in self.issuer_keys[issuer][use]:                 <MASK>                     flag = 0                     break             if flag:                 self.issuer_keys[issuer][use].append((typ, key))     else:         self.issuer_keys[issuer][use] = keys",if _typ == typ and key is _key :,if typ not in self . issuer_keys [ issuer ] :,4.368583925857938,4.368583925857938,0.0
"def run(self):     while True:         message = self.in_queue.get()         <MASK>             self.reset()         elif message == EXIT:             return         else:             index, transaction = message             self.results_queue.put((index, self.validate(transaction)))",if message == RESET :,if message == EXIT :,53.7284965911771,53.7284965911771,0.0
"def __run(self):     threads = self.parameters()[""threads""].getTypedValue()     with IECore.tbb_global_control(         IECore.tbb_global_control.parameter.max_allowed_parallelism,         IECore.hardwareConcurrency() if threads == 0 else threads,     ):         self._executeStartupFiles(self.root().getName())         # Append DEBUG message with process information to all messages         defaultMessageHandler = IECore.MessageHandler.getDefaultHandler()         <MASK>             IECore.MessageHandler.setDefaultHandler(                 Gaffer.ProcessMessageHandler(defaultMessageHandler)             )         return self._run(self.parameters().getValidatedValue())","if not isinstance ( defaultMessageHandler , Gaffer . ProcessMessageHandler ) :",if defaultMessageHandler :,2.7574525891364066,0.0,0.0
"def adjust_uri(self, uri, relativeto):     """"""Adjust the given ``uri`` based on the given relative URI.""""""     key = (uri, relativeto)     if key in self._uri_cache:         return self._uri_cache[key]     if uri[0] != ""/"":         <MASK>             v = self._uri_cache[key] = posixpath.join(                 posixpath.dirname(relativeto), uri             )         else:             v = self._uri_cache[key] = ""/"" + uri     else:         v = self._uri_cache[key] = uri     return v",if relativeto is not None :,"if uri [ 0 ] != ""/"" :",4.456882760699063,4.456882760699063,0.0
"def decoder(s):     r = []     decode = []     for c in s:         <MASK>             decode.append(""&"")         elif c == ""-"" and decode:             if len(decode) == 1:                 r.append(""&"")             else:                 r.append(modified_unbase64("""".join(decode[1:])))             decode = []         elif decode:             decode.append(c)         else:             r.append(c)     if decode:         r.append(modified_unbase64("""".join(decode[1:])))     bin_str = """".join(r)     return (bin_str, len(s))","if c == ""&"" and not decode :","if c == ""+"" and decode :",45.561621331146846,45.561621331146846,0.0
"def _process_file(self, content):     args = []     for line in content.splitlines():         line = line.strip()         if line.startswith(""-""):             args.extend(self._split_option(line))         <MASK>             args.append(line)     return args","elif line and not line . startswith ( ""#"" ) :","if line . startswith ( ""-"" ) :",38.921254277570085,38.921254277570085,0.0
"def _method_events_callback(self, values):     try:         previous_echoed = (             values[""child_result_list""][-1].decode().split(""\n"")[-2].strip()         )         if previous_echoed.endswith(""foo1""):             return ""echo foo2\n""         elif previous_echoed.endswith(""foo2""):             return ""echo foo3\n""         <MASK>             return ""exit\n""         else:             raise Exception(""Unexpected output {0!r}"".format(previous_echoed))     except IndexError:         return ""echo foo1\n""","elif previous_echoed . endswith ( ""foo3"" ) :","if previous_echoed . endswith ( ""exit"" ) :",63.40466277046863,63.40466277046863,0.0
"def __delete_hook(self, rpc):     try:         rpc.check_success()     except apiproxy_errors.Error:         return None     result = []     for status in rpc.response.delete_status_list():         <MASK>             result.append(DELETE_SUCCESSFUL)         elif status == MemcacheDeleteResponse.NOT_FOUND:             result.append(DELETE_ITEM_MISSING)         else:             result.append(DELETE_NETWORK_FAILURE)     return result",if status == MemcacheDeleteResponse . DELETED :,if status == MemcacheDeleteResponse . SUCCESS :,70.71067811865478,70.71067811865478,0.0
"def __createRandom(plug):     node = plug.node()     parentNode = node.ancestor(Gaffer.Node)     with Gaffer.UndoScope(node.scriptNode()):         randomNode = Gaffer.Random()         parentNode.addChild(randomNode)         <MASK>             plug.setInput(randomNode[""outFloat""])         elif isinstance(plug, Gaffer.Color3fPlug):             plug.setInput(randomNode[""outColor""])     GafferUI.NodeEditor.acquire(randomNode)","if isinstance ( plug , ( Gaffer . FloatPlug , Gaffer . IntPlug ) ) :","if isinstance ( plug , Gaffer . FloatPlug ) :",34.64082076796053,34.64082076796053,0.0
"def escapeentities(self, line):     ""Escape all Unicode characters to HTML entities.""     result = """"     pos = TextPosition(line)     while not pos.finished():         if ord(pos.current()) > 128:             codepoint = hex(ord(pos.current()))             <MASK>                 codepoint = hex(ord(pos.next()) + 0xF800)             result += ""&#"" + codepoint[1:] + "";""         else:             result += pos.current()         pos.skipcurrent()     return result","if codepoint == ""0xd835"" :",if codepoint [ 0 ] == 0x00 :,14.25876976452075,14.25876976452075,0.0
def get_and_set_all_aliases(self):     all_aliases = []     for page in self.pages:         <MASK>             all_aliases.extend(page.relations.aliases_norm)         if page.relations.aliases is not None:             all_aliases.extend(page.relations.aliases)     return set(all_aliases),if page . relations . aliases_norm is not None :,if page . relations . aliases_norm is not None :,100.00000000000004,100.00000000000004,1.0
"def _list_cases(suite):     for test in suite:         <MASK>             _list_cases(test)         elif isinstance(test, unittest.TestCase):             if support.match_test(test):                 print(test.id())","if isinstance ( test , unittest . TestSuite ) :",if test . id ( ) in test . test_cases :,4.834632845440431,4.834632845440431,0.0
"def get_next_requests(self, max_n_requests, **kwargs):     next_pages = []     partitions = set(kwargs.pop(""partitions"", []))     for partition_id in range(0, self.queue_partitions):         <MASK>             continue         results = self.queue.get_next_requests(max_n_requests, partition_id)         next_pages.extend(results)         self.logger.debug(             ""Got %d requests for partition id %d"", len(results), partition_id         )     return next_pages",if partition_id not in partitions :,if partition_id not in partitions :,100.00000000000004,100.00000000000004,1.0
"def __iter__(self):     if (self.query is not None) and sqlite.is_read_only_query(self.query):         cur = self.connection.cursor()         results = cur.execute(self.query)         <MASK>             yield [col[0] for col in cur.description]         for i, row in enumerate(results):             if i >= self.limit:                 break             yield [val for val in row]     else:         yield",if self . headers :,if not results :,14.794015674776452,14.794015674776452,0.0
"def rollback(self):     for operation, values in self.current_transaction_state[::-1]:         if operation == ""insert"":             values.remove()         <MASK>             old_value, new_value = values             if new_value.full_filename != old_value.full_filename:                 os.unlink(new_value.full_filename)             old_value.write()     self._post_xact_cleanup()","elif operation == ""update"" :","if operation == ""delete"" :",41.11336169005198,41.11336169005198,0.0
"def index(self, value):     if self._growing:         if self._start <= value < self._stop:             q, r = divmod(value - self._start, self._step)             <MASK>                 return int(q)     else:         if self._start >= value > self._stop:             q, r = divmod(self._start - value, -self._step)             if r == self._zero:                 return int(q)     raise ValueError(""{} is not in numeric range"".format(value))",if r == self . _zero :,if r == self . _zero :,100.00000000000004,100.00000000000004,1.0
"def validate_name_and_description(body, check_length=True):     for attribute in [""name"", ""description"", ""display_name"", ""display_description""]:         value = body.get(attribute)         <MASK>             if isinstance(value, six.string_types):                 body[attribute] = value.strip()             if check_length:                 try:                     utils.check_string_length(                         body[attribute], attribute, min_length=0, max_length=255                     )                 except exception.InvalidInput as error:                     raise webob.exc.HTTPBadRequest(explanation=error.msg)",if value is not None :,if value is not None :,100.00000000000004,100.00000000000004,1.0
"def printWiki():     firstHeading = False     for m in protocol:         if m[0] == """":             <MASK>                 output(""|}"")             __printWikiHeader(m[1], m[2])             firstHeading = True         else:             output(""|-"")             output(                 '| <span style=""white-space:nowrap;""><tt>'                 + m[0]                 + ""</tt></span> || || ""                 + m[1]             )     output(""|}"")",if firstHeading :,if firstHeading :,100.00000000000004,0.0,1.0
"def _get_platforms(data):     platform_list = []     for item in data:         if item.startswith(""PlatformEdit.html?""):             parameter_list = item.split(""PlatformEdit.html?"", 1)[1].split(""&"")             for parameter in parameter_list:                 <MASK>                     platform_list.append(parameter.split(""="")[1])     return platform_list","if parameter . startswith ( ""platformName"" ) :","if parameter . startswith ( ""PlatformEdit.html?"" ) :",48.44273237963865,48.44273237963865,0.0
"def find_scintilla_constants(f):     lexers = []     states = []     for name in f.order:         v = f.features[name]         if v[""Category""] != ""Deprecated"":             if v[""FeatureType""] == ""val"":                 <MASK>                     states.append((name, v[""Value""]))                 elif name.startswith(""SCLEX_""):                     lexers.append((name, v[""Value""]))     return (lexers, states)","if name . startswith ( ""SCE_"" ) :","if name . startswith ( ""SCINTAL_STATE"" ) :",54.52469119630866,54.52469119630866,0.0
"def get_operation_ast(document_ast, operation_name=None):     operation = None     for definition in document_ast.definitions:         if isinstance(definition, ast.OperationDefinition):             <MASK>                 # If no operation name is provided, only return an Operation if it is the only one present in the                 # document. This means that if we've encountered a second operation as we were iterating over the                 # definitions in the document, there are more than one Operation defined, and we should return None.                 if operation:                     return None                 operation = definition             elif definition.name and definition.name.value == operation_name:                 return definition     return operation",if not operation_name :,if operation_name is None :,27.77619034011791,27.77619034011791,0.0
"def _insertNewItemAtParent(self, targetIndex):     if not self.isContainer(targetIndex):         return     elif not self.isContainerOpen(targetIndex):         uri = self._rows[targetIndex].uri         modelNode = self.getNodeForURI(uri)         <MASK>             modelNode.markForRefreshing()         return     self.refreshView(targetIndex)",if modelNode :,if modelNode :,100.00000000000004,0.0,1.0
"def _get_trace(self, model, guide, args, kwargs):     model_trace, guide_trace = super()._get_trace(model, guide, args, kwargs)     # Mark all sample sites with require_backward to gather enumerated     # sites and adjust cond_indep_stack of all sample sites.     for node in model_trace.nodes.values():         <MASK>             log_prob = node[""packed""][""unscaled_log_prob""]             require_backward(log_prob)     self._saved_state = model, model_trace, guide_trace, args, kwargs     return model_trace, guide_trace","if node [ ""type"" ] == ""sample"" and not node [ ""is_observed"" ] :","if node [ ""packed"" ] [ ""unscaled_log_prob"" ] :",18.108091492137547,18.108091492137547,0.0
"def _url_encode_impl(obj, charset, encode_keys, sort, key):     from .datastructures import iter_multi_items     iterable = iter_multi_items(obj)     if sort:         iterable = sorted(iterable, key=key)     for key, value in iterable:         if value is None:             continue         if not isinstance(key, bytes):             key = text_type(key).encode(charset)         <MASK>             value = text_type(value).encode(charset)         yield _fast_url_quote_plus(key) + ""="" + _fast_url_quote_plus(value)","if not isinstance ( value , bytes ) :","if not isinstance ( value , bytes ) :",100.00000000000004,100.00000000000004,1.0
"def handle_parse_result(self, ctx, opts, args):     with augment_usage_errors(ctx, param=self):         value = self.consume_value(ctx, opts)         try:             value = self.full_process_value(ctx, value)         except Exception:             <MASK>                 raise             value = None         if self.callback is not None:             try:                 value = invoke_param_callback(self.callback, ctx, self, value)             except Exception:                 if not ctx.resilient_parsing:                     raise     if self.expose_value:         ctx.params[self.name] = value     return value, args",if not ctx . resilient_parsing :,if ctx . resilient_parsing :,72.89545183625967,72.89545183625967,0.0
"def word_pattern(pattern, str):     dict = {}     set_value = set()     list_str = str.split()     if len(list_str) != len(pattern):         return False     for i in range(len(pattern)):         if pattern[i] not in dict:             <MASK>                 return False             dict[pattern[i]] = list_str[i]             set_value.add(list_str[i])         else:             if dict[pattern[i]] != list_str[i]:                 return False     return True",if list_str [ i ] in set_value :,if set_value . add ( list_str [ i ] ) :,43.33207865423753,43.33207865423753,0.0
"def create(self, path, wipe=False):     # type: (Text, bool) -> bool     _path = self.validatepath(path)     with ftp_errors(self, path):         <MASK>             empty_file = io.BytesIO()             self.ftp.storbinary(                 str(""STOR "") + _encode(_path, self.ftp.encoding), empty_file             )             return True     return False",if wipe or not self . isfile ( path ) :,if wipe :,4.377183140747567,0.0,0.0
"def build_output_for_item(self, item):     output = []     for field in self.fields:         values = self._get_item(item, field)         <MASK>             values = [values]         for value in values:             if value:                 output.append(self.build_output_for_single_value(value))     return """".join(output)","if not isinstance ( values , list ) :","if not isinstance ( values , list ) :",100.00000000000004,100.00000000000004,1.0
"def get_resource_public_actions(resource_class):     resource_class_members = inspect.getmembers(resource_class)     resource_methods = {}     for name, member in resource_class_members:         if not name.startswith(""_""):             if not name[0].isupper():                 <MASK>                     if is_resource_action(member):                         resource_methods[name] = member     return resource_methods","if not name . startswith ( ""wait_until"" ) :",if member . __class__ == resource_class :,4.016138436407654,4.016138436407654,0.0
"def get_command(cls):     ifconfig_cmd = ""ifconfig""     for path in [""/sbin"", ""/usr/sbin"", ""/bin"", ""/usr/bin""]:         <MASK>             ifconfig_cmd = os.path.join(path, ifconfig_cmd)             break     ifconfig_cmd = ifconfig_cmd + "" -a""     return ifconfig_cmd","if os . path . exists ( os . path . join ( path , ifconfig_cmd ) ) :","ifconfig_cmd : if config_cmd = ifconfig_cmd + "" -a"" + path",8.047708401675957,8.047708401675957,0.0
"def main():     base_dir = os.path.join(os.path.split(__file__)[0], "".."", "".."")     for path in PATHS:         path = os.path.join(base_dir, path)         for root, _, files in os.walk(path):             for file in files:                 extension = os.path.splitext(file)[1]                 <MASK>                     path = os.path.join(root, file)                     validate_header(path)",if extension in EXTENSIONS :,"if extension == ""header"" :",12.22307556087252,12.22307556087252,0.0
"def auth_login(request):     form = RegistrationForm(request.POST or None)     if form.is_valid():         authed_user = authenticate(             username=form.cleaned_data[""username""],             password=form.cleaned_data[""password""],         )         <MASK>             login(request, authed_user)             return HttpResponse(""Success"")     raise Http404",if authed_user :,if authed_user :,100.00000000000004,100.00000000000004,1.0
"def set(self, _key, _new_login=True):     with self.lock:         user = self.users.get(current_user.id, None)         if user is None:             self.users[current_user.id] = dict(session_count=1, key=_key)         else:             <MASK>                 user[""session_count""] += 1             user[""key""] = _key",if _new_login :,if _new_login :,100.00000000000004,100.00000000000004,1.0
"def fetch(self, fingerprints):     to_fetch = [f for f in fingerprints if f not in self._cache]     self._logger.debug(""cache size %s"" % len(self._cache))     self._logger.debug(""to fetch %d from %d"" % (len(to_fetch), len(fingerprints)))     [self._redis_pipeline.hgetall(key) for key in to_fetch]     responses = self._redis_pipeline.execute()     for index, key in enumerate(to_fetch):         response = responses[index]         <MASK>             self._cache[key] = response[FIELD_STATE]         else:             self._cache[key] = self.NOT_CRAWLED",if len ( response ) > 0 and FIELD_STATE in response :,if response [ FIELD_STATE ] :,11.547544133164605,11.547544133164605,0.0
"def _append_to_io_queue(self, data, stream_name):     # Make sure ANSI CSI codes and object links are stored as separate events     # TODO: try to complete previously submitted incomplete code     parts = re.split(OUTPUT_SPLIT_REGEX, data)     for part in parts:         if part:  # split may produce empty string in the beginning or start             # split the data so that very long lines separated             for block in re.split(                 ""(.{%d,})"" % (self._get_squeeze_threshold() + 1), part             ):                 <MASK>                     self._queued_io_events.append((block, stream_name))",if block :,if block :,100.00000000000004,0.0,1.0
"def find_file_at_path_with_indexes(self, path, url):     if url.endswith(""/""):         path = os.path.join(path, self.index_file)         return self.get_static_file(path, url)     elif url.endswith(""/"" + self.index_file):         <MASK>             return self.redirect(url, url[: -len(self.index_file)])     else:         try:             return self.get_static_file(path, url)         except IsDirectoryError:             if os.path.isfile(os.path.join(path, self.index_file)):                 return self.redirect(url, url + ""/"")     raise MissingFileError(path)",if os . path . isfile ( path ) :,if os . path . isdir ( path ) :,65.80370064762461,65.80370064762461,0.0
"def module_list(target, fast):     """"""Find the list of modules to be compiled""""""     modules = []     native = native_modules(target)     basedir = os.path.join(ouroboros_repo_folder(), ""ouroboros"")     for name in os.listdir(basedir):         module_name, ext = os.path.splitext(name)         if ext == "".py"" or ext == """" and os.path.isdir(os.path.join(basedir, name)):             if module_name not in IGNORE_MODULES and module_name not in native:                 <MASK>                     modules.append(module_name)     return set(modules)",if not ( fast and module_name in KNOWN_PROBLEM_MODULES ) :,if fast :,0.5208155200691346,0.0,0.0
"def housenumber(self):     if self.address:         expression = r""\d+""         pattern = re.compile(expression)         match = pattern.search(self.address)         <MASK>             return int(match.group(0))",if match :,if match :,100.00000000000004,0.0,1.0
"def get_pip_version(import_path=BASE_IMPORT_PATH):     try:         pip = importlib.import_module(import_path)     except ImportError:         <MASK>             return get_pip_version(import_path=""pip"")         else:             import subprocess             version = subprocess.check_output([""pip"", ""--version""])             if version:                 version = version.decode(""utf-8"").split()[1]                 return version             return ""0.0.0""     version = getattr(pip, ""__version__"", None)     return version","if import_path != ""pip"" :",if not version :,4.238556455648295,4.238556455648295,0.0
"def __animate_progress(self):     """"""Change the status message, mostly used to animate progress.""""""     while True:         sleep_time = ThreadPool.PROGRESS_IDLE_DELAY         with self.__progress_lock:             if not self.__progress_status:                 sleep_time = ThreadPool.PROGRESS_IDLE_DELAY             <MASK>                 self.__progress_status.update_progress(self.__current_operation_name)                 sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY             else:                 self.__progress_status.show_as_ready()                 sleep_time = ThreadPool.PROGRESS_IDLE_DELAY         # Allow some time for progress status to be updated.         time.sleep(sleep_time)",elif self . __show_animation :,if self . __current_operation_name :,25.965358893403383,25.965358893403383,0.0
"def range_key_names(self):     keys = [self.range_key_attr]     for index in self.global_indexes:         range_key = None         for key in index.schema:             <MASK>                 range_key = keys.append(key[""AttributeName""])         keys.append(range_key)     return keys","if key [ ""KeyType"" ] == ""RANGE"" :","if key [ ""AttributeName"" ] in range_key :",24.107473040766184,24.107473040766184,0.0
"def run(self):     dist = self.distribution     commands = dist.command_options.keys()     settings = {}     for cmd in commands:         if cmd == ""saveopts"":             continue  # don't save our own options!         for opt, (src, val) in dist.get_option_dict(cmd).items():             <MASK>                 settings.setdefault(cmd, {})[opt] = val     edit_config(self.filename, settings, self.dry_run)","if src == ""command line"" :","if src == ""saveopts"" :",52.47357977607325,52.47357977607325,0.0
"def parse_move(self, node):     old, new = """", """"     for child in node:         tag, text = child.tag, child.text         text = text.strip() if text else None         if tag == ""Old"" and text:             old = text         <MASK>             new = text     return Move(old, new)","elif tag == ""New"" and text :","if tag == ""New"" and text :",88.01117367933934,88.01117367933934,0.0
"def __codeanalysis_settings_changed(self, current_finfo):     if self.data:         run_pyflakes, run_pep8 = self.pyflakes_enabled, self.pep8_enabled         for finfo in self.data:             self.__update_editor_margins(finfo.editor)             finfo.cleanup_analysis_results()             if (run_pyflakes or run_pep8) and current_finfo is not None:                 <MASK>                     finfo.run_code_analysis(run_pyflakes, run_pep8)",if current_finfo is not finfo :,if current_finfo . editor is self :,33.03164318013809,33.03164318013809,0.0
"def tchg(var, width):     ""Convert time string to given length""     ret = ""%2dh%02d"" % (var / 60, var % 60)     <MASK>         ret = ""%2dh"" % (var / 60)         if len(ret) > width:             ret = ""%2dd"" % (var / 60 / 24)             if len(ret) > width:                 ret = ""%2dw"" % (var / 60 / 24 / 7)     return ret",if len ( ret ) > width :,if len ( ret ) > width :,100.00000000000004,100.00000000000004,1.0
"def spider_log_activity(self, messages):     for i in range(0, messages):         <MASK>             self.sp_sl_p.send(                 sha1(str(randint(1, 1000))),                 b""http://helloworld.com/way/to/the/sun/"" + b""0"",             )         else:             self.sp_sl_p.send(                 sha1(str(randint(1, 1000))), b""http://way.to.the.sun"" + b""0""             )     self.sp_sl_p.flush()",if i % 2 == 0 :,if i == 0 :,43.29820146406896,43.29820146406896,0.0
"def decode_serial(self, offset):     serialnum = (         (self.cache[offset + 3] << 24)         + (self.cache[offset + 2] << 16)         + (self.cache[offset + 1] << 8)         + self.cache[offset]     )     serialstr = """"     is_alnum = True     for i in range(4):         <MASK>             is_alnum = False             break         serialstr += chr(self.cache[offset + 3 - i])     serial = serialstr if is_alnum else str(serialnum)     self.ann_field(offset, offset + 3, ""Serial "" + serial)",if not chr ( self . cache [ offset + 3 - i ] ) . isalnum ( ) :,if i == 0 :,1.0358715463283363,1.0358715463283363,0.0
def gettext(rv):     for child in rv.childNodes:         if child.nodeType == child.TEXT_NODE:             yield child.nodeValue         <MASK>             for item in gettext(child):                 yield item,if child . nodeType == child . ELEMENT_NODE :,if child . nodeType == child . ELEMENT_NODE :,100.00000000000004,100.00000000000004,1.0
"def determine_block_hints(self, text):     hints = """"     if text:         if text[0] in "" \n\x85\u2028\u2029"":             hints += str(self.best_indent)         if text[-1] not in ""\n\x85\u2028\u2029"":             hints += ""-""         <MASK>             hints += ""+""     return hints","elif len ( text ) == 1 or text [ - 2 ] in ""\n\x85\u2028\u2029"" :","if text [ 0 ] in ""\n\x85\u2028\u2029"" :",46.10405942733444,46.10405942733444,0.0
"def _infer_return_type(*args):     """"""Look at the type of all args and divine their implied return type.""""""     return_type = None     for arg in args:         if arg is None:             continue         <MASK>             if return_type is str:                 raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")             return_type = bytes         else:             if return_type is bytes:                 raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")             return_type = str     if return_type is None:         return str  # tempfile APIs return a str by default.     return return_type","if isinstance ( arg , bytes ) :",if return_type is bytes :,7.492442692259767,7.492442692259767,0.0
"def as_iconbitmap(cls, rkey):     """"""Get image path for use in iconbitmap property""""""     img = None     if rkey in cls._stock:         data = cls._stock[rkey]         <MASK>             fpath = data[""filename""]             fname = os.path.basename(fpath)             name, file_ext = os.path.splitext(fname)             file_ext = str(file_ext).lower()             if file_ext in TK_BITMAP_FORMATS:                 img = BITMAP_TEMPLATE.format(fpath)     return img","if data [ ""type"" ] not in ( ""stock"" , ""data"" , ""image"" ) :",if data :,0.08017090575578772,0.0,0.0
"def anonymize_ip(ip):     if ip:         match = RE_FIRST_THREE_OCTETS_OF_IP.findall(str(ip))         <MASK>             return ""%s%s"" % (match[0][0], ""0"")     return """"",if match :,if match :,100.00000000000004,0.0,1.0
"def serialize_tail(self):     msg = bytearray()     for v in self.info:         <MASK>             value = v[""value""].encode(""utf-8"")         elif v[""type""] == BMP_TERM_TYPE_REASON:             value = struct.pack(""!H"", v[""value""])         v[""len""] = len(value)         msg += struct.pack(self._TLV_PACK_STR, v[""type""], v[""len""])         msg += value     return msg","if v [ ""type"" ] == BMP_TERM_TYPE_STRING :","if v [ ""type"" ] == BMP_TERM_TYPE_DATA :",88.43946454355333,88.43946454355333,0.0
"def get_django_comment(text: str, i: int) -> str:     end = i + 4     unclosed_end = 0     while end <= len(text):         <MASK>             return text[i:end]         if not unclosed_end and text[end] == ""<"":             unclosed_end = end         end += 1     raise TokenizationException(""Unclosed comment"", text[i:unclosed_end])","if text [ end - 2 : end ] == ""#}"" :","if text [ i : end ] == "">"" :",41.81117364345026,41.81117364345026,0.0
"def ComboBoxDroppedHeightTest(windows):     ""Check if each combobox height is the same as the reference""     bugs = []     for win in windows:         if not win.ref:             continue         if win.Class() != ""ComboBox"" or win.ref.Class() != ""ComboBox"":             continue         <MASK>             bugs.append(                 (                     [                         win,                     ],                     {},                     testname,                     0,                 )             )     return bugs",if win . DroppedRect ( ) . height ( ) != win . ref . DroppedRect ( ) . height ( ) :,"if win . ref . Class ( ) == ""ComboBox"" :",13.403300919489984,13.403300919489984,0.0
"def testBadModeArgument(self):     # verify that we get a sensible error message for bad mode argument     bad_mode = ""qwerty""     try:         f = self.open(TESTFN, bad_mode)     except ValueError as msg:         <MASK>             s = str(msg)             if TESTFN in s or bad_mode not in s:                 self.fail(""bad error message for invalid mode: %s"" % s)         # if msg.args[0] == 0, we're probably on Windows where there may be         # no obvious way to discover why open() failed.     else:         f.close()         self.fail(""no error for invalid mode: %s"" % bad_mode)",if msg . args [ 0 ] != 0 :,if msg . args [ 0 ] == 0 :,70.16879391277372,70.16879391277372,0.0
"def command_group_expired(self, command_group_name):     try:         deprecate_info = self._command_loader.command_group_table[             command_group_name         ].group_kwargs.get(""deprecate_info"", None)         <MASK>             return deprecate_info.expired()     except AttributeError:         # Items with only token presence in the command table will not have any data. They can't be expired.         pass     return False",if deprecate_info :,if deprecate_info :,100.00000000000004,100.00000000000004,1.0
"def test_non_uniform_probabilities_over_elements(self):     param = iap.Choice([0, 1], p=[0.25, 0.75])     samples = param.draw_samples((10000,))     unique, counts = np.unique(samples, return_counts=True)     assert len(unique) == 2     for val, count in zip(unique, counts):         if val == 0:             assert 2500 - 500 < count < 2500 + 500         <MASK>             assert 7500 - 500 < count < 7500 + 500         else:             assert False",elif val == 1 :,if val == 1 :,75.98356856515926,75.98356856515926,0.0
"def get_labels(directory):     cache = get_labels.__cache     if directory not in cache:         l = {}         for t in get_visual_configs(directory)[0][LABEL_SECTION]:             <MASK>                 Messager.warning(                     ""In configuration, labels for '%s' defined more than once. Only using the last set.""                     % t.storage_form(),                     -1,                 )             # first is storage for, rest are labels.             l[t.storage_form()] = t.terms[1:]         cache[directory] = l     return cache[directory]",if t . storage_form ( ) in l :,if t . storage_form ( ) in l :,100.00000000000004,100.00000000000004,1.0
"def try_split(self, split_text: List[str]):     ret = []     for i in split_text:         if len(i) == 0:             continue         val = int(i, 2)         <MASK>             return None         ret.append(val)     if len(ret) != 0:         ret = bytes(ret)         logger.debug(f""binary successful, returning {ret.__repr__()}"")         return ret",if val > 255 or val < 0 :,if val < 0 or val > 255 :,36.55552228545125,36.55552228545125,0.0
"def setCellValue(self, row_idx, col, value):     assert col.id == ""repls-marked""     with self._lock:         rgroup = self.events[row_idx]         <MASK>             return         rgroup._marked = value == ""true"" and True or False     if self._tree:         self._tree.invalidateCell(row_idx, col)","if not isinstance ( rgroup , findlib2 . ReplaceHitGroup ) :",if rgroup . _marked :,4.988641679706251,4.988641679706251,0.0
"def create(cls, settlement_manager, resource_id):     """"""Create a production chain that can produce the given resource.""""""     resource_producer = {}     for abstract_building in AbstractBuilding.buildings.values():         for resource, production_line in abstract_building.lines.items():             <MASK>                 resource_producer[resource] = []             resource_producer[resource].append((production_line, abstract_building))     return ProductionChain(settlement_manager, resource_id, resource_producer)",if resource not in resource_producer :,if production_line is None :,7.492442692259767,7.492442692259767,0.0
def get_all_partition_sets(self):     partition_sets = []     if self.partitions_handle:         partition_sets.extend(self.partitions_handle.get_partition_sets())     if self.scheduler_handle:         partition_sets.extend(             [                 schedule_def.get_partition_set()                 for schedule_def in self.scheduler_handle.all_schedule_defs()                 <MASK>             ]         )     return partition_sets,"if isinstance ( schedule_def , PartitionScheduleDefinition )",if schedule_def . is_partition_set ( ) :,13.674406678232565,13.674406678232565,0.0
"def _sendDatapointsNow(self, datapoints):     metrics = {}     payload_pb = Payload()     for metric, datapoint in datapoints:         <MASK>             metric_pb = payload_pb.metrics.add()             metric_pb.metric = metric             metrics[metric] = metric_pb         else:             metric_pb = metrics[metric]         point_pb = metric_pb.points.add()         point_pb.timestamp = int(datapoint[0])         point_pb.value = datapoint[1]     self.sendString(payload_pb.SerializeToString())",if metric not in metrics :,if metric not in metrics :,100.00000000000004,100.00000000000004,1.0
"def execute(self):     if self._dirty or not self._qr:         model_class = self.model_class         query_meta = self.get_query_meta()         if self._tuples:             ResultWrapper = TuplesQueryResultWrapper         <MASK>             ResultWrapper = DictQueryResultWrapper         elif self._naive or not self._joins or self.verify_naive():             ResultWrapper = NaiveQueryResultWrapper         elif self._aggregate_rows:             ResultWrapper = AggregateQueryResultWrapper         else:             ResultWrapper = ModelQueryResultWrapper         self._qr = ResultWrapper(model_class, self._execute(), query_meta)         self._dirty = False         return self._qr     else:         return self._qr",elif self . _dicts :,if self . _dict :,32.46679154750991,32.46679154750991,0.0
"def get_metrics():     classifier, feature_labels = load_classifier()     available_metrics = ImgageMetrics.get_metric_classes()     # todo review: DONE IN DOCS     #  effective_metrics isn't used after filling it with values     #  in the loops below     effective_metrics = []     for metric in available_metrics:         for label in feature_labels:             for label_part in metric.get_labels():                 <MASK>                     effective_metrics.append(metric)     return (classifier, feature_labels, available_metrics)",if label_part == label and metric not in effective_metrics :,if label_part . lower ( ) == label :,24.689800508009657,24.689800508009657,0.0
"def test_nic_names(self):     p = subprocess.Popen([""ipconfig"", ""/all""], stdout=subprocess.PIPE)     out = p.communicate()[0]     if PY3:         out = str(out, sys.stdout.encoding)     nics = psutil.net_io_counters(pernic=True).keys()     for nic in nics:         if ""pseudo-interface"" in nic.replace("" "", ""-"").lower():             continue         <MASK>             self.fail(""%r nic wasn't found in 'ipconfig /all' output"" % nic)",if nic not in out :,if nic not in out :,100.00000000000004,100.00000000000004,1.0
"def convert_with_key(self, key, value, replace=True):     result = self.configurator.convert(value)     # If the converted value is different, save for next time     if value is not result:         if replace:             self[key] = result         <MASK>             result.parent = self             result.key = key     return result","if type ( result ) in ( ConvertingDict , ConvertingList , ConvertingTuple ) :",if result . parent is self :,3.1795892263857453,3.1795892263857453,0.0
"def _EvaluateFile(self, test_list, file):     (name, ext) = os.path.splitext(file)     if ext == "".cc"" or ext == "".cpp"" or ext == "".c"":         <MASK>             logger.SilentLog(""Found native test file %s"" % file)             test_list.append(name)","if re . search ( ""_test$|_test_$|_unittest$|_unittest_$|^test_|Tests$"" , name ) :",if name in test_list :,0.30249782419446397,0.30249782419446397,0.0
"def leading_whitespace(self, inputstring):     """"""Get leading whitespace.""""""     leading_ws = []     for i, c in enumerate(inputstring):         <MASK>             leading_ws.append(c)         else:             break         if self.indchar is None:             self.indchar = c         elif c != self.indchar:             self.strict_err_or_warn(""found mixing of tabs and spaces"", inputstring, i)     return """".join(leading_ws)",if c in legal_indent_chars :,if i == 0 :,5.854497694024015,5.854497694024015,0.0
"def ident_values(self):     value = self._ident_values     if value is False:         value = None         # XXX: how will this interact with orig_prefix ?         #      not exposing attrs for now if orig_prefix is set.         <MASK>             wrapped = self.wrapped             idents = getattr(wrapped, ""ident_values"", None)             if idents:                 value = [self._wrap_hash(ident) for ident in idents]             ##else:             ##    ident = self.ident             ##    if ident is not None:             ##        value = [ident]         self._ident_values = value     return value",if not self . orig_prefix :,if self . prefix is not None :,14.535768424205482,14.535768424205482,0.0
"def _available_symbols(self, scoperef, expr):     cplns = []     found_names = set()     while scoperef:         elem = self._elem_from_scoperef(scoperef)         for child in elem:             name = child.get(""name"", """")             <MASK>                 if name not in found_names:                     found_names.add(name)                     ilk = child.get(""ilk"") or child.tag                     cplns.append((ilk, name))         scoperef = self.parent_scoperef_from_scoperef(scoperef)         if not scoperef:             break     return sorted(cplns, key=operator.itemgetter(1))",if name . startswith ( expr ) :,if name in found_names :,12.600736402830258,12.600736402830258,0.0
"def pid_from_name(name):     # quick and dirty, works with all linux not depending on ps output     for pid in os.listdir(""/proc""):         try:             int(pid)         except:             continue         pname = """"         with open(""/proc/%s/cmdline"" % pid, ""r"") as f:             pname = f.read()         <MASK>             return int(pid)     raise ProcessException(""No process with such name: %s"" % name)",if name in pname :,if pname == name :,11.478744233307168,11.478744233307168,0.0
"def touch(self):     if not self.exists():         try:             self.parent().touch()         except ValueError:             pass         node = self._fs.touch(self.pathnames, {})         <MASK>             raise AssertionError(""Not a folder: %s"" % self.path)         if self.watcher:             self.watcher.emit(""created"", self)",if not node . isdir :,if not node :,38.75385825373298,38.75385825373298,0.0
"def setUp(self):     BaseTestCase.setUp(self)     self.rawData = []     self.dataByKey = {}     for i in range(1, 11):         stringCol = ""String %d"" % i         fixedCharCol = (""Fixed Char %d"" % i).ljust(40)         rawCol = ""Raw %d"" % i         <MASK>             nullableCol = ""Nullable %d"" % i         else:             nullableCol = None         dataTuple = (i, stringCol, rawCol, fixedCharCol, nullableCol)         self.rawData.append(dataTuple)         self.dataByKey[i] = dataTuple",if i % 2 :,if i % 2 :,100.00000000000004,100.00000000000004,1.0
"def GenerateVector(self, hits, vector, level):     """"""Generate possible hit vectors which match the rules.""""""     for item in hits.get(level, []):         if vector:             if item < vector[-1]:                 continue             if item > self.max_separation + vector[-1]:                 break         new_vector = vector + [item]         <MASK>             yield new_vector         elif level + 1 < len(hits):             for result in self.GenerateVector(hits, new_vector, level + 1):                 yield result",if level + 1 == len ( hits ) :,if level + 1 < len ( new_vector ) :,28.646290158800984,28.646290158800984,0.0
"def __repr__(self):     attrs = []     for k in self.keydata:         <MASK>             attrs.append(""p(%d)"" % (self.size() + 1,))         elif hasattr(self.key, k):             attrs.append(k)     if self.has_private():         attrs.append(""private"")     return ""<%s @0x%x %s>"" % (self.__class__.__name__, id(self), "","".join(attrs))","if k == ""p"" :","if k == ""p"" :",100.00000000000004,100.00000000000004,1.0
"def autoload(self):     if self._app.config.THEME == ""auto"":         <MASK>             if get_osx_theme() == 1:                 theme = DARK             else:                 theme = LIGHT         else:             theme = self.guess_system_theme()             if theme == Dark:                 theme = MacOSDark     else:  # user settings have highest priority         theme = self._app.config.THEME     self.load_theme(theme)","if sys . platform == ""darwin"" :",if theme == LIGHT :,9.911450612811139,9.911450612811139,0.0
"def _get_matching_bracket(self, s, pos):     if s[pos] != ""{"":         return None     end = len(s)     depth = 1     pos += 1     while pos != end:         c = s[pos]         if c == ""{"":             depth += 1         elif c == ""}"":             depth -= 1         <MASK>             break         pos += 1     if pos < end and s[pos] == ""}"":         return pos     return None",if depth == 0 :,if depth > 1 :,19.3576934939088,19.3576934939088,0.0
"def update_meter(self, output, target, meters={""accuracy""}):     output = self.__to_tensor(output)     target = self.__to_tensor(target)     for meter in meters:         <MASK>             self.__addmeter(meter)         if meter in [""ap"", ""map"", ""confusion""]:             target_th = self._ver2tensor(target)             self.meter[meter].add(output, target_th)         else:             self.meter[meter].add(output, target)",if meter not in self . meter . keys ( ) :,"if meter in [ ""ap"" , ""confusion"" ] :",7.347053125977879,7.347053125977879,0.0
"def _reinit_optimizers_with_oss(self):     optimizers = self.lightning_module.trainer.optimizers     for x, optimizer in enumerate(optimizers):         if is_lightning_optimizer(optimizer):             optimizer = optimizer._optimizer         <MASK>             optim_class = type(optimizer)             zero_optimizer = OSS(                 params=optimizer.param_groups, optim=optim_class, **optimizer.defaults             )             optimizers[x] = zero_optimizer             del optimizer     trainer = self.lightning_module.trainer     trainer.optimizers = optimizers     trainer.convert_to_lightning_optimizers()","if not isinstance ( optimizer , OSS ) :","if not isinstance ( optimizer , OSS ) :",100.00000000000004,100.00000000000004,1.0
"def OnSelChanged(self, event):     self.item = event.GetItem()     if self.item:         self.log.write(""OnSelChanged: %s"" % self.GetItemText(self.item))         <MASK>             self.log.write(                 "", BoundingRect: %s\n"" % self.GetBoundingRect(self.item, True)             )         else:             self.log.write(""\n"")     event.Skip()","if wx . Platform == ""__WXMSW__"" :",if self . item . HasBoundingRect :,3.1795892263857453,3.1795892263857453,0.0
"def parse_batch(args):     errmsg = ""Invalid batch definition: batch entry has to be defined as RULE=BATCH/BATCHES (with integers BATCH <= BATCHES, BATCH >= 1).""     if args.batch is not None:         rule, batchdef = parse_key_value_arg(args.batch, errmsg=errmsg)         try:             batch, batches = batchdef.split(""/"")             batch = int(batch)             batches = int(batches)         except ValueError:             raise ValueError(errmsg)         <MASK>             raise ValueError(errmsg)         return Batch(rule, batch, batches)     return None",if batch > batches or batch < 1 :,if rule is not None :,5.854497694024015,5.854497694024015,0.0
"def get_foreign_key_columns(self, engine, table_name):     foreign_keys = set()     table = db_utils.get_table(engine, table_name)     inspector = reflection.Inspector.from_engine(engine)     for column_dict in inspector.get_columns(table_name):         column_name = column_dict[""name""]         column = getattr(table.c, column_name)         <MASK>             foreign_keys.add(column_name)     return foreign_keys",if column . foreign_keys :,if column :,16.605579150202516,0.0,0.0
"def update(self, t):     l = int(t * self.nr_of_tiles)     for i in range(self.nr_of_tiles):         t = self.tiles_order[i]         <MASK>             self.turn_off_tile(t)         else:             self.turn_on_tile(t)",if i < l :,if l > t :,14.058533129758727,14.058533129758727,0.0
"def read(self, amt=None):     # the _rbuf test is only in this first if for speed.  It's not     # logically necessary     if self._rbuf and not amt is None:         L = len(self._rbuf)         <MASK>             amt -= L         else:             s = self._rbuf[:amt]             self._rbuf = self._rbuf[amt:]             return s     s = self._rbuf + self._raw_read(amt)     self._rbuf = b""""     return s",if amt > L :,if L > 0 :,15.106876986783844,15.106876986783844,0.0
"def draw_menu_button(self, context, layout, node, text):     if (         hasattr(node.id_data, ""sv_show_socket_menus"")         and node.id_data.sv_show_socket_menus     ):         <MASK>             layout.menu(""SV_MT_SocketOptionsMenu"", text="""", icon=""TRIA_DOWN"")",if self . is_output or self . is_linked or not self . use_prop :,if node . id_data . sv_show_socket_menus :,3.0628710710467613,3.0628710710467613,0.0
"def __enter__(self):     with DB.connection_context():         session_record = SessionRecord()         session_record.f_session_id = self._session_id         session_record.f_engine_name = self._engine_name         session_record.f_engine_type = EngineType.STORAGE         # TODO: engine address         session_record.f_engine_address = {}         session_record.f_create_time = current_timestamp()         rows = session_record.save(force_insert=True)         <MASK>             raise Exception(f""create session record {self._session_id} failed"")         LOGGER.debug(f""save session {self._session_id} record"")     self.create()     return self",if rows != 1 :,if not rows :,12.750736437345598,12.750736437345598,0.0
"def tearDown(self):     """"""Shutdown the server.""""""     try:         if self.server:             self.server.stop(2.0)         <MASK>             self.root_logger.removeHandler(self.sl_hdlr)             self.sl_hdlr.close()     finally:         BaseTest.tearDown(self)",if self . sl_hdlr :,if self . sl_hdlr :,100.00000000000004,100.00000000000004,1.0
"def _dec_device(self, srcdev, dstdev):     if srcdev:         self.srcdevs[srcdev] -= 1         <MASK>             del self.srcdevs[srcdev]         self._set_limits(""read"", self.srcdevs)     if dstdev:         self.dstdevs[dstdev] -= 1         if self.dstdevs[dstdev] == 0:             del self.dstdevs[dstdev]         self._set_limits(""write"", self.dstdevs)",if self . srcdevs [ srcdev ] == 0 :,if self . srcdevs [ srcdev ] == 0 :,100.00000000000004,100.00000000000004,1.0
"def array_for(self, i):     if 0 <= i < self._cnt:         <MASK>             return self._tail         node = self._root         level = self._shift         while level > 0:             assert isinstance(node, Node)             node = node._array[(i >> level) & 0x01F]             level -= 5         return node._array     affirm(False, u""Index out of Range"")",if i >= self . tailoff ( ) :,if i == self . _cnt :,20.90067144241745,20.90067144241745,0.0
"def convert_tensor(self, offsets, sizes):     results = []     for b, batch in enumerate(offsets):         utterances = []         for p, utt in enumerate(batch):             size = sizes[b][p]             <MASK>                 utterances.append(utt[0:size])             else:                 utterances.append(torch.tensor([], dtype=torch.int))         results.append(utterances)     return results",if sizes [ b ] [ p ] > 0 :,if size > 0 :,12.869637315183779,12.869637315183779,0.0
"def _predict_proba(self, X, preprocess=True):     if preprocess:         X = self.preprocess(X)     if self.problem_type == REGRESSION:         return self.model.predict(X)     y_pred_proba = self.model.predict_proba(X)     if self.problem_type == BINARY:         if len(y_pred_proba.shape) == 1:             return y_pred_proba         <MASK>             return y_pred_proba[:, 1]         else:             return y_pred_proba     elif y_pred_proba.shape[1] > 2:         return y_pred_proba     else:         return y_pred_proba[:, 1]",elif y_pred_proba . shape [ 1 ] > 1 :,if y_pred_proba . shape [ 0 ] > 2 :,57.57575636202256,57.57575636202256,0.0
def timeout(self):     now = ptime.time()     dt = now - self.lastPlayTime     if dt < 0:         return     n = int(self.playRate * dt)     if n != 0:         self.lastPlayTime += float(n) / self.playRate         <MASK>             self.play(0)         self.jumpFrames(n),"if self . currentIndex + n > self . image . shape [ self . axes [ ""t"" ] ] :",if n > self . playRate :,4.627353756943341,4.627353756943341,0.0
"def __init__(self, data, weights=None, ddof=0):     self.data = np.asarray(data)     if weights is None:         self.weights = np.ones(self.data.shape[0])     else:         self.weights = np.asarray(weights).astype(float)         # TODO: why squeeze?         <MASK>             self.weights = self.weights.squeeze()     self.ddof = ddof",if len ( self . weights . shape ) > 1 and len ( self . weights ) > 1 :,if ddof is not None :,0.792320103628836,0.792320103628836,0.0
"def writerow(self, row):     unicode_row = []     for col in row:         <MASK>             unicode_row.append(col.encode(""utf-8"").strip())         else:             unicode_row.append(col)     self.writer.writerow(unicode_row)     # Fetch UTF-8 output from the queue ...     data = self.queue.getvalue()     data = data.decode(""utf-8"")     # ... and reencode it into the target encoding     data = self.encoder.encode(data)     # write to the target stream     self.stream.write(data)     # empty queue     self.queue.truncate(0)",if type ( col ) == str or type ( col ) == unicode :,if self . encoder . strip ( ) :,2.771947612153099,2.771947612153099,0.0
"def __init__(self, choices, allow_blank=False, **kwargs):     self.choiceset = choices     self.allow_blank = allow_blank     self._choices = dict()     # Unpack grouped choices     for k, v in choices:         <MASK>             for k2, v2 in v:                 self._choices[k2] = v2         else:             self._choices[k] = v     super().__init__(**kwargs)","if type ( v ) in [ list , tuple ] :","if k == ""blank"" :",3.983253478176822,3.983253478176822,0.0
"def simp_ext(_, expr):     if expr.op.startswith(""zeroExt_""):         arg = expr.args[0]         <MASK>             return arg         return ExprCompose(arg, ExprInt(0, expr.size - arg.size))     if expr.op.startswith(""signExt_""):         arg = expr.args[0]         add_size = expr.size - arg.size         new_expr = ExprCompose(             arg,             ExprCond(                 arg.msb(), ExprInt(size2mask(add_size), add_size), ExprInt(0, add_size)             ),         )         return new_expr     return expr",if expr . size == arg . size :,if arg . size == 0 :,36.827215283744195,36.827215283744195,0.0
"def mark_differences(value: str, compare_against: str):     result = []     for i, char in enumerate(value):         try:             <MASK>                 result.append('<font color=""red"">{}</font>'.format(char))             else:                 result.append(char)         except IndexError:             result.append(char)     return """".join(result)",if char != compare_against [ i ] :,if compare_against :,13.607984667977698,13.607984667977698,0.0
"def run_query(self, query, user):     url = ""%s%s"" % (self.base_url, ""&"".join(query.split(""\n"")))     error = None     data = None     try:         response = requests.get(url, auth=self.auth, verify=self.verify)         <MASK>             data = _transform_result(response)         else:             error = ""Failed getting results (%d)"" % response.status_code     except Exception as ex:         data = None         error = str(ex)     return data, error",if response . status_code == 200 :,if response . status_code == 200 :,100.00000000000004,100.00000000000004,1.0
"def on_enter(self):     """"""Fired when mouse enter the bbox of the widget.""""""     if hasattr(self, ""md_bg_color"") and self.focus_behavior:         <MASK>             self.md_bg_color = self.theme_cls.bg_normal         else:             if not self.focus_color:                 self.md_bg_color = App.get_running_app().theme_cls.bg_normal             else:                 self.md_bg_color = self.focus_color","if hasattr ( self , ""theme_cls"" ) and not self . focus_color :",if self . focus_color :,15.14389796999661,15.14389796999661,0.0
"def tearDown(self):     if not self.is_playback():         try:             <MASK>                 self.sms.delete_hosted_service(self.hosted_service_name)         except:             pass         try:             if self.storage_account_name is not None:                 self.sms.delete_storage_account(self.storage_account_name)         except:             pass         try:             self.sms.delete_affinity_group(self.affinity_group_name)         except:             pass     return super(LegacyMgmtAffinityGroupTest, self).tearDown()",if self . hosted_service_name is not None :,if self . hosted_service_name is not None :,100.00000000000004,100.00000000000004,1.0
"def name2cp(k):     if k == ""apos"":         return ord(""'"")     if hasattr(htmlentitydefs, ""name2codepoint""):  # requires Python 2.3         return htmlentitydefs.name2codepoint[k]     else:         k = htmlentitydefs.entitydefs[k]         <MASK>             return int(k[2:-1])  # not in latin-1         return ord(codecs.latin_1_decode(k)[0])","if k . startswith ( ""&#"" ) and k . endswith ( "";"" ) :","if k [ 0 ] == ""apos"" :",4.158692987846551,4.158692987846551,0.0
"def _para_set(self, params, part):     if len(params) == 0:         result = suggest([i.get_name() for i in self._options], part)         return result     elif len(params) == 1:         paramName = params[0]         if paramName not in self._options:             return []         opt = self._options[paramName]         paramType = opt.get_type()         <MASK>             values = [opt.get_default_value() == ""True"" and ""False"" or ""True""]         else:             values = self._memory[paramName]         return suggest(values, part)     else:         return []","if paramType == ""boolean"" :","if paramType == ""boolean"" :",100.00000000000004,100.00000000000004,1.0
"def hexcmp(x, y):     try:         a = int(x, 16)         b = int(y, 16)         <MASK>             return -1         if a > b:             return 1         return 0     except:         return cmp(x, y)",if a < b :,if a < b :,100.00000000000004,100.00000000000004,1.0
"def execute(self, statement, arguments=None):     while True:         try:             if arguments:                 self.cursor.execute(statement, arguments)             else:                 self.cursor.execute(statement)         except sqlite3.OperationalError as ex:             <MASK>                 raise         else:             break     if statement.lstrip().upper().startswith(""SELECT""):         return self.cursor.fetchall()","if ""locked"" not in getSafeExString ( ex ) :",if ex . errno == sqlite3 . EEXIST :,4.996872151825361,4.996872151825361,0.0
"def _test_forever(self, tests):     while True:         for test_name in tests:             yield test_name             <MASK>                 return             if self.ns.fail_env_changed and self.environment_changed:                 return",if self . bad :,if self . ns . fail_test_changed and self . environment_changed :,9.147827112247601,9.147827112247601,0.0
"def removeUser(self, username):     hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD     if username in self._users:         user = self._users[username]         if user.room:             <MASK>                 hideFromOSD = not constants.SHOW_SAME_ROOM_OSD     if username in self._users:         self._users.pop(username)         message = getMessage(""left-notification"").format(username)         self.ui.showMessage(message, hideFromOSD)         self._client.lastLeftTime = time.time()         self._client.lastLeftUser = username     self.userListChange()",if self . isRoomSame ( user . room ) :,if user . room . is_active ( ) :,19.72940627795883,19.72940627795883,0.0
"def AutoTest():     with open(sys.argv[1], ""rb"") as f:         for line in f.read().split(b""\n""):             line = BYTES2SYSTEMSTR(line.strip())             if not line:                 continue             elif line.startswith(""#""):                 print(line)             else:                 print("">>> "" + line)                 os.system(line)                 sys.stdout.write(""\npress enter to continue..."")                 <MASK>                     input()                 else:                     raw_input()                 sys.stdout.write(""\n"")",if PY3 :,"if line . startswith ( ""input"" ) :",4.990049701936832,4.990049701936832,0.0
"def get_first_field(layout, clz):     for layout_object in layout.fields:         <MASK>             return layout_object         elif hasattr(layout_object, ""get_field_names""):             gf = get_first_field(layout_object, clz)             if gf:                 return gf","if issubclass ( layout_object . __class__ , clz ) :",if layout_object . get_field_names ( ) :,20.745378949098622,20.745378949098622,0.0
"def sanitize_event_keys(kwargs, valid_keys):     # Sanity check: Don't honor keys that we don't recognize.     for key in list(kwargs.keys()):         if key not in valid_keys:             kwargs.pop(key)     # Truncate certain values over 1k     for key in [""play"", ""role"", ""task"", ""playbook""]:         if isinstance(kwargs.get(""event_data"", {}).get(key), str):             <MASK>                 kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars(                     1024                 )","if len ( kwargs [ ""event_data"" ] [ key ] ) > 1024 :","if kwargs [ ""event_data"" ] [ key ] :",58.49965463119502,58.49965463119502,0.0
"def visit_productionlist(self, node):     self.new_state()     names = []     for production in node:         names.append(production[""tokenname""])     maxlen = max(len(name) for name in names)     for production in node:         <MASK>             self.add_text(production[""tokenname""].ljust(maxlen) + "" ::="")             lastname = production[""tokenname""]         else:             self.add_text(""%s    "" % ("" "" * len(lastname)))         self.add_text(production.astext() + self.nl)     self.end_state(wrap=False)     raise nodes.SkipNode","if production [ ""tokenname"" ] :","if len ( production [ ""tokenname"" ] ) > maxlen :",40.89601472043678,40.89601472043678,0.0
"def uuid(self):     if not getattr(self, ""_uuid"", None):         <MASK>             self._uuid = self.repository._kp_uuid(                 self.path             )  # Use repository UUID (even if None)         else:             self._uuid = str(uuid.uuid4())     return self._uuid",if self . repository is not None :,if self . repository :,38.80684294761701,38.80684294761701,0.0
"def remove(self, values):     if not isinstance(values, (list, tuple, set)):         values = [values]     for v in values:         v = str(v)         <MASK>             self._definition.pop(v, None)         elif self._definition == ""ANY"":             if v == ""ANY"":                 self._definition = []         elif v in self._definition:             self._definition.remove(v)     if (         self._value is not None         and self._value not in self._definition         and self._not_any()     ):         raise ConanException(bad_value_msg(self._name, self._value, self.values_range))","if isinstance ( self . _definition , dict ) :",if self . _definition is not None :,26.449672174138467,26.449672174138467,0.0
"def make(self):     pygments_dir = join(self.dir, ""externals"", ""pygments"")     if exists(pygments_dir):         run_in_dir(""hg pull"", pygments_dir, self.log.info)         run_in_dir(""hg update"", pygments_dir, self.log.info)     else:         <MASK>             os.makedirs(dirname(pygments_dir))         run_in_dir(             ""hg clone http://dev.pocoo.org/hg/pygments-main %s""             % basename(pygments_dir),             dirname(pygments_dir),             self.log.info,         )",if not exists ( dirname ( pygments_dir ) ) :,if not os . path . isdir ( pygments_dir ) :,35.993062543531785,35.993062543531785,0.0
def set_field(self):     i = 0     for string in self.display_string:         <MASK>             self.config[self.field + str(i)] = self.conversion_fn(self.str[i])         else:             self.config[self.field + str(i)] = self.str[i]         i = i + 1,if self . conversion_fn :,if string . isdigit ( ) :,8.643019616048525,8.643019616048525,0.0
"def cleanup(self):     with self.lock:         for proc in self.processes:             <MASK>                 continue             proc.join()             self.processes.remove(proc)             log.debug(""Subprocess %s cleaned up"", proc.name)",if proc . is_alive ( ) :,if proc . is_alive ( ) :,100.00000000000004,100.00000000000004,1.0
"def setup(self, gen):     Node.setup(self, gen)     for c in self.children:         c.setup(gen)     if not self.accepts_epsilon:         # If it's not already accepting epsilon, it might now do so.         for c in self.children:             # any non-epsilon means all is non-epsilon             <MASK>                 break         else:             self.accepts_epsilon = 1             gen.changed()",if not c . accepts_epsilon :,if c . is_epsilon :,27.890014303843827,27.890014303843827,0.0
"def __call__(self, message):     with self._lock:         self._pending_ack += 1         self.max_pending_ack = max(self.max_pending_ack, self._pending_ack)         self.seen_message_ids.append(int(message.attributes[""seq_num""]))     time.sleep(self._processing_time)     with self._lock:         self._pending_ack -= 1         message.ack()         self.completed_calls += 1         if self.completed_calls >= self._resolve_at_msg_count:             <MASK>                 self.done_future.set_result(None)",if not self . done_future . done ( ) :,if self . done_future :,32.73762387107808,32.73762387107808,0.0
"def build_canned_image_list(path):     layers_path = get_bitbake_var(""BBLAYERS"")     canned_wks_layer_dirs = []     if layers_path is not None:         for layer_path in layers_path.split():             for wks_path in (WIC_DIR, SCRIPTS_CANNED_IMAGE_DIR):                 cpath = os.path.join(layer_path, wks_path)                 <MASK>                     canned_wks_layer_dirs.append(cpath)     cpath = os.path.join(path, CANNED_IMAGE_DIR)     canned_wks_layer_dirs.append(cpath)     return canned_wks_layer_dirs",if os . path . isdir ( cpath ) :,if os . path . isdir ( cpath ) :,100.00000000000004,100.00000000000004,1.0
"def _recv_loop(self) -> None:     async with self._ws as connection:         self._connected = True         self.connection = connection         while self._connected:             try:                 resp = await self.connection.recv()                 <MASK>                     await self._on_message(resp)             except (websockets.ConnectionClosed, ConnectionResetError):                 logger.info(""connection closed"")                 break             await asyncio.sleep(0)     if self._connected:         self._loop.create_task(self.dispose())",if resp :,if resp is None :,23.643540225079384,23.643540225079384,0.0
"def _get_between(content, start, end=None):     should_yield = False     for line in content.split(""\n""):         if start in line:             should_yield = True             continue         <MASK>             return         if should_yield and line:             yield line.strip().split("" "")[0]",if end and end in line :,if end is None :,15.848738972120703,15.848738972120703,0.0
"def handle_parse_result(self, ctx, opts, args):     if self.name in opts:         if self.mutually_exclusive.intersection(opts):             self._raise_exclusive_error()         <MASK>             self._raise_exclusive_error()     return super(MutuallyExclusiveOption, self).handle_parse_result(ctx, opts, args)",if self . multiple and len ( set ( opts [ self . name ] ) ) > 1 :,if self . mutually_exclusive . all ( ) :,7.607839918902365,7.607839918902365,0.0
"def write(self, s):     if self.interactive:         <MASK>             self.active_mode.write(s)         else:             component.get(""CmdLine"").add_line(s, False)             self.events.append(s)     else:         print(colors.strip_colors(s))","if isinstance ( self . active_mode , deluge . ui . console . modes . cmdline . CmdLine ) :",if self . active_mode :,7.845605491344882,7.845605491344882,0.0
"def findfiles(path):     files = []     for name in os.listdir(path):         # ignore hidden files/dirs and other unwanted files         if name.startswith(""."") or name == ""lastsnap.jpg"":             continue         pathname = os.path.join(path, name)         st = os.lstat(pathname)         mode = st.st_mode         <MASK>             files.extend(findfiles(pathname))         elif stat.S_ISREG(mode):             files.append((pathname, name, st))     return files",if stat . S_ISDIR ( mode ) :,if stat . S_ISDIR ( mode ) :,100.00000000000004,100.00000000000004,1.0
"def _get_documented_completions(self, table, startswith=None):     names = []     for key, command in table.items():         if getattr(command, ""_UNDOCUMENTED"", False):             # Don't tab complete undocumented commands/params             continue         if startswith is not None and not key.startswith(startswith):             continue         <MASK>             continue         names.append(key)     return names","if getattr ( command , ""positional_arg"" , False ) :",if not command . _documented :,3.4166909782241994,3.4166909782241994,0.0
"def fix_newlines(lines):     """"""Convert newlines to unix.""""""     for i, line in enumerate(lines):         <MASK>             lines[i] = line[:-2] + ""\n""         elif line.endswith(""\r""):             lines[i] = line[:-1] + ""\n""","if line . endswith ( ""\r\n"" ) :","if line . endswith ( ""\n"" ) :",75.33808072882876,75.33808072882876,0.0
"def GeneratePageMetatadata(self, task):     address_space = self.session.GetParameter(""default_address_space"")     for vma in task.mm.mmap.walk_list(""vm_next""):         start = vma.vm_start         end = vma.vm_end         # Skip the entire region.         if end < self.plugin_args.start:             continue         # Done.         if start > self.plugin_args.end:             break         for vaddr in utils.xrange(start, end, 0x1000):             <MASK>                 yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))",if self . plugin_args . start <= vaddr <= self . plugin_args . end :,if start > self . plugin_args . end :,28.910816880783106,28.910816880783106,0.0
"def get_shape_at_node(self, node, assumptions):     for k, v in assumptions.items():         <MASK>             return v     if node.inputs:         return node.container.shape(             input_shapes=[                 self.get_shape_at_node(input_node, assumptions)                 for input_node in node.inputs             ]         )     else:         return node.container.shape(None)",if k in node . names :,"if k == ""input"" :",12.22307556087252,12.22307556087252,0.0
"def fix_doc(self, doc):     type = doc.get(""type"", {}).get(""key"")     if type == ""/type/work"":         <MASK>             # some record got empty author records because of an error             # temporary hack to fix             doc[""authors""] = [                 a for a in doc[""authors""] if ""author"" in a and ""key"" in a[""author""]             ]     elif type == ""/type/edition"":         # get rid of title_prefix.         if ""title_prefix"" in doc:             title = doc[""title_prefix""].strip() + "" "" + doc.get(""title"", """")             doc[""title""] = title.strip()             del doc[""title_prefix""]     return doc","if doc . get ( ""authors"" ) :","if ""author"" in doc :",6.3973701491755195,6.3973701491755195,0.0
"def modify_column(self, column: List[Optional[""Cell""]]):     for i in range(len(column)):         gate = column[i]         <MASK>             continue         elif isinstance(gate, ParityControlCell):             # The first parity control to modify the column must merge all             # of the other parity controls into itself.             column[i] = None             self._basis_change += gate._basis_change             self.qubits += gate.qubits         elif gate is not None:             column[i] = gate.controlled_by(self.qubits[0])",if gate is self :,if gate is None :,42.72870063962342,42.72870063962342,0.0
"def onSync(self, auto=False, reload=True):     if not auto or (         self.pm.profile[""syncKey""] and self.pm.profile[""autoSync""] and not self.safeMode     ):         from aqt.sync import SyncManager         if not self.unloadCollection():             return         # set a sync state so the refresh timer doesn't fire while deck         # unloaded         self.state = ""sync""         self.syncer = SyncManager(self, self.pm)         self.syncer.sync()     if reload:         <MASK>             self.loadCollection()",if not self . col :,if self . syncer . refreshTimer ( ) :,11.339582221952005,11.339582221952005,0.0
"def _has_url_match(self, match, request_url):     url = match[""url""]     if _is_string(url):         <MASK>             return self._has_strict_url_match(url, request_url)         else:             url_without_qs = request_url.split(""?"", 1)[0]             return url == url_without_qs     elif isinstance(url, re._pattern_type) and url.match(request_url):         return True     else:         return False","if match [ ""match_querystring"" ] :",if _is_strict ( url ) :,5.61480827173619,5.61480827173619,0.0
"def pool_image(self, image):     if self.count < self.pool_size:         self.pool.append(image)         self.count += 1         return image     else:         p = random.random()         <MASK>             random_id = random.randint(0, self.pool_size - 1)             temp = self.pool[random_id]             self.pool[random_id] = image             return temp         else:             return image",if p > 0.5 :,if p == self . pool_size :,9.287528999566801,9.287528999566801,0.0
"def get_target_dimensions(self):     width, height = self.engine.size     for operation in self.operations:         <MASK>             width = operation[""right""] - operation[""left""]             height = operation[""bottom""] - operation[""top""]         if operation[""type""] == ""resize"":             width = operation[""width""]             height = operation[""height""]     return (width, height)","if operation [ ""type"" ] == ""crop"" :","if operation [ ""type"" ] == ""resize"" :",79.10665071754353,79.10665071754353,0.0
"def validate_matrix(matrix):     if not matrix:         return None     for key, value in matrix.items():         <MASK>             raise ValidationError(                 ""`{}` defines a non uniform distribution, ""                 ""and it cannot be used with bayesian optimization."".format(key)             )     return matrix",if value . is_distribution and not value . is_uniform :,if value is None :,4.199688916946863,4.199688916946863,0.0
"def scm_to_conandata(self):     try:         scm_to_conandata = get_env(""CONAN_SCM_TO_CONANDATA"")         <MASK>             scm_to_conandata = self.get_item(""general.scm_to_conandata"")         return scm_to_conandata.lower() in (""1"", ""true"")     except ConanException:         return False",if scm_to_conandata is None :,if scm_to_conandata is None :,100.00000000000004,100.00000000000004,1.0
"def _link_vrf_table(self, vrf_table, rt_list):     route_family = vrf_table.route_family     for rt in rt_list:         rt_rf_id = rt + "":"" + str(route_family)         table_set = self._tables_for_rt.get(rt_rf_id)         <MASK>             table_set = set()             self._tables_for_rt[rt_rf_id] = table_set         table_set.add(vrf_table)         LOG.debug(""Added VrfTable %s to import RT table list: %s"", vrf_table, rt)",if table_set is None :,if table_set is None :,100.00000000000004,100.00000000000004,1.0
"def add_tags(     self, cve_results: Dict[str, Dict[str, Dict[str, str]]], file_object: FileObject ):     # results structure: {'component': {'cve_id': {'score2': '6.4', 'score3': 'N/A'}}}     for component in cve_results:         for cve_id in cve_results[component]:             entry = cve_results[component][cve_id]             <MASK>                 self.add_analysis_tag(                     file_object, ""CVE"", ""critical CVE"", TagColor.RED, True                 )                 return",if self . _entry_has_critical_rating ( entry ) :,if entry . score2 == '6.4' :,3.255629778647324,3.255629778647324,0.0
"def _validate(self):     try:         super(CustomClassifier, self)._validate()     except UnsupportedDataType:         if self.dtype in FACTOR_DTYPES:             raise UnsupportedDataType(                 typename=type(self).__name__,                 dtype=self.dtype,                 hint=""Did you mean to create a CustomFactor?"",             )         <MASK>             raise UnsupportedDataType(                 typename=type(self).__name__,                 dtype=self.dtype,                 hint=""Did you mean to create a CustomFilter?"",             )         raise",elif self . dtype in FILTER_DTYPES :,if self . dtype in FILTER_DTYPES :,86.33400213704509,86.33400213704509,0.0
"def formatMessage(self, record):     recordcopy = copy(record)     levelname = recordcopy.levelname     seperator = "" "" * (8 - len(recordcopy.levelname))     if self.use_colors:         levelname = self.color_level_name(levelname, recordcopy.levelno)         <MASK>             recordcopy.msg = recordcopy.__dict__[""color_message""]             recordcopy.__dict__[""message""] = recordcopy.getMessage()     recordcopy.__dict__[""levelprefix""] = levelname + "":"" + seperator     return super().formatMessage(recordcopy)","if ""color_message"" in recordcopy . __dict__ :",if self . use_colors :,2.961853899298388,2.961853899298388,0.0
"def dumpregs(self):     for reg in (         list(self.regs.retaddr)         + list(self.regs.misc)         + list(self.regs.common)         + list(self.regs.flags)     ):         enum = self.get_reg_enum(reg)         <MASK>             debug(""# Could not dump register %r"" % reg)             continue         name = ""U.x86_const.UC_X86_REG_%s"" % reg.upper()         value = self.uc.reg_read(enum)         debug(""uc.reg_read(%(name)s) ==> %(value)x"" % locals())",if not reg or enum is None :,if enum is None :,38.80684294761701,38.80684294761701,0.0
"def filter(self, lexer, stream):     current_type = None     current_value = None     for ttype, value in stream:         if ttype is current_type:             current_value += value         else:             <MASK>                 yield current_type, current_value             current_type = ttype             current_value = value     if current_type is not None:         yield current_type, current_value",if current_type is not None :,if current_value is not None :,50.000000000000014,50.000000000000014,0.0
"def _get_between(content, start, end=None):     should_yield = False     for line in content.split(""\n""):         <MASK>             should_yield = True             continue         if end and end in line:             return         if should_yield and line:             yield line.strip().split("" "")[0]",if start in line :,if start and start in line :,46.713797772820016,46.713797772820016,0.0
"def parse_git_config(path):     """"""Parse git config file.""""""     config = dict()     section = None     with open(os.path.join(path, ""config""), ""r"") as f:         for line in f:             line = line.strip()             if line.startswith(""[""):                 section = line[1:-1].strip()                 config[section] = dict()             <MASK>                 key, value = line.replace("" "", """").split(""="")                 config[section][key] = value     return config",elif section :,if line :,27.516060407455225,0.0,0.0
"def test_has_arg(fn, name, accept_all, expected):     if isinstance(fn, str):         context = dict()         try:             exec(""def {}: pass"".format(fn), context)         except SyntaxError:             <MASK>                 raise             pytest.skip(""Function is not compatible with Python 2"")         # Sometimes exec adds builtins to the context         context.pop(""__builtins__"", None)         (fn,) = context.values()     assert has_arg(fn, name, accept_all) is expected","if sys . version_info >= ( 3 , ) :",if pytest . version_info [ 0 ] == 2 :,22.242469397936766,22.242469397936766,0.0
"def ObjectExpression(self, properties, **kwargs):     data = []     for prop in properties:         self.emit(prop[""value""])         <MASK>             raise NotImplementedError(                 ""ECMA 5.1 does not support computed object properties!""             )         data.append((to_key(prop[""key""]), prop[""kind""][0]))     self.emit(""LOAD_OBJECT"", tuple(data))","if prop [ ""computed"" ] :","if prop [ ""kind"" ] not in [ ""computed"" , ""computed"" ] :",34.6697783111003,34.6697783111003,0.0
"def run(self):     for domain, locale, po in self.locales:         <MASK>             path = os.path.join(""locale"", locale, ""LC_MESSAGES"")         else:             path = os.path.join(self.build_dir, locale, ""LC_MESSAGES"")         mo = os.path.join(path, ""%s.mo"" % domain)         self.mkpath(path)         self.spawn([""msgfmt"", ""-o"", mo, po])",if self . inplace :,"if domain == ""default"" :",6.567274736060395,6.567274736060395,0.0
"def _compute_map(self, first_byte, second_byte=None):     if first_byte != 0x0F:         return ""XED_ILD_MAP0""     else:         if second_byte == None:             return ""XED_ILD_MAP1""         if second_byte == 0x38:             return ""XED_ILD_MAP2""         <MASK>             return ""XED_ILD_MAP3""         if second_byte == 0x0F and self.amd_enabled:             return ""XED_ILD_MAPAMD""     die(""Unhandled escape {} / map {} bytes"".format(first_byte, second_byte))",if second_byte == 0x3A :,if first_byte == 0x38 :,38.260294162784454,38.260294162784454,0.0
"def parse_tag(self):     buf = []     escaped = False     for c in self.get_next_chars():         <MASK>             buf.append(c)         elif c == ""\\"":             escaped = True         elif c == "">"":             return """".join(buf)         else:             buf.append(c)     raise Exception(""Unclosed tag "" + """".join(buf))",if escaped :,if escaped :,100.00000000000004,0.0,1.0
"def print_pairs(attrs=None, offset_y=0):     fmt = "" ({0}:{1}) ""     fmt_len = len(fmt)     for bg, fg in get_fg_bg():         try:             color = curses.color_pair(pair_number(fg, bg))             <MASK>                 for attr in attrs:                     color |= attr             screen.addstr(offset_y + bg, fg * fmt_len, fmt.format(fg, bg), color)             pass         except curses.error:             pass",if not attrs is None :,if attrs :,14.599305297825525,0.0,0.0
"def _impl(inputs, input_types):     data = inputs[0]     axis = None     keepdims = False     if len(inputs) > 2:  # default, torch have only data, axis=None, keepdims=False         <MASK>             axis = int(inputs[1])         elif _is_int_seq(inputs[1]):             axis = inputs[1]         else:             axis = list(_infer_shape(inputs[1]))         keepdims = bool(inputs[2])     return get_relay_op(name)(data, axis=axis, keepdims=keepdims)","if isinstance ( inputs [ 1 ] , int ) :",if _is_int_seq ( inputs [ 1 ] ) :,32.55964126200301,32.55964126200301,0.0
"def run(self, args, **kwargs):     # Filtering options     if args.trace_tag:         kwargs[""trace_tag""] = args.trace_tag     if args.trigger_instance:         kwargs[""trigger_instance""] = args.trigger_instance     if args.execution:         kwargs[""execution""] = args.execution     if args.rule:         kwargs[""rule""] = args.rule     if args.sort_order:         <MASK>             kwargs[""sort_asc""] = True         elif args.sort_order in [""desc"", ""descending""]:             kwargs[""sort_desc""] = True     return self.manager.query_with_count(limit=args.last, **kwargs)","if args . sort_order in [ ""asc"" , ""ascending"" ] :","if args . sort_order in [ ""asc"" , ""descending"" ] :",82.82477531331043,82.82477531331043,0.0
def retaddr():     sp = pwndbg.regs.sp     stack = pwndbg.vmmap.find(sp)     # Enumerate all return addresses     frame = gdb.newest_frame()     addresses = []     while frame:         addresses.append(frame.pc())         frame = frame.older()     # Find all of them on the stack     start = stack.vaddr     stop = start + stack.memsz     while addresses and start < sp < stop:         value = pwndbg.memory.u(sp)         <MASK>             index = addresses.index(value)             del addresses[:index]             print(pwndbg.chain.format(sp))         sp += pwndbg.arch.ptrsize,if value in addresses :,if value in addresses :,100.00000000000004,100.00000000000004,1.0
"def update_from_dictio(self, dictio_item):     for index, dictio_payload in enumerate(dictio_item, 1):         fuzz_payload = None         for fuzz_payload in self.payloads[index]:             fuzz_payload.content = dictio_payload.content             fuzz_payload.type = dictio_payload.type         # payload generated not used in seed but in filters         <MASK>             self.add(                 {""full_marker"": None, ""word"": None, ""index"": index, ""field"": None},                 dictio_item[index - 1],             )",if fuzz_payload is None :,if fuzz_payload . full_marker :,31.55984539112946,31.55984539112946,0.0
"def check_expected(result, expected, contains=False):     if sys.version_info[0] >= 3:         <MASK>             result = result.encode(""ascii"")         if isinstance(expected, str):             expected = expected.encode(""ascii"")     resultlines = result.splitlines()     expectedlines = expected.splitlines()     if len(resultlines) != len(expectedlines):         return False     for rline, eline in zip(resultlines, expectedlines):         if contains:             if eline not in rline:                 return False         else:             if not rline.endswith(eline):                 return False     return True","if isinstance ( result , str ) :","if isinstance ( result , str ) :",100.00000000000004,100.00000000000004,1.0
"def execute_sql(self, sql, params=None, commit=True):     try:         cursor = super(RetryOperationalError, self).execute_sql(sql, params, commit)     except OperationalError:         if not self.is_closed():             self.close()         with __exception_wrapper__:             cursor = self.cursor()             cursor.execute(sql, params or ())             <MASK>                 self.commit()     return cursor",if commit and not self . in_transaction ( ) :,if not self . is_closed :,14.827340167306767,14.827340167306767,0.0
"def get_operation_ast(document_ast, operation_name=None):     operation = None     for definition in document_ast.definitions:         if isinstance(definition, ast.OperationDefinition):             if not operation_name:                 # If no operation name is provided, only return an Operation if it is the only one present in the                 # document. This means that if we've encountered a second operation as we were iterating over the                 # definitions in the document, there are more than one Operation defined, and we should return None.                 <MASK>                     return None                 operation = definition             elif definition.name and definition.name.value == operation_name:                 return definition     return operation",if operation :,if definition . name . value == operation_name :,4.456882760699063,4.456882760699063,0.0
"def removeTrailingWs(self, aList):     i = 0     while i < len(aList):         if self.is_ws(aList[i]):             j = i             i = self.skip_ws(aList, i)             assert j < i             <MASK>                 # print ""removing trailing ws:"", `i-j`                 del aList[j:i]                 i = j         else:             i += 1","if i >= len ( aList ) or aList [ i ] == ""\n"" :",if i > 0 :,2.1273367400149232,2.1273367400149232,0.0
"def _process_filter(self, query, host_state):     """"""Recursively parse the query structure.""""""     if not query:         return True     cmd = query[0]     method = self.commands[cmd]     cooked_args = []     for arg in query[1:]:         <MASK>             arg = self._process_filter(arg, host_state)         elif isinstance(arg, basestring):             arg = self._parse_string(arg, host_state)         if arg is not None:             cooked_args.append(arg)     result = method(self, cooked_args)     return result","if isinstance ( arg , list ) :","if isinstance ( arg , ( list , tuple ) ) :",37.70063804549471,37.70063804549471,0.0
"def handle_sent(self, elt):     sent = []     for child in elt:         if child.tag in (""mw"", ""hi"", ""corr"", ""trunc""):             sent += [self.handle_word(w) for w in child]         elif child.tag in (""w"", ""c""):             sent.append(self.handle_word(child))         <MASK>             raise ValueError(""Unexpected element %s"" % child.tag)     return BNCSentence(elt.attrib[""n""], sent)",elif child . tag not in self . tags_to_ignore :,"if child . tag in ( ""h"" , ""c"" ) :",11.114924776032012,11.114924776032012,0.0
"def get_display_price(     base: Union[TaxedMoney, TaxedMoneyRange], display_gross: bool = False ) -> Money:     """"""Return the price amount that should be displayed based on settings.""""""     if not display_gross:         display_gross = display_gross_prices()     if isinstance(base, TaxedMoneyRange):         <MASK>             base = MoneyRange(start=base.start.gross, stop=base.stop.gross)         else:             base = MoneyRange(start=base.start.net, stop=base.stop.net)     if isinstance(base, TaxedMoney):         base = base.gross if display_gross else base.net     return base",if display_gross :,if base . stop . gross :,14.535768424205482,14.535768424205482,0.0
"def check_classes(self, node):     if isinstance(node, nodes.Element):         for class_value in node[""classes""][:]:             if class_value in self.strip_classes:                 node[""classes""].remove(class_value)             <MASK>                 return 1",if class_value in self . strip_elements :,if node . nodeType == nodes . Node :,4.996872151825361,4.996872151825361,0.0
"def validate(outfile=sys.stdout, silent_success=False):     ""Validates all installed models.""     try:         num_errors = get_validation_errors(outfile)         <MASK>             return         outfile.write(             ""%s error%s found.\n"" % (num_errors, num_errors != 1 and ""s"" or """")         )     except ImproperlyConfigured:         outfile.write(""Skipping validation because things aren't configured properly."")",if silent_success and num_errors == 0 :,if silent_success :,17.437038542312457,17.437038542312457,0.0
"def check_basename_conflicts(self, targets):     """"""Apps' basenames are used as bundle directory names. Ensure they are all unique.""""""     basename_seen = {}     for target in targets:         <MASK>             raise self.BasenameConflictError(                 ""Basename must be unique, found two targets use ""                 ""the same basename: {}'\n\t{} and \n\t{}"".format(                     target.basename,                     basename_seen[target.basename].address.spec,                     target.address.spec,                 )             )         basename_seen[target.basename] = target",if target . basename in basename_seen :,if target . basename in basename_seen :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, api_version_str):     try:         self.latest = self.preview = False         self.yyyy = self.mm = self.dd = None         <MASK>             self.latest = True         else:             if ""preview"" in api_version_str:                 self.preview = True             parts = api_version_str.split(""-"")             self.yyyy = int(parts[0])             self.mm = int(parts[1])             self.dd = int(parts[2])     except (ValueError, TypeError):         raise ValueError(             ""The API version {} is not in a "" ""supported format"".format(api_version_str)         )","if api_version_str == ""latest"" :","if ""latest"" in api_version_str :",45.305163015763085,45.305163015763085,0.0
"def _osp2ec(self, bytes):     compressed = self._from_bytes(bytes)     y = compressed >> self._bits     x = compressed & (1 << self._bits) - 1     if x == 0:         y = self._curve.b     else:         result = self.sqrtp(             x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p         )         <MASK>             y = result[0]         elif len(result) == 2:             y1, y2 = result             y = y1 if (y1 & 1 == y) else y2         else:             return None     return ec.Point(self._curve, x, y)",if len ( result ) == 1 :,if len ( result ) == 1 :,100.00000000000004,100.00000000000004,1.0
"def _visit_import_alike(self, node: Union[cst.Import, cst.ImportFrom]) -> bool:     names = node.names     if isinstance(names, cst.ImportStar):         return False     # make sure node.names is Sequence[ImportAlias]     for name in names:         self.provider.set_metadata(name, self.scope)         asname = name.asname         <MASK>             name_values = _gen_dotted_names(cst.ensure_type(asname.name, cst.Name))         else:             name_values = _gen_dotted_names(name.name)         for name_value, _ in name_values:             self.scope.record_assignment(name_value, node)     return False",if asname is not None :,if asname . name :,19.3576934939088,19.3576934939088,0.0
"def test_sanity_no_unmatched_parentheses(CorpusType: Type[ColumnCorpus]):     corpus = CorpusType()     unbalanced_entities = []     for sentence in corpus.get_all_sentences():         entities = sentence.get_spans(""ner"")         for entity in entities:             entity_text = """".join(t.text for t in entity.tokens)             <MASK>                 unbalanced_entities.append(entity_text)     assert unbalanced_entities == []",if not has_balanced_parantheses ( entity_text ) :,"if entity_text != """" :",12.929367642051732,12.929367642051732,0.0
"def _learn_rate_adjust(self):     if self.learn_rate_decays == 1.0:         return     learn_rate_decays = self._vp(self.learn_rate_decays)     learn_rate_minimums = self._vp(self.learn_rate_minimums)     for index, decay in enumerate(learn_rate_decays):         new_learn_rate = self.net_.learnRates[index] * decay         <MASK>             self.net_.learnRates[index] = new_learn_rate     if self.verbose >= 2:         print(""Learn rates: {}"".format(self.net_.learnRates))",if new_learn_rate >= learn_rate_minimums [ index ] :,if new_learn_rate < learn_rate_minimums :,50.3633439149258,50.3633439149258,0.0
"def set_attr_from_xmp_tag(self, attr, xmp_tags, tags, cast=None):     v = self.get_xmp_tag(xmp_tags, tags)     if v is not None:         <MASK>             setattr(self, attr, v)         else:             # Handle fractions             if (cast == float or cast == int) and ""/"" in v:                 v = self.try_parse_fraction(v)             setattr(self, attr, cast(v))",if cast is None :,if cast is None :,100.00000000000004,100.00000000000004,1.0
"def _merge_scientific_float_tokens(tokens: Iterable[str]) -> List[str]:     tokens = list(tokens)     i = 0     while ""e"" in tokens[i + 1 :]:         i = tokens.index(""e"", i + 1)         s = i - 1         e = i + 1         <MASK>             continue         if re.match(""[+-]"", str(tokens[e])):             e += 1         if re.match(""[0-9]"", str(tokens[e])):             e += 1             tokens[s:e] = ["""".join(tokens[s:e])]             i -= 1     return tokens","if not re . match ( ""[0-9]"" , str ( tokens [ s ] ) ) :",if s == 0 :,0.6751392346890166,0.6751392346890166,0.0
"def anypython(request):     name = request.param     executable = getexecutable(name)     if executable is None:         <MASK>             executable = winpymap.get(name, None)             if executable:                 executable = py.path.local(executable)                 if executable.check():                     return executable         pytest.skip(""no suitable %s found"" % (name,))     return executable","if sys . platform == ""win32"" :",if name in winpymap :,4.673289785800722,4.673289785800722,0.0
"def set_meta(self, dataset, overwrite=True, **kwd):     super().set_meta(dataset, overwrite=overwrite, **kwd)     try:         <MASK>             with tarfile.open(dataset.file_name, ""r"") as temptar:                 dataset.metadata.fast5_count = sum(                     1 for f in temptar if f.name.endswith("".fast5"")                 )     except Exception as e:         log.warning(""%s, set_meta Exception: %s"", self, e)",if dataset and tarfile . is_tarfile ( dataset . file_name ) :,if dataset . file_name :,19.548182395144593,19.548182395144593,0.0
"def run(self):     for k in list(iterkeys(self.objs)):         <MASK>             continue         v = self.objs[k]         if v[""_class""] == ""User"":             self.split_user(k, v)         elif v[""_class""] in [             ""Message"",             ""PrintJob"",             ""Question"",             ""Submission"",             ""UserTest"",         ]:             v[""participation""] = v[""user""]             del v[""user""]     return self.objs","if k . startswith ( ""_"" ) :",if k not in self . objs :,10.229197414177778,10.229197414177778,0.0
"def _findInTree(t, n):     ret = []     if type(t) is dict:         <MASK>             ret.append(t)         for k, v in t.items():             ret += _findInTree(v, n)     if type(t) is list:         for v in t:             ret += _findInTree(v, n)     return ret","if ""_name"" in t and t [ ""_name"" ] == n :",if type ( t ) is list :,1.8376089065437553,1.8376089065437553,0.0
"def parseArrayPattern(self):     node = Node()     elements = []     self.expect(""["")     while not self.match(""]""):         <MASK>             self.lex()             elements.append(null)         else:             if self.match(""...""):                 restNode = Node()                 self.lex()                 rest = self.parseVariableIdentifier()                 elements.append(restNode.finishRestElement(rest))                 break             else:                 elements.append(self.parsePatternWithDefault())             if not self.match(""]""):                 self.expect("","")     self.expect(""]"")     return node.finishArrayPattern(elements)","if self . match ( "","" ) :","if self . match ( ""null"" ) :",65.80370064762461,65.80370064762461,0.0
"def _set_log_writer(self):     if self.config[""logging""]:         config = self.config[""log_writer_config""]         <MASK>             self.log_writer = LogWriter(**config)         elif config[""writer""] == ""tensorboard"":             self.log_writer = TensorBoardWriter(**config)         else:             raise ValueError(f""Unrecognized writer option: {config['writer']}"")     else:         self.log_writer = None","if config [ ""writer"" ] == ""json"" :","if config [ ""writer"" ] == ""log"" :",79.10665071754353,79.10665071754353,0.0
"def _parse(self, contents):     entries = []     hostnames_found = set()     for line in contents.splitlines():         <MASK>             entries.append((""blank"", [line]))             continue         (head, tail) = chop_comment(line.strip(), ""#"")         if not len(head):             entries.append((""all_comment"", [line]))             continue         entries.append((""hostname"", [head, tail]))         hostnames_found.add(head)     if len(hostnames_found) > 1:         raise IOError(""Multiple hostnames (%s) found!"" % (hostnames_found))     return entries",if not len ( line . strip ( ) ) :,if not line :,6.6019821735025035,6.6019821735025035,0.0
"def get_all_values(self, project):     if isinstance(project, models.Model):         project_id = project.id     else:         project_id = project     if project_id not in self.__cache:         cache_key = self._make_key(project_id)         result = cache.get(cache_key)         <MASK>             result = self.reload_cache(project_id)         else:             self.__cache[project_id] = result     return self.__cache.get(project_id, {})",if result is None :,if result is None :,100.00000000000004,100.00000000000004,1.0
"def needed_libraries(self):     for cmd in self.load_commands_of_type(0xC):  # LC_LOAD_DYLIB         tname = self._get_typename(""dylib_command"")         dylib_command = cmd.cast(tname)         name_addr = cmd.obj_offset + dylib_command.name         dylib_name = self.obj_vm.read(name_addr, 256)         <MASK>             idx = dylib_name.find(""\x00"")             if idx != -1:                 dylib_name = dylib_name[:idx]             yield dylib_name",if dylib_name :,if dylib_name :,100.00000000000004,100.00000000000004,1.0
"def compress(self, data_list):     warn_untested()     if data_list:         <MASK>             error = self.error_messages[""invalid_year""]             raise forms.ValidationError(error)         if data_list[0] in forms.fields.EMPTY_VALUES:             error = self.error_messages[""invalid_month""]             raise forms.ValidationError(error)         year = int(data_list[1])         month = int(data_list[0])         # find last day of the month         day = monthrange(year, month)[1]         return date(year, month, day)     return None",if data_list [ 1 ] in forms . fields . EMPTY_VALUES :,if data_list [ 0 ] in forms . fields . DATE_VALUES :,61.28081331864041,61.28081331864041,0.0
"def put(self, obj, block=True, timeout=None):     assert not self._closed     if not self._sem.acquire(block, timeout):         raise Full     with self._notempty:         with self._cond:             <MASK>                 self._start_thread()             self._buffer.append(obj)             self._unfinished_tasks.release()             self._notempty.notify()",if self . _thread is None :,if self . _finished_tasks :,36.55552228545123,36.55552228545123,0.0
"def has_module(self, module, version):     has_module = False     for directory in self.directories:         module_directory = join(directory, module)         has_module_directory = isdir(module_directory)         if not version:             has_module = has_module_directory or exists(                 module_directory             )  # could be a bare modulefile         else:             modulefile = join(module_directory, version)             has_modulefile = exists(modulefile)             has_module = has_module_directory and has_modulefile         <MASK>             break     return has_module",if has_module :,if not has_module :,53.7284965911771,53.7284965911771,0.0
"def expanduser(path):     if path[:1] == ""~"":         c = path[1:2]         <MASK>             return gethome()         if c == os.sep:             return asPyString(File(gethome(), path[2:]).getPath())     return path",if not c :,if c == os . path . sep :,5.522397783539471,5.522397783539471,0.0
"def mock_touch(self, bearer, version=None, revision=None, **kwargs):     if version:         <MASK>             try:                 return self.versions[int(version) - 1]             except (IndexError, ValueError):                 return None         else:             return None     return file_models.FileVersion()",if self . versions :,if revision :,17.799177396293473,0.0,0.0
"def _get_field_value(self, test, key, match):     if test.ver == ofproto_v1_0.OFP_VERSION:         members = inspect.getmembers(match)         for member in members:             if member[0] == key:                 field_value = member[1]             <MASK>                 wildcards = member[1]         if key == ""nw_src"":             field_value = test.nw_src_to_str(wildcards, field_value)         elif key == ""nw_dst"":             field_value = test.nw_dst_to_str(wildcards, field_value)     else:         field_value = match[key]     return field_value","elif member [ 0 ] == ""wildcards"" :","if key == ""wildcards"" :",46.76143972269792,46.76143972269792,0.0
"def check_expected(result, expected, contains=False):     if sys.version_info[0] >= 3:         if isinstance(result, str):             result = result.encode(""ascii"")         if isinstance(expected, str):             expected = expected.encode(""ascii"")     resultlines = result.splitlines()     expectedlines = expected.splitlines()     if len(resultlines) != len(expectedlines):         return False     for rline, eline in zip(resultlines, expectedlines):         <MASK>             if eline not in rline:                 return False         else:             if not rline.endswith(eline):                 return False     return True",if contains :,if contains :,100.00000000000004,0.0,1.0
"def OnKeyUp(self, event):     if self._properties.modifiable:         if event.GetKeyCode() == wx.WXK_ESCAPE:             self._cancel_editing()         <MASK>             self._update_value()         elif event.GetKeyCode() == wx.WXK_DELETE:             self.SetValue("""")     if event.GetKeyCode() != wx.WXK_RETURN:         # Don't send skip event if enter key is pressed         # On some platforms this event is sent too late and causes crash         event.Skip()",elif event . GetKeyCode ( ) == wx . WXK_RETURN :,if event . GetKeyCode ( ) == wx . WXK_ENTER :,77.4403141014203,77.4403141014203,0.0
"def load_modules(     to_load, load, attr, modules_dict, excluded_aliases, loading_message=None ):     if loading_message:         print(loading_message)     for name in to_load:         module = load(name)         <MASK>             continue         cls = getattr(module, attr)         if hasattr(cls, ""initialize"") and not cls.initialize():             continue         if hasattr(module, ""aliases""):             for alias in module.aliases():                 if alias not in excluded_aliases:                     modules_dict[alias] = module         else:             modules_dict[name] = module     if loading_message:         print()","if module is None or not hasattr ( module , attr ) :",if not module :,2.3809737623256155,2.3809737623256155,0.0
def eventIterator():     while True:         yield eventmodule.wait()         while True:             event = eventmodule.poll()             <MASK>                 break             else:                 yield event,if event . type == NOEVENT :,if event is None :,12.975849993980741,12.975849993980741,0.0
"def _get_state_without_padding(self, state_with_padding, padding):     lean_state = {}     for key, value in state_with_padding.items():         <MASK>             lean_length = value.numel() - padding             lean_state[key] = value[:lean_length]         else:             lean_state[key] = value     return lean_state",if torch . is_tensor ( value ) :,if padding :,3.361830360737634,0.0,0.0
"def _get_validate(data):     """"""Retrieve items to validate, from single samples or from combined joint calls.""""""     if data.get(""vrn_file"") and tz.get_in([""config"", ""algorithm"", ""validate""], data):         return utils.deepish_copy(data)     elif ""group_orig"" in data:         for sub in multi.get_orig_items(data):             <MASK>                 sub_val = utils.deepish_copy(sub)                 sub_val[""vrn_file""] = data[""vrn_file""]                 return sub_val     return None","if ""validate"" in sub [ ""config"" ] [ ""algorithm"" ] :","if sub . get ( ""vrn_file"" ) :",3.3383922484634225,3.3383922484634225,0.0
"def OnPopup(self, form, popup_handle):     for num, action_name, menu_name, shortcut in self.actions:         <MASK>             ida_kernwin.attach_action_to_popup(form, popup_handle, None)         else:             handler = command_handler_t(self, num, 2)             desc = ida_kernwin.action_desc_t(action_name, menu_name, handler, shortcut)             ida_kernwin.attach_dynamic_action_to_popup(form, popup_handle, desc)",if menu_name is None :,if num == 0 :,8.170609724417774,8.170609724417774,0.0
"def show(self, indent=0):     """"""Pretty print this structure.""""""     if indent == 0:         print(""struct {}"".format(self.name))     for field in self.fields:         <MASK>             offset = ""0x??""         else:             offset = ""0x{:02x}"".format(field.offset)         print(""{}+{} {} {}"".format("" "" * indent, offset, field.name, field.type))         if isinstance(field.type, Structure):             field.type.show(indent + 1)",if field . offset is None :,if field . offset == 0 :,36.55552228545123,36.55552228545123,0.0
"def get_operation_ast(document_ast, operation_name=None):     operation = None     for definition in document_ast.definitions:         <MASK>             if not operation_name:                 # If no operation name is provided, only return an Operation if it is the only one present in the                 # document. This means that if we've encountered a second operation as we were iterating over the                 # definitions in the document, there are more than one Operation defined, and we should return None.                 if operation:                     return None                 operation = definition             elif definition.name and definition.name.value == operation_name:                 return definition     return operation","if isinstance ( definition , ast . OperationDefinition ) :",if definition . name and definition . name . value == operation_name :,3.4585921141027365,3.4585921141027365,0.0
"def getSubMenu(self, callingWindow, context, mainItem, selection, rootMenu, i, pitem):     msw = True if ""wxMSW"" in wx.PlatformInfo else False     self.context = context     self.abilityIds = {}     sub = wx.Menu()     for ability in self.fighter.abilities:         <MASK>             continue         menuItem = self.addAbility(rootMenu if msw else sub, ability)         sub.Append(menuItem)         menuItem.Check(ability.active)     return sub",if not ability . effect . isImplemented :,if ability . active == 0 :,13.134549472120788,13.134549472120788,0.0
"def consume(self, event: Dict[str, Any]) -> None:     with self.lock:         logging.debug(""Received missedmessage_emails event: %s"", event)         # When we process an event, just put it into the queue and ensure we have a timer going.         user_profile_id = event[""user_profile_id""]         <MASK>             self.batch_start_by_recipient[user_profile_id] = time.time()         self.events_by_recipient[user_profile_id].append(event)         self.ensure_timer()",if user_profile_id not in self . batch_start_by_recipient :,if user_profile_id in self . events_by_recipient :,53.91384533067121,53.91384533067121,0.0
"def __init__(self, start_enabled=False, use_hardware=True):     self._use_hardware = use_hardware     if use_hardware:         self._button = Button(BUTTON_GPIO_PIN)         self._enabled = start_enabled         <MASK>             self._button.when_pressed = self._enable",if not start_enabled :,if start_enabled :,57.89300674674101,57.89300674674101,0.0
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""):     try:         pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na)         <MASK>             cls._execute_map(ctx, op)         elif op.stage == OperandStage.combine:             cls._execute_combine(ctx, op)         elif op.stage == OperandStage.agg:             cls._execute_agg(ctx, op)         else:  # pragma: no cover             raise ValueError(""Aggregation operand not executable"")     finally:         pd.reset_option(""mode.use_inf_as_na"")",if op . stage == OperandStage . map :,if op . stage == OperandStage . map :,100.00000000000004,100.00000000000004,1.0
"def load_package(name, path):     if os.path.isdir(path):         extensions = machinery.SOURCE_SUFFIXES[:] + machinery.BYTECODE_SUFFIXES[:]         for extension in extensions:             init_path = os.path.join(path, ""__init__"" + extension)             <MASK>                 path = init_path                 break         else:             raise ValueError(""{!r} is not a package"".format(path))     spec = util.spec_from_file_location(name, path, submodule_search_locations=[])     if name in sys.modules:         return _exec(spec, sys.modules[name])     else:         return _load(spec)",if os . path . exists ( init_path ) :,if os . path . isfile ( init_path ) :,73.48889200874659,73.48889200874659,0.0
def setup(level=None):     from pipeline.logging import pipeline_logger as logger     from pipeline.log.handlers import EngineLogHandler     if level in set(logging._levelToName.values()):         logger.setLevel(level)     logging._acquireLock()     try:         for hdl in logger.handlers:             <MASK>                 break         else:             hdl = EngineLogHandler()             hdl.setLevel(logger.level)             logger.addHandler(hdl)     finally:         logging._releaseLock(),"if isinstance ( hdl , EngineLogHandler ) :",if hdl . is_registered ( ) :,11.99014838091355,11.99014838091355,0.0
"def find_approximant(x):     c = 1e-4     it = sympy.ntheory.continued_fraction_convergents(         sympy.ntheory.continued_fraction_iterator(x)     )     for i in it:         p, q = i.as_numer_denom()         tol = c / q ** 2         if abs(i - x) <= tol:             return i         <MASK>             break     return x",if tol < machine_epsilon :,if p > tol :,9.423716574733431,9.423716574733431,0.0
"def resolve(     self, debug: bool = False, silent: bool = False, level: Optional[int] = None ) -> bool:     if silent:         spinner = nullcontext(type(""Mock"", (), {}))     else:         spinner = yaspin(text=""resolving..."")     with spinner as spinner:         while True:             resolved = self._resolve(                 debug=debug, silent=silent, level=level, spinner=spinner             )             <MASK>                 continue             self.graph.clear()  # remove unused deps from graph             return resolved",if resolved is None :,if resolved is None :,100.00000000000004,100.00000000000004,1.0
"def canonicalize_instruction_name(instr):     name = instr.insn_name().upper()     # XXX bypass a capstone bug that incorrectly labels some insns as mov     if name == ""MOV"":         <MASK>             return ""LSR""         elif instr.mnemonic.startswith(""lsl""):             return ""LSL""         elif instr.mnemonic.startswith(""asr""):             return ""ASR""     return OP_NAME_MAP.get(name, name)","if instr . mnemonic . startswith ( ""lsr"" ) :","if instr . mnemonic . startswith ( ""lsr"" ) :",100.00000000000004,100.00000000000004,1.0
"def run_all(rule_list, defined_variables, defined_actions, stop_on_first_trigger=False):     rule_was_triggered = False     for rule in rule_list:         result = run(rule, defined_variables, defined_actions)         if result:             rule_was_triggered = True             <MASK>                 return True     return rule_was_triggered",if stop_on_first_trigger :,if stop_on_first_trigger :,100.00000000000004,100.00000000000004,1.0
"def get_filters(self, request):     filter_specs = []     if self.lookup_opts.admin.list_filter and not self.opts.one_to_one_field:         filter_fields = [             self.lookup_opts.get_field(field_name)             for field_name in self.lookup_opts.admin.list_filter         ]         for f in filter_fields:             spec = FilterSpec.create(f, request, self.params, self.model)             <MASK>                 filter_specs.append(spec)     return filter_specs, bool(filter_specs)",if spec and spec . has_output ( ) :,if spec . is_valid ( ) :,20.88702936655045,20.88702936655045,0.0
"def get_type(type_ref):     kind = type_ref.get(""kind"")     if kind == TypeKind.LIST:         item_ref = type_ref.get(""ofType"")         <MASK>             raise Exception(""Decorated type deeper than introspection query."")         return GraphQLList(get_type(item_ref))     elif kind == TypeKind.NON_NULL:         nullable_ref = type_ref.get(""ofType"")         if not nullable_ref:             raise Exception(""Decorated type deeper than introspection query."")         return GraphQLNonNull(get_type(nullable_ref))     return get_named_type(type_ref[""name""])",if not item_ref :,if not item_ref :,100.00000000000004,100.00000000000004,1.0
"def _1_0_cloud_ips_cip_jsjc5_map(self, method, url, body, headers):     if method == ""POST"":         body = json.loads(body)         <MASK>             return self.test_response(httplib.ACCEPTED, """")         else:             data = '{""error_name"":""bad destination"", ""errors"": [""Bad destination""]}'             return self.test_response(httplib.BAD_REQUEST, data)","if ""destination"" in body :",if not body :,16.70067963244422,16.70067963244422,0.0
"def _get_prefixed_values(data, prefix):     """"""Collect lines which start with prefix; with trimming""""""     matches = []     for line in data.splitlines():         line = line.strip()         <MASK>             match = line[len(prefix) :]             match = match.strip()             matches.append(match)     return matches",if line . startswith ( prefix ) :,if len ( prefix ) > 0 :,23.356898886410015,23.356898886410015,0.0
"def _power_exact(y, xc, yc, xe):     yc, ye = y.int, y.exp     while yc % 10 == 0:         yc //= 10         ye += 1     if xc == 1:         xe *= yc         while xe % 10 == 0:             xe //= 10             ye += 1         <MASK>             return None         exponent = xe * 10 ** ye         if y and xe:             xc = exponent         else:             xc = 0         return 5",if ye < 0 :,if xc == 0 :,17.965205598154213,17.965205598154213,0.0
"def init(self, view, items=None):     selections = []     if view.sel():         for region in view.sel():             selections.append(view.substr(region))     values = []     for idx, index in enumerate(map(int, items)):         if idx >= len(selections):             break         i = index - 1         if i >= 0 and i < len(selections):             values.append(selections[i])         else:             values.append(None)     # fill up     for idx, value in enumerate(selections):         <MASK>             values.append(value)     self.stack = values",if len ( values ) + 1 < idx :,if idx == 0 :,5.484411595600381,5.484411595600381,0.0
"def toggleFactorReload(self, value=None):     self.serviceFittingOptions[""useGlobalForceReload""] = (         value         if value is not None         else not self.serviceFittingOptions[""useGlobalForceReload""]     )     fitIDs = set()     for fit in set(self._loadedFits):         if fit is None:             continue         <MASK>             fit.factorReload = self.serviceFittingOptions[""useGlobalForceReload""]             fit.clearFactorReloadDependentData()             fitIDs.add(fit.ID)     return fitIDs",if fit . calculated :,if fit . factorReload != value :,22.089591134157878,22.089591134157878,0.0
"def init_weights(self):     """"""Initialize model weights.""""""     for m in self.predict_layers.modules():         if isinstance(m, nn.Conv2d):             kaiming_init(m)         <MASK>             constant_init(m, 1)         elif isinstance(m, nn.Linear):             normal_init(m, std=0.01)","elif isinstance ( m , nn . BatchNorm2d ) :","if isinstance ( m , nn . Constant :",54.627576446464936,54.627576446464936,0.0
"def _unzip_file(self, filepath, ext):     try:         <MASK>             zf = zipfile.ZipFile(filepath)             zf.extractall(os.path.dirname(filepath))             zf.close()         elif ext == "".tar"":             tf = tarfile.open(filepath)             tf.extractall(os.path.dirname(filepath))             tf.close()     except Exception as e:         raise ValueError(""Error reading file %r!\n%s"" % (filepath, e))","if ext == "".zip"" :","if ext == "".zip"" :",100.00000000000004,100.00000000000004,1.0
"def add_multiple_tasks(data, parent):     data = json.loads(data)     new_doc = {         ""doctype"": ""Task"",         ""parent_task"": parent if parent != ""All Tasks"" else """",     }     new_doc[""project""] = frappe.db.get_value(""Task"", {""name"": parent}, ""project"") or """"     for d in data:         <MASK>             continue         new_doc[""subject""] = d.get(""subject"")         new_task = frappe.get_doc(new_doc)         new_task.insert()","if not d . get ( ""subject"" ) :","if not d . get ( ""subject"" ) :",100.00000000000004,100.00000000000004,1.0
"def filterSimilarKeywords(keyword, kwdsIterator):     """"""Return a sorted list of keywords similar to the one given.""""""     seenDict = {}     kwdSndx = soundex(keyword.encode(""ascii"", ""ignore""))     matches = []     matchesappend = matches.append     checkContained = False     if len(keyword) > 4:         checkContained = True     for movieID, key in kwdsIterator:         <MASK>             continue         seenDict[key] = None         if checkContained and keyword in key:             matchesappend(key)             continue         if kwdSndx == soundex(key.encode(""ascii"", ""ignore"")):             matchesappend(key)     return _sortKeywords(keyword, matches)",if key in seenDict :,if key not in seenDict :,37.99178428257963,37.99178428257963,0.0
"def visit_If(self, node):     self.newline()     self.write(""if "")     self.visit(node.test)     self.write("":"")     self.body(node.body)     while True:         else_ = node.orelse         <MASK>             node = else_[0]             self.newline()             self.write(""elif "")             self.visit(node.test)             self.write("":"")             self.body(node.body)         else:             self.newline()             self.write(""else:"")             self.body(else_)             break","if len ( else_ ) == 1 and isinstance ( else_ [ 0 ] , If ) :",if else_ :,0.5419236976269574,0.5419236976269574,0.0
"def _eyeLinkHardwareAndSoftwareVersion(self):     try:         tracker_software_ver = 0         eyelink_ver = self._eyelink.getTrackerVersion()         <MASK>             tvstr = self._eyelink.getTrackerVersionString()             vindex = tvstr.find(""EYELINK CL"")             tracker_software_ver = int(                 float(tvstr[(vindex + len(""EYELINK CL"")) :].strip())             )         return eyelink_ver, tracker_software_ver     except Exception:         print2err(""EYELINK Error during _eyeLinkHardwareAndSoftwareVersion:"")         printExceptionDetailsToStdErr()         return EyeTrackerConstants.EYETRACKER_ERROR",if eyelink_ver == 3 :,if self . _eyelink . hasTrackerVersion ( ) :,5.934202609760488,5.934202609760488,0.0
"def execute(self, context):     for monad in context.blend_data.node_groups:         if monad.bl_idname == ""SverchGroupTreeType"":             <MASK>                 try:                     monad.update_cls()                 except Exception as err:                     print(err)                     print(""{} group class could not be created"".format(monad.name))     return {""FINISHED""}","if not getattr ( bpy . types , monad . cls_bl_idname , None ) :","if monad . bl_idname == ""SverchGroupTree"" :",9.621765470834335,9.621765470834335,0.0
"def word_pattern(pattern, str):     dict = {}     set_value = set()     list_str = str.split()     if len(list_str) != len(pattern):         return False     for i in range(len(pattern)):         if pattern[i] not in dict:             if list_str[i] in set_value:                 return False             dict[pattern[i]] = list_str[i]             set_value.add(list_str[i])         else:             <MASK>                 return False     return True",if dict [ pattern [ i ] ] != list_str [ i ] :,if list_str [ i ] not in dict :,29.616193937749262,29.616193937749262,0.0
"def decorator_handle(tokens):     """"""Process decorators.""""""     defs = []     decorates = []     for i, tok in enumerate(tokens):         if ""simple"" in tok and len(tok) == 1:             decorates.append(""@"" + tok[0])         <MASK>             varname = decorator_var + ""_"" + str(i)             defs.append(varname + "" = "" + tok[0])             decorates.append(""@"" + varname)         else:             raise CoconutInternalException(""invalid decorator tokens"", tok)     return ""\n"".join(defs + decorates) + ""\n""","elif ""test"" in tok and len ( tok ) == 1 :","if ""simple"" in tok :",8.858009236942326,8.858009236942326,0.0
"def wait_impl(self, cpid):     for i in range(10):         # wait3() shouldn't hang, but some of the buildbots seem to hang         # in the forking tests.  This is an attempt to fix the problem.         spid, status, rusage = os.wait3(os.WNOHANG)         <MASK>             break         time.sleep(1.0)     self.assertEqual(spid, cpid)     self.assertEqual(status, 0, ""cause = %d, exit = %d"" % (status & 0xFF, status >> 8))     self.assertTrue(rusage)",if spid == cpid :,if not rusage :,11.521590992286539,11.521590992286539,0.0
"def test_non_uniform_probabilities_over_elements(self):     param = iap.Choice([0, 1], p=[0.25, 0.75])     samples = param.draw_samples((10000,))     unique, counts = np.unique(samples, return_counts=True)     assert len(unique) == 2     for val, count in zip(unique, counts):         <MASK>             assert 2500 - 500 < count < 2500 + 500         elif val == 1:             assert 7500 - 500 < count < 7500 + 500         else:             assert False",if val == 0 :,if val == 0 :,100.00000000000004,100.00000000000004,1.0
"def dispatch_return(self, frame, arg):     if self.stop_here(frame) or frame == self.returnframe:         # Ignore return events in generator except when stepping.         if self.stopframe and frame.f_code.co_flags & CO_GENERATOR:             return self.trace_dispatch         try:             self.frame_returning = frame             self.user_return(frame, arg)         finally:             self.frame_returning = None         <MASK>             raise BdbQuit         # The user issued a 'next' or 'until' command.         if self.stopframe is frame and self.stoplineno != -1:             self._set_stopinfo(None, None)     return self.trace_dispatch",if self . quitting :,if self . stopframe is None :,26.269098944241588,26.269098944241588,0.0
"def mouse(self, button, mods, x, y):     if button == 1:         for i in range(4):             <MASK>                 self.hit = i     elif button == -1:         self.hit = None     elif self.hit != None:         self.coords[self.hit] = (x, y)         self.view.dirty()","if hypot ( x - self . coords [ i ] [ 0 ] , y - self . coords [ i ] [ 1 ] ) < 4 :",if mods [ i ] == 0 :,2.046626413521275,2.046626413521275,0.0
"def __init__(self, *commands):     self.all_cmds = list(         map(lambda cmd: cmd[0] if isinstance(cmd, list) else cmd, commands)     )     for command in commands:         self.cmd = command if isinstance(command, list) else [command]         self.cmd_path = pwndbg.which.which(self.cmd[0])         <MASK>             break",if self . cmd_path :,if not self . cmd_path :,70.71067811865478,70.71067811865478,0.0
"def _recv_obj(self, suppress_error=False):     """"""Receive a (picklable) object""""""     if self.conn.closed:         raise OSError(""handle is closed"")     try:         buf = self.conn.recv_bytes()     except (ConnectionError, EOFError) as e:         <MASK>             return         logger.debug(""receive has failed"", exc_info=e)         try:             self._set_remote_close_cause(e)             raise PipeShutdownError()         finally:             self._close()     obj = RemoteObjectUnpickler.loads(buf, self)     logger.debug(""received %r"", obj)     return obj",if suppress_error :,if suppress_error :,100.00000000000004,100.00000000000004,1.0
"def act(self, obs):     with chainer.no_backprop_mode():         batch_obs = self.batch_states([obs], self.xp, self.phi)         action_distrib = self.model(batch_obs)         <MASK>             return chainer.cuda.to_cpu(action_distrib.most_probable.array)[0]         else:             return chainer.cuda.to_cpu(action_distrib.sample().array)[0]",if self . act_deterministically :,if action_distrib . most_probable :,6.742555929751843,6.742555929751843,0.0
"def _classify(nodes_by_level):     missing, invalid, downloads = [], [], []     for level in nodes_by_level:         for node in level:             if node.binary == BINARY_MISSING:                 missing.append(node)             elif node.binary == BINARY_INVALID:                 invalid.append(node)             <MASK>                 downloads.append(node)     return missing, invalid, downloads","elif node . binary in ( BINARY_UPDATE , BINARY_DOWNLOAD ) :",if node . binary == BINARY_DOWNLOAD :,16.55902007954154,16.55902007954154,0.0
"def persist(self, *_):     for key, obj in self._objects.items():         try:             state = obj.get_state()             <MASK>                 continue             md5 = hashlib.md5(state).hexdigest()             if self._last_state.get(key) == md5:                 continue             self._persist_provider.store(key, state)         except Exception as e:             system_log.exception(""PersistHelper.persist fail"")         else:             self._last_state[key] = md5",if not state :,if state is None :,14.058533129758727,14.058533129758727,0.0
"def enter(self, doc, **kwds):     """"""Enters the mode, arranging for necessary grabs ASAP""""""     super(ColorPickMode, self).enter(doc, **kwds)     if self._started_from_key_press:         # Pick now using the last recorded event position         doc = self.doc         tdw = self.doc.tdw         t, x, y = doc.get_last_event_info(tdw)         <MASK>             self._pick_color_mode(tdw, x, y, self._pickmode)         # Start the drag when possible         self._start_drag_on_next_motion_event = True         self._needs_drag_start = True","if None not in ( x , y ) :",if t == self . _last_event_position :,3.673526562988939,3.673526562988939,0.0
"def on_profiles_loaded(self, profiles):     cb = self.builder.get_object(""cbProfile"")     model = cb.get_model()     model.clear()     for f in profiles:         name = f.get_basename()         <MASK>             continue         if name.endswith("".sccprofile""):             name = name[0:-11]         model.append((name, f, None))     cb.set_active(0)","if name . endswith ( "".mod"" ) :","if name == ""sccprofile"" :",9.545138913210204,9.545138913210204,0.0
"def subprocess_post_check(     completed_process: subprocess.CompletedProcess, raise_error: bool = True ) -> None:     if completed_process.returncode:         if completed_process.stdout is not None:             print(completed_process.stdout, file=sys.stdout, end="""")         <MASK>             print(completed_process.stderr, file=sys.stderr, end="""")         if raise_error:             raise PipxError(                 f""{' '.join([str(x) for x in completed_process.args])!r} failed""             )         else:             logger.info(f""{' '.join(completed_process.args)!r} failed"")",if completed_process . stderr is not None :,if completed_process . stderr is not None :,100.00000000000004,100.00000000000004,1.0
"def test_connect(     ipaddr, port, device, partition, method, path, headers=None, query_string=None ):     if path == ""/a"":         for k, v in headers.iteritems():             <MASK>                 break         else:             test_errors.append(""%s: %s not in %s"" % (test_header, test_value, headers))",if k . lower ( ) == test_header . lower ( ) and v == test_value :,if k == test_value :,11.734812285622054,11.734812285622054,0.0
"def test_stat_result_pickle(self):     result = os.stat(self.fname)     for proto in range(pickle.HIGHEST_PROTOCOL + 1):         p = pickle.dumps(result, proto)         self.assertIn(b""stat_result"", p)         <MASK>             self.assertIn(b""cos\nstat_result\n"", p)         unpickled = pickle.loads(p)         self.assertEqual(result, unpickled)",if proto < 4 :,if proto == pickle . HIGHEST_PROTOCOL :,9.287528999566801,9.287528999566801,0.0
"def run_sql(sql):     table = sql.split("" "")[5]     logger.info(""Updating table {}"".format(table))     with transaction.atomic():         with connection.cursor() as cursor:             cursor.execute(sql)             rows = cursor.fetchall()             <MASK>                 raise Exception(""Sentry notification that {} is migrated"".format(table))",if not rows :,if not rows :,100.00000000000004,100.00000000000004,1.0
"def countbox(self):     self.box = [1000, 1000, -1000, -1000]     for x, y in self.body:         if x < self.box[0]:             self.box[0] = x         <MASK>             self.box[2] = x         if y < self.box[1]:             self.box[1] = y         if y > self.box[3]:             self.box[3] = y",if x > self . box [ 2 ] :,if x > self . box [ 2 ] :,100.00000000000004,100.00000000000004,1.0
"def _packageFocusOutViaKeyPress(self, row, column, txt):     if txt:         self._set_current_cell(row + 1, column)     else:         widget = self.cellWidget(row + 1, column)         <MASK>             self._delete_cell(row, column)         new_request = self.get_request()         self.context_model.set_request(new_request)         self._update_request_column(column, self.context_model)","if widget and isinstance ( widget , PackageSelectWidget ) :",if widget . is_focus :,9.469167282754096,9.469167282754096,0.0
"def parse_bash_set_output(output):     """"""Parse Bash-like 'set' output""""""     if not sys.platform.startswith(""win""):         # Replace ""\""-continued lines in *Linux* environment dumps.         # Cannot do this on Windows because a ""\"" at the end of the         # line does not imply a continuation.         output = output.replace(""\\\n"", """")     environ = {}     for line in output.splitlines(0):         line = line.rstrip()         <MASK>             continue  # skip black lines         item = _ParseBashEnvStr(line)         if item:             environ[item[0]] = item[1]     return environ",if not line :,if not line :,100.00000000000004,100.00000000000004,1.0
"def _get(self, domain):     with self.lock:         try:             record = self.cache[domain]             time_now = time.time()             <MASK>                 record = None         except KeyError:             record = None         if not record:             record = {""r"": ""unknown"", ""dns"": {}, ""g"": 1, ""query_count"": 0}         # self.cache[domain] = record         return record","if time_now - record [ ""update"" ] > self . ttl :",if time_now > self . query_count :,22.269344484776322,22.269344484776322,0.0
"def test_filehash(self):     """"""tests the hashes of the files in data/""""""     fp = self.get_data_path()     for fn in os.listdir(fp):         <MASK>             # file used for something else             continue         expected_hash = fn         fullp = os.path.join(fp, fn)         output = self.run_command(""sha1sum "" + fullp, exitcode=0)         result = output.split("" "")[0]         self.assertEqual(result, expected_hash)","if ""."" in fn :",if not os . path . isfile ( fn ) :,5.300156689756295,5.300156689756295,0.0
"def test_new_vs_reference_code_stream_read_during_iter(read_idx, read_len, bytecode):     reference = SlowCodeStream(bytecode)     latest = CodeStream(bytecode)     for index, (actual, expected) in enumerate(zip(latest, reference)):         assert actual == expected         if index == read_idx:             readout_actual = latest.read(read_len)             readout_expected = reference.read(read_len)             assert readout_expected == readout_actual         <MASK>             assert latest.program_counter >= len(reference)         else:             assert latest.program_counter == reference.program_counter",if reference . program_counter >= len ( reference ) :,if index == read_idx :,4.18031138310865,4.18031138310865,0.0
"def setup_logging():     try:         logconfig = config.get(""logging_config_file"")         <MASK>             logging.config.fileConfig(logconfig, disable_existing_loggers=False)         logger.info(""logging initialized"")         logger.debug(""debug"")     except Exception as e:         print(""Unable to set logging configuration:"", str(e), file=sys.stderr)         raise",if logconfig and os . path . exists ( logconfig ) :,if logconfig :,3.136388772461349,0.0,0.0
"def all_words(filename):     start_char = True     for c in characters(filename):         <MASK>             word = """"             if c.isalnum():                 # We found the start of a word                 word = c.lower()                 start_char = False             else:                 pass         else:             if c.isalnum():                 word += c.lower()             else:                 # We found end of word, emit it                 start_char = True                 yield word",if start_char == True :,if start_char :,38.80684294761701,38.80684294761701,0.0
"def _get_nonce(self, url, new_nonce_url):     if not self._nonces:         logger.debug(""Requesting fresh nonce"")         <MASK>             response = self.head(url)         else:             # request a new nonce from the acme newNonce endpoint             response = self._check_response(self.head(new_nonce_url), content_type=None)         self._add_nonce(response)     return self._nonces.pop()",if new_nonce_url is None :,if new_nonce_url is None :,100.00000000000004,100.00000000000004,1.0
"def paragraph_is_fully_commented(lines, comment, main_language):     """"""Is the paragraph fully commented?""""""     for i, line in enumerate(lines):         if line.startswith(comment):             <MASK>                 continue             if is_magic(line, main_language):                 return False             continue         return i > 0 and _BLANK_LINE.match(line)     return True",if line [ len ( comment ) : ] . lstrip ( ) . startswith ( comment ) :,if i == 0 :,1.1057717812699017,1.1057717812699017,0.0
"def gvariant_args(args: List[Any]) -> str:     """"""Convert args into gvariant.""""""     gvariant = """"     for arg in args:         if isinstance(arg, bool):             gvariant += "" {}"".format(str(arg).lower())         <MASK>             gvariant += f"" {arg}""         elif isinstance(arg, str):             gvariant += f' ""{arg}""'         else:             gvariant += f"" {arg!s}""     return gvariant.lstrip()","elif isinstance ( arg , ( int , float ) ) :","if isinstance ( arg , int ) :",25.9162669876144,25.9162669876144,0.0
"def _SkipGroup(buffer, pos, end):     """"""Skip sub-group.  Returns the new position.""""""     while 1:         (tag_bytes, pos) = ReadTag(buffer, pos)         new_pos = SkipField(buffer, pos, end, tag_bytes)         <MASK>             return pos         pos = new_pos",if new_pos == - 1 :,if new_pos == end :,62.401954419369176,62.401954419369176,0.0
"def update_participants(self, refresh=True):     for participant in list(self.participants_dict):         if participant is None or participant == self.simulator_config.broadcast_part:             continue         self.removeItem(self.participants_dict[participant])         self.participant_items.remove(self.participants_dict[participant])         del self.participants_dict[participant]     for participant in self.simulator_config.participants:         <MASK>             self.participants_dict[participant].refresh()         else:             self.insert_participant(participant)     if refresh:         self.update_view()",if participant in self . participants_dict :,if participant in self . participants_dict :,100.00000000000004,100.00000000000004,1.0
"def feature_reddit(layer_data, graph):     feature = {}     times = {}     indxs = {}     for _type in layer_data:         if len(layer_data[_type]) == 0:             continue         idxs = np.array(list(layer_data[_type].keys()))         tims = np.array(list(layer_data[_type].values()))[:, 1]         feature[_type] = np.array(             list(graph.node_feature[_type].loc[idxs, ""emb""]), dtype=np.float         )         times[_type] = tims         indxs[_type] = idxs         <MASK>             attr = feature[_type]     return feature, times, indxs, attr","if _type == ""def"" :",if len ( feature [ _type ] ) == 1 :,9.669265690880861,9.669265690880861,0.0
"def _get_sort_map(tags):     """"""See TAG_TO_SORT""""""     tts = {}     for name, tag in tags.items():         if tag.has_sort:             <MASK>                 tts[name] = ""%ssort"" % name             if tag.internal:                 tts[""~%s"" % name] = ""~%ssort"" % name     return tts",if tag . user :,if tag . internal :,42.72870063962342,42.72870063962342,0.0
"def max_radius(iterator):     radius_result = dict()     for k, v in iterator:         if v[0] not in radius_result:             radius_result[v[0]] = v[1]         <MASK>             radius_result[v[0]] = v[1]     return radius_result",elif v [ 1 ] >= radius_result [ v [ 0 ] ] :,if v [ 1 ] not in radius_result :,20.334154400185707,20.334154400185707,0.0
"def run(self):     pwd_found = []     if constant.user_dpapi and constant.user_dpapi.unlocked:         main_vault_directory = os.path.join(             constant.profile[""APPDATA""], u"".."", u""Local"", u""Microsoft"", u""Vault""         )         if os.path.exists(main_vault_directory):             for vault_directory in os.listdir(main_vault_directory):                 cred = constant.user_dpapi.decrypt_vault(                     os.path.join(main_vault_directory, vault_directory)                 )                 <MASK>                     pwd_found.append(cred)     return pwd_found",if cred :,if cred :,100.00000000000004,0.0,1.0
"def disconnect_sync(self, connection, close_connection=False):     key = id(connection)     ts = self.in_use.pop(key)     if close_connection:         self.connections_map.pop(key)         self._connection_close_sync(connection)     else:         <MASK>             self.connections_map.pop(key)             self._connection_close_sync(connection)         else:             with self._lock_sync:                 heapq.heappush(self.connections_sync, (ts, key))",if self . stale_timeout and self . is_stale ( ts ) :,if ts == self . in_use :,6.061512325492642,6.061512325492642,0.0
"def _populate_tree(self, element, d):     """"""Populates an etree with attributes & elements, given a dict.""""""     for k, v in d.iteritems():         if isinstance(v, dict):             self._populate_dict(element, k, v)         elif isinstance(v, list):             self._populate_list(element, k, v)         elif isinstance(v, bool):             self._populate_bool(element, k, v)         elif isinstance(v, basestring):             self._populate_str(element, k, v)         <MASK>             self._populate_number(element, k, v)","elif type ( v ) in [ int , float , long , complex ] :","if isinstance ( v , int ) :",5.34741036489421,5.34741036489421,0.0
"def readframes(self, nframes):     if self._ssnd_seek_needed:         self._ssnd_chunk.seek(0)         dummy = self._ssnd_chunk.read(8)         pos = self._soundpos * self._framesize         <MASK>             self._ssnd_chunk.seek(pos + 8)         self._ssnd_seek_needed = 0     if nframes == 0:         return """"     data = self._ssnd_chunk.read(nframes * self._framesize)     if self._convert and data:         data = self._convert(data)     self._soundpos = self._soundpos + len(data) / (self._nchannels * self._sampwidth)     return data",if pos :,if dummy :,34.66806371753173,0.0,0.0
"def target_glob(tgt, hosts):     ret = {}     for host in hosts:         <MASK>             ret[host] = copy.deepcopy(__opts__.get(""roster_defaults"", {}))             ret[host].update({""host"": host})             if __opts__.get(""ssh_user""):                 ret[host].update({""user"": __opts__[""ssh_user""]})     return ret","if fnmatch . fnmatch ( tgt , host ) :","if __opts__ . get ( ""roster_defaults"" ) :",6.150343144231885,6.150343144231885,0.0
"def get_attribute_value(self, nodeid, attr):     with self._lock:         self.logger.debug(""get attr val: %s %s"", nodeid, attr)         if nodeid not in self._nodes:             dv = ua.DataValue()             dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown)             return dv         node = self._nodes[nodeid]         if attr not in node.attributes:             dv = ua.DataValue()             dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid)             return dv         attval = node.attributes[attr]         <MASK>             return attval.value_callback()         return attval.value",if attval . value_callback :,if attval . value_callback :,100.00000000000004,100.00000000000004,1.0
"def remove_property(self, key):  # type: (str) -> None     with self.secure() as config:         keys = key.split(""."")         current_config = config         for i, key in enumerate(keys):             <MASK>                 return             if i == len(keys) - 1:                 del current_config[key]                 break             current_config = current_config[key]",if key not in current_config :,if not current_config [ key ] :,21.93456688254155,21.93456688254155,0.0
"def _class_browser(parent):  # Wrapper for htest     try:         file = __file__     except NameError:         file = sys.argv[0]         <MASK>             file = sys.argv[1]         else:             file = sys.argv[0]     dir, file = os.path.split(file)     name = os.path.splitext(file)[0]     flist = PyShell.PyShellFileList(parent)     global file_open     file_open = flist.open     ClassBrowser(flist, name, [dir], _htest=True)",if sys . argv [ 1 : ] :,if os . path . isdir ( file ) :,5.522397783539471,5.522397783539471,0.0
"def get_only_text_part(self, msg):     count = 0     only_text_part = None     for part in msg.walk():         if part.is_multipart():             continue         count += 1         mimetype = part.get_content_type() or ""text/plain""         <MASK>             return False         else:             only_text_part = part     return only_text_part","if mimetype != ""text/plain"" or count != 1 :",if not count :,1.3439177330889347,1.3439177330889347,0.0
"def should_keep_alive(commit_msg):     result = False     ci = get_current_ci() or """"     for line in commit_msg.splitlines():         parts = line.strip(""# "").split("":"", 1)         (key, val) = parts if len(parts) > 1 else (parts[0], """")         if key == ""CI_KEEP_ALIVE"":             ci_names = val.replace("","", "" "").lower().split() if val else []             <MASK>                 result = True     return result",if len ( ci_names ) == 0 or ci . lower ( ) in ci_names :,if ci_names [ 0 ] == ci :,8.186324982349669,8.186324982349669,0.0
"def _calc_block_io(self, blkio):     """"""Calculate block IO stats.""""""     for stats in blkio[""io_service_bytes_recursive""]:         if stats[""op""] == ""Read"":             self._blk_read += stats[""value""]         <MASK>             self._blk_write += stats[""value""]","elif stats [ ""op"" ] == ""Write"" :","if stats [ ""op"" ] == ""Write"" :",91.21679090703874,91.21679090703874,0.0
"def value_to_db_datetime(self, value):     if value is None:         return None     # Oracle doesn't support tz-aware datetimes     if timezone.is_aware(value):         <MASK>             value = value.astimezone(timezone.utc).replace(tzinfo=None)         else:             raise ValueError(                 ""Oracle backend does not support timezone-aware datetimes when USE_TZ is False.""             )     return six.text_type(value)",if settings . USE_TZ :,if USE_TZ :,47.39878501170795,47.39878501170795,0.0
"def load_state_dict(self, state_dict):     for module_name, module_state_dict in state_dict.items():         if module_name in self.module_pool:             <MASK>                 self.module_pool[module_name].module.load_state_dict(module_state_dict)             else:                 self.module_pool[module_name].load_state_dict(module_state_dict)         else:             logging.info(f""Missing {module_name} in module_pool, skip it.."")","if self . config [ ""dataparallel"" ] :","if module_state_dict [ ""state"" ] == state_dict :",7.158561577277536,7.158561577277536,0.0
"def _unpack_scales(scales, vidxs):     scaleData = [None, None, None]     for i in range(3):         if i >= min(len(scales), len(vidxs) // 2):             break         scale = scales[i]         <MASK>             vidx1, vidx2 = vidxs[i * 2], vidxs[i * 2 + 1]             scaleData[i] = (int(vidx1), int(vidx2), float(scale))     return scaleData",if not math . isnan ( scale ) :,if scale != 0 :,6.4790667469036025,6.4790667469036025,0.0
"def __init__(self, factors, contrast_matrices, num_columns):     self.factors = tuple(factors)     factor_set = frozenset(factors)     if not isinstance(contrast_matrices, dict):         raise ValueError(""contrast_matrices must be dict"")     for factor, contrast_matrix in six.iteritems(contrast_matrices):         <MASK>             raise ValueError(""Unexpected factor in contrast_matrices dict"")         if not isinstance(contrast_matrix, ContrastMatrix):             raise ValueError(""Expected a ContrastMatrix, not %r"" % (contrast_matrix,))     self.contrast_matrices = contrast_matrices     if not isinstance(num_columns, six.integer_types):         raise ValueError(""num_columns must be an integer"")     self.num_columns = num_columns",if factor not in factor_set :,if factor not in factor_set :,100.00000000000004,100.00000000000004,1.0
"def app(scope, receive, send):     while True:         message = await receive()         <MASK>             await send({""type"": ""websocket.accept""})         elif message[""type""] == ""websocket.receive"":             pass         elif message[""type""] == ""websocket.disconnect"":             break","if message [ ""type"" ] == ""websocket.connect"" :","if message [ ""type"" ] == ""websocket.accept"" :",82.42367502646057,82.42367502646057,0.0
"def value__set(self, value):     for i, (option, checked) in enumerate(self.options):         <MASK>             self.selectedIndex = i             break     else:         raise ValueError(             ""Option %r not found (from %s)""             % (value, "", "".join([repr(o) for o, c in self.options]))         )",if option == str ( value ) :,if option == value :,34.1077254951379,34.1077254951379,0.0
"def init_links(self):     links = LinkCallback.find_links(self)     callbacks = []     for link, src_plot, tgt_plot in links:         cb = Link._callbacks[""bokeh""][type(link)]         <MASK>             continue         callbacks.append(cb(self.root, link, src_plot, tgt_plot))     return callbacks",if src_plot is None or ( link . _requires_target and tgt_plot is None ) :,if not cb :,0.21102530003841274,0.21102530003841274,0.0
"def _validate_scalar_extensions(self) -> List[str]:     errors = []     for extension in [         x for x in self.extensions if isinstance(x, GraphQLScalarTypeExtension)     ]:         extended = self.type_definitions.get(extension.name)         ext_errors = _validate_extension(             extended, extension.name, GraphQLScalarType, ""SCALAR""         )         errors.extend(ext_errors)         <MASK>             errors.extend(_validate_extension_directives(extension, extended, ""SCALAR""))     return errors",if not ext_errors :,if extension . directives :,10.400597689005304,10.400597689005304,0.0
"def copy_tcltk(src, dest, symlink):     """"""copy tcl/tk libraries on Windows (issue #93)""""""     for libversion in ""8.5"", ""8.6"":         for libname in ""tcl"", ""tk"":             srcdir = join(src, ""tcl"", libname + libversion)             destdir = join(dest, ""tcl"", libname + libversion)             # Only copy the dirs from the above combinations that exist             <MASK>                 copyfileordir(srcdir, destdir, symlink)",if os . path . exists ( srcdir ) and not os . path . exists ( destdir ) :,"if libname in ""tcl"" :",1.2192584915912204,1.2192584915912204,0.0
"def parse(self, response):     try:         content = response.content.decode(""utf-8"", ""ignore"")         content = json.loads(content, strict=False)     except:         self.logger.error(""Fail to parse the response in json format"")         return     for item in content[""data""]:         if ""objURL"" in item:             img_url = self._decode_url(item[""objURL""])         <MASK>             img_url = item[""hoverURL""]         else:             continue         yield dict(file_url=img_url)","elif ""hoverURL"" in item :","if ""hoverURL"" in item :",80.91067115702207,80.91067115702207,0.0
"def check_and_reload(self):     # Check if tables have been modified, if so reload     for table_name, table_version in self._table_versions.items():         table = self.app.tool_data_tables.get(table_name, None)         <MASK>             return self.reload_genomes()",if table is not None and not table . is_current_version ( table_version ) :,if table . version == table_version :,8.732284559507212,8.732284559507212,0.0
"def _get_query_defaults(self, query_defns):     defaults = {}     for k, v in query_defns.items():         try:             <MASK>                 defaults[k] = self._get_default_obj(v[""schema""])             else:                 defaults[k] = v[""schema""][""default""]         except KeyError:             pass     return defaults","if v [ ""schema"" ] [ ""type"" ] == ""object"" :","if v [ ""schema"" ] is not None :",31.22606771110883,31.22606771110883,0.0
"def ftp_login(host, port, username=None, password=None, anonymous=False):     ret = False     try:         ftp = ftplib.FTP()         ftp.connect(host, port, timeout=6)         <MASK>             ftp.login()         else:             ftp.login(username, password)         ret = True         ftp.quit()     except Exception:         pass     return ret",if anonymous :,if anonymous :,100.00000000000004,0.0,1.0
"def _getVolumeScalar(self):     if self._volumeScalar is not None:         return self._volumeScalar     # use default     elif self._value in dynamicStrToScalar:         return dynamicStrToScalar[self._value]     else:         thisDynamic = self._value         # ignore leading s like in sf         if ""s"" in thisDynamic:             thisDynamic = thisDynamic[1:]         # ignore closing z like in fz         if thisDynamic[-1] == ""z"":             thisDynamic = thisDynamic[:-1]         <MASK>             return dynamicStrToScalar[thisDynamic]         else:             return dynamicStrToScalar[None]",if thisDynamic in dynamicStrToScalar :,"if thisDynamic [ 0 ] == ""s"" :",8.29519350710986,8.29519350710986,0.0
"def processCoords(coords):     newcoords = deque()     for (x, y, z) in coords:         for _dir, offsets in faceDirections:             if _dir == FaceYIncreasing:                 continue             dx, dy, dz = offsets             p = (x + dx, y + dy, z + dz)             <MASK>                 continue             nx, ny, nz = p             if level.blockAt(nx, ny, nz) == 0:                 level.setBlockAt(nx, ny, nz, waterID)                 newcoords.append(p)     return newcoords",if p not in box :,if p == 0 :,17.965205598154213,17.965205598154213,0.0
"def _set_property(self, target_widget, pname, value):     if pname == ""text"":         wstate = str(target_widget[""state""])         <MASK>             # change state temporarily             target_widget[""state""] = ""normal""         target_widget.delete(""0"", tk.END)         target_widget.insert(""0"", value)         target_widget[""state""] = wstate     else:         super(EntryBaseBO, self)._set_property(target_widget, pname, value)","if wstate != ""normal"" :",if wstate == tk . START :,13.134549472120788,13.134549472120788,0.0
"def teardown():     try:         time.sleep(1)     except KeyboardInterrupt:         return     while launchers:         p = launchers.pop()         <MASK>             try:                 p.stop()             except Exception as e:                 print(e)                 pass         if p.poll() is None:             try:                 time.sleep(0.25)             except KeyboardInterrupt:                 return         if p.poll() is None:             try:                 print(""cleaning up test process..."")                 p.signal(SIGKILL)             except:                 print(""couldn't shutdown process: "", p)",if p . poll ( ) is None :,if p . is_alive ( ) :,24.274588585366175,24.274588585366175,0.0
"def checkAndRemoveDuplicate(self, node):     for bucket in self.buckets:         for n in bucket.getNodes():             <MASK>                 self.removeContact(n)","if ( n . ip , n . port ) == ( node . ip , node . port ) and n . id != node . id :",if node . id == node . id :,6.579399787958061,6.579399787958061,0.0
"def toString():     flags = u""""     try:         if this.glob:             flags += u""g""         <MASK>             flags += u""i""         if this.multiline:             flags += u""m""     except:         pass     v = this.value if this.value else ""(?:)""     return u""/%s/"" % v + flags",if this . ignore_case :,if this . ignoreCase :,28.641904579795423,28.641904579795423,0.0
"def import_submodules(package_name):     package = sys.modules[package_name]     results = {}     for loader, name, is_pkg in pkgutil.iter_modules(package.__path__):         full_name = package_name + ""."" + name         module = importlib.import_module(full_name)         setattr(sys.modules[__name__], name, module)         results[full_name] = module         if is_pkg:             valid_pkg = import_submodules(full_name)             <MASK>                 results.update(valid_pkg)     return results",if valid_pkg :,if valid_pkg :,100.00000000000004,100.00000000000004,1.0
"def _call(self, cmd):     what = cmd[""command""]     if what == ""list"":         name = cmd[""properties""].get(""name"")         <MASK>             return {""watchers"": [""one"", ""two"", ""three""]}         return {""pids"": [123, 456]}     elif what == ""dstats"":         return {""info"": {""pid"": 789}}     elif what == ""listsockets"":         return {             ""status"": ""ok"",             ""sockets"": [{""path"": self._unix, ""fd"": 5, ""name"": ""XXXX"", ""backlog"": 2048}],             ""time"": 1369647058.967524,         }     raise NotImplementedError(cmd)",if name is None :,"if name == ""watchers"" :",12.22307556087252,12.22307556087252,0.0
"def select(self):     e = xlib.XEvent()     while xlib.XPending(self._display):         xlib.XNextEvent(self._display, e)         # Key events are filtered by the xlib window event         # handler so they get a shot at the prefiltered event.         if e.xany.type not in (xlib.KeyPress, xlib.KeyRelease):             <MASK>                 continue         try:             dispatch = self._window_map[e.xany.window]         except KeyError:             continue         dispatch(e)","if xlib . XFilterEvent ( e , e . xany . window ) :",if e . xany . window is None :,27.855256475695366,27.855256475695366,0.0
"def translate(self, line):     parsed = self.RE_LINE_PARSER.match(line)     if parsed:         value = parsed.group(3)         stage = parsed.group(1)         <MASK>  # query string is rendered here             return ""\n# HTTP Request:\n"" + self.stripslashes(value)         elif stage == ""reply"":             return ""\n\n# HTTP Response:\n"" + self.stripslashes(value)         elif stage == ""header"":             return value + ""\n""         else:             return value     return line","if stage == ""send"" :","if stage == ""query"" :",59.4603557501361,59.4603557501361,0.0
"def toString():     flags = u""""     try:         <MASK>             flags += u""g""         if this.ignore_case:             flags += u""i""         if this.multiline:             flags += u""m""     except:         pass     v = this.value if this.value else ""(?:)""     return u""/%s/"" % v + flags",if this . glob :,if this . ignoreCase :,42.72870063962342,42.72870063962342,0.0
"def __exit__(self, *exc_info):     super(WarningsChecker, self).__exit__(*exc_info)     # only check if we're not currently handling an exception     if all(a is None for a in exc_info):         if self.expected_warning is not None:             <MASK>                 __tracebackhide__ = True                 pytest.fail(""DID NOT WARN"")",if not any ( r . category in self . expected_warning for r in self ) :,if self . expected_warning is not None :,17.96191510244705,17.96191510244705,0.0
"def run(self):     for k, v in iteritems(self.objs):         if k.startswith(""_""):             continue         <MASK>             if v[""email""] == """":                 v[""email""] = None             if v[""ip""] == ""0.0.0.0"":                 v[""ip""] = None     return self.objs","if v [ ""_class"" ] == ""User"" :","if v [ ""email"" ] == """" :",48.871645172969465,48.871645172969465,0.0
"def list_stuff(self, upto=10, start_after=-1):     for i in range(upto):         if i <= start_after:             continue         <MASK>             self.count += 1             raise TemporaryProblem         if i == 7 and self.count < 4:             self.count += 1             raise TemporaryProblem         yield i",if i == 2 and self . count < 1 :,if i == 0 :,19.765609300943975,19.765609300943975,0.0
"def check(self):     tcp_client = self.tcp_create()     if tcp_client.connect():         tcp_client.send(b""ABCDE"")         response = tcp_client.recv(5)         tcp_client.close()         if response:             <MASK>                 self.endianness = "">""  # BE             elif response.startswith(b""ScMM""):                 self.endianness = ""<""  # LE             return True  # target is vulnerable     return False  # target is not vulnerable","if response . startswith ( b""MMcS"" ) :","if response . startswith ( b""BCC"" ) :",70.16879391277372,70.16879391277372,0.0
"def copy_tree(self, src_dir, dst_dir, skip_variables=False):     for src_root, _, files in os.walk(src_dir):         if src_root != src_dir:             rel_root = os.path.relpath(src_root, src_dir)         else:             rel_root = """"         if skip_variables and rel_root.startswith(""variables""):             continue         dst_root = os.path.join(dst_dir, rel_root)         <MASK>             os.makedirs(dst_root)         for f in files:             shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))",if not os . path . exists ( dst_root ) :,if not os . path . isdir ( dst_root ) :,76.11606003349888,76.11606003349888,0.0
"def _set_hostport(self, host, port):     if port is None:         i = host.rfind("":"")         j = host.rfind(""]"")  # ipv6 addresses have [...]         <MASK>             try:                 port = int(host[i + 1 :])             except ValueError:                 raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1 :])             host = host[:i]         else:             port = self.default_port         if host and host[0] == ""["" and host[-1] == ""]"":             host = host[1:-1]     self.host = host     self.port = port",if i > j :,if i < j :,30.213753973567677,30.213753973567677,0.0
"def _get_field_value(self, test, key, match):     if test.ver == ofproto_v1_0.OFP_VERSION:         members = inspect.getmembers(match)         for member in members:             if member[0] == key:                 field_value = member[1]             elif member[0] == ""wildcards"":                 wildcards = member[1]         <MASK>             field_value = test.nw_src_to_str(wildcards, field_value)         elif key == ""nw_dst"":             field_value = test.nw_dst_to_str(wildcards, field_value)     else:         field_value = match[key]     return field_value","if key == ""nw_src"" :","if key == ""nw_src"" :",100.00000000000004,100.00000000000004,1.0
"def _clear_storage():     """"""Clear old files from storage.""""""     hacs = get_hacs()     storagefiles = [""hacs""]     for s_f in storagefiles:         path = f""{hacs.core.config_path}/.storage/{s_f}""         <MASK>             hacs.log.info(f""Cleaning up old storage file {path}"")             os.remove(path)",if os . path . isfile ( path ) :,if os . path . isfile ( path ) :,100.00000000000004,100.00000000000004,1.0
"def action_delete(self, ids):     try:         count = 0         # TODO: Optimize me         for pk in ids:             <MASK>                 count += 1         flash(             ngettext(                 ""Record was successfully deleted."",                 ""%(count)s records were successfully deleted."",                 count,                 count=count,             ),             ""success"",         )     except Exception as ex:         flash(gettext(""Failed to delete records. %(error)s"", error=str(ex)), ""error"")",if self . delete_model ( self . get_one ( pk ) ) :,if pk . id == self . id :,5.484683161991908,5.484683161991908,0.0
"def test_inclusion(all_values):     for values in [{""guid_2"", ""guid_1""}, {""guid_5"", ""guid_XXX""}, {""guid_2""}]:         test_predicate = in_set(values, ""volume_guid"")         included_values = set()         for val in all_values:             <MASK>                 included_values.add(val)         assert included_values == all_values.intersection(values)","if test_predicate . do_include ( { ""volume_guid"" : val } ) :",if test_predicate ( val ) :,9.857708014755968,9.857708014755968,0.0
"def _get_attr(sdk_path, mod_attr_path, checked=True):     try:         attr_mod, attr_path = (             mod_attr_path.split(""#"") if ""#"" in mod_attr_path else (mod_attr_path, """")         )         full_mod_path = ""{}.{}"".format(sdk_path, attr_mod) if attr_mod else sdk_path         op = import_module(full_mod_path)         if attr_path:             # Only load attributes if needed             for part in attr_path.split("".""):                 op = getattr(op, part)         return op     except (ImportError, AttributeError) as ex:         <MASK>             return None         raise ex",if checked :,if not checked :,35.35533905932737,35.35533905932737,0.0
"def __exit__(self, exc_type, exc_val, exc_tb):     if self.fusefat is not None:         self.fusefat.send_signal(signal.SIGINT)         # Allow 1s to return without sending terminate         for count in range(10):             time.sleep(0.1)             <MASK>                 break         else:             self.fusefat.terminate()         time.sleep(self.delay)         assert not os.path.exists(self.canary)     self.dev_null.close()     shutil.rmtree(self.tmpdir)",if self . fusefat . poll ( ) is not None :,if count == 1 :,3.550932348642477,3.550932348642477,0.0
"def check_context_processors(output):     with output.section(""Context processors"") as section:         processors = list(             chain(                 *[                     template[""OPTIONS""].get(""context_processors"", [])                     for template in settings.TEMPLATES                 ]             )         )         required_processors = (""cms.context_processors.cms_settings"",)         for processor in required_processors:             <MASK>                 section.error(                     ""%s context processor must be in TEMPLATES option context_processors""                     % processor                 )",if processor not in processors :,if processor not in processors :,100.00000000000004,100.00000000000004,1.0
"def test_converters(self):     response = self._get(""datatypes/converters"")     self._assert_status_code_is(response, 200)     converters_list = response.json()     found_fasta_to_tabular = False     for converter in converters_list:         self._assert_has_key(converter, ""source"", ""target"", ""tool_id"")         <MASK>             found_fasta_to_tabular = True     assert found_fasta_to_tabular","if converter [ ""source"" ] == ""fasta"" and converter [ ""target"" ] == ""tabular"" :",if converter . tool_id == self . tool_id :,3.8711036051829204,3.8711036051829204,0.0
"def remove_pid(self, watcher, pid):     if pid in self._pids[watcher]:         logger.debug(""Removing %d from %s"" % (pid, watcher))         self._pids[watcher].remove(pid)         <MASK>             logger.debug(""Stopping the periodic callback for {0}"".format(watcher))             self._callbacks[watcher].stop()",if len ( self . _pids [ watcher ] ) == 0 :,if self . _callbacks [ watcher ] :,16.466920669770428,16.466920669770428,0.0
"def _fc_layer(self, sess, bottom, name, trainable=True, relu=True):     with tf.variable_scope(name) as scope:         shape = bottom.get_shape().as_list()         dim = 1         for d in shape[1:]:             dim *= d         x = tf.reshape(bottom, [-1, dim])         weight = self._get_fc_weight(sess, name, trainable=trainable)         bias = self._get_bias(sess, name, trainable=trainable)         fc = tf.nn.bias_add(tf.matmul(x, weight), bias)         <MASK>             fc = tf.nn.relu(fc)         return fc",if relu :,if relu :,100.00000000000004,0.0,1.0
"def get_drive(self, root_path="""", volume_guid_path=""""):     for drive in self.drives:         if root_path:             config_root_path = drive.get(""root_path"")             <MASK>                 return drive         elif volume_guid_path:             config_volume_guid_path = drive.get(""volume_guid_path"")             if config_volume_guid_path and config_volume_guid_path == volume_guid_path:                 return drive",if config_root_path and root_path == config_root_path :,if config_root_path == root_path :,50.61208977839847,50.61208977839847,0.0
"def rewire_init(expr):     new_args = []     if expr[0] == HySymbol(""setv""):         pairs = expr[1:]         while len(pairs) > 0:             k, v = (pairs.pop(0), pairs.pop(0))             <MASK>                 v.append(HySymbol(""None""))             new_args.append(k)             new_args.append(v)         expr = HyExpression([HySymbol(""setv"")] + new_args).replace(expr)     return expr","if k == HySymbol ( ""__init__"" ) :","if k . startswith ( ""None"" ) :",13.92442062500076,13.92442062500076,0.0
"def doDir(elem):     for child in elem.childNodes:         if not isinstance(child, minidom.Element):             continue         <MASK>             doDir(child)         elif child.tagName == ""Component"":             for grandchild in child.childNodes:                 if not isinstance(grandchild, minidom.Element):                     continue                 if grandchild.tagName != ""File"":                     continue                 files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))","if child . tagName == ""Directory"" :","if child . tagName == ""Directory"" :",100.00000000000004,100.00000000000004,1.0
"def _v2_common(self, cfg):     LOG.debug(""v2_common: handling config:\n%s"", cfg)     if ""nameservers"" in cfg:         search = cfg.get(""nameservers"").get(""search"", [])         dns = cfg.get(""nameservers"").get(""addresses"", [])         name_cmd = {""type"": ""nameserver""}         <MASK>             name_cmd.update({""search"": search})         if len(dns) > 0:             name_cmd.update({""addresses"": dns})         LOG.debug(""v2(nameserver) -> v1(nameserver):\n%s"", name_cmd)         self.handle_nameserver(name_cmd)",if len ( search ) > 0 :,if len ( search ) > 0 :,100.00000000000004,100.00000000000004,1.0
"def __start_element_handler(self, name, attrs):     if name == ""mime-type"":         if self.type:             for extension in self.extensions:                 self[extension] = self.type         self.type = attrs[""type""].lower()         self.extensions = []     elif name == ""glob"":         pattern = attrs[""pattern""]         <MASK>             self.extensions.append(pattern[1:].lower())","if pattern . startswith ( ""*."" ) :",if pattern :,4.377183140747567,0.0,0.0
"def get_attr_by_data_model(self, dmodel, exclude_record=False):     if exclude_record:         return list(             filter(                 lambda x: x.data_model == dmodel and x.value == """"                 <MASK>                 else False,                 self._inferred_intent,             )         )     else:         return list(             filter(                 lambda x: x.data_model == dmodel and x.value == """"                 if hasattr(x, ""data_model"")                 else False,                 self._inferred_intent,             )         )","if x . attribute != ""Record"" and hasattr ( x , ""data_model"" )","if hasattr ( x , ""data_model"" ) and hasattr ( x , ""data_model"" ) :",47.479483628535654,47.479483628535654,0.0
"def general(metadata, value):     if metadata.get(""commands"") and value:         if not metadata.get(""nargs""):             v = quote(value)         else:             v = value         return u""{0} {1}"".format(metadata[""commands""][0], v)     else:         if not value:             return None         <MASK>             return quote(value)         else:             return value","elif not metadata . get ( ""nargs"" ) :","if not metadata . get ( ""nargs"" ) :",89.31539818068698,89.31539818068698,0.0
"def get_images(self):     images = []     try:         tag = MP4(self[""~filename""])     except Exception:         return []     for cover in tag.get(""covr"", []):         <MASK>             mime = ""image/jpeg""         elif cover.imageformat == MP4Cover.FORMAT_PNG:             mime = ""image/png""         else:             mime = ""image/""         f = get_temp_cover_file(cover)         images.append(EmbeddedImage(f, mime))     return images",if cover . imageformat == MP4Cover . FORMAT_JPEG :,if cover . imageformat == MP4Cover . FORMAT_JPEG :,100.00000000000004,100.00000000000004,1.0
"def run_cmd(self, util, value):     state = util.state     if not state.argument_supplied:         state.argument_supplied = True         if value == ""by_four"":             state.argument_value = 4         <MASK>             state.argument_negative = True         else:             state.argument_value = value     elif value == ""by_four"":         state.argument_value *= 4     elif isinstance(value, int):         state.argument_value *= 10         state.argument_value += value     elif value == ""negative"":         state.argument_value = -state.argument_value","elif value == ""negative"" :",if state . argument_negative :,6.770186228657864,6.770186228657864,0.0
"def finish_character_data(self):     if self.character_data:         <MASK>             line, column = self.character_pos             token = XmlToken(                 XML_CHARACTER_DATA, self.character_data, None, line, column             )             self.tokens.append(token)         self.character_data = """"",if not self . skip_ws or not self . character_data . isspace ( ) :,if self . character_pos :,8.194094675927117,8.194094675927117,0.0
"def check_syntax(filename, raise_error=False):     """"""Return True if syntax is okay.""""""     with autopep8.open_with_encoding(filename) as input_file:         try:             compile(input_file.read(), ""<string>"", ""exec"", dont_inherit=True)             return True         except (SyntaxError, TypeError, UnicodeDecodeError):             <MASK>                 raise             else:                 return False",if raise_error :,if raise_error :,100.00000000000004,100.00000000000004,1.0
"def write(self, file):     if not self._been_written:         self._been_written = True         for attribute, value in self.__dict__.items():             <MASK>                 self.write_recursive(value, file)         w = file.write         w(""\t%s = {\n"" % self._id)         w(""\t\tisa = %s;\n"" % self.__class__.__name__)         for attribute, value in self.__dict__.items():             if attribute[0] != ""_"":                 w(""\t\t%s = %s;\n"" % (attribute, self.tostring(value)))         w(""\t};\n\n"")","if attribute [ 0 ] != ""_"" :","if attribute [ 0 ] != ""_id"" :",76.91605673134588,76.91605673134588,0.0
"def update_service_key(kid, name=None, metadata=None):     try:         with db_transaction():             key = db_for_update(ServiceKey.select().where(ServiceKey.kid == kid)).get()             if name is not None:                 key.name = name             <MASK>                 key.metadata.update(metadata)             key.save()     except ServiceKey.DoesNotExist:         raise ServiceKeyDoesNotExist",if metadata is not None :,if metadata is not None :,100.00000000000004,100.00000000000004,1.0
"def fill_buf(self, db, len_=None):     with open(""/dev/urandom"", ""rb"") as rfh:         first = True         for (id_,) in db.query(""SELECT id FROM test""):             if len_ is None and first:                 val = b""""  # We always want to check this case                 first = False             <MASK>                 val = rfh.read(random.randint(0, 140))             else:                 val = rfh.read(len_)             db.execute(""UPDATE test SET buf=? WHERE id=?"", (val, id_))",elif len_ is None :,if len_ == 0 :,14.535768424205482,14.535768424205482,0.0
"def load_category_from_parser(self, parser):     for cate in parser.keys():         id = parser.get_id(cate)         <MASK>             self._data[""cates""][id] = 0         else:             self._data[""cates""][id] = self.count_unread(id)     self._is_init = False     self.save()",if self . _is_init :,if id == self . count_read ( id ) :,7.768562846380176,7.768562846380176,0.0
"def after_insert(self):     if self.prescription:         frappe.db.set_value(             ""Lab Prescription"", self.prescription, ""lab_test_created"", 1         )         <MASK>             self.invoiced = True     if not self.lab_test_name and self.template:         self.load_test_from_template()         self.reload()","if frappe . db . get_value ( ""Lab Prescription"" , self . prescription , ""invoiced"" ) :",if self . invoiced :,0.734031264183814,0.734031264183814,0.0
"def sync_terminology(self):     if self.is_source:         return     store = self.store     missing = []     for source in self.component.get_all_sources():         <MASK>             continue         try:             _unit, add = store.find_unit(source.context, source.source)         except UnitNotFound:             add = True         # Unit is already present         if not add:             continue         missing.append((source.context, source.source, """"))     if missing:         self.add_units(None, missing)","if ""terminology"" not in source . all_flags :",if not source . context :,7.509307647752128,7.509307647752128,0.0
def refresh(self):     if self._obj:         base = self._db.get_media_from_handle(self._obj.get_reference_handle())         <MASK>             self._title = base.get_description()             self._value = base.get_path(),if base :,if base :,100.00000000000004,0.0,1.0
"def _set_parse_context(self, tag, tag_attrs):     # special case: script or style parse context     if not self._wb_parse_context:         if tag == ""style"":             self._wb_parse_context = ""style""         <MASK>             if self._allow_js_type(tag_attrs):                 self._wb_parse_context = ""script""","elif tag == ""script"" :","if tag == ""script"" :",84.08964152537145,84.08964152537145,0.0
"def can_read(self):     if hasattr(self.file, ""__iter__""):         iterator = iter(self.file)         head = next(iterator, None)         if head is None:             self.repaired = []             return True         <MASK>             self.repaired = itertools.chain([head], iterator)             return True         else:             # We may have mangled a generator at this point, so just abort             raise IOSourceError(                 ""Could not open source: %r (mode: %r)""                 % (self.file, self.options[""mode""])             )     return False","if isinstance ( head , str ) :",if head . next ( ) is not None :,6.27465531099474,6.27465531099474,0.0
"def wrapped_request_method(*args, **kwargs):     """"""Modifies HTTP headers to include a specified user-agent.""""""     if kwargs.get(""headers"") is not None:         if kwargs[""headers""].get(""user-agent""):             <MASK>                 # Save the existing user-agent header and tack on our own.                 kwargs[""headers""][""user-agent""] = (                     f""{user_agent} "" f'{kwargs[""headers""][""user-agent""]}'                 )         else:             kwargs[""headers""][""user-agent""] = user_agent     else:         kwargs[""headers""] = {""user-agent"": user_agent}     return request_method(*args, **kwargs)","if user_agent not in kwargs [ ""headers"" ] [ ""user-agent"" ] :","if user_agent in kwargs [ ""headers"" ] :",48.35695637717113,48.35695637717113,0.0
"def execute(self):     if self._dirty or not self._qr:         model_class = self.model_class         query_meta = self.get_query_meta()         if self._tuples:             ResultWrapper = TuplesQueryResultWrapper         elif self._dicts:             ResultWrapper = DictQueryResultWrapper         <MASK>             ResultWrapper = NaiveQueryResultWrapper         elif self._aggregate_rows:             ResultWrapper = AggregateQueryResultWrapper         else:             ResultWrapper = ModelQueryResultWrapper         self._qr = ResultWrapper(model_class, self._execute(), query_meta)         self._dirty = False         return self._qr     else:         return self._qr",elif self . _naive or not self . _joins or self . verify_naive ( ) :,if self . _naive_rows :,8.53702556386813,8.53702556386813,0.0
"def populate_data(apps, schema_editor):     Menu = apps.get_model(""menu"", ""Menu"")     for menu in Menu.objects.all():         <MASK>             json_str = menu.json_content             while isinstance(json_str, str):                 json_str = json.loads(json_str)             menu.json_content_new = json_str             menu.save()","if isinstance ( menu . json_content , str ) :",if menu . json_content_new is not None :,34.48444257953326,34.48444257953326,0.0
"def virtualenv_exists(self):     if os.path.exists(self.virtualenv_location):         <MASK>             extra = [""Scripts"", ""activate.bat""]         else:             extra = [""bin"", ""activate""]         return os.path.isfile(os.sep.join([self.virtualenv_location] + extra))     return False","if os . name == ""nt"" :",if self . is_script :,5.630400552901077,5.630400552901077,0.0
"def get_minkowski_function(name, variable):     fn_name = name + get_postfix(variable)     if hasattr(MEB, fn_name):         return getattr(MEB, fn_name)     else:         <MASK>             raise ValueError(                 f""Function {fn_name} not available. Please compile MinkowskiEngine with `torch.cuda.is_available()` is `True`.""             )         else:             raise ValueError(f""Function {fn_name} not available."")",if variable . is_cuda :,if not self . is_available ( ) :,17.747405280050266,17.747405280050266,0.0
"def build_temp_workspace(files):     tempdir = tempfile.mkdtemp(prefix=""yamllint-tests-"")     for path, content in files.items():         path = os.path.join(tempdir, path).encode(""utf-8"")         <MASK>             os.makedirs(os.path.dirname(path))         if type(content) is list:             os.mkdir(path)         else:             mode = ""wb"" if isinstance(content, bytes) else ""w""             with open(path, mode) as f:                 f.write(content)     return tempdir",if not os . path . exists ( os . path . dirname ( path ) ) :,if not os . path . isdir ( path ) :,33.01754666797577,33.01754666797577,0.0
"def clean_form(self, request, user, form, cleaned_data):     for field in self.get_fields():         <MASK>             continue         try:             cleaned_data[field.fieldname] = field.clean(                 request, user, cleaned_data[field.fieldname]             )         except ValidationError as e:             form.add_error(field.fieldname, e)     return cleaned_data",if field . fieldname not in cleaned_data :,if not field . is_valid ( ) :,11.044795567078939,11.044795567078939,0.0
"def setUp(self):     self.realm = service.InMemoryWordsRealm(""realmname"")     self.checker = checkers.InMemoryUsernamePasswordDatabaseDontUse()     self.portal = portal.Portal(self.realm, [self.checker])     self.factory = service.IRCFactory(self.realm, self.portal)     c = []     for nick in self.STATIC_USERS:         <MASK>             nick = nick.decode(""utf-8"")         c.append(self.realm.createUser(nick))         self.checker.addUser(nick, nick + ""_password"")     return DeferredList(c)","if isinstance ( nick , bytes ) :","if nick . startswith ( ""password"" ) :",10.552670315936318,10.552670315936318,0.0
"def __call__(self, message):     with self._lock:         self._pending_ack += 1         self.max_pending_ack = max(self.max_pending_ack, self._pending_ack)         self.seen_message_ids.append(int(message.attributes[""seq_num""]))     time.sleep(self._processing_time)     with self._lock:         self._pending_ack -= 1         message.ack()         self.completed_calls += 1         <MASK>             if not self.done_future.done():                 self.done_future.set_result(None)",if self . completed_calls >= self . _resolve_at_msg_count :,if self . done_future :,5.0022783410134535,5.0022783410134535,0.0
"def fill_in_standard_formats(book):     for x in std_format_code_types.keys():         <MASK>             ty = std_format_code_types[x]             # Note: many standard format codes (mostly CJK date formats) have             # format strings that vary by locale; xlrd does not (yet)             # handle those; the type (date or numeric) is recorded but the fmt_str will be None.             fmt_str = std_format_strings.get(x)             fmtobj = Format(x, ty, fmt_str)             book.format_map[x] = fmtobj",if x not in book . format_map :,if x in book . format_map :,71.89393375176813,71.89393375176813,0.0
"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None):     result = []     for i in range(10):         # This line introduces a bug.         if bigger_than_3_only and less_than_7_only and i == 4:             continue         if bigger_than_3_only and i <= 3:             continue         <MASK>             continue         if even_only and i % 2 != 0:             continue         result.append(i)     return result",if less_than_7_only and i >= 7 :,if less_than_7_only and i > 7 :,81.49492131269727,81.49492131269727,0.0
"def next_instruction_is_function_or_class(lines):     """"""Is the first non-empty, non-commented line of the cell either a function or a class?""""""     parser = StringParser(""python"")     for i, line in enumerate(lines):         <MASK>             parser.read_line(line)             continue         parser.read_line(line)         if not line.strip():  # empty line             if i > 0 and not lines[i - 1].strip():                 return False             continue         if line.startswith(""def "") or line.startswith(""class ""):             return True         if line.startswith((""#"", ""@"", "" "", "")"")):             continue         return False     return False",if parser . is_quoted ( ) :,"if line . startswith ( ""function "" ) :",10.552670315936318,10.552670315936318,0.0
"def __getattr__(self, key):     for tag in self.tag.children:         if tag.name not in (""input"",):             continue         <MASK>             from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation             return DOMImplementation.createHTMLElement(self.doc, tag)     raise AttributeError","if ""name"" in tag . attrs and tag . attrs [ ""name"" ] in ( key , ) :",if key in self . tags :,0.9987449228520247,0.9987449228520247,0.0
"def process_signature(app, what, name, obj, options, signature, return_annotation):     if signature:         # replace Mock function names         signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature)         signature = re.sub(""tensorflow"", ""tf"", signature)         # add scope name to layer signatures:         <MASK>             if obj.use_scope:                 signature = signature[0] + ""variable_scope_name, "" + signature[1:]             elif obj.use_scope is None:                 signature = signature[0] + ""[variable_scope_name,] "" + signature[1:]     # signature: arg list     return signature, return_annotation","if hasattr ( obj , ""use_scope"" ) :",if obj . use_scope is not None :,15.20797122409784,15.20797122409784,0.0
"def countbox(self):     self.box = [1000, 1000, -1000, -1000]     for x, y in self.body:         if x < self.box[0]:             self.box[0] = x         if x > self.box[2]:             self.box[2] = x         if y < self.box[1]:             self.box[1] = y         <MASK>             self.box[3] = y",if y > self . box [ 3 ] :,if y > self . box [ 3 ] :,100.00000000000004,100.00000000000004,1.0
"def find_shell():     global DEFAULT_SHELL     if not DEFAULT_SHELL:         for shell in propose_shell():             <MASK>                 DEFAULT_SHELL = shell                 break     if not DEFAULT_SHELL:         DEFAULT_SHELL = ""/bin/sh""     return DEFAULT_SHELL","if os . path . isfile ( shell ) and os . access ( shell , os . X_OK ) :","if shell . strip ( ) == ""bash"" :",2.119265820372585,2.119265820372585,0.0
"def addAggregators(sheet, cols, aggrnames):     ""Add each aggregator in list of *aggrnames* to each of *cols*.""     for aggrname in aggrnames:         aggrs = vd.aggregators.get(aggrname)         aggrs = aggrs if isinstance(aggrs, list) else [aggrs]         for aggr in aggrs:             for c in cols:                 if not hasattr(c, ""aggregators""):                     c.aggregators = []                 <MASK>                     c.aggregators += [aggr]",if aggr and aggr not in c . aggregators :,if aggr not in aggrs :,19.505632433269746,19.505632433269746,0.0
"def run(self, paths=[]):     items = []     for item in SideBarSelection(paths).getSelectedItems():         items.append(item.pathAbsoluteFromProjectEncoded())     if len(items) > 0:         sublime.set_clipboard(""\n"".join(items))         <MASK>             sublime.status_message(""Items copied"")         else:             sublime.status_message(""Item copied"")",if len ( items ) > 1 :,if len ( items ) > 0 :,70.71067811865478,70.71067811865478,0.0
"def social_user(backend, uid, user=None, *args, **kwargs):     provider = backend.name     social = backend.strategy.storage.user.get_social_auth(provider, uid)     if social:         <MASK>             msg = ""This account is already in use.""             raise AuthAlreadyAssociated(backend, msg)         elif not user:             user = social.user     return {         ""social"": social,         ""user"": user,         ""is_new"": user is None,         ""new_association"": social is None,     }",if user and social . user != user :,if user is not None :,9.22364410103253,9.22364410103253,0.0
"def _text(bitlist):     out = """"     for typ, text in bitlist:         if not typ:             out += text         elif typ == ""em"":             out += ""\\fI%s\\fR"" % text         <MASK>             out += ""\\fB%s\\fR"" % text         else:             raise ValueError(""unexpected tag %r inside text"" % (typ,))     out = out.strip()     out = re.sub(re.compile(r""^\s+"", re.M), """", out)     return out","elif typ in [ ""strong"" , ""code"" ] :","if typ == ""b"" :",4.18031138310865,4.18031138310865,0.0
"def OnRadioSelect(self, event):     fitID = self.mainFrame.getActiveFit()     if fitID is not None:         self.mainFrame.command.Submit(             cmd.GuiChangeImplantLocationCommand(                 fitID=fitID,                 source=ImplantLocation.FIT                 <MASK>                 else ImplantLocation.CHARACTER,             )         )",if self . rbFit . GetValue ( ),if fitID != self . mainFrame . activeFitID :,9.980099403873663,9.980099403873663,0.0
"def hexdump(data):     """"""yield lines with hexdump of data""""""     values = []     ascii = []     offset = 0     for h, a in sixteen(data):         <MASK>             yield (offset, "" "".join(["""".join(values), """".join(ascii)]))             del values[:]             del ascii[:]             offset += 0x10         else:             values.append(h)             ascii.append(a)",if h is None :,if h == 0x10 :,17.965205598154213,17.965205598154213,0.0
"def submit(self):     bot_token = self.config[""bot_token""]     chat_ids = self.config[""chat_id""]     chat_ids = [chat_ids] if isinstance(chat_ids, str) else chat_ids     text = ""\n"".join(super().submit())     if not text:         logger.debug(""Not calling telegram API (no changes)"")         return     result = None     for chunk in chunkstring(text, self.MAX_LENGTH, numbering=True):         for chat_id in chat_ids:             res = self.submitToTelegram(bot_token, chat_id, chunk)             <MASK>                 result = res     return result",if res . status_code != requests . codes . ok or res is None :,if res :,0.42446406286118865,0.0,0.0
"def onMessage(self, payload, isBinary):     if not isBinary:         self.result = ""Expected binary message with payload, but got binary.""     else:         <MASK>             self.result = (                 ""Expected binary message with payload of length %d, but got %d.""                 % (self.DATALEN, len(payload))             )         else:             ## FIXME : check actual content             ##             self.behavior = Case.OK             self.result = ""Received binary message of length %d."" % len(payload)     self.p.createWirelog = True     self.p.sendClose(self.p.CLOSE_STATUS_CODE_NORMAL)",if len ( payload ) != self . DATALEN :,if self . DATALEN :,21.297646969725616,21.297646969725616,0.0
"def verify_output(actual, expected):     actual = _read_file(actual, ""Actual"")     expected = _read_file(join(CURDIR, expected), ""Expected"")     if len(expected) != len(actual):         raise AssertionError(             ""Lengths differ. Expected %d lines but got %d""             % (len(expected), len(actual))         )     for exp, act in zip(expected, actual):         tester = fnmatchcase if ""*"" in exp else eq         <MASK>             raise AssertionError(                 ""Lines differ.\nExpected: %s\nActual:   %s"" % (exp, act)             )","if not tester ( act . rstrip ( ) , exp . rstrip ( ) ) :",if not tester :,2.4774512716690604,2.4774512716690604,0.0
"def _in_out_vector_helper(self, name1, name2, ceil):     vector = []     stats = self.record     if ceil is None:         ceil = self._get_max_rate(name1, name2)     maxlen = self.config.get_stats_history_length()     for n in [name1, name2]:         for i in range(maxlen + 1):             <MASK>                 vector.append(float(stats[i][n]) / ceil)             else:                 vector.append(0.0)     return vector",if i < len ( stats ) :,if stats [ i ] [ n ] > ceil :,5.300156689756295,5.300156689756295,0.0
"def _init_param(param, mode):     if isinstance(param, str):         param = _resolve(param)     elif isinstance(param, (list, tuple)):         param = [_init_param(p, mode) for p in param]     elif isinstance(param, dict):         <MASK>             param = from_params(param, mode=mode)         else:             param = {k: _init_param(v, mode) for k, v in param.items()}     return param","if { ""ref"" , ""class_name"" , ""config_path"" } . intersection ( param . keys ( ) ) :","if isinstance ( param , ( list , tuple ) ) :",4.839717618719305,4.839717618719305,0.0
"def link_pantsrefs(soups, precomputed):     """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">""""""     for (page, soup) in soups.items():         for a in soup.find_all(""a""):             if not a.has_attr(""pantsref""):                 continue             pantsref = a[""pantsref""]             <MASK>                 raise TaskError(                     f'Page {page} has pantsref ""{pantsref}"" and I cannot find pantsmark for it'                 )             a[""href""] = rel_href(page, precomputed.pantsref[pantsref])",if pantsref not in precomputed . pantsref :,if pantsref not in precomputed . pantsref :,100.00000000000004,100.00000000000004,1.0
"def _gridconvvalue(self, value):     if isinstance(value, (str, _tkinter.Tcl_Obj)):         try:             svalue = str(value)             if not svalue:                 return None             <MASK>                 return getdouble(svalue)             else:                 return getint(svalue)         except ValueError:             pass     return value","elif ""."" in svalue :","if svalue == ""double"" :",7.809849842300637,7.809849842300637,0.0
"def default(self, o):     try:         <MASK>             return str(o)         else:             # remove unwanted attributes from the provider object during conversion to json             if hasattr(o, ""profile""):                 del o.profile             if hasattr(o, ""credentials""):                 del o.credentials             if hasattr(o, ""metadata_path""):                 del o.metadata_path             if hasattr(o, ""services_config""):                 del o.services_config             return vars(o)     except Exception as e:         return str(o)",if type ( o ) == datetime . datetime :,if o . __class__ is not None :,4.789232204309912,4.789232204309912,0.0
"def transform_kwarg(self, name, value, split_single_char_options):     if len(name) == 1:         <MASK>             return [""-%s"" % name]         elif value not in (False, None):             if split_single_char_options:                 return [""-%s"" % name, ""%s"" % value]             else:                 return [""-%s%s"" % (name, value)]     else:         if value is True:             return [""--%s"" % dashify(name)]         elif value is not False and value is not None:             return [""--%s=%s"" % (dashify(name), value)]     return []",if value is True :,"if value in ( True , None ) :",11.339582221952005,11.339582221952005,0.0
"def handle(self, context, sign, *args):     if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP):         return Infsign[sign]     if sign == 0:         <MASK>             return Infsign[sign]         return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))     if sign == 1:         if context.rounding == ROUND_FLOOR:             return Infsign[sign]         return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))",if context . rounding == ROUND_CEILING :,if context . rounding == ROUND_FLOOR :,78.25422900366438,78.25422900366438,0.0
"def OnLeftUp(self, event):     # Stop Drawing     if self.Drawing:         self.Drawing = False         <MASK>             world_rect = (                 self.Canvas.PixelToWorld(self.RBRect[0]),                 self.Canvas.ScalePixelToWorld(self.RBRect[1]),             )             wx.CallAfter(self.CallBack, world_rect)     self.RBRect = None",if self . RBRect :,if self . Canvas :,42.72870063962342,42.72870063962342,0.0
"def _map_answers(answers):     result = []     for a in answers.split(""|""):         user_answers = []         result.append(dict(sourcerAnswers=user_answers))         for r in a.split("",""):             <MASK>                 user_answers.append(dict(noAnswer=True))             else:                 start_, end_ = map(int, r.split("":""))                 user_answers.append(dict(s=start_, e=end_))     return result","if r == ""None"" :","if r == ""yesAnswer"" :",59.4603557501361,59.4603557501361,0.0
"def parse_edges(self, pcb):     edges = []     drawings = list(pcb.GetDrawings())     bbox = None     for m in pcb.GetModules():         for g in m.GraphicalItems():             drawings.append(g)     for d in drawings:         <MASK>             parsed_drawing = self.parse_drawing(d)             if parsed_drawing:                 edges.append(parsed_drawing)                 if bbox is None:                     bbox = d.GetBoundingBox()                 else:                     bbox.Merge(d.GetBoundingBox())     if bbox:         bbox.Normalize()     return edges, bbox",if d . GetLayer ( ) == pcbnew . Edge_Cuts :,if self . is_drawing :,3.4166909782241994,3.4166909782241994,0.0
"def get_size(self):     size = self.start_size     for operation in self.ran_operations:         <MASK>             size = operation[1][0]         elif operation[0] == ""crop"":             crop = operation[1][0]             size = crop[2] - crop[0], crop[3] - crop[1]     return size","if operation [ 0 ] == ""resize"" :","if operation [ 0 ] == ""crop"" :",74.19446627365011,74.19446627365011,0.0
"def migrate_account_metadata(account_id):     from inbox.models.session import session_scope     from inbox.models import Account     with session_scope(versioned=False) as db_session:         account = db_session.query(Account).get(account_id)         <MASK>             create_categories_for_easfoldersyncstatuses(account, db_session)         else:             create_categories_for_folders(account, db_session)         if account.discriminator == ""gmailaccount"":             set_labels_for_imapuids(account, db_session)         db_session.commit()","if account . discriminator == ""easaccount"" :","if account . discriminator == ""easfoldersyncstatuses"" :",70.71067811865478,70.71067811865478,0.0
"def OnEndDrag(self, event):     self.StopDragging()     dropTarget = event.GetItem()     if not dropTarget:         dropTarget = self.GetRootItem()     if self.IsValidDropTarget(dropTarget):         self.UnselectAll()         <MASK>             self.SelectItem(dropTarget)         self.OnDrop(dropTarget, self._dragItem)",if dropTarget != self . GetRootItem ( ) :,if self . IsValidDropTarget ( dropTarget ) :,13.991316187881289,13.991316187881289,0.0
"def validate(self, frame, value):     if self.sep and isinstance(value, string_types):         value = value.split(self.sep)     if isinstance(value, list):         <MASK>             return [self.specs[0].validate(frame, v) for v in value]         else:             return [                 [s.validate(frame, v) for (v, s) in izip(val, self.specs)]                 for val in value             ]     raise ValueError(""Invalid MultiSpec data: %r"" % value)",if len ( self . specs ) == 1 :,if len ( value ) == 1 :,47.79995354275012,47.79995354275012,0.0
"def __init__(self, action_space=None, network=None, network_kwargs=None, hparams=None):     QNetBase.__init__(self, hparams=hparams)     with tf.variable_scope(self.variable_scope):         <MASK>             action_space = Space(low=0, high=self._hparams.action_space, dtype=np.int32)         self._action_space = action_space         self._append_output_layer()",if action_space is None :,if action_space is None :,100.00000000000004,100.00000000000004,1.0
"def n_weights(self):     """"""Return the number of weights (parameters) in this network.""""""     n_weights = 0     for i, w in enumerate(self.all_weights):         n = 1         # for s in p.eval().shape:         for s in w.get_shape():             try:                 s = int(s)             except:                 s = 1             <MASK>                 n = n * s         n_weights = n_weights + n     # print(""num of weights (parameters) %d"" % n_weights)     return n_weights",if s :,if i == 0 :,9.652434877402245,9.652434877402245,0.0
"def _arg_desc(name, ctx):     for param in ctx.command.params:         if param.name == name:             desc = param.opts[-1]             <MASK>                 desc = param.human_readable_name             return desc     raise AssertionError(name)","if desc [ 0 ] != ""-"" :",if desc is None :,7.121297464907233,7.121297464907233,0.0
"def walk(directory, path_so_far):     for name in sorted(os.listdir(directory)):         if any(fnmatch(name, pattern) for pattern in basename_ignore):             continue         path = path_so_far + ""/"" + name if path_so_far else name         if any(fnmatch(path, pattern) for pattern in path_ignore):             continue         full_name = os.path.join(directory, name)         if os.path.isdir(full_name):             for file_path in walk(full_name, path):                 yield file_path         <MASK>             yield path",elif os . path . isfile ( full_name ) :,if os . path . isdir ( path ) :,26.581560693718632,26.581560693718632,0.0
"def cache_dst(self):     final_dst = None     final_linenb = None     for linenb, assignblk in enumerate(self):         for dst, src in viewitems(assignblk):             if dst.is_id(""IRDst""):                 <MASK>                     raise ValueError(""Multiple destinations!"")                 final_dst = src                 final_linenb = linenb     self._dst = final_dst     self._dst_linenb = final_linenb     return final_dst",if final_dst is not None :,if linenb == linenb :,6.916271812933183,6.916271812933183,0.0
"def run(self, args, **kwargs):     if args.resource_ref or args.policy_type:         filters = {}         <MASK>             filters[""resource_ref""] = args.resource_ref         if args.policy_type:             filters[""policy_type""] = args.policy_type         filters.update(**kwargs)         return self.manager.query(**filters)     else:         return self.manager.get_all(**kwargs)",if args . resource_ref :,if args . resource_ref :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, folders):     self.folders = folders     self.duplicates = {}     for folder, path in folders.items():         duplicates = []         for other_folder, other_path in folders.items():             if other_folder == folder:                 continue             if other_path == path:                 duplicates.append(other_folder)         <MASK>             self.duplicates[folder] = duplicates",if len ( duplicates ) :,if folder in duplicates :,11.51015341649912,11.51015341649912,0.0
"def limit_clause(self, select, **kw):     text = """"     if select._limit_clause is not None:         text += ""\n LIMIT "" + self.process(select._limit_clause, **kw)     if select._offset_clause is not None:         <MASK>             text += ""\n LIMIT "" + self.process(sql.literal(-1))         text += "" OFFSET "" + self.process(select._offset_clause, **kw)     else:         text += "" OFFSET "" + self.process(sql.literal(0), **kw)     return text",if select . _limit_clause is None :,if select . _offset_clause is not None :,37.81790427652475,37.81790427652475,0.0
"def _get_activation(self, act):     """"""Get activation block based on the name.""""""     if isinstance(act, str):         if act.lower() == ""gelu"":             return GELU()         <MASK>             return GELU(approximate=True)         else:             return gluon.nn.Activation(act)     assert isinstance(act, gluon.Block)     return act","elif act . lower ( ) == ""approx_gelu"" :","if act . lower ( ) == ""gelu"" :",62.20700406786387,62.20700406786387,0.0
"def __eq__(self, other):     try:         if self.type != other.type:             return False         if self.type == ""ASK"":             return self.askAnswer == other.askAnswer         <MASK>             return self.vars == other.vars and self.bindings == other.bindings         else:             return self.graph == other.graph     except:         return False","elif self . type == ""SELECT"" :","if self . type == ""VARIABLE"" :",58.14307369682194,58.14307369682194,0.0
"def _get_text_nodes(nodes, html_body):     text = []     open_tags = 0     for node in nodes:         if isinstance(node, HtmlTag):             <MASK>                 open_tags += 1             elif node.tag_type == CLOSE_TAG:                 open_tags -= 1         elif (             isinstance(node, HtmlDataFragment)             and node.is_text_content             and open_tags == 0         ):             text.append(html_body[node.start : node.end])     return text",if node . tag_type == OPEN_TAG :,if node . tag_type == OPEN_TAG :,100.00000000000004,100.00000000000004,1.0
"def test_do_change(self):     """"""Test if VTK object changes when trait is changed.""""""     p = Prop()     p.edge_visibility = not p.edge_visibility     p.representation = ""p""     p.opacity = 0.5     p.color = (0, 1, 0)     p.diffuse_color = (1, 1, 1)     p.specular_color = (1, 1, 0)     for t, g in p._updateable_traits_:         val = getattr(p._vtk_obj, g)()         <MASK>             self.assertEqual(val, getattr(p, t + ""_""))         else:             self.assertEqual(val, getattr(p, t))","if t == ""representation"" :","if t + ""_"" in val :",11.99014838091355,11.99014838091355,0.0
"def update_item(source_doc, target_doc, source_parent):     target_doc.t_warehouse = """"     if source_doc.material_request_item and source_doc.material_request:         add_to_transit = frappe.db.get_value(             ""Stock Entry"", source_name, ""add_to_transit""         )         <MASK>             warehouse = frappe.get_value(                 ""Material Request Item"", source_doc.material_request_item, ""warehouse""             )             target_doc.t_warehouse = warehouse     target_doc.s_warehouse = source_doc.t_warehouse     target_doc.qty = source_doc.qty - source_doc.transferred_qty",if add_to_transit :,if add_to_transit :,100.00000000000004,100.00000000000004,1.0
"def get_drive(self, root_path="""", volume_guid_path=""""):     for drive in self.drives:         <MASK>             config_root_path = drive.get(""root_path"")             if config_root_path and root_path == config_root_path:                 return drive         elif volume_guid_path:             config_volume_guid_path = drive.get(""volume_guid_path"")             if config_volume_guid_path and config_volume_guid_path == volume_guid_path:                 return drive",if root_path :,if root_path :,100.00000000000004,100.00000000000004,1.0
"def f_freeze(_):     repos = utils.get_repos()     for name, path in repos.items():         url = """"         cp = subprocess.run([""git"", ""remote"", ""-v""], cwd=path, capture_output=True)         <MASK>             url = cp.stdout.decode(""utf-8"").split(""\n"")[0].split()[1]         print(f""{url},{name},{path}"")",if cp . returncode == 0 :,if cp . returncode == 0 :,100.00000000000004,100.00000000000004,1.0
"def conj(self):     dtype = self.dtype     if issubclass(self.dtype.type, np.complexfloating):         <MASK>             raise RuntimeError(                 ""only contiguous arrays may "" ""be used as arguments to this operation""             )         if self.flags.f_contiguous:             order = ""F""         else:             order = ""C""         result = self._new_like_me(order=order)         func = elementwise.get_conj_kernel(dtype)         func.prepared_async_call(             self._grid, self._block, None, self.gpudata, result.gpudata, self.mem_size         )         return result     else:         return self",if not self . flags . forc :,if self . flags . contiguous :,39.44243648327556,39.44243648327556,0.0
"def detect_reentrancy(self, contract):     for function in contract.functions_and_modifiers_declared:         if function.is_implemented:             <MASK>                 continue             self._explore(function.entry_point, [])             function.context[self.KEY] = True",if self . KEY in function . context :,if self . key not in self . context :,27.301208627090666,27.301208627090666,0.0
"def test_default_configuration_no_encoding(self):     transformations = []     for i in range(2):         transformation, original = _test_preprocessing(NoEncoding)         self.assertEqual(transformation.shape, original.shape)         self.assertTrue((transformation == original).all())         transformations.append(transformation)         <MASK>             self.assertTrue((transformations[-1] == transformations[-2]).all())",if len ( transformations ) > 1 :,if transformations [ i ] == 1 :,11.339582221952005,11.339582221952005,0.0
"def main():     """"""main function""""""     # todo: lookuo real description     parser = argparse.ArgumentParser(description=""Let a cow speak for you"")     parser.add_argument(""text"", nargs=""*"", default=None, help=""text to say"")     ns = parser.parse_args()     if (ns.text is None) or (len(ns.text) == 0):         text = """"         while True:             inp = sys.stdin.read(4096)             if inp.endswith(""\n""):                 inp = inp[:-1]             <MASK>                 break             text += inp     else:         text = "" "".join(ns.text)     cow = get_cow(text)     print(cow)",if not inp :,if not inp :,100.00000000000004,100.00000000000004,1.0
"def prehook(self, emu, op, eip):     if op in self.badops:         emu.stopEmu()         raise v_exc.BadOpBytes(op.va)     if op.mnem in STOS:         if self.arch == ""i386"":             reg = emu.getRegister(envi.archs.i386.REG_EDI)         elif self.arch == ""amd64"":             reg = emu.getRegister(envi.archs.amd64.REG_RDI)         <MASK>             self.vw.makePointer(reg, follow=True)",if self . vw . isValidPointer ( reg ) and self . vw . getLocation ( reg ) is None :,if op . va :,0.5730567950718444,0.5730567950718444,0.0
"def get_boarding_status(project):     status = ""Pending""     if project:         doc = frappe.get_doc(""Project"", project)         if flt(doc.percent_complete) > 0.0 and flt(doc.percent_complete) < 100.0:             status = ""In Process""         <MASK>             status = ""Completed""         return status",elif flt ( doc . percent_complete ) == 100.0 :,"if doc . status == ""Pending"" :",9.29675796576443,9.29675796576443,0.0
"def set_weights(self, new_weights):     weights = self.get_weights()     if len(weights) != len(new_weights):         raise ValueError(""len of lists mismatch"")     tuples = []     for w, new_w in zip(weights, new_weights):         <MASK>             new_w = new_w.reshape(w.shape)         tuples.append((w, new_w))     nn.batch_set_value(tuples)",if len ( w . shape ) != new_w . shape :,"if isinstance ( new_w , nn . Batch ) :",12.545696183524145,12.545696183524145,0.0
"def reload_json_api_settings(*args, **kwargs):     django_setting = kwargs[""setting""]     setting = django_setting.replace(JSON_API_SETTINGS_PREFIX, """")     value = kwargs[""value""]     if setting in DEFAULTS.keys():         <MASK>             setattr(json_api_settings, setting, value)         elif hasattr(json_api_settings, setting):             delattr(json_api_settings, setting)",if value is not None :,if value :,23.174952587773145,0.0,0.0
"def knamn(self, sup, cdict):     cname = cdict[sup].class_name     if not cname:         (namesp, tag) = cdict[sup].name.split(""."")         <MASK>             ctag = self.root.modul[namesp].factory(tag).__class__.__name__             cname = ""%s.%s"" % (namesp, ctag)         else:             cname = tag + ""_""     return cname",if namesp :,if namesp :,100.00000000000004,0.0,1.0
"def setdefault(self, key, default=None):     try:         o = self.data[key]()     except KeyError:         o = None     if o is None:         <MASK>             self._commit_removals()         self.data[key] = KeyedRef(default, self._remove, key)         return default     else:         return o",if self . _pending_removals :,if default is not None :,6.916271812933183,6.916271812933183,0.0
"def __on_item_activated(self, event):     if self.__module_view:         module = self.get_event_module(event)         self.__module_view.set_selection(module.module_num)         <MASK>             self.input_list_ctrl.deactivate_active_item()         else:             self.list_ctrl.deactivate_active_item()             for index in range(self.list_ctrl.GetItemCount()):                 if self.list_ctrl.IsSelected(index):                     self.list_ctrl.Select(index, False)     self.__controller.enable_module_controls_panel_buttons()",if event . EventObject is self . list_ctrl :,if self . input_list_ctrl :,29.53872020786076,29.53872020786076,0.0
"def _create_valid_graph(graph):     nodes = graph.nodes()     for i in range(len(nodes)):         for j in range(len(nodes)):             if i == j:                 continue             edge = (nodes[i], nodes[j])             <MASK>                 graph.del_edge(edge)             graph.add_edge(edge, 1)",if graph . has_edge ( edge ) :,if edge not in graph :,5.893383794376546,5.893383794376546,0.0
"def _parse_param_value(name, datatype, default):     if datatype == ""bool"":         <MASK>             return True         elif default.lower() == ""false"":             return False         else:             _s = ""{}: Invalid default value '{}' for bool parameter {}""             raise SyntaxError(_s.format(self.name, default, p))     elif datatype == ""int"":         if type(default) == int:             return default         else:             return int(default, 0)     elif datatype == ""real"":         if type(default) == float:             return default         else:             return float(default)     else:         return str(default)","if default . lower ( ) == ""true"" :","if default . lower ( ) == ""true"" :",100.00000000000004,100.00000000000004,1.0
"def get_size(self, shape_info):     # The size is the data, that have constant size.     state = np.random.RandomState().get_state()     size = 0     for elem in state:         <MASK>             size += len(elem)         elif isinstance(elem, np.ndarray):             size += elem.size * elem.itemsize         elif isinstance(elem, int):             size += np.dtype(""int"").itemsize         elif isinstance(elem, float):             size += np.dtype(""float"").itemsize         else:             raise NotImplementedError()     return size","if isinstance ( elem , str ) :","if isinstance ( elem , int ) :",59.4603557501361,59.4603557501361,0.0
"def _merge_substs(self, subst, new_substs):     subst = subst.copy()     for new_subst in new_substs:         for name, var in new_subst.items():             if name not in subst:                 subst[name] = var             <MASK>                 subst[name].PasteVariable(var)     return subst",elif subst [ name ] is not var :,if var . is_paste :,6.495032985064742,6.495032985064742,0.0
"def _load_weights_if_possible(self, model, init_weight_path=None):     """"""Loads model weights when it is provided.""""""     if init_weight_path:         logging.info(""Load weights: {}"".format(init_weight_path))         <MASK>             checkpoint = tf.train.Checkpoint(                 model=model, optimizer=self._create_optimizer()             )             checkpoint.restore(init_weight_path)         else:             model.load_weights(init_weight_path)     else:         logging.info(""Weights not loaded from path:{}"".format(init_weight_path))",if self . use_tpu :,if model . is_checkpoint ( ) :,6.742555929751843,6.742555929751843,0.0
"def _cleanup_inactive_receivexlogs(self, site):     if site in self.receivexlogs:         if not self.receivexlogs[site].running:             <MASK>                 self.receivexlogs[site].join()             del self.receivexlogs[site]",if self . receivexlogs [ site ] . is_alive ( ) :,if self . receiptlogs [ site ] . running :,26.331153487510537,26.331153487510537,0.0
"def get_asset(self, path):     """"""Loads an asset by path.""""""     clean_path = cleanup_path(path).strip(""/"")     nodes = [self.asset_root] + self.theme_asset_roots     for node in nodes:         for piece in clean_path.split(""/""):             node = node.get_child(piece)             <MASK>                 break         if node is not None:             return node     return None",if node is None :,if node is None :,100.00000000000004,100.00000000000004,1.0
"def palindromic_substrings(s):     if not s:         return [[]]     results = []     for i in range(len(s), 0, -1):         sub = s[:i]         <MASK>             for rest in palindromic_substrings(s[i:]):                 results.append([sub] + rest)     return results",if sub == sub [ : : - 1 ] :,if sub in results :,5.830425236335824,5.830425236335824,0.0
"def debug_tree(tree):     l = []     for elt in tree:         <MASK>             l.append(_names.get(elt, elt))         elif isinstance(elt, str):             l.append(elt)         else:             l.append(debug_tree(elt))     return l","if isinstance ( elt , ( int , long ) ) :","if isinstance ( elt , ( int , float , float ) ) :",61.62607099729587,61.62607099729587,0.0
"def shared_username(account):     username = os.environ.get(""SHARED_USERNAME"", ""PKKid"")     for user in account.users():         <MASK>             return username         elif (             user.username             and user.email             and user.id             and username.lower()             in (user.username.lower(), user.email.lower(), str(user.id))         ):             return username     pytest.skip(""Shared user %s wasn`t found in your MyPlex account"" % username)",if user . title . lower ( ) == username . lower ( ) :,if user . id :,4.73447498358895,4.73447498358895,0.0
"def process_schema_element(self, e):     if e.name is None:         return     self.debug1(""adding element: %s"", e.name)     t = self.get_type(e.type)     if t:         <MASK>             del self.pending_elements[e.name]         self.retval[self.tns].elements[e.name] = e     else:         self.pending_elements[e.name] = e",if e . name in self . pending_elements :,if t . name in self . pending_elements :,80.70557274927978,80.70557274927978,0.0
"def __setitem__(self, key, value):     with self._lock:         try:             link = self._get_link_and_move_to_front_of_ll(key)         except KeyError:             <MASK>                 self._set_key_and_add_to_front_of_ll(key, value)             else:                 evicted = self._set_key_and_evict_last_in_ll(key, value)                 super(LRI, self).__delitem__(evicted)             super(LRI, self).__setitem__(key, value)         else:             link[VALUE] = value",if len ( self ) < self . max_size :,if link [ KEY ] == value :,4.062582855427254,4.062582855427254,0.0
"def __delattr__(self, name):     if name == ""__dict__"":         raise AttributeError(             ""%r object attribute '__dict__' is read-only"" % self.__class__.__name__         )     if name in self._local_type_vars:         <MASK>             # A data descriptor, like a property or a slot.             type_attr = getattr(self._local_type, name, _marker)             type(type_attr).__delete__(type_attr, self)             return     # Otherwise it goes directly in the dict     # Begin inlined function _get_dict()     dct = _local_get_dict(self)     try:         del dct[name]     except KeyError:         raise AttributeError(name)",if name in self . _local_type_del_descriptors :,"if isinstance ( self . _local_type , type ) :",37.868117902707674,37.868117902707674,0.0
"def update_participants(self, refresh=True):     for participant in list(self.participants_dict):         <MASK>             continue         self.removeItem(self.participants_dict[participant])         self.participant_items.remove(self.participants_dict[participant])         del self.participants_dict[participant]     for participant in self.simulator_config.participants:         if participant in self.participants_dict:             self.participants_dict[participant].refresh()         else:             self.insert_participant(participant)     if refresh:         self.update_view()",if participant is None or participant == self . simulator_config . broadcast_part :,if participant in self . participant_items :,5.705839925298489,5.705839925298489,0.0
"def insert_bigger_b_add(node):     if node.op == theano.tensor.add:         inputs = list(node.inputs)         <MASK>             inputs[-1] = theano.tensor.concatenate((inputs[-1], inputs[-1]))             return [node.op(*inputs)]     return False",if inputs [ - 1 ] . owner is None :,if inputs . count ( ) > 0 :,9.080027618567454,9.080027618567454,0.0
"def _activate_cancel_status(self, cancel_status):     if self._cancel_status is not None:         self._cancel_status._tasks.remove(self)     self._cancel_status = cancel_status     if self._cancel_status is not None:         self._cancel_status._tasks.add(self)         <MASK>             self._attempt_delivery_of_any_pending_cancel()",if self . _cancel_status . effectively_cancelled :,if cancel_status is None :,13.597602315271134,13.597602315271134,0.0
"def writeLibraryGeometry(fp, meshes, config, shapes=None):     progress = Progress(len(meshes), None)     fp.write(""\n  <library_geometries>\n"")     for mIdx, mesh in enumerate(meshes):         <MASK>             shape = None         else:             shape = shapes[mIdx]         writeGeometry(fp, mesh, config, shape)         progress.step()     fp.write(""  </library_geometries>\n"")",if shapes is None :,if shapes is None :,100.00000000000004,100.00000000000004,1.0
"def init_module_config(module_json, config, config_path=default_config_path):     if ""config"" in module_json[""meta""]:         if module_json[""meta""][""config""]:             if module_json[""name""] not in config:                 config.add_section(module_json[""name""])             for config_var in module_json[""meta""][""config""]:                 <MASK>                     config.set(module_json[""name""], config_var, """")     return config","if config_var not in config [ module_json [ ""name"" ] ] :",if config_var in config_path :,14.03153115483935,14.03153115483935,0.0
"def get_const_defines(flags, prefix=""""):     defs = []     for k, v in globals().items():         if isinstance(v, int):             if v & flags:                 <MASK>                     if k.startswith(prefix):                         defs.append(k)                 else:                     defs.append(k)     return defs",if prefix :,if v == 0 :,9.652434877402245,9.652434877402245,0.0
"def __init__(self, source, encoding=DEFAULT_ENCODING):     self.data = {}     with open(source, encoding=encoding) as file_:         for line in file_:             line = line.strip()             <MASK>                 continue             k, v = line.split(""="", 1)             k = k.strip()             v = v.strip()             if len(v) >= 2 and (                 (v[0] == ""'"" and v[-1] == ""'"") or (v[0] == '""' and v[-1] == '""')             ):                 v = v.strip(""'\"""")             self.data[k] = v","if not line or line . startswith ( ""#"" ) or ""="" not in line :",if not line :,1.2951112459987986,1.2951112459987986,0.0
"def __detect_console_logger(self):     logger = self.log     while logger:         for handler in logger.handlers[:]:             <MASK>                 if handler.stream in (sys.stdout, sys.stderr):                     self.logger_handlers.append(handler)         if logger.root == logger:             break         else:             logger = logger.root","if isinstance ( handler , StreamHandler ) :",if handler . stream == sys . stdin :,5.522397783539471,5.522397783539471,0.0
"def check_heuristic_in_sql():     heurs = set()     excluded = [""Equal assembly or pseudo-code"", ""All or most attributes""]     for heur in HEURISTICS:         name = heur[""name""]         <MASK>             continue         sql = heur[""sql""]         if sql.lower().find(name.lower()) == -1:             print((""SQL command not correctly associated to %s"" % repr(name)))             print(sql)             assert sql.find(name) != -1         heurs.add(name)     print(""Heuristics:"")     import pprint     pprint.pprint(heurs)",if name in excluded :,if name in excluded :,100.00000000000004,100.00000000000004,1.0
"def read(self, size=-1):     buf = bytearray()     while size != 0 and self.cursor < self.maxpos:         <MASK>             self.seek_to_block(self.cursor)         part = self.current_stream.read(size)         if size > 0:             if len(part) == 0:                 raise EOFError()             size -= len(part)         self.cursor += len(part)         buf += part     return bytes(buf)",if not self . in_current_block ( self . cursor ) :,if self . current_stream . eof ( ) :,9.987617065555241,9.987617065555241,0.0
"def get_project_dir(env):     project_file = workon_home / env / "".project""     if project_file.exists():         with project_file.open() as f:             project_dir = f.readline().strip()             <MASK>                 return project_dir             else:                 err(                     ""Corrupted or outdated:"",                     project_file,                     ""\nDirectory"",                     project_dir,                     ""doesn't exist."",                 )",if os . path . exists ( project_dir ) :,if project_dir :,11.141275535087015,11.141275535087015,0.0
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None):     """"""cache hidden states into memory.""""""     if mem_len is None or mem_len == 0:         return None     else:         <MASK>             curr_out = curr_out[:reuse_len]         if prev_mem is None:             new_mem = curr_out[-mem_len:]         else:             new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]     return tf.keras.backend.stop_gradient(new_mem)",if reuse_len is not None and reuse_len > 0 :,if reuse_len is not None :,41.06951993704473,41.06951993704473,0.0
"def cleanup_channel(self, to_cleanup):     public_key, id_ = to_cleanup     # TODO: Maybe run it threaded?     try:         with db_session:             channel = self.session.mds.ChannelMetadata.get_for_update(                 public_key=public_key, id_=id_             )             <MASK>                 return             channel.local_version = 0             channel.contents.delete(bulk=True)     except Exception as e:         self._logger.warning(""Exception while cleaning unsubscribed channel: %"", str(e))",if not channel :,if not channel :,100.00000000000004,100.00000000000004,1.0
"def best_image(width, height):     # A heuristic for finding closest sized image to required size.     image = images[0]     for img in images:         if img.width == width and img.height == height:             # Exact match always used             return img         <MASK>             # At least wide enough, and largest area             image = img     return image",elif img . width >= width and img . width * img . height > image . width * image . height :,if img . width > width :,4.011350706220156,4.011350706220156,0.0
"def add_peer_to_blob(self, contact: ""KademliaPeer"", key: bytes) -> None:     now = self.loop.time()     if key in self._data_store:         current = list(filter(lambda x: x[0] == contact, self._data_store[key]))         <MASK>             self._data_store[key][self._data_store[key].index(current[0])] = (                 contact,                 now,             )         else:             self._data_store[key].append((contact, now))     else:         self._data_store[key] = [(contact, now)]",if len ( current ) > 0 :,if current :,7.49553326588684,0.0,0.0
"def dump(self):     self.ql.log.info(""[*] Dumping object: %s"" % (self.sf_name))     for field in self._fields_:         if isinstance(getattr(self, field[0]), POINTER64):             self.ql.log.info(""%s: 0x%x"" % (field[0], getattr(self, field[0]).value))         elif isinstance(getattr(self, field[0]), int):             self.ql.log.info(""%s: %d"" % (field[0], getattr(self, field[0])))         <MASK>             self.ql.log.info(""%s: %s"" % (field[0], getattr(self, field[0]).decode()))","elif isinstance ( getattr ( self , field [ 0 ] ) , bytes ) :","if isinstance ( getattr ( self , field [ 0 ] , bytes ) :",74.8705249982442,74.8705249982442,0.0
"def GeneratePageMetatadata(self, task):     address_space = self.session.GetParameter(""default_address_space"")     for vma in task.mm.mmap.walk_list(""vm_next""):         start = vma.vm_start         end = vma.vm_end         # Skip the entire region.         <MASK>             continue         # Done.         if start > self.plugin_args.end:             break         for vaddr in utils.xrange(start, end, 0x1000):             if self.plugin_args.start <= vaddr <= self.plugin_args.end:                 yield vaddr, self._CreateMetadata(address_space.describe_vtop(vaddr))",if end < self . plugin_args . start :,if start < self . plugin_args . start :,80.70557274927978,80.70557274927978,0.0
"def _available_symbols(self, scoperef, expr):     cplns = []     found_names = set()     while scoperef:         elem = self._elem_from_scoperef(scoperef)         for child in elem:             name = child.get(""name"", """")             if name.startswith(expr):                 if name not in found_names:                     found_names.add(name)                     ilk = child.get(""ilk"") or child.tag                     cplns.append((ilk, name))         scoperef = self.parent_scoperef_from_scoperef(scoperef)         <MASK>             break     return sorted(cplns, key=operator.itemgetter(1))",if not scoperef :,if not scoperef :,100.00000000000004,100.00000000000004,1.0
"def get_xenapi_host(self):     """"""Return the xenapi host on which nova-compute runs on.""""""     with self._get_session() as session:         <MASK>             return session.xenapi.host.get_by_uuid(self.host_uuid)         else:             return session.xenapi.session.get_this_host(session.handle)",if self . host_uuid :,if self . host_uuid :,100.00000000000004,100.00000000000004,1.0
"def stream_docker_log(log_stream):     async for line in log_stream:         <MASK>             logger.debug(line[""stream""].strip())         elif ""status"" in line:             logger.debug(line[""status""].strip())         elif ""error"" in line:             logger.error(line[""error""].strip())             raise DockerBuildError","if ""stream"" in line and line [ ""stream"" ] . strip ( ) :","if ""stream"" in line :",17.469470584451173,17.469470584451173,0.0
"def test_wildcard_import():     bonobo = __import__(""bonobo"")     assert bonobo.__version__     for name in dir(bonobo):         # ignore attributes starting by underscores         <MASK>             continue         attr = getattr(bonobo, name)         if inspect.ismodule(attr):             continue         assert name in bonobo.__all__","if name . startswith ( ""_"" ) :","if name . startswith ( ""_"" ) :",100.00000000000004,100.00000000000004,1.0
"def _coerce_to_bool(self, node, var, true_val=True):     """"""Coerce the values in a variable to bools.""""""     bool_var = self.program.NewVariable()     for b in var.bindings:         v = b.data         if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool):             const = v.pyval is true_val         <MASK>             const = not true_val         elif not compare.compatible_with(v, False):             const = true_val         else:             const = None         bool_var.AddBinding(self.convert.bool_values[const], {b}, node)     return bool_var","elif not compare . compatible_with ( v , True ) :",if const :,0.9816077559181531,0.0,0.0
"def _parse_policies(self, policies_yaml):     for item in policies_yaml:         id_ = required_key(item, ""id"")         controls_ids = required_key(item, ""controls"")         if not isinstance(controls_ids, list):             <MASK>                 msg = ""Policy {id_} contains invalid controls list {controls}."".format(                     id_=id_, controls=str(controls_ids)                 )                 raise ValueError(msg)         self.policies[id_] = controls_ids","if controls_ids != ""all"" :",if controls_ids not in self . policies :,27.77619034011791,27.77619034011791,0.0
"def pong(self, payload: Union[str, bytes] = """") -> None:     if self.trace_enabled and self.ping_pong_trace_enabled:         <MASK>             payload = payload.decode(""utf-8"")         self.logger.debug(             ""Sending a pong data frame ""             f""(session id: {self.session_id}, payload: {payload})""         )     data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PONG)     with self.sock_send_lock:         self.sock.send(data)","if isinstance ( payload , bytes ) :",if payload :,7.49553326588684,0.0,0.0
"def _extract_curve_feature_log(arg):     """"""extract sampled curve feature for log items""""""     try:         inp, res = arg         config = inp.config         with inp.target:             sch, args = inp.task.instantiate(config)         fea = feature.get_buffer_curve_sample_flatten(sch, args, sample_n=20)         x = np.concatenate((fea, list(config.get_other_option().values())))         <MASK>             y = inp.task.flop / np.mean(res.costs)         else:             y = 0.0         return x, y     except Exception:  # pylint: disable=broad-except         return None",if res . error_no == 0 :,if res . costs :,15.719010513286515,15.719010513286515,0.0
"def messageSourceStamps(self, source_stamps):     text = """"     for ss in source_stamps:         source = """"         if ss[""branch""]:             source += ""[branch %s] "" % ss[""branch""]         <MASK>             source += str(ss[""revision""])         else:             source += ""HEAD""         if ss[""patch""] is not None:             source += "" (plus patch)""         discriminator = """"         if ss[""codebase""]:             discriminator = "" '%s'"" % ss[""codebase""]         text += ""Build Source Stamp%s: %s\n"" % (discriminator, source)     return text","if ss [ ""revision"" ] :","if ss [ ""revision"" ] :",100.00000000000004,100.00000000000004,1.0
"def find_repository():     orig_path = path = os.path.realpath(""."")     drive, path = os.path.splitdrive(path)     while path:         current_path = os.path.join(drive, path)         current_repo = LocalRepository(current_path)         if current_repo.isValid():             return current_repo         path, path_tail = os.path.split(current_path)         <MASK>             raise CannotFindRepository(""Cannot find repository for %s"" % (orig_path,))",if not path_tail :,if path_tail . startswith ( orig_path ) :,14.323145079400492,14.323145079400492,0.0
"def compute_indices(text: str, tokens):     indices = []     for i, token in enumerate(tokens):         <MASK>             current_index = indices[-1] + len(tokens[i - 1][0])             indices.append(current_index + text[current_index:].find(token[0]))         else:             indices.append(text.find(token[0]))     return indices",if 1 <= i :,if i > 0 :,11.51015341649912,11.51015341649912,0.0
"def _add_defaults_data_files(self):     # getting distribution.data_files     if self.distribution.has_data_files():         for item in self.distribution.data_files:             if isinstance(item, str):                 # plain file                 item = convert_path(item)                 <MASK>                     self.filelist.append(item)             else:                 # a (dirname, filenames) tuple                 dirname, filenames = item                 for f in filenames:                     f = convert_path(f)                     if os.path.isfile(f):                         self.filelist.append(f)",if os . path . isfile ( item ) :,if os . path . isfile ( item ) :,100.00000000000004,100.00000000000004,1.0
"def libcxx_define(settings):     compiler = _base_compiler(settings)     libcxx = settings.get_safe(""compiler.libcxx"")     if not compiler or not libcxx:         return """"     if str(compiler) in GCC_LIKE:         <MASK>             return ""_GLIBCXX_USE_CXX11_ABI=0""         elif str(libcxx) == ""libstdc++11"":             return ""_GLIBCXX_USE_CXX11_ABI=1""     return """"","if str ( libcxx ) == ""libstdc++"" :","if str ( libcxx ) == ""libstdc++11"" :",80.91067115702207,80.91067115702207,0.0
"def _populate_tree(self, element, d):     """"""Populates an etree with attributes & elements, given a dict.""""""     for k, v in d.iteritems():         if isinstance(v, dict):             self._populate_dict(element, k, v)         elif isinstance(v, list):             self._populate_list(element, k, v)         elif isinstance(v, bool):             self._populate_bool(element, k, v)         <MASK>             self._populate_str(element, k, v)         elif type(v) in [int, float, long, complex]:             self._populate_number(element, k, v)","elif isinstance ( v , basestring ) :","if isinstance ( v , str ) :",41.11336169005198,41.11336169005198,0.0
"def test_seek(self):     <MASK>         print(""create large file via seek (may be sparse file) ..."")     with self.open(TESTFN, ""wb"") as f:         f.write(b""z"")         f.seek(0)         f.seek(size)         f.write(b""a"")         f.flush()         if verbose:             print(""check file size with os.fstat"")         self.assertEqual(os.fstat(f.fileno())[stat.ST_SIZE], size + 1)",if verbose :,if verbose :,100.00000000000004,0.0,1.0
"def serialize_review_url_field(self, obj, **kwargs):     if obj.review_ui:         review_request = obj.get_review_request()         <MASK>             local_site_name = review_request.local_site.name         else:             local_site_name = None         return local_site_reverse(             ""file-attachment"",             local_site_name=local_site_name,             kwargs={                 ""review_request_id"": review_request.display_id,                 ""file_attachment_id"": obj.pk,             },         )     return """"",if review_request . local_site_id :,if review_request . local_site :,71.19674182275,71.19674182275,0.0
"def on_item_down_clicked(self, button):     model = self.treeview.get_model()     for s in self._get_selected():         <MASK>  # XXX need model.swap             old = model.get_iter(s[0])             iter = model.insert(s[0] + 2)             for i in range(3):                 model.set_value(iter, i, model.get_value(old, i))             model.remove(old)             self.treeview.get_selection().select_iter(iter)     self._update_filter_string()",if s [ 0 ] < len ( model ) - 1 :,if s [ 0 ] == 1 :,32.91467773009902,32.91467773009902,0.0
"def writer(self):     """"""loop forever and copy socket->serial""""""     while self.alive:         try:             data = self.socket.recv(1024)             <MASK>                 break             self.serial.write(b"""".join(self.rfc2217.filter(data)))         except socket.error as msg:             self.log.error(""{}"".format(msg))             # probably got disconnected             break     self.stop()",if not data :,if not data :,100.00000000000004,100.00000000000004,1.0
"def __getitem__(self, key):     if key == 1:         return self.get_value()     elif key == 0:         return self.cell[0]     elif isinstance(key, slice):         s = list(self.cell.__getitem__(key))         <MASK>             s[s.index(self.cell[1])] = self.get_value()         return s     else:         raise IndexError(key)",if self . cell [ 1 ] in s :,if s . index ( self . cell [ 0 ] ) == key :,18.20705281109213,18.20705281109213,0.0
"def test_error_stream(environ, start_response):     writer = start_response(""200 OK"", [])     wsgi_errors = environ[""wsgi.errors""]     error_msg = None     for method in [         ""flush"",         ""write"",         ""writelines"",     ]:         <MASK>             error_msg = ""wsgi.errors has no '%s' attr"" % method         if not error_msg and not callable(getattr(wsgi_errors, method)):             error_msg = ""wsgi.errors.%s attr is not callable"" % method         if error_msg:             break     return_msg = error_msg or ""success""     writer(return_msg)     return []","if not hasattr ( wsgi_errors , method ) :","if not error_msg and not hasattr ( wsgi_errors , method ) :",61.28081331864041,61.28081331864041,0.0
"def job_rule_modules(app):     rules_module_list = []     for rules_module_name in __job_rule_module_names(app):         rules_module = sys.modules.get(rules_module_name, None)         <MASK>             # if using a non-default module, it's not imported until a JobRunnerMapper is instantiated when the first             # JobWrapper is created             rules_module = importlib.import_module(rules_module_name)         rules_module_list.append(rules_module)     return rules_module_list",if not rules_module :,if rules_module is None :,27.77619034011791,27.77619034011791,0.0
"def discover_hdfstore(f):     d = dict()     for key in f.keys():         d2 = d         key2 = key.lstrip(""/"")         while ""/"" in key2:             group, key2 = key2.split(""/"", 1)             <MASK>                 d2[group] = dict()             d2 = d2[group]         d2[key2] = f.get_storer(key)     return discover(d)",if group not in d2 :,if group not in d2 :,100.00000000000004,100.00000000000004,1.0
"def test_update_zone(self):     zone = self.driver.list_zones()[0]     updated_zone = self.driver.update_zone(zone=zone, domain="""", extra={""paused"": True})     self.assertEqual(zone.id, updated_zone.id)     self.assertEqual(zone.domain, updated_zone.domain)     self.assertEqual(zone.type, updated_zone.type)     self.assertEqual(zone.ttl, updated_zone.ttl)     for key in set(zone.extra) | set(updated_zone.extra):         <MASK>             self.assertNotEqual(zone.extra[key], updated_zone.extra[key])         else:             self.assertEqual(zone.extra[key], updated_zone.extra[key])","if key in ( ""paused"" , ""modified_on"" ) :",if key in zone . extra :,8.377387908310832,8.377387908310832,0.0
"def ESP(phrase):     for num, name in enumerate(devname):         <MASK>             dev = devid[num]             if custom_action_keyword[""Dict""][""On""] in phrase:                 ctrl = ""=ON""                 say(""Turning On "" + name)             elif custom_action_keyword[""Dict""][""Off""] in phrase:                 ctrl = ""=OFF""                 say(""Turning Off "" + name)             rq = requests.head(""https://"" + ip + dev + ctrl, verify=False)",if name . lower ( ) in phrase :,if name in devid :,11.415938068117505,11.415938068117505,0.0
"def filter_ports(self, dpid, in_port, nw_id, allow_nw_id_external=None):     assert nw_id != self.nw_id_unknown     ret = []     for port in self.get_ports(dpid):         nw_id_ = port.network_id         if port.port_no == in_port:             continue         if nw_id_ == nw_id:             ret.append(port.port_no)         <MASK>             ret.append(port.port_no)     return ret",elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external :,if allow_nw_id_external :,13.057133736325447,13.057133736325447,0.0
"def tail(filename):     if os.path.isfile(filename):         file = open(filename, ""r"")         st_results = os.stat(filename)         st_size = st_results[6]         file.seek(st_size)         while 1:             where = file.tell()             line = file.readline()             <MASK>                 time.sleep(1)                 file.seek(where)             else:                 print(                     line,                 )  # already has newline     else:         print_error(""File not found, cannot tail."")",if not line :,if where :,24.840753130578644,0.0,0.0
"def proc_day_of_week(d):     if expanded[4][0] != ""*"":         diff_day_of_week = nearest_diff_method(d.isoweekday() % 7, expanded[4], 7)         if diff_day_of_week is not None and diff_day_of_week != 0:             <MASK>                 d += relativedelta(days=diff_day_of_week, hour=23, minute=59, second=59)             else:                 d += relativedelta(days=diff_day_of_week, hour=0, minute=0, second=0)             return True, d     return False, d",if is_prev :,if diff_day_of_week < 7 :,4.9323515694897075,4.9323515694897075,0.0
"def __call__(self):     """"""Run all check_* methods.""""""     if self.on:         oldformatwarning = warnings.formatwarning         warnings.formatwarning = self.formatwarning         try:             for name in dir(self):                 if name.startswith(""check_""):                     method = getattr(self, name)                     <MASK>                         method()         finally:             warnings.formatwarning = oldformatwarning",if method and callable ( method ) :,if method :,11.898417391331403,0.0,0.0
"def get(self, request, *args, **kwargs):     if self.revision:         <MASK>             try:                 return send_file(                     request,                     self.revision.file.path,                     self.revision.created,                     self.attachment.original_filename,                 )             except OSError:                 pass         else:             return HttpResponseRedirect(self.revision.file.url)     raise Http404",if settings . USE_LOCAL_PATH :,if self . attachment :,6.316906128202129,6.316906128202129,0.0
"def _close(self):     super(Recording, self)._close()     if self._log_n is not None:         for i in range(self.n):             <MASK>                 self._log_n[i].close()                 self._log_n[i] = None",if self . _log_n [ i ] is not None :,if self . _log_n [ i ] is not None :,100.00000000000004,100.00000000000004,1.0
"def addTags(self, rpcObjects=None):     hosts = self._getOnlyHostObjects(rpcObjects)     if hosts:         title = ""Add Tags""         body = ""What tags should be added?\n\nUse a comma or space between each""         (tags, choice) = self.getText(title, body, """")         <MASK>             tags = str(tags).replace("" "", "","").split("","")             for host in hosts:                 self.cuebotCall(                     host.addTags, ""Add Tags to %s Failed"" % host.data.name, tags                 )             self._update()",if choice :,if choice :,100.00000000000004,0.0,1.0
"def available_datasets(self):     """"""Automatically determine datasets provided by this file""""""     res = self.resolution     coordinates = [""pixel_longitude"", ""pixel_latitude""]     for var_name, val in self.file_content.items():         <MASK>             ds_info = {                 ""file_type"": self.filetype_info[""file_type""],                 ""resolution"": res,             }             if not self.is_geo:                 ds_info[""coordinates""] = coordinates             yield DatasetID(name=var_name, resolution=res), ds_info","if isinstance ( val , netCDF4 . Variable ) :",if val :,3.848335094984576,0.0,0.0
"def extract_from_file(fname: PathIsh) -> Iterator[Extraction]:     path = Path(fname)     fallback_dt = file_mtime(path)     p = Parser(path)     for r in p.walk():         <MASK>             yield r         else:             yield Visit(                 url=r.url,                 dt=fallback_dt,                 locator=Loc.file(fname),  # TODO line number                 context=r.context,             )","if isinstance ( r , Exception ) :",if r . url is None :,7.492442692259767,7.492442692259767,0.0
"def init_module_config(module_json, config, config_path=default_config_path):     if ""config"" in module_json[""meta""]:         if module_json[""meta""][""config""]:             <MASK>                 config.add_section(module_json[""name""])             for config_var in module_json[""meta""][""config""]:                 if config_var not in config[module_json[""name""]]:                     config.set(module_json[""name""], config_var, """")     return config","if module_json [ ""name"" ] not in config :",if config_path :,3.0500258614052496,3.0500258614052496,0.0
"def _create_entities(parsed_entities, sidx, eidx):     entities = []     for k, vs in parsed_entities.items():         <MASK>             vs = [vs]         for value in vs:             entities.append(                 {                     ""entity"": k,                     ""start"": sidx,                     ""end"": eidx,  # can't be more specific                     ""value"": value,                 }             )     return entities","if not isinstance ( vs , list ) :","if isinstance ( vs , list ) :",76.72796459606589,76.72796459606589,0.0
"def _telegram_upload_stream(self, stream, **kwargs):     """"""Perform upload defined in a stream.""""""     msg = None     try:         stream.accept()         msg = self._telegram_special_message(             chat_id=stream.identifier.id,             content=stream.raw,             msg_type=stream.stream_type,             **kwargs,         )     except Exception:         log.exception(f""Upload of {stream.name} to {stream.identifier} failed."")     else:         <MASK>             stream.error()         else:             stream.success()",if msg is None :,if msg is None :,100.00000000000004,100.00000000000004,1.0
"def readlines(self, size=-1):     if self._nbr == self._size:         return []     # leave all additional logic to our readline method, we just check the size     out = []     nbr = 0     while True:         line = self.readline()         <MASK>             break         out.append(line)         if size > -1:             nbr += len(line)             if nbr > size:                 break         # END handle size constraint     # END readline loop     return out",if not line :,if not line :,100.00000000000004,100.00000000000004,1.0
"def clean_permissions(     cls,     requestor: ""User"",     group: auth_models.Group,     errors: Dict[Optional[str], List[ValidationError]],     cleaned_input: dict, ):     field = ""add_permissions""     permission_items = cleaned_input.get(field)     if permission_items:         cleaned_input[field] = get_permissions(permission_items)         <MASK>             cls.ensure_can_manage_permissions(                 requestor, errors, field, permission_items             )",if not requestor . is_superuser :,if group . id == group . id :,5.522397783539471,5.522397783539471,0.0
"def _bwd(subj=None, obj=None, seen=None):     seen.add(obj)     for s, o in evalPath(graph, (None, self.path, obj)):         <MASK>             yield s, o         if self.more:             if s in seen:                 continue             for s2, o2 in _bwd(None, s, seen):                 yield s2, o",if not subj or subj == s :,if subj :,5.370784274455332,0.0,0.0
"def generate_data(self, request):     """"""Generate data for the widget.""""""     uptime = {}     cache_stats = get_cache_stats()     if cache_stats:         for hosts, stats in cache_stats:             <MASK>                 uptime[""value""] = stats[""uptime""] / 60 / 60 / 24                 uptime[""unit""] = _(""days"")             elif stats[""uptime""] > 3600:                 uptime[""value""] = stats[""uptime""] / 60 / 60                 uptime[""unit""] = _(""hours"")             else:                 uptime[""value""] = stats[""uptime""] / 60                 uptime[""unit""] = _(""minutes"")     return {""cache_stats"": cache_stats, ""uptime"": uptime}","if stats [ ""uptime"" ] > 86400 :","if stats [ ""uptime"" ] > 3600 :",78.25422900366438,78.25422900366438,0.0
def refresh(self):     if self._handle:         source = self._db.get_repository_from_handle(self._handle)         <MASK>             self._title = str(source.get_type())             self._value = source.get_name(),if source :,if source :,100.00000000000004,0.0,1.0
"def _gridconvvalue(self, value):     if isinstance(value, (str, _tkinter.Tcl_Obj)):         try:             svalue = str(value)             <MASK>                 return None             elif ""."" in svalue:                 return getdouble(svalue)             else:                 return getint(svalue)         except ValueError:             pass     return value",if not svalue :,"if ""-"" in svalue :",14.535768424205482,14.535768424205482,0.0
"def parseGrants(self, tree):     for grant in tree.findall("".//Grant""):         grantee = Grantee()         g = grant.find("".//Grantee"")         grantee.xsi_type = g.attrib[""{http://www.w3.org/2001/XMLSchema-instance}type""]         grantee.permission = grant.find(""Permission"").text         for el in g:             <MASK>                 grantee.display_name = el.text             else:                 grantee.tag = el.tag                 grantee.name = el.text         self.grantees.append(grantee)","if el . tag == ""DisplayName"" :","if el . tag == ""display"" :",70.71067811865478,70.71067811865478,0.0
"def __init__(self, name: Optional[str] = None, order: int = 0):     if name is None:         if order == 0:             name = ""std_dev""         <MASK>             name = ""sample_std_dev""         else:             name = f""std_dev{order})""     super().__init__(name=name, order=order)     self.order = order",elif order == 1 :,if order == 1 :,75.98356856515926,75.98356856515926,0.0
"def _shouldRollover(self):     if self.maxBytes > 0:  # are we rolling over?         try:             self.stream.seek(0, 2)  # due to non-posix-compliant Windows feature         except IOError:             return True         <MASK>             return True         else:             self._degrade(False, ""Rotation done or not needed at this time"")     return False",if self . stream . tell ( ) >= self . maxBytes :,if self . stream . tell ( ) == self . maxBytes :,78.25422900366432,78.25422900366432,0.0
"def userfullname():     """"""Get the user's full name.""""""     global _userfullname     if not _userfullname:         uid = os.getuid()         entry = pwd_from_uid(uid)         <MASK>             _userfullname = entry[4].split("","")[0] or entry[0]         if not _userfullname:             _userfullname = ""user%d"" % uid     return _userfullname",if entry :,if entry :,100.00000000000004,0.0,1.0
"def drop(self):     # mssql     sql = ""if object_id('%s') is not null drop table %s"" % (self.tname, self.tname)     try:         self.execute(sql)     except Exception as e:         self.conn.rollback()         <MASK>             raise         # sqlite         sql = ""drop table if exists %s"" % self.tname         self.execute(sql)","if ""syntax error"" not in str ( e ) :",if e . errno == 404 :,4.408194605881708,4.408194605881708,0.0
"def _find_delimiter(f, block_size=2 ** 16):     delimiter = b""\n""     if f.tell() == 0:         return 0     while True:         b = f.read(block_size)         <MASK>             return f.tell()         elif delimiter in b:             return f.tell() - len(b) + b.index(delimiter) + 1",if not b :,if b is None :,14.058533129758727,14.058533129758727,0.0
"def _convert(container):     if _value_marker in container:         force_list = False         values = container.pop(_value_marker)         <MASK>             force_list = True             values.extend(_convert(x[1]) for x in sorted(container.items()))         if not force_list and len(values) == 1:             values = values[0]         if not container:             return values         return _convert(container)     elif container.pop(_list_marker, False):         return [_convert(x[1]) for x in sorted(container.items())]     return dict_cls((k, _convert(v)) for k, v in iteritems(container))","if container . pop ( _list_marker , False ) :",if _list_marker in values :,20.479259709128794,20.479259709128794,0.0
"def fitting(self, value):     self._fitting = value     if self._fitting is not None:         <MASK>             try:                 os.makedirs(dirname(self.checkpoint_path()))             except FileExistsError as ex:                 pass  # race to create         if not os.path.exists(dirname(self.tensorboard_path())):             try:                 os.makedirs(dirname(self.tensorboard_path()))             except FileExistsError as ex:                 pass  # race to create",if not os . path . exists ( dirname ( self . checkpoint_path ( ) ) ) :,if not os . path . exists ( self . checkpoint_path ( ) ) :,77.56845723911358,77.56845723911358,0.0
"def _make_headers(self):     libraries = self._df.columns.to_list()     columns = []     for library in libraries:         version = self._package_versions[library]         library_description = self._libraries_description.get(library)         <MASK>             library += "" {}"".format(library_description)         columns.append(             ""{library}<br><small>{version}</small>"".format(                 library=library, version=version             )         )     return [""""] + columns",if library_description :,if library_description :,100.00000000000004,100.00000000000004,1.0
"def plugin_on_song_ended(self, song, stopped):     if song is not None:         poll = self.rating_box.poll_vote()         <MASK>             ups = int(song.get(""~#wins"") or 0)             downs = int(song.get(""~#losses"") or 0)             ups += poll[0]             downs += poll[1]             song[""~#wins""] = ups             song[""~#losses""] = downs             song[""~#rating""] = ups / max((ups + downs), 2)             # note: ^^^ Look into implementing w/ confidence intervals!             song[""~#score""] = ups - downs",if poll [ 0 ] >= 1 or poll [ 1 ] >= 1 :,if poll is not None :,2.8722725093023906,2.8722725093023906,0.0
"def submit(self, pig_script, params):     workflow = None     try:         workflow = self._create_workflow(pig_script, params)         mapping = dict(             [(param[""name""], param[""value""]) for param in workflow.get_parameters()]         )         oozie_wf = _submit_workflow(self.user, self.fs, self.jt, workflow, mapping)     finally:         <MASK>             workflow.delete(skip_trash=True)     return oozie_wf",if workflow :,if oozie_wf . is_trash :,5.669791110976001,5.669791110976001,0.0
"def test_parse(self):     correct = 0     for example in EXAMPLES:         try:             schema.parse(example.schema_string)             <MASK>                 correct += 1             else:                 self.fail(""Invalid schema was parsed: "" + example.schema_string)         except:             if not example.valid:                 correct += 1             else:                 self.fail(""Valid schema failed to parse: "" + example.schema_string)     fail_msg = ""Parse behavior correct on %d out of %d schemas."" % (         correct,         len(EXAMPLES),     )     self.assertEqual(correct, len(EXAMPLES), fail_msg)",if example . valid :,if example . valid :,100.00000000000004,100.00000000000004,1.0
"def handle_sent(self, elt):     sent = []     for child in elt:         if child.tag in (""wf"", ""punc""):             itm = self.handle_word(child)             <MASK>                 sent.extend(itm)             else:                 sent.append(itm)         else:             raise ValueError(""Unexpected element %s"" % child.tag)     return SemcorSentence(elt.attrib[""snum""], sent)","if self . _unit == ""word"" :","if ""snum"" in itm :",5.244835934727967,5.244835934727967,0.0
"def _set_property(self, target_widget, pname, value):     if pname == ""text"":         state = target_widget.cget(""state"")         <MASK>             target_widget.configure(state=tk.NORMAL)             target_widget.insert(""0.0"", value)             target_widget.configure(state=tk.DISABLED)         else:             target_widget.insert(""0.0"", value)     else:         super(TKText, self)._set_property(target_widget, pname, value)",if state == tk . DISABLED :,if state == tk . NORMAL :,70.71067811865478,70.71067811865478,0.0
"def get_vrf_tables(self, vrf_rf=None):     vrf_tables = {}     for (scope_id, table_id), table in self._tables.items():         if scope_id is None:             continue         <MASK>             continue         vrf_tables[(scope_id, table_id)] = table     return vrf_tables",if vrf_rf is not None and table_id != vrf_rf :,if table_rf is None :,8.01321535989944,8.01321535989944,0.0
"def new_f(self, *args, **kwargs):     for obj in f(self, *args, **kwargs):         if self.protected == False:             if ""user"" in obj and obj[""user""][""protected""]:                 continue             <MASK>                 continue         yield obj","elif ""protected"" in obj and obj [ ""protected"" ] :","if ""user"" in obj :",10.218289380194191,10.218289380194191,0.0
"def draw(self, context):     col = self.layout.column()     col.operator(""node.sv_show_latest_commits"")     if context.scene.sv_new_version:         col_alert = self.layout.column()         col_alert.alert = True         col_alert.operator(""node.sverchok_update_addon"", text=""Upgrade Sverchok addon"")     else:         col.operator(""node.sverchok_check_for_upgrades_wsha"", text=""Check for updates"")     with sv_preferences() as prefs:         <MASK>             col.operator(""node.sv_run_pydoc"")",if prefs . developer_mode :,if context . scene . sv_new_version :,5.300156689756295,5.300156689756295,0.0
"def generate_tag_1_data(ids):     if len(ids) != SAMPLE_NUM:         raise ValueError(""len ids should equal to sample number"")     counter = 0     for sample_i in range(SAMPLE_NUM):         one_data = [ids[sample_i]]         valid_set = [x for x in range(TAG_INTERVAL[0], TAG_INTERVAL[1])]         features = np.random.choice(valid_set, FEATURE_NUM, replace=False)         one_data += ["":"".join([x, ""1.0""]) for x in features]         counter += 1         <MASK>             print(""generate data {}"".format(counter))         yield one_data",if counter % 10000 == 0 :,if counter < TAG_INTERVAL [ 0 ] :,9.980099403873663,9.980099403873663,0.0
"def handle_api_languages(self, http_context):     mgr = PluginManager.get(aj.context)     languages = set()     for id in mgr:         locale_dir = mgr.get_content_path(id, ""locale"")         <MASK>             for lang in os.listdir(locale_dir):                 if lang != ""app.pot"":                     languages.add(lang)     return sorted(list(languages))",if os . path . isdir ( locale_dir ) :,if locale_dir :,11.141275535087015,11.141275535087015,0.0
"def update(self, t):     # direction right - up     for i in range(self.grid.x):         for j in range(self.grid.y):             distance = self.test_func(i, j, t)             <MASK>                 self.turn_off_tile(i, j)             elif distance < 1:                 self.transform_tile(i, j, distance)             else:                 self.turn_on_tile(i, j)",if distance == 0 :,if distance > 0 :,24.736929544091932,24.736929544091932,0.0
"def _handle_autocomplete_request_for_text(text):     if not hasattr(text, ""autocompleter""):         if isinstance(text, (CodeViewText, ShellText)) and text.is_python_text():             if isinstance(text, CodeViewText):                 text.autocompleter = Completer(text)             <MASK>                 text.autocompleter = ShellCompleter(text)             text.bind(""<1>"", text.autocompleter.on_text_click)         else:             return     text.autocompleter.handle_autocomplete_request()","elif isinstance ( text , ShellText ) :",if text . is_text_click :,5.669791110976001,5.669791110976001,0.0
"def test_create_repository(repo_name, expected_status, client):     with client_with_identity(""devtable"", client) as cl:         body = {             ""namespace"": ""devtable"",             ""repository"": repo_name,             ""visibility"": ""public"",             ""description"": ""foo"",         }         result = conduct_api_call(             client, RepositoryList, ""post"", None, body, expected_code=expected_status         ).json         <MASK>             assert result[""name""] == repo_name             assert (                 model.repository.get_repository(""devtable"", repo_name).name == repo_name             )",if expected_status == 201 :,"if ""name"" in result :",6.770186228657864,6.770186228657864,0.0
"def _apply_filter(filter_item, filter_list):     for filter_method in filter_list:         try:             <MASK>                 return False         except Exception as e:             raise MessageException(                 ""Toolbox filter exception from '{}': {}."".format(                     filter_method.__name__, unicodify(e)                 )             )     return True","if not filter_method ( context , filter_item ) :",if filter_method ( filter_item ) is False :,41.24914892312111,41.24914892312111,0.0
"def printsumfp(fp, filename, out=sys.stdout):     m = md5()     try:         while 1:             data = fp.read(bufsize)             if not data:                 break             <MASK>                 data = data.encode(fp.encoding)             m.update(data)     except IOError as msg:         sys.stderr.write(""%s: I/O error: %s\n"" % (filename, msg))         return 1     out.write(""%s %s\n"" % (m.hexdigest(), filename))     return 0","if isinstance ( data , str ) :",if fp . encoding :,6.9717291216921975,6.9717291216921975,0.0
"def get_block_loc_keys(block):     """"""Extract loc_keys used by @block""""""     symbols = set()     for instr in block.lines:         <MASK>             if isinstance(instr.raw, list):                 for expr in instr.raw:                     symbols.update(get_expr_locs(expr))         else:             for arg in instr.args:                 symbols.update(get_expr_locs(arg))     return symbols","if isinstance ( instr , AsmRaw ) :","if instr . type == ""block"" :",5.522397783539471,5.522397783539471,0.0
"def get_operations(cls, info, operations: List[ProductAttributeAssignInput]):     """"""Resolve all passed global ids into integer PKs of the Attribute type.""""""     product_attrs_pks = []     variant_attrs_pks = []     for operation in operations:         pk = from_global_id_strict_type(             operation.id, only_type=Attribute, field=""operations""         )         <MASK>             product_attrs_pks.append(pk)         else:             variant_attrs_pks.append(pk)     return product_attrs_pks, variant_attrs_pks",if operation . type == ProductAttributeType . PRODUCT :,if pk is not None :,4.955725306405571,4.955725306405571,0.0
"def _collect_manual_intervention_nodes(pipeline_tree):     for act in pipeline_tree[""activities""].values():         if act[""type""] == ""SubProcess"":             _collect_manual_intervention_nodes(act[""pipeline""])         <MASK>             manual_intervention_nodes.add(act[""id""])","elif act [ ""component"" ] [ ""code"" ] in MANUAL_INTERVENTION_COMP_CODES :","if act [ ""id"" ] in manual_intervention_nodes :",11.933478484692971,11.933478484692971,0.0
"def prompt_authorization(self, stacks: List[Stack]):     auth_required_per_resource = auth_per_resource(stacks)     for resource, authorization_required in auth_required_per_resource:         <MASK>             auth_confirm = confirm(                 f""\t{self.start_bold}{resource} may not have authorization defined, Is this okay?{self.end_bold}"",                 default=False,             )             if not auth_confirm:                 raise GuidedDeployFailedError(msg=""Security Constraints Not Satisfied!"")",if not authorization_required :,if authorization_required :,57.89300674674101,57.89300674674101,0.0
"def get_cloud_credential(self):     """"""Return the credential which is directly tied to the inventory source type.""""""     credential = None     for cred in self.credentials.all():         if self.source in CLOUD_PROVIDERS:             <MASK>                 credential = cred                 break         else:             # these need to be returned in the API credential field             if cred.credential_type.kind != ""vault"":                 credential = cred                 break     return credential","if cred . kind == self . source . replace ( ""ec2"" , ""aws"" ) :","if cred . credential_type . kind == ""vault"" :",17.04960194455123,17.04960194455123,0.0
"def validate_party_details(self):     if self.party:         if not frappe.db.exists(self.party_type, self.party):             frappe.throw(_(""Invalid {0}: {1}"").format(self.party_type, self.party))         <MASK>             self.validate_account_type(                 self.party_account, [erpnext.get_party_account_type(self.party_type)]             )","if self . party_account and self . party_type in ( ""Customer"" , ""Supplier"" ) :",if self . party_account :,8.552033621493605,8.552033621493605,0.0
"def __iter__(self):     it = DiskHashMerger.__iter__(self)     direct_upstreams = self.direct_upstreams     for k, groups in it:         t = list([[] for _ in range(self.size)])         for i, g in enumerate(groups):             <MASK>                 if i in direct_upstreams:                     t[i] = g                 else:                     g.sort(key=itemgetter(0))                     g1 = []                     for _, vs in g:                         g1.extend(vs)                     t[i] = g1         yield k, tuple(t)",if g :,if i in t :,12.703318703865365,12.703318703865365,0.0
"def _unpack_scales(scales, vidxs):     scaleData = [None, None, None]     for i in range(3):         <MASK>             break         scale = scales[i]         if not math.isnan(scale):             vidx1, vidx2 = vidxs[i * 2], vidxs[i * 2 + 1]             scaleData[i] = (int(vidx1), int(vidx2), float(scale))     return scaleData","if i >= min ( len ( scales ) , len ( vidxs ) // 2 ) :",if scales [ i ] == 0 :,2.1001373276485245,2.1001373276485245,0.0
"def _make_ext_obj(self, obj):     ext = self._get_ext_class(obj.objname)()     for name, val in obj.body:         <MASK>             raise Exception(                 ""Error val should be a list, this is a python-opcua bug"",                 name,                 type(val),                 val,             )         else:             for attname, v in val:                 self._set_attr(ext, attname, v)     return ext","if not isinstance ( val , list ) :","if not isinstance ( val , list ) :",100.00000000000004,100.00000000000004,1.0
"def insertLine(self, refnum, linenum, line):     i = -1     for i, row in enumerate(self.rows):         if row[0] == linenum:             if row[refnum + 1] is None:                 row[refnum + 1] = line                 return             # else keep looking         <MASK>             break     self.rows.insert(i, self.newRow(linenum, refnum, line))",elif row [ 0 ] > linenum :,if i == - 1 :,5.693025330278465,5.693025330278465,0.0
"def valid_localparts(strip_delimiters=False):     for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""):         # strip line, skip over empty lines         line = line.strip()         if line == """":             continue         # skip over comments or empty lines         match = COMMENT.match(line)         if match:             continue         # skip over localparts with delimiters         if strip_delimiters:             <MASK>                 continue         yield line","if "","" in line or "";"" in line :",if not match :,2.002152301552759,2.002152301552759,0.0
"def encodingChanged(self, idx):     encoding = str(self.mode_combo.currentText())     validator = None     if encoding == ""hex"":         # only clear the box if there are non-hex chars         # before setting the validator.         txt = str(self.data_edit.text())         <MASK>             self.data_edit.setText("""")         regex = QtCore.QRegExp(""^[0-9A-Fa-f]+$"")         validator = QtGui.QRegExpValidator(regex)     self.data_edit.setValidator(validator)     self.renderMemory()",if not all ( c in string . hexdigits for c in txt ) :,"if txt == ""hex"" :",3.0297048914466935,3.0297048914466935,0.0
"def _compare_single_run(self, compares_done):     try:         compare_id, redo = self.in_queue.get(             timeout=float(self.config[""ExpertSettings""][""block_delay""])         )     except Empty:         pass     else:         if self._decide_whether_to_process(compare_id, redo, compares_done):             <MASK>                 self.db_interface.delete_old_compare_result(compare_id)             compares_done.add(compare_id)             self._process_compare(compare_id)             if self.callback:                 self.callback()",if redo :,if self . db_interface . has_compare_result ( compare_id ) :,2.5540496664715904,2.5540496664715904,0.0
"def _transform_bin(self, X: DataFrame):     if self._bin_map:         <MASK>             X = X.copy(deep=True)         with pd.option_context(""mode.chained_assignment"", None):             # Pandas complains about SettingWithCopyWarning, but this should be valid.             for column in self._bin_map:                 X[column] = binning.bin_column(                     series=X[column],                     mapping=self._bin_map[column],                     dtype=self._astype_map[column],                 )     return X",if not self . inplace :,if self . _astype_map :,13.134549472120788,13.134549472120788,0.0
"def escape(text, newline=False):     """"""Escape special html characters.""""""     if isinstance(text, str):         if ""&"" in text:             text = text.replace(""&"", ""&amp;"")         if "">"" in text:             text = text.replace("">"", ""&gt;"")         if ""<"" in text:             text = text.replace(""<"", ""&lt;"")         if '""' in text:             text = text.replace('""', ""&quot;"")         <MASK>             text = text.replace(""'"", ""&quot;"")         if newline:             if ""\n"" in text:                 text = text.replace(""\n"", ""<br>"")     return text","if ""'"" in text :","if ""'"" in text :",100.00000000000004,100.00000000000004,1.0
"def read(self):     """"""Reads the robots.txt URL and feeds it to the parser.""""""     try:         f = urllib.request.urlopen(self.url)     except urllib.error.HTTPError as err:         <MASK>             self.disallow_all = True         elif err.code >= 400 and err.code < 500:             self.allow_all = True     else:         raw = f.read()         self.parse(raw.decode(""utf-8"").splitlines())","if err . code in ( 401 , 403 ) :",if err . code == 404 :,25.124218547395092,25.124218547395092,0.0
"def post_create(self, user, billing=None):     from weblate.trans.models import Change     if billing:         billing.projects.add(self)         <MASK>             self.access_control = Project.ACCESS_PRIVATE         else:             self.access_control = Project.ACCESS_PUBLIC         self.save()     if not user.is_superuser:         self.add_user(user, ""@Administration"")     Change.objects.create(         action=Change.ACTION_CREATE_PROJECT, project=self, user=user, author=user     )",if billing . plan . change_access_control :,if user . is_superuser :,5.244835934727967,5.244835934727967,0.0
"def visitConst(self, node):     if self.documentable:         <MASK>             self.documentable.append(make_docstring(node.value, node.lineno))         else:             self.documentable = None","if type ( node . value ) in ( StringType , UnicodeType ) :",if node . value :,7.468220329575271,7.468220329575271,0.0
"def requires(self):     requires = copy.deepcopy(self._requires)     # Auto add dependencies when parameters reference the Ouptuts of     # another stack.     parameters = self.parameters     for value in parameters.values():         if isinstance(value, basestring) and ""::"" in value:             stack_name, _ = value.split(""::"")         else:             continue         <MASK>             requires.add(stack_name)     return requires",if stack_name not in requires :,if stack_name :,38.80684294761701,38.80684294761701,0.0
"def __load_protos():     g = globals()     for k, v in g.items():         <MASK>             name = k[4:]             modname = name.lower()             try:                 mod = __import__(modname, g, level=1)                 PPP.set_p(v, getattr(mod, name))             except (ImportError, AttributeError):                 continue","if k . startswith ( ""PPP_"" ) :","if k [ 0 ] == ""protos"" :",9.425159511373677,9.425159511373677,0.0
"def init_weights(self):     """"""Initialize model weights.""""""     for m in self.predict_layers.modules():         if isinstance(m, nn.Conv2d):             kaiming_init(m)         elif isinstance(m, nn.BatchNorm2d):             constant_init(m, 1)         <MASK>             normal_init(m, std=0.01)","elif isinstance ( m , nn . Linear ) :","if isinstance ( m , nn . BatchNorm2d ) :",58.14307369682194,58.14307369682194,0.0
"def get_data(self):     """"""get all data from sockets""""""     si = self.inputs     parameters = []     for socket in si:         <MASK>             parameters.append(socket.sv_get())         else:             parameters.append(socket.sv_get(default=[[]]))     return match_long_repeat(parameters)",if len ( socket . prop_name ) > 0 :,if socket . sv_get :,8.085182710148953,8.085182710148953,0.0
"def test_parse_query_params_comparable_field(self):     query_params = {""filter[int_field][gt]"": 42, ""filter[int_field][lte]"": 9000}     fields = self.view.parse_query_params(query_params)     for key, field_name in fields.items():         if field_name[""int_field""][""op""] == ""gt"":             assert_equal(field_name[""int_field""][""value""], 42)         <MASK>             assert_equal(field_name[""int_field""][""value""], 9000)         else:             self.fail()","elif field_name [ ""int_field"" ] [ ""op"" ] == ""lte"" :","if field_name [ ""int_field"" ] [ ""op"" ] == ""lte"" :",95.10699415570296,95.10699415570296,0.0
"def _create_examples(self, lines, set_type):     """"""Creates examples for the training and dev sets.""""""     examples = []     for (i, line) in enumerate(lines):         <MASK>             continue         guid = ""%s-%s"" % (set_type, i)         text = line[0]         bbox = line[1]         label = line[2]         examples.append(             DocExample(guid=guid, text_a=text, text_b=None, bbox=bbox, label=label)         )     return examples",if i == 0 :,if not line :,11.521590992286539,11.521590992286539,0.0
"def _get_attr(sdk_path, mod_attr_path, checked=True):     try:         attr_mod, attr_path = (             mod_attr_path.split(""#"") if ""#"" in mod_attr_path else (mod_attr_path, """")         )         full_mod_path = ""{}.{}"".format(sdk_path, attr_mod) if attr_mod else sdk_path         op = import_module(full_mod_path)         <MASK>             # Only load attributes if needed             for part in attr_path.split("".""):                 op = getattr(op, part)         return op     except (ImportError, AttributeError) as ex:         if checked:             return None         raise ex",if attr_path :,if op :,17.799177396293473,0.0,0.0
"def _load_ui_modules(self, modules: Any) -> None:     if isinstance(modules, types.ModuleType):         self._load_ui_modules(dict((n, getattr(modules, n)) for n in dir(modules)))     elif isinstance(modules, list):         for m in modules:             self._load_ui_modules(m)     else:         assert isinstance(modules, dict)         for name, cls in modules.items():             try:                 <MASK>                     self.ui_modules[name] = cls             except TypeError:                 pass","if issubclass ( cls , UIModule ) :","if isinstance ( cls , ( list , tuple ) ) :",16.59038701421971,16.59038701421971,0.0
"def _remove_obsolete_leafs(input_dict):     if not isinstance(input_dict, dict):         return     if input_dict[LEAF_MARKER]:         bottom_leafs = input_dict[LEAF_MARKER]         for leaf in bottom_leafs:             <MASK>                 input_dict[LEAF_MARKER].remove(leaf)     for subtree in input_dict.keys():         _remove_obsolete_leafs(input_dict[subtree])",if leaf in input_dict :,if leaf in input_dict [ LEAF_MARKER ] :,43.36189090348677,43.36189090348677,0.0
"def decode(self, value, force=False):     ""Return a unicode string from the bytes-like representation""     if self.decode_responses or force:         <MASK>             value = value.tobytes()         if isinstance(value, bytes):             value = value.decode(self.encoding, self.encoding_errors)     return value","if isinstance ( value , memoryview ) :","if isinstance ( value , bytes ) :",59.4603557501361,59.4603557501361,0.0
"def audit(self, directive):     value = _get_value(directive)     if not value:         return     server_side = directive.name.startswith(""proxy_"")     for var in compile_script(value):         char = """"         <MASK>             char = ""\\n""         elif not server_side and var.can_contain(""\r""):             char = ""\\r""         else:             continue         reason = 'At least variable ""${var}"" can contain ""{char}""'.format(             var=var.name, char=char         )         self.add_issue(directive=[directive] + var.providers, reason=reason)","if var . can_contain ( ""\n"" ) :","if server_side and var . can_contain ( ""\n"" ) :",68.89656775362826,68.89656775362826,0.0
"def checkFilename(filename):  # useful in case of drag and drop     while True:         if filename[0] == ""'"":             filename = filename[1:]         <MASK>             filename = filename[:-1]         if os.path.exists(filename):             return filename         filename = input(             ""[!] Cannot find '%s'.\n[*] Enter a valid name of the file containing the paths to test -> ""             % filename         )","if filename [ len ( filename ) - 1 ] == ""'"" :","if filename [ - 1 ] == ""'"" :",59.79111927660196,59.79111927660196,0.0
"def findfiles(self, dir, base, rec):     try:         names = os.listdir(dir or os.curdir)     except os.error as msg:         print(msg)         return []     list = []     subdirs = []     for name in names:         fn = os.path.join(dir, name)         <MASK>             subdirs.append(fn)         else:             if fnmatch.fnmatch(name, base):                 list.append(fn)     if rec:         for subdir in subdirs:             list.extend(self.findfiles(subdir, base, rec))     return list",if os . path . isdir ( fn ) :,"if fnmatch . fnmatch ( name , base ) :",10.552670315936318,10.552670315936318,0.0
"def loop(handler, obj):     handler.response.write(""<table>"")     for k, v in obj.__dict__.items():         <MASK>             style = ""color: red"" if not v else """"             handler.response.write(                 '<tr style=""{}""><td>{}:</td><td>{}</td></tr>'.format(style, k, v)             )     handler.response.write(""</table>"")","if not k in ( ""data"" , ""gae_user"" , ""credentials"" , ""content"" , ""config"" ) :","if k not in ( ""color"" , ""green"" , ""blue"" ) :",13.718130539288866,13.718130539288866,0.0
"def anypython(request):     name = request.param     executable = getexecutable(name)     if executable is None:         if sys.platform == ""win32"":             executable = winpymap.get(name, None)             if executable:                 executable = py.path.local(executable)                 <MASK>                     return executable         pytest.skip(""no suitable %s found"" % (name,))     return executable",if executable . check ( ) :,if not os . path . exists ( executable ) :,9.864703138979419,9.864703138979419,0.0
"def __init__(self, socketpath=None):     if socketpath is None:         <MASK>             socketpath = ""/var/run/usbmuxd""         else:             socketpath = ""/var/run/usbmuxd""     self.socketpath = socketpath     self.listener = MuxConnection(socketpath, BinaryProtocol)     try:         self.listener.listen()         self.version = 0         self.protoclass = BinaryProtocol     except MuxVersionError:         self.listener = MuxConnection(socketpath, PlistProtocol)         self.listener.listen()         self.protoclass = PlistProtocol         self.version = 1     self.devices = self.listener.devices","if sys . platform == ""darwin"" :",if self . version == 1 :,10.816059393812111,10.816059393812111,0.0
"def _validate_distinct_on_different_types_and_field_orders(     self, collection, query, expected_results, get_mock_result ):     self.count = 0     self.get_mock_result = get_mock_result     query_iterable = collection.query_items(query, enable_cross_partition_query=True)     results = list(query_iterable)     for i in range(len(expected_results)):         <MASK>             self.assertDictEqual(results[i], expected_results[i])         elif isinstance(results[i], list):             self.assertListEqual(results[i], expected_results[i])         else:             self.assertEqual(results[i], expected_results[i])     self.count = 0","if isinstance ( results [ i ] , dict ) :","if isinstance ( results [ i ] , dict ) :",100.00000000000004,100.00000000000004,1.0
"def getRootId(self, id):     with self.connect() as cu:         while True:             stmt = ""select parent_path_id from hierarchy where path_id = ?""             cu.execute(stmt, (id,))             parent_id = cu.fetchone()[0]             <MASK>                 return id             id = parent_id",if parent_id is None or parent_id == id :,if parent_id is None :,30.93485033266056,30.93485033266056,0.0
"def add(self, path):     with self.get_lock(path):         <MASK>             self.entries[path] = {}             self.entries[path][""lock""] = self.new_locks[path]             del self.new_locks[path]             self.lru.append(path)",if not path in self . entries :,if self . entries [ path ] is None :,18.575057999133595,18.575057999133595,0.0
"def _get_coordinates_for_dataset_key(self, dsid):     """"""Get the coordinate dataset keys for *dsid*.""""""     ds_info = self.ids[dsid]     cids = []     for cinfo in ds_info.get(""coordinates"", []):         if not isinstance(cinfo, dict):             cinfo = {""name"": cinfo}         cinfo[""resolution""] = ds_info[""resolution""]         <MASK>             cinfo[""polarization""] = ds_info[""polarization""]         cid = DatasetID(**cinfo)         cids.append(self.get_dataset_key(cid))     return cids","if ""polarization"" in ds_info :","if ds_info [ ""polarization"" ] :",28.227983861579556,28.227983861579556,0.0
"def build_from_gdobj(cls, gdobj, steal=False):     # Avoid calling cls.__init__ by first instanciating a placeholder, then     # overloading it __class__ to turn it into an instance of the right class     ret = BuiltinInitPlaceholder()     if steal:         assert ffi.typeof(gdobj).kind == ""pointer""         ret._gd_ptr = gdobj     else:         <MASK>             ret._gd_ptr = cls._copy_gdobj(gdobj)         else:             ret._gd_ptr = cls._copy_gdobj(ffi.addressof(gdobj))     ret.__class__ = cls     return ret","if ffi . typeof ( gdobj ) . kind == ""pointer"" :","if gdobj . kind == ""pointer"" :",48.730396897437764,48.730396897437764,0.0
"def _listen_output(self):     ""NB! works in background thread""     try:         while True:             chars = self._proc.read(1)             <MASK>                 as_bytes = chars.encode(self.encoding)                 self._make_output_available(as_bytes)             else:                 self._error = ""EOF""                 break     except Exception as e:         self._error = str(e)",if len ( chars ) > 0 :,if chars :,7.49553326588684,0.0,0.0
"def result(     metrics: Dict[metric_types.MetricKey, Any] ) -> Dict[metric_types.AttributionsKey, Dict[Text, Union[float, np.ndarray]]]:     """"""Returns mean attributions.""""""     total_attributions = metrics[total_attributions_key]     weighted_count = metrics[weighted_example_count_key]     attributions = {}     for k, v in total_attributions.items():         <MASK>             attributions[k] = float(""nan"")         else:             attributions[k] = v / weighted_count     return {key: attributions}","if np . isclose ( weighted_count , 0.0 ) :",if v == 0 :,3.550932348642477,3.550932348642477,0.0
"def write_if_changed(path, data):     if isinstance(data, str):         data = data.encode()     changed = False     with open(os.open(path, os.O_CREAT | os.O_RDWR), ""wb+"") as f:         f.seek(0)         current = f.read()         <MASK>             changed = True             f.seek(0)             f.write(data)             f.truncate()         os.fsync(f)     return changed",if current != data :,if current != data :,100.00000000000004,100.00000000000004,1.0
"def detect_ssl_option(self):     for option in self.ssl_options():         if scan_argv(self.argv, option) is not None:             for other_option in self.ssl_options():                 <MASK>                     if scan_argv(self.argv, other_option) is not None:                         raise ConfigurationError(                             ""Cannot give both %s and %s"" % (option, other_option)                         )             return option",if option != other_option :,if other_option != option :,39.28146509005134,39.28146509005134,0.0
"def _infer_return_type(*args):     """"""Look at the type of all args and divine their implied return type.""""""     return_type = None     for arg in args:         <MASK>             continue         if isinstance(arg, bytes):             if return_type is str:                 raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")             return_type = bytes         else:             if return_type is bytes:                 raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")             return_type = str     if return_type is None:         return str  # tempfile APIs return a str by default.     return return_type",if arg is None :,if arg is None :,100.00000000000004,100.00000000000004,1.0
"def _get_app(self, body=None):     app = self._app     if app is None:         try:             tasks = self.tasks.tasks  # is a group         except AttributeError:             tasks = self.tasks         <MASK>             app = tasks[0]._app         if app is None and body is not None:             app = body._app     return app if app is not None else current_app",if len ( tasks ) :,if len ( tasks ) == 1 :,46.713797772819994,46.713797772819994,0.0
"def add_field(self, field):     self.remove_field(field.name)     self.fields[field.name] = field     self.columns[field.db_column] = field     self._sorted_field_list.insert(field)     self._update_field_lists()     if field.default is not None:         self.defaults[field] = field.default         <MASK>             self._default_callables[field] = field.default             self._default_callable_list.append((field.name, field.default))         else:             self._default_dict[field] = field.default             self._default_by_name[field.name] = field.default",if callable ( field . default ) :,if field . callable :,14.74341205953811,14.74341205953811,0.0
"def _get_families(self):     families = []     for name, ext in self._get_family_dirs():         <MASK>  # is a directory             family = self.get_resource(                 FileSystemPackageFamilyResource.key, location=self.location, name=name             )         else:             family = self.get_resource(                 FileSystemCombinedPackageFamilyResource.key,                 location=self.location,                 name=name,                 ext=ext,             )         families.append(family)     return families",if ext is None :,"if name == ""package"" :",6.567274736060395,6.567274736060395,0.0
"def test(model, data_loader, device=None):     device = device or torch.device(""cpu"")     model.eval()     correct = 0     total = 0     with torch.no_grad():         for batch_idx, (data, target) in enumerate(data_loader):             <MASK>                 break             data, target = data.to(device), target.to(device)             outputs = model(data)             _, predicted = torch.max(outputs.data, 1)             total += target.size(0)             correct += (predicted == target).sum().item()     return correct / total",if batch_idx * len ( data ) > TEST_SIZE :,if batch_idx == 0 :,17.267606045625936,17.267606045625936,0.0
"def __animate_progress(self):     """"""Change the status message, mostly used to animate progress.""""""     while True:         sleep_time = ThreadPool.PROGRESS_IDLE_DELAY         with self.__progress_lock:             <MASK>                 sleep_time = ThreadPool.PROGRESS_IDLE_DELAY             elif self.__show_animation:                 self.__progress_status.update_progress(self.__current_operation_name)                 sleep_time = ThreadPool.PROGRESS_UPDATE_DELAY             else:                 self.__progress_status.show_as_ready()                 sleep_time = ThreadPool.PROGRESS_IDLE_DELAY         # Allow some time for progress status to be updated.         time.sleep(sleep_time)",if not self . __progress_status :,if self . __show_animation :,30.719343730842187,30.719343730842187,0.0
"def _parse_subtitles(self, video_data, url_key):     subtitles = {}     for translation in video_data.get(""translations"", []):         vtt_path = translation.get(url_key)         <MASK>             continue         lang = translation.get(""language_w3c"") or ISO639Utils.long2short(             translation[""language_medium""]         )         subtitles.setdefault(lang, []).append(             {                 ""ext"": ""vtt"",                 ""url"": vtt_path,             }         )     return subtitles",if not vtt_path :,if not vtt_path or not vtt_path :,46.17366309441026,46.17366309441026,0.0
"def postprocess_message(self, msg):     if msg[""type""] == ""sample"" and msg[""value""] is not None:         fn, value = msg[""fn""], msg[""value""]         value_batch_ndims = jnp.ndim(value) - fn.event_dim         fn_batch_ndim = len(fn.batch_shape)         <MASK>             prepend_shapes = (1,) * (value_batch_ndims - fn_batch_ndim)             msg[""fn""] = tree_map(                 lambda x: jnp.reshape(x, prepend_shapes + jnp.shape(x)), fn             )",if fn_batch_ndim < value_batch_ndims :,if fn_batch_ndim > 0 :,39.14236894465539,39.14236894465539,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             self.set_filename(d.getPrefixedString())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 10 :,if tt == 0 :,53.7284965911771,53.7284965911771,0.0
"def createError(self, line, pos, description):     global ENABLE_PYIMPORT     msg = ""Line "" + unicode(line) + "": "" + unicode(description)     if ENABLE_JS2PY_ERRORS:         <MASK>             import js2py.base             return js2py.base.MakeError(""SyntaxError"", msg)         else:             return ENABLE_JS2PY_ERRORS(msg)     else:         return JsSyntaxError(msg)","if isinstance ( ENABLE_JS2PY_ERRORS , bool ) :",if self . _is_js2py :,4.736913377107212,4.736913377107212,0.0
"def extract(self, page, start_index=0, end_index=None):     items = []     for extractor in self.extractors:         extracted = extractor.extract(             page, start_index, end_index, self.template.ignored_regions         )         for item in arg_to_iter(extracted):             <MASK>                 if isinstance(item, (ItemProcessor, dict)):                     item[u""_template""] = self.template.id                 items.append(item)     return items",if item :,"if not isinstance ( item , ( ItemProcessor , list ) ) :",4.065425428798724,4.065425428798724,0.0
"def create_volume(self, volume):     """"""Create a volume.""""""     try:         cmd = [""volume"", ""create"", volume[""name""], ""%sG"" % (volume[""size""])]         <MASK>             cmd.append(""pool"")             cmd.append(self.configuration.eqlx_pool)         if self.configuration.san_thin_provision:             cmd.append(""thin-provision"")         out = self._eql_execute(*cmd)         self.add_multihost_access(volume)         return self._get_volume_data(out)     except Exception:         with excutils.save_and_reraise_exception():             LOG.error('Failed to create volume ""%s"".', volume[""name""])","if self . configuration . eqlx_pool != ""default"" :",if self . configuration . san_pool :,30.453180144425207,30.453180144425207,0.0
"def clean(self):     # TODO: check for clashes if the random code is already taken     if not self.code:         self.code = u""static-%s"" % uuid.uuid4()     if not self.site:         placeholders = StaticPlaceholder.objects.filter(             code=self.code, site__isnull=True         )         <MASK>             placeholders = placeholders.exclude(pk=self.pk)         if placeholders.exists():             raise ValidationError(                 _(""A static placeholder with the same site and code already exists"")             )",if self . pk :,if self . site :,42.72870063962342,42.72870063962342,0.0
"def spawnMenu(self, event):     clickedPos = self.getRowByAbs(event.Position)     self.ensureSelection(clickedPos)     selection = self.getSelectedBoosters()     mainBooster = None     if clickedPos != -1:         try:             booster = self.boosters[clickedPos]         except IndexError:             pass         else:             <MASK>                 mainBooster = booster     itemContext = None if mainBooster is None else _t(""Booster"")     menu = ContextMenu.getMenu(         self,         mainBooster,         selection,         (""boosterItem"", itemContext),         (""boosterItemMisc"", itemContext),     )     if menu:         self.PopupMenu(menu)",if booster in self . original :,if booster is not None :,15.207218222740094,15.207218222740094,0.0
"def init_errorhandler():     # http error handling     for ex in default_exceptions:         <MASK>             app.register_error_handler(ex, error_http)         elif ex == 500:             app.register_error_handler(ex, internal_error)     if services.ldap:         # Only way of catching the LDAPException upon logging in with LDAP server down         @app.errorhandler(services.ldap.LDAPException)         def handle_exception(e):             log.debug(""LDAP server not accessible while trying to login to opds feed"")             return error_http(FailedDependency())",if ex < 500 :,if ex in 400 :,23.643540225079384,23.643540225079384,0.0
"def reloadCols(self):     self.columns = []     for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr):         <MASK>             t = anytype         elif ""M"" in fmt:             self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i])))             continue         elif ""i"" in fmt:             t = int         elif ""f"" in fmt:             t = float         else:             t = anytype         self.addColumn(ColumnItem(name, i, type=t))",if shape :,"if ""d"" in fmt :",7.809849842300637,7.809849842300637,0.0
"def Proc2(IntParIO):     IntLoc = IntParIO + 10     while True:         if Char1Glob == ""A"":             IntLoc = IntLoc - 1             IntParIO = IntLoc - IntGlob             EnumLoc = Ident1         <MASK>             break     return IntParIO",if EnumLoc == Ident1 :,if EnumLoc == Ident1 :,100.00000000000004,100.00000000000004,1.0
"def opengroup(self, name=None):     gid = self.groups     self.groupwidths.append(None)     if self.groups > MAXGROUPS:         raise error(""too many groups"")     if name is not None:         ogid = self.groupdict.get(name, None)         <MASK>             raise error(                 ""redefinition of group name %r as group %d; ""                 ""was group %d"" % (name, gid, ogid)             )         self.groupdict[name] = gid     return gid",if ogid is not None :,if gid != ogid :,10.682175159905853,10.682175159905853,0.0
"def __setattr__(self, name: str, val: Any):     if name.startswith(""COMPUTED_""):         if name in self:             old_val = self[name]             <MASK>                 return             raise KeyError(                 ""Computed attributed '{}' already exists ""                 ""with a different value! old={}, new={}."".format(name, old_val, val)             )         self[name] = val     else:         super().__setattr__(name, val)",if old_val == val :,if old_val != val :,50.000000000000014,50.000000000000014,0.0
"def get_all_function_symbols(self, module=""kernel""):     """"""Gets all the function tuples for the given module""""""     ret = []     symtable = self.type_map     if module in symtable:         mod = symtable[module]         for (addr, (name, _sym_types)) in mod.items():             <MASK>                 addr = addr + self.shift_address             ret.append([name, addr])     else:         debug.info(""All symbols requested for non-existent module %s"" % module)     return ret",if self . shift_address and addr :,if addr in ret :,6.316906128202129,6.316906128202129,0.0
"def __call__(self, frame: FrameType, event: str, arg: Any) -> ""CallTracer"":     code = frame.f_code     if (         event not in SUPPORTED_EVENTS         or code.co_name == ""trace_types""         or self.should_trace         and not self.should_trace(code)     ):         return self     try:         <MASK>             self.handle_call(frame)         elif event == EVENT_RETURN:             self.handle_return(frame, arg)         else:             logger.error(""Cannot handle event %s"", event)     except Exception:         logger.exception(""Failed collecting trace"")     return self",if event == EVENT_CALL :,if event == EVENT_CALL :,100.00000000000004,100.00000000000004,1.0
"def test_update_topic(self):     async with self.chat_client:         await self._create_thread()         topic = ""update topic""         async with self.chat_thread_client:             await self.chat_thread_client.update_topic(topic=topic)         # delete chat threads         <MASK>             await self.chat_client.delete_chat_thread(self.thread_id)",if not self . is_playback ( ) :,if self . thread_id :,10.759051250985632,10.759051250985632,0.0
"def render_observation(self):     x = self.read_head_position     label = ""Observation Grid    : ""     x_str = """"     for j in range(-1, self.rows + 1):         if j != -1:             x_str += "" "" * len(label)         for i in range(-2, self.input_width + 2):             <MASK>                 x_str += colorize(self._get_str_obs((i, j)), ""green"", highlight=True)             else:                 x_str += self._get_str_obs((i, j))         x_str += ""\n""     x_str = label + x_str     return x_str",if i == x [ 0 ] and j == x [ 1 ] :,if i != - 1 :,3.9580337617475023,3.9580337617475023,0.0
"def build(opt):     dpath = os.path.join(opt[""datapath""], ""QA-ZRE"")     version = None     if not build_data.built(dpath, version_string=version):         print(""[building data: "" + dpath + ""]"")         <MASK>             # An older version exists, so remove these outdated files.             build_data.remove_dir(dpath)         build_data.make_dir(dpath)         # Download the data.         for downloadable_file in RESOURCES:             downloadable_file.download_file(dpath)         # Mark the data as built.         build_data.mark_done(dpath, version_string=version)",if build_data . built ( dpath ) :,if version_string in VERSIONS :,5.630400552901077,5.630400552901077,0.0
"def git_pull(args):     if len(args) <= 1:         repo = _get_repo()         _confirm_dangerous()         url = args[0] if len(args) == 1 else repo.remotes.get(""origin"", """")         <MASK>             origin = url             url = repo.remotes.get(origin)         if url:             repo.pull(origin_uri=url)         else:             print(""No pull URL."")     else:         print(command_help[""git pull""])",if url in repo . remotes :,if origin :,9.138402379955025,0.0,0.0
"def FindAndDelete(script, sig):     """"""Consensus critical, see FindAndDelete() in Satoshi codebase""""""     r = b""""     last_sop_idx = sop_idx = 0     skip = True     for (opcode, data, sop_idx) in script.raw_iter():         <MASK>             r += script[last_sop_idx:sop_idx]         last_sop_idx = sop_idx         if script[sop_idx : sop_idx + len(sig)] == sig:             skip = True         else:             skip = False     if not skip:         r += script[last_sop_idx:]     return CScript(r)",if not skip :,if data [ 0 ] == sig :,5.669791110976001,5.669791110976001,0.0
"def get_ip_info(ipaddress):     """"""Returns device information by IP address""""""     result = {}     try:         ip = IPAddress.objects.select_related().get(address=ipaddress)     except IPAddress.DoesNotExist:         pass     else:         if ip.venture is not None:             result[""venture_id""] = ip.venture.id         if ip.device is not None:             result[""device_id""] = ip.device.id             <MASK>                 result[""venture_id""] = ip.device.venture.id     return result",if ip . device . venture is not None :,if ip . device . eventure is not None :,65.80370064762461,65.80370064762461,0.0
"def restore(self, state):     """"""Restore the state of a mesh previously saved using save()""""""     import pickle     state = pickle.loads(state)     for k in state:         if isinstance(state[k], list):             <MASK>                 state[k] = [[v.x(), v.y(), v.z()] for v in state[k]]             state[k] = np.array(state[k])         setattr(self, k, state[k])","if isinstance ( state [ k ] [ 0 ] , QtGui . QVector3D ) :","if isinstance ( state [ k ] , np . ndarray ) :",44.92903001150463,44.92903001150463,0.0
"def get_extra_lines(tup):     ext_name, pyopencl_ver = tup     if ext_name is not None:         <MASK>             # capital letters -> CL version, not extension             yield """"             yield ""    Available with OpenCL %s."" % (ext_name[3:])             yield """"         else:             yield """"             yield ""    Available with the ``%s`` extension."" % ext_name             yield """"     if pyopencl_ver is not None:         yield """"         yield ""    .. versionadded:: %s"" % pyopencl_ver         yield """"","if ext_name . startswith ( ""CL_"" ) :","if ext_name [ 0 ] == ""OpenCL"" :",22.242469397936766,22.242469397936766,0.0
"def _gen_remote_uri(     fileobj: IO[bytes],     remote_uri: Optional[ParseResult],     remote_path_prefix: Optional[str],     remote_path_suffix: Optional[str],     sha256sum: Optional[str], ) -> ParseResult:     if remote_uri is None:         assert remote_path_prefix is not None and remote_path_suffix is not None         <MASK>             sha256sum = _hash_fileobj(fileobj)         return urlparse(             os.path.join(remote_path_prefix, f""{sha256sum}{remote_path_suffix}"")         )     else:         return remote_uri",if sha256sum is None :,if sha256sum is None :,100.00000000000004,100.00000000000004,1.0
"def queries(self):     if DEV:         cmd = ShellCommand(""docker"", ""ps"", ""-qf"", ""name=%s"" % self.path.k8s)         if not cmd.check(f""docker check for {self.path.k8s}""):             <MASK>                 log_cmd = ShellCommand(                     ""docker"", ""logs"", self.path.k8s, stderr=subprocess.STDOUT                 )                 if log_cmd.check(f""docker logs for {self.path.k8s}""):                     print(cmd.stdout)                 pytest.exit(f""container failed to start for {self.path.k8s}"")     return ()",if not cmd . stdout . strip ( ) :,"if cmd . check ( f""docker check for {self.path.k8s}"" ) :",6.228496954013446,6.228496954013446,0.0
"def get_range(self):     present = self.xml.find(""{%s}range"" % self.namespace)     if present is not None:         attributes = present.attrib         return_value = dict()         <MASK>             return_value[""minimum""] = attributes[""min""]         if ""max"" in attributes:             return_value[""maximum""] = attributes[""max""]         return return_value     return False","if ""min"" in attributes :","if ""min"" in attributes :",100.00000000000004,100.00000000000004,1.0
"def _configuredOn(self, workerid, builderid=None, masterid=None):     cfg = []     for cs in itervalues(self.configured):         <MASK>             continue         bid, mid = self.db.builders.builder_masters[cs[""buildermasterid""]]         if builderid is not None and bid != builderid:             continue         if masterid is not None and mid != masterid:             continue         cfg.append({""builderid"": bid, ""masterid"": mid})     return cfg","if cs [ ""workerid"" ] != workerid :","if cs [ ""workerid"" ] != workerid :",100.00000000000004,100.00000000000004,1.0
"def __exit__(self, type, value, traceback):     try:         if type is not None:             return self.exception_handler(type, value, traceback)     finally:         final_contexts = _state.contexts         _state.contexts = self.old_contexts         <MASK>             raise StackContextInconsistentError(                 ""stack_context inconsistency (may be caused by yield ""                 'within a ""with StackContext"" block)'             )         # Break up a reference to itself to allow for faster GC on CPython.         self.new_contexts = None",if final_contexts is not self . new_contexts :,if final_contexts != self . new_contexts :,63.40466277046863,63.40466277046863,0.0
"def del_(self, key):     initial_hash = hash_ = self.hash(key)     while True:         <MASK>             # That key was never assigned             return None         elif self._keys[hash_] == key:             # key found, assign with deleted sentinel             self._keys[hash_] = self._deleted             self._values[hash_] = self._deleted             self._len -= 1             return         hash_ = self._rehash(hash_)         if initial_hash == hash_:             # table is full and wrapped around             return None",if self . _keys [ hash_ ] is self . _empty :,if hash_ == self . _keys [ hash_ ] :,51.84664169856239,51.84664169856239,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             self.set_logout_url(d.getPrefixedString())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 10 :,if tt == 0 :,53.7284965911771,53.7284965911771,0.0
"def data_generator():     i = 0     max_batch_index = len(X_train) // batch_size     tot = 0     while 1:         <MASK>             yield (                 np.ones([batch_size, input_dim]) * np.nan,                 np.ones([batch_size, num_classes]) * np.nan,             )         else:             yield (                 X_train[i * batch_size : (i + 1) * batch_size],                 y_train[i * batch_size : (i + 1) * batch_size],             )         i += 1         tot += 1         i = i % max_batch_index",if tot > 3 * len ( X_train ) :,if tot > max_batch_index :,14.448814886766836,14.448814886766836,0.0
"def title(self):     ret = theme[""title""]     if isinstance(self.name, six.string_types):         width = self.statwidth()         return (             ret + self.name[0:width].center(width).replace("" "", ""-"") + theme[""default""]         )     for i, name in enumerate(self.name):         width = self.colwidth()         ret = ret + name[0:width].center(width).replace("" "", ""-"")         <MASK>             if op.color:                 ret = ret + theme[""frame""] + char[""dash""] + theme[""title""]             else:                 ret = ret + char[""space""]     return ret",if i + 1 != len ( self . vars ) :,if i == 0 :,6.011598678897526,6.011598678897526,0.0
"def get_container_from_dport(dport, docker_client):     for container in docker_client.containers():         try:             ports = container[""Ports""]             for port in ports:                 <MASK>                     if port[""PublicPort""] == int(dport):                         return container         except KeyError:             print(ports)             pass","if ""PublicPort"" in port :","if port [ ""PublicPort"" ] == int ( dport ) :",12.571192676522521,12.571192676522521,0.0
"def _get_parents_data(self, data):     parents = 0     if data[COLUMN_PARENT]:         family = self.db.get_family_from_handle(data[COLUMN_PARENT][0])         if family.get_father_handle():             parents += 1         <MASK>             parents += 1     return parents",if family . get_mother_handle ( ) :,if family . get_father_handle ( ) :,70.16879391277372,70.16879391277372,0.0
"def wrapper(filename):     mtime = getmtime(filename)     with lock:         if filename in cache:             old_mtime, result = cache.pop(filename)             if old_mtime == mtime:                 # Move to the end                 cache[filename] = old_mtime, result                 return result     result = function(filename)     with lock:         cache[filename] = mtime, result  # at the end         <MASK>             cache.popitem(last=False)     return result",if len ( cache ) > max_size :,if cache [ filename ] == mtime :,5.61480827173619,5.61480827173619,0.0
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""):     try:         pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na)         if op.stage == OperandStage.map:             cls._execute_map(ctx, op)         elif op.stage == OperandStage.combine:             cls._execute_combine(ctx, op)         <MASK>             cls._execute_agg(ctx, op)         else:  # pragma: no cover             raise ValueError(""Aggregation operand not executable"")     finally:         pd.reset_option(""mode.use_inf_as_na"")",elif op . stage == OperandStage . agg :,if op . stage == OperandStage . agg :,88.01117367933934,88.01117367933934,0.0
"def FindAndDelete(script, sig):     """"""Consensus critical, see FindAndDelete() in Satoshi codebase""""""     r = b""""     last_sop_idx = sop_idx = 0     skip = True     for (opcode, data, sop_idx) in script.raw_iter():         if not skip:             r += script[last_sop_idx:sop_idx]         last_sop_idx = sop_idx         <MASK>             skip = True         else:             skip = False     if not skip:         r += script[last_sop_idx:]     return CScript(r)",if script [ sop_idx : sop_idx + len ( sig ) ] == sig :,if data [ 0 ] == sig :,14.301365529665468,14.301365529665468,0.0
"def extractall(zip: typing.Any, path: str) -> NoneType:     for name in zip.namelist():         member = zip.getinfo(name)         extracted_path = zip._extract_member(member, path, None)         attr = member.external_attr >> 16         <MASK>             os.chmod(extracted_path, attr)",if attr != 0 :,if attr != 0 :,100.00000000000004,100.00000000000004,1.0
"def find_all_gyptest_files(directory):     result = []     for root, dirs, files in os.walk(directory):         <MASK>             dirs.remove("".svn"")         result.extend([os.path.join(root, f) for f in files if is_test_name(f)])     result.sort()     return result","if "".svn"" in dirs :",if os . path . isdir ( root ) :,5.522397783539471,5.522397783539471,0.0
"def load(cls, storefile, template_store):     # Did we get file or filename?     if not hasattr(storefile, ""read""):         storefile = open(storefile, ""rb"")     # Adjust store to have translations     store = cls.convertfile(storefile, template_store)     for unit in store.units:         if unit.isheader():             continue         # HTML does this properly on loading, others need it         <MASK>             unit.target = unit.source             unit.rich_target = unit.rich_source     return store",if cls . needs_target_sync :,if unit . ishtml ( ) :,6.495032985064742,6.495032985064742,0.0
"def postOptions(self):     _BasicOptions.postOptions(self)     if self[""jobs""]:         conflicts = [""debug"", ""profile"", ""debug-stacktraces"", ""exitfirst""]         for option in conflicts:             if self[option]:                 raise usage.UsageError(                     ""You can't specify --%s when using --jobs"" % option                 )     if self[""nopm""]:         <MASK>             raise usage.UsageError(""You must specify --debug when using "" ""--nopm "")         failure.DO_POST_MORTEM = False","if not self [ ""debug"" ] :",if self [ option ] :,14.723282228934908,14.723282228934908,0.0
"def filterTokenLocation():     i = None     entry = None     token = None     tokens = []     i = 0     while 1:         if not (i < len(extra.tokens)):             break         entry = extra.tokens[i]         token = jsdict(             {                 ""type"": entry.type,                 ""value"": entry.value,             }         )         if extra.range:             token.range = entry.range         <MASK>             token.loc = entry.loc         tokens.append(token)         i += 1     extra.tokens = tokens",if extra . loc :,if entry . loc :,42.72870063962342,42.72870063962342,0.0
"def on_rebalance_end(self) -> None:     """"""Call when rebalancing is done.""""""     self.rebalancing = False     if self._rebalancing_span:         self._rebalancing_span.finish()     self._rebalancing_span = None     sensor_state = self._rebalancing_sensor_state     try:         <MASK>             self.log.warning(                 ""Missing sensor state for rebalance #%s"", self.rebalancing_count             )         else:             self.sensors.on_rebalance_end(self, sensor_state)     finally:         self._rebalancing_sensor_state = None",if not sensor_state :,if sensor_state is None :,27.77619034011791,27.77619034011791,0.0
"def decorator(request, *args, **kwargs):     if CALENDAR_VIEW_PERM:         user = request.user         if not user:             return HttpResponseRedirect(settings.LOGIN_URL)         occurrence, event, calendar = get_objects(request, **kwargs)         if calendar:             allowed = CHECK_CALENDAR_PERM_FUNC(calendar, user)             <MASK>                 return HttpResponseRedirect(settings.LOGIN_URL)             # all checks passed             return function(request, *args, **kwargs)         return HttpResponseNotFound(""<h1>Page not found</h1>"")     return function(request, *args, **kwargs)",if not allowed :,if not allowed :,100.00000000000004,100.00000000000004,1.0
"def reduce_arguments(self, args):     assert isinstance(args, nodes.Arguments)     if args.incorrect_order():         raise InvalidArguments(             ""All keyword arguments must be after positional arguments.""         )     reduced_pos = [self.reduce_single(arg) for arg in args.arguments]     reduced_kw = {}     for key in args.kwargs.keys():         <MASK>             raise InvalidArguments(""Keyword argument name is not a string."")         a = args.kwargs[key]         reduced_kw[key] = self.reduce_single(a)     return (reduced_pos, reduced_kw)","if not isinstance ( key , str ) :","if not isinstance ( key , string_types ) :",51.93071778680675,51.93071778680675,0.0
"def _encode(n, nbytes, little_endian=False):     retval = []     n = long(n)     for i in range(nbytes):         <MASK>             retval.append(chr(n & 0xFF))         else:             retval.insert(0, chr(n & 0xFF))         n >>= 8     return """".join(retval)",if little_endian :,if little_endian :,100.00000000000004,100.00000000000004,1.0
"def copy_shell(self):     cls = self.__class__     old_id = cls.id     new_i = cls()  # create a new group     new_i.id = self.id  # with the same id     cls.id = old_id  # Reset the Class counter     # Copy all properties     for prop in cls.properties:         if prop is not ""members"":             <MASK>                 val = getattr(self, prop)                 setattr(new_i, prop, val)     # but no members     new_i.members = []     return new_i",if self . has ( prop ) :,if prop in new_i . members :,6.742555929751843,6.742555929751843,0.0
"def dataspec(config):     master = yield fakemaster.make_master()     data = connector.DataConnector()     data.setServiceParent(master)     if config[""out""] != ""--"":         dirs = os.path.dirname(config[""out""])         <MASK>             os.makedirs(dirs)         f = open(config[""out""], ""w"")     else:         f = sys.stdout     if config[""global""] is not None:         f.write(""window."" + config[""global""] + ""="")     f.write(json.dumps(data.allEndpoints(), indent=2))     f.close()     defer.returnValue(0)",if dirs and not os . path . exists ( dirs ) :,if dirs :,2.247320757600649,0.0,0.0
"def _parseSCDOCDC(self, src):     """"""[S|CDO|CDC]*""""""     while 1:         src = src.lstrip()         <MASK>             src = src[4:]         elif src.startswith(""-->""):             src = src[3:]         else:             break     return src","if src . startswith ( ""<!--"" ) :","if src . startswith ( ""-->"" ) :",55.097857671324185,55.097857671324185,0.0
"def command(filenames, dirnames, fix):     for filename in gather_files(dirnames, filenames):         visitor = process_file(filename)         if visitor.needs_fix():             print(""%s: %s"" % (filename, visitor.get_stats()))             <MASK>                 print(""Fixing: %s"" % filename)                 fix_file(filename)",if fix :,if fix :,100.00000000000004,0.0,1.0
"def shutdown(self):     """"""Shutdown host system.""""""     self._check_dbus(MANAGER)     use_logind = self.sys_dbus.logind.is_connected     _LOGGER.info(""Initialize host power off %s"", ""logind"" if use_logind else ""systemd"")     try:         await self.sys_core.shutdown()     finally:         <MASK>             await self.sys_dbus.logind.power_off()         else:             await self.sys_dbus.systemd.power_off()",if use_logind :,if use_logind :,100.00000000000004,100.00000000000004,1.0
"def _run_split_on_punc(self, text, never_split=None):     """"""Splits punctuation on a piece of text.""""""     if never_split is not None and text in never_split:         return [text]     chars = list(text)     i = 0     start_new_word = True     output = []     while i < len(chars):         char = chars[i]         if _is_punctuation(char):             output.append([char])             start_new_word = True         else:             <MASK>                 output.append([])             start_new_word = False             output[-1].append(char)         i += 1     return ["""".join(x) for x in output]",if start_new_word :,if start_new_word :,100.00000000000004,100.00000000000004,1.0
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout):     try:         if tp == ""write"":             out.write(msg)         <MASK>             out.flush()         elif tp == ""write_flush"":             out.write(msg)             out.flush()         elif tp == ""print"":             print(msg, file=out)         else:             raise ValueError(""Unsupported type: "" + tp)     except IOError as e:         logger.critical(""{}: {}"".format(type(e).__name__, ucd(e)))         pass","elif tp == ""flush"" :","if tp == ""flush"" :",84.08964152537145,84.08964152537145,0.0
"def checkClassDeclation(file):     localResult = []     with open(file, ""rb"") as f:         lineNumber = 0         for line in f:             m = re.search(""class\s+[^\(]*:"", line)             <MASK>                 localResult.append(                     ""Old class definition found on {0}"".format(m.group())                 )     return localResult",if m :,if m :,100.00000000000004,0.0,1.0
"def _evaluate_local_single(self, iterator):     for batch in iterator:         in_arrays = convert._call_converter(self.converter, batch, self.device)         with function.no_backprop_mode():             if isinstance(in_arrays, tuple):                 results = self.calc_local(*in_arrays)             <MASK>                 results = self.calc_local(**in_arrays)             else:                 results = self.calc_local(in_arrays)         if self._progress_hook:             self._progress_hook(batch)         yield results","elif isinstance ( in_arrays , dict ) :",if self . _progress_hook :,5.11459870708889,5.11459870708889,0.0
"def check_billing_view(user, permission, obj):     if hasattr(obj, ""all_projects""):         <MASK>             return True         # This is a billing object         return any(check_permission(user, permission, prj) for prj in obj.all_projects)     return check_permission(user, permission, obj)",if user . is_superuser or obj . owners . filter ( pk = user . pk ) . exists ( ) :,if not obj . all_projects :,1.8795492934569615,1.8795492934569615,0.0
"def ensure_output_spaces_contain_the_same_data(self, y, y_ensured):     stride = y.shape[1]     self.assertEqual(y.shape[0] * y.shape[1], y_ensured.shape[0])     self.assertEqual(len(y_ensured.shape), 1)     for row in range(y.shape[0]):         for column in range(y.shape[1]):             <MASK>                 self.assertEqual(y[row, column], y_ensured[row * stride + column])             else:                 self.assertEqual(y[row][column], y_ensured[row * stride + column])",if sp . issparse ( y ) :,if row == stride :,6.916271812933183,6.916271812933183,0.0
"def train(     self,     training_data: TrainingData,     config: Optional[RasaNLUModelConfig] = None,     **kwargs: Any, ) -> None:     """"""Tokenize all training data.""""""     for example in training_data.training_examples:         for attribute in MESSAGE_ATTRIBUTES:             if example.get(attribute) is not None and not example.get(attribute) == """":                 <MASK>                     tokens = self._split_name(example, attribute)                 else:                     tokens = self.tokenize(example, attribute)                 example.set(TOKENS_NAMES[attribute], tokens)","if attribute in [ INTENT , ACTION_NAME , INTENT_RESPONSE_KEY ] :","if TOKENS_NAMES [ attribute ] == """" :",3.494078636316037,3.494078636316037,0.0
"def refresh_token(self, strategy, *args, **kwargs):     token = self.extra_data.get(""refresh_token"") or self.extra_data.get(""access_token"")     backend = self.get_backend(strategy)     if token and backend and hasattr(backend, ""refresh_token""):         backend = backend(strategy=strategy)         response = backend.refresh_token(token, *args, **kwargs)         extra_data = backend.extra_data(self, self.uid, response, self.extra_data)         <MASK>             self.save()",if self . set_extra_data ( extra_data ) :,if extra_data :,7.468220329575271,7.468220329575271,0.0
"def _verify_environ(_collected_environ):     try:         yield     finally:         new_environ = dict(os.environ)         current_test = new_environ.pop(""PYTEST_CURRENT_TEST"", None)         old_environ = dict(_collected_environ)         old_environ.pop(""PYTEST_CURRENT_TEST"", None)         <MASK>             raise DirtyTest(                 ""Left over environment variables"",                 current_test,                 _compare_eq_dict(new_environ, old_environ, verbose=2),             )",if new_environ != old_environ :,if current_test != old_test :,30.213753973567687,30.213753973567687,0.0
"def clean_len(self, line):     """"""Calculate wisible length of string""""""     if isinstance(line, basestring):         return len(self.screen.markup.clean_markup(line))     elif isinstance(line, tuple) or isinstance(line, list):         markups = self.screen.markup.get_markup_vars()         length = 0         for i in line:             <MASK>                 length += len(i)         return length",if i not in markups :,if i in markups :,40.93653765389909,40.93653765389909,0.0
"def _build_merged_dataset_args(datasets):     merged_dataset_args = []     for dataset in datasets:         dataset_code_column = _parse_dataset_code(dataset)         arg = dataset_code_column[""code""]         column_index = dataset_code_column[""column_index""]         <MASK>             arg = (dataset_code_column[""code""], {""column_index"": [column_index]})         merged_dataset_args.append(arg)     return merged_dataset_args",if column_index is not None :,if column_index :,38.80684294761701,38.80684294761701,0.0
"def update_watch_data_table_paths(self):     if hasattr(self.tool_data_watcher, ""monitored_dirs""):         for tool_data_table_path in self.tool_data_paths:             <MASK>                 self.tool_data_watcher.watch_directory(tool_data_table_path)",if tool_data_table_path not in self . tool_data_watcher . monitored_dirs :,if tool_data_table_path not in self . monitored_dirs :,64.7076654809797,64.7076654809797,0.0
"def getsource(obj):     """"""Wrapper around inspect.getsource""""""     try:         try:             src = encoding.to_unicode(inspect.getsource(obj))         except TypeError:             <MASK>                 src = encoding.to_unicode(inspect.getsource(obj.__class__))             else:                 # Bindings like VTK or ITK require this case                 src = getdoc(obj)         return src     except (TypeError, IOError):         return","if hasattr ( obj , ""__class__"" ) :","if isinstance ( obj , ( str , unicode ) ) :",14.043459416399545,14.043459416399545,0.0
"def __iter__(self):     for model in self.app_config.get_models():         admin_model = AdminModel(model, **self.options)         for model_re in self.model_res:             if model_re.search(admin_model.name):                 break         else:             <MASK>                 continue         yield admin_model",if self . model_res :,if not admin_model . is_active ( ) :,5.063996506781411,5.063996506781411,0.0
"def run(self):     while True:         try:             with DelayedKeyboardInterrupt():                 raw_inputs = self._parent_task_queue.get()                 if self._has_stop_signal(raw_inputs):                     self._rq.put(raw_inputs, block=True)                     break                 if self._flow_type == BATCH:                     self._rq.put(raw_inputs, block=True)                 <MASK>                     try:                         self._rq.put(raw_inputs, block=False)                     except:                         pass         except KeyboardInterrupt:             continue",elif self . _flow_type == REALTIME :,if self . _flow_type == STOP :,69.89307622784945,69.89307622784945,0.0
"def dump(self):     self.ql.log.info(""[*] Dumping object: %s"" % (self.sf_name))     for field in self._fields_:         <MASK>             self.ql.log.info(""%s: 0x%x"" % (field[0], getattr(self, field[0]).value))         elif isinstance(getattr(self, field[0]), int):             self.ql.log.info(""%s: %d"" % (field[0], getattr(self, field[0])))         elif isinstance(getattr(self, field[0]), bytes):             self.ql.log.info(""%s: %s"" % (field[0], getattr(self, field[0]).decode()))","if isinstance ( getattr ( self , field [ 0 ] ) , POINTER64 ) :","if isinstance ( field , ( int , bytes ) ) :",13.506843794103194,13.506843794103194,0.0
"def validate_configuration(self, configuration: Optional[ExpectationConfiguration]):     """"""Validating that user has inputted a value set and that configuration has been initialized""""""     super().validate_configuration(configuration)     try:         assert ""value_set"" in configuration.kwargs, ""value_set is required""         assert isinstance(             configuration.kwargs[""value_set""], (list, set, dict)         ), ""value_set must be a list or a set""         <MASK>             assert (                 ""$PARAMETER"" in configuration.kwargs[""value_set""]             ), 'Evaluation Parameter dict for value_set kwarg must have ""$PARAMETER"" key'     except AssertionError as e:         raise InvalidExpectationConfigurationError(str(e))     return True","if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :","if ""$PARAMETER"" in configuration . kwargs :",9.586514611843183,9.586514611843183,0.0
def test_one_dead_branch():     with deterministic_PRNG():         seen = set()         @run_to_buffer         def x(data):             i = data.draw_bytes(1)[0]             if i > 0:                 data.mark_invalid()             i = data.draw_bytes(1)[0]             if len(seen) < 255:                 seen.add(i)             <MASK>                 data.mark_interesting(),elif i not in seen :,if i < 0 :,10.400597689005304,10.400597689005304,0.0
"def __on_item_activated(self, event):     if self.__module_view:         module = self.get_event_module(event)         self.__module_view.set_selection(module.module_num)         if event.EventObject is self.list_ctrl:             self.input_list_ctrl.deactivate_active_item()         else:             self.list_ctrl.deactivate_active_item()             for index in range(self.list_ctrl.GetItemCount()):                 <MASK>                     self.list_ctrl.Select(index, False)     self.__controller.enable_module_controls_panel_buttons()",if self . list_ctrl . IsSelected ( index ) :,if self . list_ctrl . GetItem ( index ) == 0 :,50.389204852596336,50.389204852596336,0.0
"def prime(self, callback):     <MASK>         # import pdb         # pdb.set_trace()         self.cbhdl = simulator.register_rwsynch_callback(callback, self)         if self.cbhdl is None:             raise_error(self, ""Unable set up %s Trigger"" % (str(self)))     Trigger.prime(self)",if self . cbhdl is None :,if self . cbhdl is None :,100.00000000000004,100.00000000000004,1.0
"def fstab_configuration(middleware):     for command in (         [             [""systemctl"", ""daemon-reload""],             [""systemctl"", ""restart"", ""local-fs.target""],         ]         if osc.IS_LINUX         else [[""mount"", ""-uw"", ""/""]]     ):         ret = subprocess.run(command, capture_output=True)         <MASK>             middleware.logger.debug(                 f'Failed to execute ""{"" "".join(command)}"": {ret.stderr.decode()}'             )",if ret . returncode :,if ret . stderr :,42.72870063962342,42.72870063962342,0.0
"def _generate_table(self, fromdesc, todesc, diffs):     if fromdesc or todesc:         yield (             simple_colorize(fromdesc, ""description""),             simple_colorize(todesc, ""description""),         )     for i, line in enumerate(diffs):         <MASK>             # mdiff yields None on separator lines; skip the bogus ones             # generated for the first line             if i > 0:                 yield (                     simple_colorize(""---"", ""separator""),                     simple_colorize(""---"", ""separator""),                 )         else:             yield line",if line is None :,if i > 0 :,12.703318703865365,12.703318703865365,0.0
"def update_completion(self):     """"""Update completion model with exist tags""""""     orig_text = self.widget.text()     text = "", "".join(orig_text.replace("", "", "","").split("","")[:-1])     tags = []     for tag in self.tags_list:         if "","" in orig_text:             <MASK>                 tags.append(""%s,%s"" % (text, tag))             tags.append(""%s, %s"" % (text, tag))         else:             tags.append(tag)     if tags != self.completer_model.stringList():         self.completer_model.setStringList(tags)","if orig_text [ - 1 ] not in ( "","" , "" "" ) :",if tag in self . completer_model . stringList :,2.561168646726282,2.561168646726282,0.0
"def cart_number_checksum_validation(cls, number):     digits = []     even = False     if not number.isdigit():         return False     for digit in reversed(number):         digit = ord(digit) - ord(""0"")         <MASK>             digit *= 2             if digit >= 10:                 digit = digit % 10 + digit // 10         digits.append(digit)         even = not even     return sum(digits) % 10 == 0 if digits else False",if even :,if even :,100.00000000000004,0.0,1.0
"def __get_param_string__(params):     params_string = []     for key in sorted(params.keys()):         <MASK>             return         value = params[key]         params_string.append("""" if value == ""null"" else str(value))     return ""|"".join(params_string)","if ""REFUND"" in params [ key ] or ""|"" in params [ key ] :","if key == ""null"" :",2.0879268276081806,2.0879268276081806,0.0
"def _map_handlers(self, session, event_class, mapfn):     for event in DOC_EVENTS:         event_handler_name = event.replace(""-"", ""_"")         <MASK>             event_handler = getattr(self, event_handler_name)             format_string = DOC_EVENTS[event]             num_args = len(format_string.split(""."")) - 2             format_args = (event_class,) + (""*"",) * num_args             event_string = event + format_string % format_args             unique_id = event_class + event_handler_name             mapfn(event_string, event_handler, unique_id)","if hasattr ( self , event_handler_name ) :",if event_handler_name in DOC_EVENTS :,34.84694488743309,34.84694488743309,0.0
"def _create_param_lr(self, param_and_grad):     # create learning rate variable for every parameter     param = param_and_grad[0]     param_lr = param.optimize_attr[""learning_rate""]     if type(param_lr) == Variable:         return param_lr     else:         <MASK>             return self._global_learning_rate()         else:             with default_main_program()._lr_schedule_guard(                 is_with_opt=True             ), framework.name_scope(""scale_with_param_lr""):                 return self._global_learning_rate() * param_lr",if param_lr == 1.0 :,if type ( param_lr ) == float :,19.081654556856684,19.081654556856684,0.0
"def __getitem__(self, key):     try:         return self._clsmap[key]     except KeyError as e:         <MASK>             self._mutex.acquire()             try:                 if not self.initialized:                     self._init()                     self.initialized = True                 return self._clsmap[key]             finally:                 self._mutex.release()         raise e",if not self . initialized :,if self . initialized :,57.89300674674101,57.89300674674101,0.0
"def save(self, force=False):     if not force:         <MASK>             return         if time.time() - self.last_save_time < 10:             return     with self.lock:         with open(self.file_path, ""w"") as fd:             for ip in self.cache:                 record = self.cache[ip]                 rule = record[""r""]                 connect_time = record[""c""]                 update_time = record[""update""]                 fd.write(""%s %s %d %d\n"" % (ip, rule, connect_time, update_time))     self.last_save_time = time.time()     self.need_save = False",if not self . need_save :,if self . need_save :,72.89545183625967,72.89545183625967,0.0
"def pick(items, sel):     for x, s in zip(items, sel):         <MASK>             yield x         elif not x.is_atom() and not s.is_atom():             yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)",if match ( s ) :,if not s . is_atom ( ) :,10.552670315936318,10.552670315936318,0.0
"def isValidFloat(config_param_name, value, constraints):     if isinstance(value, float):         constraints.setdefault(""min"", MIN_VALID_FLOAT_VALUE)         constraints.setdefault(""max"", MAX_VALID_FLOAT_VALUE)         minv = float(constraints.get(""min""))         maxv = float(constraints.get(""max""))         <MASK>             if value <= maxv:                 return value     raise FloatValueError(config_param_name, value, constraints)",if value >= minv :,if value >= minv :,100.00000000000004,100.00000000000004,1.0
"def get_files(d):     f = []     for root, dirs, files in os.walk(d):         for name in files:             if ""meta-environment"" in root or ""cross-canadian"" in root:                 continue             <MASK>                 continue             if ""do_build"" not in name and ""do_populate_sdk"" not in name:                 f.append(os.path.join(root, name))     return f","if ""qemux86copy-"" in root or ""qemux86-"" in root :","if ""do_build"" in name :",8.181017927901259,8.181017927901259,0.0
"def __get_photo(self, person_or_marriage):     """"""returns the first photo in the media list or None""""""     media_list = person_or_marriage.get_media_list()     for media_ref in media_list:         media_handle = media_ref.get_reference_handle()         media = self.database.get_media_from_handle(media_handle)         mime_type = media.get_mime_type()         <MASK>             return media     return None","if mime_type and mime_type . startswith ( ""image"" ) :","if mime_type == ""photo"" :",16.581659750776073,16.581659750776073,0.0
"def filter(this, args):     array = to_object(this, args.space)     callbackfn = get_arg(args, 0)     arr_len = js_arr_length(array)     if not is_callable(callbackfn):         raise MakeError(""TypeError"", ""callbackfn must be a function"")     _this = get_arg(args, 1)     k = 0     res = []     while k < arr_len:         if array.has_property(unicode(k)):             kValue = array.get(unicode(k))             <MASK>                 res.append(kValue)         k += 1     return args.space.ConstructArray(res)","if to_boolean ( callbackfn . call ( _this , ( kValue , float ( k ) , array ) ) ) :",if kValue is not None :,0.45018791827775173,0.45018791827775173,0.0
"def optimize(self, graph: Graph):     for v in graph.inputs:         if not v.has_attribute(SplitTarget):             continue         <MASK>             DumpGraph().optimize(graph)         raise NotImplementedError(             f""Input Variable {v} is too large to handle in WebGL backend""         )     return graph, False",if flags . DEBUG :,if not v . has_attribute ( DumpGraph ) :,4.9323515694897075,4.9323515694897075,0.0
"def detach_volume(self, volume):     # We need to find the node using this volume     for node in self.list_nodes():         if type(node.image) is not list:             # This node has only one associated image. It is not the one we             # are after.             continue         for disk in node.image:             <MASK>                 # Node found. We can now detach the volume                 disk_id = disk.extra[""disk_id""]                 return self._do_detach_volume(node.id, disk_id)     return False",if disk . id == volume . id :,if disk . id == volume . id :,100.00000000000004,100.00000000000004,1.0
"def Yield(value, level=1):     g = greenlet.getcurrent()     while level != 0:         if not isinstance(g, genlet):             raise RuntimeError(""yield outside a genlet"")         <MASK>             g.parent.set_child(g)         g = g.parent         level -= 1     g.switch(value)",if level > 1 :,if g . parent :,12.703318703865365,12.703318703865365,0.0
"def get_all_pipeline_nodes(     pipeline: pipeline_pb2.Pipeline, ) -> List[pipeline_pb2.PipelineNode]:     """"""Returns all pipeline nodes in the given pipeline.""""""     result = []     for pipeline_or_node in pipeline.nodes:         which = pipeline_or_node.WhichOneof(""node"")         # TODO(goutham): Handle sub-pipelines.         # TODO(goutham): Handle system nodes.         <MASK>             result.append(pipeline_or_node.pipeline_node)         else:             raise NotImplementedError(""Only pipeline nodes supported."")     return result","if which == ""pipeline_node"" :","if which == ""system"" :",46.307771619910305,46.307771619910305,0.0
"def __init__(self, **settings):     default_settings = self.get_default_settings()     for name, value in default_settings.items():         if not hasattr(self, name):             setattr(self, name, value)     for name, value in settings.items():         <MASK>             raise ImproperlyConfigured(                 ""Invalid setting '{}' for {}"".format(                     name,                     self.__class__.__name__,                 )             )         setattr(self, name, value)",if name not in default_settings :,"if not hasattr ( self , name ) :",6.742555929751843,6.742555929751843,0.0
"def _check_choice(self):     if self.type == ""choice"":         <MASK>             raise OptionError(""must supply a list of choices for type 'choice'"", self)         elif type(self.choices) not in (types.TupleType, types.ListType):             raise OptionError(                 ""choices must be a list of strings ('%s' supplied)""                 % str(type(self.choices)).split(""'"")[1],                 self,             )     elif self.choices is not None:         raise OptionError(""must not supply choices for type %r"" % self.type, self)",if self . choices is None :,if self . choices is not None :,59.4603557501361,59.4603557501361,0.0
"def prepare(self, size=None):     if _is_seekable(self.file):         start_pos = self.file.tell()         self.file.seek(0, 2)         end_pos = self.file.tell()         self.file.seek(start_pos)         fsize = end_pos - start_pos         <MASK>             self.remain = fsize         else:             self.remain = min(fsize, size)     return self.remain",if size is None :,if size is None :,100.00000000000004,100.00000000000004,1.0
"def _setSitemapTargets():     if not conf.sitemapUrl:         return     infoMsg = ""parsing sitemap '%s'"" % conf.sitemapUrl     logger.info(infoMsg)     found = False     for item in parseSitemap(conf.sitemapUrl):         <MASK>             found = True             kb.targets.add((item.strip(), None, None, None, None))     if not found and not conf.forms and not conf.crawlDepth:         warnMsg = ""no usable links found (with GET parameters)""         logger.warn(warnMsg)","if re . match ( r""[^ ]+\?(.+)"" , item , re . I ) :",if item . strip ( ) == conf . links :,1.7153332687076943,1.7153332687076943,0.0
"def test_CY_decomposition(self, tol):     """"""Tests that the decomposition of the CY gate is correct""""""     op = qml.CY(wires=[0, 1])     res = op.decomposition(op.wires)     mats = []     for i in reversed(res):         <MASK>             mats.append(np.kron(i.matrix, np.eye(2)))         else:             mats.append(i.matrix)     decomposed_matrix = np.linalg.multi_dot(mats)     assert np.allclose(decomposed_matrix, op.matrix, atol=tol, rtol=0)",if len ( i . wires ) == 1 :,if i . ndim == 2 :,11.880509436980429,11.880509436980429,0.0
"def _line_ranges(statements, lines):     """"""Produce a list of ranges for `format_lines`.""""""     statements = sorted(statements)     lines = sorted(lines)     pairs = []     start = None     lidx = 0     for stmt in statements:         if lidx >= len(lines):             break         <MASK>             lidx += 1             if not start:                 start = stmt             end = stmt         elif start:             pairs.append((start, end))             start = None     if start:         pairs.append((start, end))     return pairs",if stmt == lines [ lidx ] :,if not end :,5.4424142191183185,5.4424142191183185,0.0
"def init_params(net):     """"""Init layer parameters.""""""     for module in net.modules():         if isinstance(module, nn.Conv2d):             init.kaiming_normal(module.weight, mode=""fan_out"")             <MASK>                 init.constant(module.bias, 0)         elif isinstance(module, nn.BatchNorm2d):             init.constant(module.weight, 1)             init.constant(module.bias, 0)         elif isinstance(module, nn.Linear):             init.normal(module.weight, std=1e-3)             if module.bias:                 init.constant(module.bias, 0)",if module . bias :,if module . bias :,100.00000000000004,100.00000000000004,1.0
"def _get_directory_size_in_bytes(directory):     total = 0     try:         for entry in os.scandir(directory):             <MASK>                 # if it's a file, use stat() function                 total += entry.stat().st_size             elif entry.is_dir():                 # if it's a directory, recursively call this function                 total += _get_directory_size_in_bytes(entry.path)     except NotADirectoryError:         # if `directory` isn't a directory, get the file size then         return os.path.getsize(directory)     except PermissionError:         # if for whatever reason we can't open the folder, return 0         return 0     return total",if entry . is_file ( ) :,if entry . isFile ( ) :,30.895757752065407,30.895757752065407,0.0
"def run_cmd(self, util, to, always_push_mark=False):     if to == ""bof"":         util.push_mark_and_goto_position(0)     elif to == ""eof"":         util.push_mark_and_goto_position(self.view.size())     elif to in (""eow"", ""bow""):         visible = self.view.visible_region()         pos = visible.a if to == ""bow"" else visible.b         <MASK>             util.push_mark_and_goto_position(pos)         else:             util.set_cursors([sublime.Region(pos)])",if always_push_mark :,if always_push_mark :,100.00000000000004,100.00000000000004,1.0
"def parse_results(cwd):     optimal_dd = None     optimal_measure = numpy.inf     for tup in tools.find_conf_files(cwd):         dd = tup[1]         if ""results.train_y_misclass"" in dd:             if dd[""results.train_y_misclass""] < optimal_measure:                 optimal_measure = dd[""results.train_y_misclass""]                 optimal_dd = dd     print(""Optimal results.train_y_misclass:"", str(optimal_measure))     for key, value in optimal_dd.items():         <MASK>             print(key + "": "" + str(value))","if ""hyper_parameters"" in key :","if ""results.train_y_misclass"" in dd :",9.669265690880861,9.669265690880861,0.0
"def clean_vc_position(self):     vc_position = self.cleaned_data[""vc_position""]     if self.validate_vc_position:         conflicting_members = Device.objects.filter(             virtual_chassis=self.instance.virtual_chassis, vc_position=vc_position         )         <MASK>             raise forms.ValidationError(                 ""A virtual chassis member already exists in position {}."".format(                     vc_position                 )             )     return vc_position",if conflicting_members . exists ( ) :,if vc_position in conflicting_members :,20.164945583740657,20.164945583740657,0.0
"def cal_pads(auto_pad, pad_shape):     spatial_size = len(pad_shape)     pads = [0] * spatial_size * 2     for i in range(spatial_size):         if auto_pad == ""SAME_LOWER"":             pads[i + spatial_size] = pad_shape[i] // 2             pads[i] = pad_shape[i] - pads[i + spatial_size]         <MASK>             pads[i] = pad_shape[i] // 2             pads[i + spatial_size] = pad_shape[i] - pads[i]     return pads","elif auto_pad == ""SAME_UPPER"" :","if auto_pad == ""SAME_UPPER"" :",90.36020036098445,90.36020036098445,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             length = d.getVarInt32()             tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)             d.skip(length)             self.add_presence_response().TryMerge(tmp)             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 10 :,if tt == 0 :,53.7284965911771,53.7284965911771,0.0
"def test_cwl_rnaseq(self, install_test_files):     with install_cwl_test_files() as work_dir:         with utils.chdir(os.path.join(work_dir, ""rnaseq"")):             <MASK>                 shutil.rmtree(""cromwell_work"")             subprocess.check_call(                 [""bcbio_vm.py"", ""cwlrun"", ""cromwell"", ""rnaseq-workflow""]             )","if os . path . exists ( ""cromwell_work"" ) :",if os . path . isdir ( work_dir ) :,33.743784579663036,33.743784579663036,0.0
"def files_per_version(self):     xpath = ""./files/file""     files = self.root.findall(xpath)     versions = {}     for file in files:         vfile = file.findall(""version"")         for version in vfile:             nb = version.attrib[""nb""]             <MASK>                 versions[nb] = []             versions[nb].append(file.attrib[""url""])     return versions",if not nb in versions :,if nb not in versions :,35.930411196308434,35.930411196308434,0.0
"def value_to_db_datetime(self, value):     if value is None:         return None     # SQLite doesn't support tz-aware datetimes     if timezone.is_aware(value):         <MASK>             value = value.astimezone(timezone.utc).replace(tzinfo=None)         else:             raise ValueError(                 ""SQLite backend does not support timezone-aware datetimes when USE_TZ is False.""             )     return six.text_type(value)",if settings . USE_TZ :,if USE_TZ :,47.39878501170795,47.39878501170795,0.0
"def _toplevelTryFunc(func, *args, status=status, **kwargs):     with ThreadProfiler(threading.current_thread()) as prof:         t = threading.current_thread()         t.name = func.__name__         try:             t.status = func(*args, **kwargs)         except EscapeException as e:  # user aborted             t.status = ""aborted by user""             <MASK>                 status(""%s aborted"" % t.name, priority=2)         except Exception as e:             t.exception = e             t.status = ""exception""             vd.exceptionCaught(e)         if t.sheet:             t.sheet.currentThreads.remove(t)",if status :,"if t . status == ""aborted"" :",5.522397783539471,5.522397783539471,0.0
"def ESP(phrase):     for num, name in enumerate(devname):         if name.lower() in phrase:             dev = devid[num]             <MASK>                 ctrl = ""=ON""                 say(""Turning On "" + name)             elif custom_action_keyword[""Dict""][""Off""] in phrase:                 ctrl = ""=OFF""                 say(""Turning Off "" + name)             rq = requests.head(""https://"" + ip + dev + ctrl, verify=False)","if custom_action_keyword [ ""Dict"" ] [ ""On"" ] in phrase :","if custom_action_keyword [ ""On"" ] in phrase :",68.31981105541473,68.31981105541473,0.0
"def _table_schema(self, table):     rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall()     # Build list of fields from table information     result = {}     for _, name, data_type, not_null, _, primary_key in rows:         parts = [data_type]         <MASK>             parts.append(""PRIMARY KEY"")         if not_null:             parts.append(""NOT NULL"")         result[name] = "" "".join(parts)     return result",if primary_key :,if primary_key :,100.00000000000004,100.00000000000004,1.0
"def _validate_forward_input(x, n_in):     if n_in != 1:         if not isinstance(x, (tuple, list)):             raise TypeError(                 f""Expected input to be a tuple or list; instead got {type(x)}.""             )         <MASK>             raise ValueError(                 f""Input tuple length ({len(x)}) does not equal required ""                 f""number of inputs ({n_in}).""             )",if len ( x ) != n_in :,if len ( x ) != n_in :,100.00000000000004,100.00000000000004,1.0
"def _table_reprfunc(self, row, col, val):     if self._table.column_names[col].endswith(""Size""):         if isinstance(val, compat.string_types):             return ""  %s"" % val         <MASK>             return ""  %.1f KB"" % (val / 1024.0 ** 1)         elif val < 1024 ** 3:             return ""  %.1f MB"" % (val / 1024.0 ** 2)         else:             return ""  %.1f GB"" % (val / 1024.0 ** 3)     if col in (0, """"):         return str(val)     else:         return ""  %s"" % val",elif val < 1024 ** 2 :,if val < 1024 ** 1 :,54.10822690539397,54.10822690539397,0.0
"def get_path_name(self):     if self.is_root():         return ""@"" + self.name     else:         parent_name = self.parent.get_path_name()         <MASK>             return ""/"".join([parent_name, ""@"" + self.name])         else:             return ""@"" + self.name",if parent_name :,if parent_name :,100.00000000000004,100.00000000000004,1.0
"def parse(cls, api, json):     lst = List(api)     setattr(lst, ""_json"", json)     for k, v in json.items():         <MASK>             setattr(lst, k, User.parse(api, v))         elif k == ""created_at"":             setattr(lst, k, parse_datetime(v))         else:             setattr(lst, k, v)     return lst","if k == ""user"" :","if k == ""user"" :",100.00000000000004,100.00000000000004,1.0
"def _bytecode_filenames(self, py_filenames):     bytecode_files = []     for py_file in py_filenames:         if not py_file.endswith("".py""):             continue         <MASK>             bytecode_files.append(py_file + ""c"")         if self.optimize > 0:             bytecode_files.append(py_file + ""o"")     return bytecode_files",if self . compile :,if self . optimize > 0 :,26.269098944241588,26.269098944241588,0.0
"def to_json_dict(self):     d = super().to_json_dict()     d[""bullet_list""] = RenderedContent.rendered_content_list_to_json(self.bullet_list)     if self.header is not None:         <MASK>             d[""header""] = self.header.to_json_dict()         else:             d[""header""] = self.header     if self.subheader is not None:         if isinstance(self.subheader, RenderedContent):             d[""subheader""] = self.subheader.to_json_dict()         else:             d[""subheader""] = self.subheader     return d","if isinstance ( self . header , RenderedContent ) :","if isinstance ( self . header , RenderedContent ) :",100.00000000000004,100.00000000000004,1.0
"def makeSomeFiles(pathobj, dirdict):     pathdict = {}     for (key, value) in dirdict.items():         child = pathobj.child(key)         if isinstance(value, bytes):             pathdict[key] = child             child.setContent(value)         <MASK>             child.createDirectory()             pathdict[key] = makeSomeFiles(child, value)         else:             raise ValueError(""only strings and dicts allowed as values"")     return pathdict","elif isinstance ( value , dict ) :","if isinstance ( value , dict ) :",84.08964152537145,84.08964152537145,0.0
"def Restore(self):     picker, obj = self._window, self._pObject     value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH)     if value is not None:         <MASK>             if type(value) == list:                 value = value[-1]         picker.SetPath(value)         return True     return False","if issubclass ( picker . __class__ , wx . FileDialog ) :",if type ( value ) == str :,3.097704314134564,3.097704314134564,0.0
"def recv(self, buffer_size):     try:         return super(SSLConnection, self).recv(buffer_size)     except ssl.SSLError as err:         <MASK>             return b""""         if err.args[0] in (ssl.SSL_ERROR_EOF, ssl.SSL_ERROR_ZERO_RETURN):             self.handle_close()             return b""""         raise","if err . args [ 0 ] in ( ssl . SSL_ERROR_WANT_READ , ssl . SSL_ERROR_WANT_WRITE ) :",if err . args [ 0 ] == ssl . SSL_ERROR_NO_CONNECT :,32.40587283786134,32.40587283786134,0.0
"def IncrementErrorCount(self, category):     """"""Bumps the module's error statistic.""""""     self.error_count += 1     if self.counting in (""toplevel"", ""detailed""):         <MASK>             category = category.split(""/"")[0]         if category not in self.errors_by_category:             self.errors_by_category[category] = 0         self.errors_by_category[category] += 1","if self . counting != ""detailed"" :",if category :,3.361830360737634,0.0,0.0
"def _get_y(self, data_inst):     if self.stratified:         y = [v for i, v in data_inst.mapValues(lambda v: v.label).collect()]         <MASK>             y = self.transform_regression_label(data_inst)     else:         # make dummy y         y = [0] * (data_inst.count())     return y",if self . need_transform :,if self . transform_regression_label :,21.10534063187263,21.10534063187263,0.0
"def test_all_project_files(self):     if sys.platform.startswith(""win""):         # XXX something with newlines goes wrong on Windows.         return     for filepath in support.all_project_files():         with open(filepath, ""rb"") as fp:             encoding = tokenize.detect_encoding(fp.readline)[0]         self.assertIsNotNone(encoding, ""can't detect encoding for %s"" % filepath)         with open(filepath, ""r"") as fp:             source = fp.read()             source = source.decode(encoding)         tree = driver.parse_string(source)         new = unicode(tree)         <MASK>             self.fail(""Idempotency failed: %s"" % filepath)","if diff ( filepath , new , encoding ) :","if new != ""utf-8"" :",5.660233915657916,5.660233915657916,0.0
"def test_resource_arn_override_generator(self):     overrides = set()     for k, v in manager.resources.items():         arn_gen = bool(v.__dict__.get(""get_arns"") or v.__dict__.get(""generate_arn""))         <MASK>             overrides.add(k)     overrides = overrides.difference(         {             ""account"",             ""s3"",             ""hostedzone"",             ""log-group"",             ""rest-api"",             ""redshift-snapshot"",             ""rest-stage"",         }     )     if overrides:         raise ValueError(""unknown arn overrides in %s"" % ("", "".join(overrides)))",if arn_gen :,if arn_gen :,100.00000000000004,100.00000000000004,1.0
"def _check_dsl_runner(self) -> None:     """"""Checks if runner in dsl is Kubeflow V2 runner.""""""     with open(self.flags_dict[labels.PIPELINE_DSL_PATH], ""r"") as f:         dsl_contents = f.read()         <MASK>             raise RuntimeError(""KubeflowV2DagRunner not found in dsl."")","if ""KubeflowV2DagRunner"" not in dsl_contents :",if not len ( dsl_contents ) == 0 :,14.991106946711685,14.991106946711685,0.0
"def create_warehouse(warehouse_name, properties=None, company=None):     if not company:         company = ""_Test Company""     warehouse_id = erpnext.encode_company_abbr(warehouse_name, company)     if not frappe.db.exists(""Warehouse"", warehouse_id):         warehouse = frappe.new_doc(""Warehouse"")         warehouse.warehouse_name = warehouse_name         warehouse.parent_warehouse = ""All Warehouses - _TCUV""         warehouse.company = company         warehouse.account = get_warehouse_account(warehouse_name, company)         <MASK>             warehouse.update(properties)         warehouse.save()         return warehouse.name     else:         return warehouse_id",if properties :,if properties :,100.00000000000004,0.0,1.0
"def _parse(self, contents):     entries = []     hostnames_found = set()     for line in contents.splitlines():         if not len(line.strip()):             entries.append((""blank"", [line]))             continue         (head, tail) = chop_comment(line.strip(), ""#"")         <MASK>             entries.append((""all_comment"", [line]))             continue         entries.append((""hostname"", [head, tail]))         hostnames_found.add(head)     if len(hostnames_found) > 1:         raise IOError(""Multiple hostnames (%s) found!"" % (hostnames_found))     return entries",if not len ( head ) :,"if tail == ""all_comment"" :",4.990049701936832,4.990049701936832,0.0
"def _get_omega(self):     if self._omega is None:         n = self.get_drift_dim() // 2         omg = sympl.calc_omega(n)         if self.oper_dtype == Qobj:             self._omega = Qobj(omg, dims=self.dyn_dims)             self._omega_qobj = self._omega         <MASK>             self._omega = sp.csr_matrix(omg)         else:             self._omega = omg     return self._omega",elif self . oper_dtype == sp . csr_matrix :,if self . op_dtype == sp . csr_matrix :,72.72454093000138,72.72454093000138,0.0
"def get_in_inputs(key, data):     if isinstance(data, dict):         for k, v in data.items():             if k == key:                 return v             <MASK>                 out = get_in_inputs(key, v)                 if out:                     return out     elif isinstance(data, (list, tuple)):         out = [get_in_inputs(key, x) for x in data]         out = [x for x in out if x]         if out:             return out[0]","elif isinstance ( v , ( list , tuple , dict ) ) :","if isinstance ( v , ( list , tuple ) ) :",62.20700406786387,62.20700406786387,0.0
def visit_binary(binary):     if binary.operator == operators.eq:         cols = util.column_set(chain(*[c.proxy_set for c in columns.difference(omit)]))         <MASK>             for c in reversed(columns):                 if c.shares_lineage(binary.right) and (                     not only_synonyms or c.name == binary.left.name                 ):                     omit.add(c)                     break,if binary . left in cols and binary . right in cols :,if not cols :,3.726425320974899,3.726425320974899,0.0
"def wait_tasks_or_abort(futures, timeout=60, kill_switch_ev=None):     try:         LazySingletonTasksCoordinator.wait_tasks(             futures, return_when=FIRST_EXCEPTION, raise_exceptions=True         )     except Exception as e:         <MASK>             # Used when we want to keep both raise the exception and wait for all tasks to finish             kill_switch_ev.set()             LazySingletonTasksCoordinator.wait_tasks(                 futures,                 return_when=ALL_COMPLETED,                 raise_exceptions=False,                 timeout=timeout,             )         raise e",if kill_switch_ev is not None :,if kill_switch_ev :,54.77927682341229,54.77927682341229,0.0
"def is_valid(sample):     if sample is None:         return False     if isinstance(sample, tuple):         for s in sample:             <MASK>                 return False             elif isinstance(s, np.ndarray) and s.size == 0:                 return False             elif isinstance(s, collections.abc.Sequence) and len(s) == 0:                 return False     return True",if s is None :,if s . shape == [ ] :,10.552670315936318,10.552670315936318,0.0
"def setVaName(self, va, parent=None):     if parent is None:         parent = self     curname = self.vw.getName(va)     if curname is None:         curname = """"     name, ok = QInputDialog.getText(parent, ""Enter..."", ""Name"", text=curname)     if ok:         name = str(name)         <MASK>             raise Exception(""Duplicate Name: %s"" % name)         self.vw.makeName(va, name)",if self . vw . vaByName ( name ) :,if name in self . vw . names :,30.719343730842187,30.719343730842187,0.0
"def generic_tag_compiler(params, defaults, name, node_class, parser, token):     ""Returns a template.Node subclass.""     bits = token.split_contents()[1:]     bmax = len(params)     def_len = defaults and len(defaults) or 0     bmin = bmax - def_len     if len(bits) < bmin or len(bits) > bmax:         <MASK>             message = ""%s takes %s arguments"" % (name, bmin)         else:             message = ""%s takes between %s and %s arguments"" % (name, bmin, bmax)         raise TemplateSyntaxError(message)     return node_class(bits)",if bmin == bmax :,if len ( bits ) > bmin :,7.267884212102741,7.267884212102741,0.0
"def extract_segmentation_mask(annotation):     poly_specs = annotation[DensePoseDataRelative.S_KEY]     if isinstance(poly_specs, torch.Tensor):         # data is already given as mask tensors, no need to decode         return poly_specs     import pycocotools.mask as mask_utils     segm = torch.zeros((DensePoseDataRelative.MASK_SIZE,) * 2, dtype=torch.float32)     for i in range(DensePoseDataRelative.N_BODY_PARTS):         poly_i = poly_specs[i]         <MASK>             mask_i = mask_utils.decode(poly_i)             segm[mask_i > 0] = i + 1     return segm",if poly_i :,if poly_i is not None :,36.55552228545123,36.55552228545123,0.0
"def module_list(target, fast):     """"""Find the list of modules to be compiled""""""     modules = []     native = native_modules(target)     basedir = os.path.join(ouroboros_repo_folder(), ""ouroboros"")     for name in os.listdir(basedir):         module_name, ext = os.path.splitext(name)         <MASK>             if module_name not in IGNORE_MODULES and module_name not in native:                 if not (fast and module_name in KNOWN_PROBLEM_MODULES):                     modules.append(module_name)     return set(modules)","if ext == "".py"" or ext == """" and os . path . isdir ( os . path . join ( basedir , name ) ) :","if ext == "".py"" :",6.178049532397588,6.178049532397588,0.0
"def filelist_from_patterns(pats, rootdir=None):     if rootdir is None:         rootdir = "".""     # filelist = []     fileset = set([])     lines = [line.strip() for line in pats]     for line in lines:         pat = line[2:]         newfiles = glob(osp.join(rootdir, pat))         if line.startswith(""+""):             fileset.update(newfiles)         <MASK>             fileset.difference_update(newfiles)         else:             raise ValueError(""line must start with + or -"")     filelist = list(fileset)     return filelist","elif line . startswith ( ""-"" ) :","if line . startswith ( ""-"" ) :",88.01117367933934,88.01117367933934,0.0
"def get_upstream_statuses_events(self, upstream: Set) -> Dict[str, V1Statuses]:     statuses_by_refs = {u: [] for u in upstream}     events = self.events or []  # type: List[V1EventTrigger]     for e in events:         entity_ref = contexts_refs.get_entity_ref(e.ref)         if not entity_ref:             continue         if entity_ref not in statuses_by_refs:             continue         for kind in e.kinds:             status = V1EventKind.events_statuses_mapping.get(kind)             <MASK>                 statuses_by_refs[entity_ref].append(status)     return statuses_by_refs",if status :,if status :,100.00000000000004,0.0,1.0
"def __setitem__(self, key, value):     if isinstance(value, (tuple, list)):         info, reference = value         <MASK>             self._reverse_infos[info] = len(self._infos)             self._infos.append(info)         if reference not in self._reverse_references:             self._reverse_references[reference] = len(self._references)             self._references.append(reference)         self._trails[key] = ""%d,%d"" % (             self._reverse_infos[info],             self._reverse_references[reference],         )     else:         raise Exception(""unsupported type '%s'"" % type(value))",if info not in self . _reverse_infos :,if info not in self . _reverse_infos :,100.00000000000004,100.00000000000004,1.0
"def ChangeStyle(self, combos):     style = 0     for combo in combos:         <MASK>             if combo.GetLabel() == ""TR_VIRTUAL"":                 style = style | HTL.TR_VIRTUAL             else:                 try:                     style = style | eval(""wx."" + combo.GetLabel())                 except:                     style = style | eval(""HTL."" + combo.GetLabel())     if self.GetAGWWindowStyleFlag() != style:         self.SetAGWWindowStyleFlag(style)",if combo . GetValue ( ) == 1 :,"if combo . GetLabel ( ) == ""TR_VIRTUAL"" :",26.58483576665878,26.58483576665878,0.0
"def _parse_csrf(self, response):     for d in response:         if d.startswith(""Set-Cookie:""):             for c in d.split("":"", 1)[1].split("";""):                 <MASK>                     self._CSRFtoken = c.strip("" \r\n"")                     log.verbose(""Got new cookie: %s"", self._CSRFtoken)                     break             if self._CSRFtoken != None:                 break","if c . strip ( ) . startswith ( ""CSRF-Token-"" ) :","if c . startswith ( ""Set-Cookie:"" ) :",32.153250955436214,32.153250955436214,0.0
"def test_page_size_matching_max_returned_rows(     app_client_returned_rows_matches_page_size, ):     fetched = []     path = ""/fixtures/no_primary_key.json""     while path:         response = app_client_returned_rows_matches_page_size.get(path)         fetched.extend(response.json[""rows""])         assert len(response.json[""rows""]) in (1, 50)         path = response.json[""next_url""]         <MASK>             path = path.replace(""http://localhost"", """")     assert 201 == len(fetched)",if path :,"if ""http://localhost"" in response . json :",3.673526562988939,3.673526562988939,0.0
"def get_mapping_exception_message(mappings: List[Tuple[Text, Text]]):     """"""Return a message given a list of duplicates.""""""     message = """"     for name, action_name in mappings:         <MASK>             message += ""\n""         message += (             ""Intent '{}' is set to trigger action '{}', which is ""             ""not defined in the domain."".format(name, action_name)         )     return message",if message :,"if name in [ ""action"" , ""trigger"" ] :",3.673526562988939,3.673526562988939,0.0
def cut(sentence):     sentence = strdecode(sentence)     blocks = re_han.split(sentence)     for blk in blocks:         if re_han.match(blk):             for word in __cut(blk):                 if word not in Force_Split_Words:                     yield word                 else:                     for c in word:                         yield c         else:             tmp = re_skip.split(blk)             for x in tmp:                 <MASK>                     yield x,if x :,if x not in Force_Split_Words :,9.287528999566801,9.287528999566801,0.0
"def chop(expr, delta=10.0 ** (-10.0)):     if isinstance(expr, Real):         if -delta < expr.get_float_value() < delta:             return Integer(0)     elif isinstance(expr, Complex) and expr.is_inexact():         real, imag = expr.real, expr.imag         if -delta < real.get_float_value() < delta:             real = Integer(0)         <MASK>             imag = Integer(0)         return Complex(real, imag)     elif isinstance(expr, Expression):         return Expression(chop(expr.head), *[chop(leaf) for leaf in expr.leaves])     return expr",if - delta < imag . get_float_value ( ) < delta :,if - delta < imag . get_float_value ( ) < delta :,100.00000000000004,100.00000000000004,1.0
"def make_row(self):     res = []     for i in range(self.num_cols):         t = sqlite3_column_type(self.stmnt, i)         # print(""type"", t)         if t == SQLITE_INTEGER:             res.append(sqlite3_column_int(self.stmnt, i))         elif t == SQLITE_FLOAT:             res.append(sqlite3_column_double(self.stmnt, i))         <MASK>             res.append(sqlite3_column_text(self.stmnt, i))         else:             raise NotImplementedError     return tuple(res)",elif t == SQLITE_TEXT :,if t == SQLITE_TEXT :,84.08964152537145,84.08964152537145,0.0
"def try_convert(self, string):     string = string.strip()     try:         return int(string)     except:         try:             return float(string)         except:             if string == ""True"":                 return True             <MASK>                 return False             return string","if string == ""False"" :","if string == ""False"" :",100.00000000000004,100.00000000000004,1.0
"def configure_create_table_epilogue(store):     for val in ["""", "" ENGINE=InnoDB""]:         store.config[""create_table_epilogue""] = val         store._set_sql_flavour()         <MASK>             store.log.info(""create_table_epilogue='%s'"", val)             return     raise Exception(""Can not create a transactional table."")",if store . _test_transaction ( ) :,if val :,3.361830360737634,0.0,0.0
"def _check_rule(self, match, target_dict, cred_dict):     """"""Recursively checks credentials based on the brains rules.""""""     try:         new_match_list = self.rules[match]     except KeyError:         <MASK>             new_match_list = (""rule:%s"" % self.default_rule,)         else:             return False     return self.check(new_match_list, target_dict, cred_dict)",if self . default_rule and match != self . default_rule :,if new_match_list is None :,3.27542352440175,3.27542352440175,0.0
"def get_civil_names(self):     congresspeople_ids = self.get_all_congresspeople_ids()     for i, congress_id in enumerate(congresspeople_ids):         if not np.math.isnan(float(congress_id)):             percentage = i / self.total * 100             msg = ""Processed {} out of {} ({:.2f}%)""             print(msg.format(i, self.total, percentage), end=""\r"")             data = self.fetch_data_repository(congress_id)             <MASK>                 yield dict(data)",if data is not None :,if data :,23.174952587773145,0.0,0.0
"def parse_network_whitelist(self, network_whitelist_location):     networks = []     with open(network_whitelist_location, ""r"") as text_file:         for line in text_file:             line = line.strip().strip(""'"").strip('""')             <MASK>                 networks.append(line)     return networks",if isIPv4 ( line ) or isIPv6 ( line ) :,if line :,2.7574525891364066,0.0,0.0
"def _pick(self, cum):     if self._isleaf():         return self.bd[0], self.s     else:         <MASK>             return self.left._pick(cum)         else:             return self.right._pick(cum - self.left.s)",if cum < self . left . s :,if self . _isleaf ( ) :,11.59119922599073,11.59119922599073,0.0
"def serialize_content_range(value):     if isinstance(value, (tuple, list)):         if len(value) not in (2, 3):             raise ValueError(                 ""When setting content_range to a list/tuple, it must ""                 ""be length 2 or 3 (not %r)"" % value             )         <MASK>             begin, end = value             length = None         else:             begin, end, length = value         value = ContentRange(begin, end, length)     value = str(value).strip()     if not value:         return None     return value",if len ( value ) == 2 :,if len ( value ) == 2 :,100.00000000000004,100.00000000000004,1.0
"def make_index_fields(rec):     fields = {}     for k, v in rec.iteritems():         <MASK>             fields[k] = v             continue         if k == ""full_title"":             fields[""title""] = [read_short_title(v)]     return fields","if k in ( ""lccn"" , ""oclc"" , ""isbn"" ) :","if k == ""title"" :",4.5088043638672195,4.5088043638672195,0.0
"def _sample_translation(reference, max_len):     translation = reference[:]     while np.random.uniform() < 0.8 and 1 < len(translation) < max_len:         trans_len = len(translation)         ind = np.random.randint(trans_len)         action = np.random.choice(actions)         if action == ""deletion"":             del translation[ind]         <MASK>             ind_rep = np.random.randint(trans_len)             translation[ind] = translation[ind_rep]         else:             ind_insert = np.random.randint(trans_len)             translation.insert(ind, translation[ind_insert])     return translation","elif action == ""replacement"" :","if action == ""insert"" :",41.11336169005198,41.11336169005198,0.0
"def __call__(self, text: str) -> str:     for t in self.cleaner_types:         if t == ""tacotron"":             text = tacotron_cleaner.cleaners.custom_english_cleaners(text)         elif t == ""jaconv"":             text = jaconv.normalize(text)         <MASK>             if vietnamese_cleaners is None:                 raise RuntimeError(""Please install underthesea"")             text = vietnamese_cleaners.vietnamese_cleaner(text)         else:             raise RuntimeError(f""Not supported: type={t}"")     return text","elif t == ""vietnamese"" :","if t == ""vietnamese"" :",84.08964152537145,84.08964152537145,0.0
"def hook_GetVariable(ql, address, params):     if params[""VariableName""] in ql.env:         var = ql.env[params[""VariableName""]]         read_len = read_int64(ql, params[""DataSize""])         <MASK>             write_int64(ql, params[""Attributes""], 0)         write_int64(ql, params[""DataSize""], len(var))         if read_len < len(var):             return EFI_BUFFER_TOO_SMALL         if params[""Data""] != 0:             ql.mem.write(params[""Data""], var)         return EFI_SUCCESS     return EFI_NOT_FOUND","if params [ ""Attributes"" ] != 0 :","if params [ ""Attributes"" ] != 0 :",100.00000000000004,100.00000000000004,1.0
"def test_setupapp(self, overrideRootMenu):     ""Call setupApp with each possible graphics type.""     root = self.root     flist = FileList(root)     for tktype in alltypes:         with self.subTest(tktype=tktype):             macosx._tk_type = tktype             macosx.setupApp(root, flist)             <MASK>                 self.assertTrue(overrideRootMenu.called)             overrideRootMenu.reset_mock()","if tktype in ( ""carbon"" , ""cocoa"" ) :",if overrideRootMenu . called :,2.564755813286796,2.564755813286796,0.0
"def names(self, persistent=None):     u = set()     result = []     for s in [         self.__storage(None),         self.__storage(self.__category),     ]:         for b in s:             if persistent is not None and b.persistent != persistent:                 continue             <MASK>                 continue             if b.name not in u:                 result.append(b.name)                 u.add(b.name)     return result","if b . name . startswith ( ""__"" ) :",if b . persistent == self . persistent :,13.147601201284163,13.147601201284163,0.0
"def _check_extra_specs(key, value=None):     extra_specs = diff.get(""extra_specs"")     specific_type = extra_specs.get(key) if extra_specs else None     old_type = None     new_type = None     if specific_type:         old_type, new_type = specific_type         <MASK>             old_type = True if old_type and old_type.upper() == value else False             new_type = True if new_type and new_type.upper() == value else False     return old_type, new_type",if value :,if value :,100.00000000000004,0.0,1.0
"def _write_lock_file(self, repo, force=True):  # type: (Repository, bool) -> None     if force or (self._update and self._write_lock):         updated_lock = self._locker.set_lock_data(self._package, repo.packages)         <MASK>             self._io.write_line("""")             self._io.write_line(""<info>Writing lock file</>"")",if updated_lock :,if updated_lock :,100.00000000000004,100.00000000000004,1.0
"def process_message(self, msg):     if msg[""type""] == ""sample"":         batch_shape = msg[""fn""].batch_shape         <MASK>             batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape)             batch_shape[self.dim] = self.size             msg[""fn""] = msg[""fn""].expand(torch.Size(batch_shape))",if len ( batch_shape ) < - self . dim or batch_shape [ self . dim ] != self . size :,if len ( batch_shape ) > self . size :,21.05492007644639,21.05492007644639,0.0
"def _test_reducibility(self):     # make a copy of the graph     graph = networkx.DiGraph(self._graph)     # preprocess: make it a super graph     self._make_supergraph(graph)     while True:         changed = False         # find a node with a back-edge, remove the edge (deleting the loop), and replace it with a MultiNode         changed |= self._remove_self_loop(graph)         # find a node that has only one predecessor, and merge it with its predecessor (replace them with a         # MultiNode)         changed |= self._merge_single_entry_node(graph)         <MASK>             # a fixed-point is reached             break",if not changed :,if not changed :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, roberta, num_classes=2, dropout=0.0, prefix=None, params=None):     super(RoBERTaClassifier, self).__init__(prefix=prefix, params=params)     self.roberta = roberta     self._units = roberta._units     with self.name_scope():         self.classifier = nn.HybridSequential(prefix=prefix)         <MASK>             self.classifier.add(nn.Dropout(rate=dropout))         self.classifier.add(nn.Dense(units=self._units, activation=""tanh""))         if dropout:             self.classifier.add(nn.Dropout(rate=dropout))         self.classifier.add(nn.Dense(units=num_classes))",if dropout :,if dropout :,100.00000000000004,0.0,1.0
"def get_object_from_name(self, name, check_symlinks=True):     if not name:         return None     name = name.rstrip(""\\"")     for a, o in self.objects.items():         if not o.name:             continue         <MASK>             return o     if check_symlinks:         m = [sl[1] for sl in self.symlinks if name.lower() == sl[0].lower()]         if m:             name = m[0]         return self.get_object_from_name(name, False)",if o . name . lower ( ) == name . lower ( ) :,if a . name == name :,10.343690622931698,10.343690622931698,0.0
"def __call__(self):     """"""Run all check_* methods.""""""     if self.on:         oldformatwarning = warnings.formatwarning         warnings.formatwarning = self.formatwarning         try:             for name in dir(self):                 <MASK>                     method = getattr(self, name)                     if method and callable(method):                         method()         finally:             warnings.formatwarning = oldformatwarning","if name . startswith ( ""check_"" ) :","if hasattr ( self , name ) :",9.545138913210204,9.545138913210204,0.0
"def __print__(self, defaults=False):     if defaults:         print_func = str     else:         print_func = repr     pieces = []     default_values = self.__defaults__     for k in self.__fields__:         value = getattr(self, k)         if not defaults and value == default_values[k]:             continue         <MASK>             print_func = repr  # keep quotes around strings         pieces.append(""%s=%s"" % (k, print_func(value)))     if pieces or self.__base__:         return ""%s(%s)"" % (self.__class__.__name__, "", "".join(pieces))     else:         return """"","if isinstance ( value , basestring ) :",if not defaults :,6.988198185490689,6.988198185490689,0.0
"def apply(self, **kwargs: Any) -> None:     for node in self.document.traverse(nodes.target):         <MASK>             continue         if (             ""ismod"" in node             and node.parent.__class__ is nodes.section             and             # index 0 is the section title node             node.parent.index(node) == 1         ):             node.parent[""ids""][0:0] = node[""ids""]             node.parent.remove(node)","if not node [ ""ids"" ] :",if node . parent is None :,6.495032985064742,6.495032985064742,0.0
"def add_special_token_2d(     values: List[List[int]], special_token: int = 0, use_first_value: bool = False ) -> List[List[int]]:     results = torch.jit.annotate(List[List[int]], [])     for value in values:         result = torch.jit.annotate(List[int], [])         <MASK>             special_token = value[0]         result.append(special_token)         result.extend(value)         result.append(special_token)         results.append(result)     return results",if use_first_value and len ( value ) > 0 :,if use_first_value :,30.93485033266056,30.93485033266056,0.0
"def test_import(self):     TIMEOUT = 5     # Test for a deadlock when importing a module that runs the     # ThreadedResolver at import-time. See resolve_test.py for     # full explanation.     command = [sys.executable, ""-c"", ""import tornado.test.resolve_test_helper""]     start = time.time()     popen = Popen(command, preexec_fn=lambda: signal.alarm(TIMEOUT))     while time.time() - start < TIMEOUT:         return_code = popen.poll()         <MASK>             self.assertEqual(0, return_code)             return  # Success.         time.sleep(0.05)     self.fail(""import timed out"")",if return_code is not None :,if return_code :,38.80684294761701,38.80684294761701,0.0
"def find_item_for_key(self, e):     for item in self._items:         if item.keycode == e.key and item.shift == e.shift and item.alt == e.alt:             focus = get_focus()             <MASK>                 return self._items.index(item)             else:                 return -1     return -1","if self . command_is_enabled ( item , focus ) :",if focus . focus == focus . focus :,3.9778149665594618,3.9778149665594618,0.0
"def check_app_config_brackets(self):     for sn, app in cherrypy.tree.apps.items():         if not isinstance(app, cherrypy.Application):             continue         <MASK>             continue         for key in app.config.keys():             if key.startswith(""["") or key.endswith(""]""):                 warnings.warn(                     ""The application mounted at %r has config ""                     ""section names with extraneous brackets: %r. ""                     ""Config *files* need brackets; config *dicts* ""                     ""(e.g. passed to tree.mount) do not."" % (sn, key)                 )",if not app . config :,if sn not in self . config_files :,10.552670315936318,10.552670315936318,0.0
"def got_arbiter_module_type_defined(self, mod_type):     for a in self.arbiters:         # Do like the linkify will do after....         for m in getattr(a, ""modules"", []):             # So look at what the arbiter try to call as module             m = m.strip()             # Ok, now look in modules...             for mod in self.modules:                 # try to see if this module is the good type                 <MASK>                     # if so, the good name?                     if getattr(mod, ""module_name"", """").strip() == m:                         return True     return False","if getattr ( mod , ""module_type"" , """" ) . strip ( ) == mod_type . strip ( ) :","if mod_type == ""module"" :",4.148774187524417,4.148774187524417,0.0
"def write_config_to_file(self, folder, filename, config):     do_not_write = [""hyperparameter_search_space_updates""]     with open(os.path.join(folder, filename), ""w"") as f:         f.write(             ""\n"".join(                 [                     (key + ""="" + str(value))                     for (key, value) in sorted(config.items(), key=lambda x: x[0])                     <MASK>                 ]             )         )",if not key in do_not_write,if do_not_write :,48.35447404743731,48.35447404743731,0.0
"def parsing(self, parsing):  # type: (bool) -> None     self._parsed = parsing     for k, v in self._body:         <MASK>             v.value.parsing(parsing)         elif isinstance(v, AoT):             for t in v.body:                 t.value.parsing(parsing)","if isinstance ( v , Table ) :","if isinstance ( v , AoT ) :",59.4603557501361,59.4603557501361,0.0
"def test_crashers_crash(self):     for fname in glob.glob(CRASHER_FILES):         <MASK>             continue         # Some ""crashers"" only trigger an exception rather than a         # segfault. Consider that an acceptable outcome.         if test.support.verbose:             print(""Checking crasher:"", fname)         assert_python_failure(fname)",if os . path . basename ( fname ) in infinite_loops :,if not test . support . test_crashers :,4.206027236923763,4.206027236923763,0.0
"def __getitem__(self, k) -> ""SimMemView"":     if isinstance(k, slice):         if k.step is not None:             raise ValueError(""Slices with strides are not supported"")         elif k.start is None:             raise ValueError(""Must specify start index"")         <MASK>             raise ValueError(""Slices with stop index are not supported"")         else:             addr = k.start     elif self._type is not None and self._type._can_refine_int:         return self._type._refine(self, k)     else:         addr = k     return self._deeper(addr=addr)",elif k . stop is not None :,if k . stop is None :,42.38365628278778,42.38365628278778,0.0
"def get_lowest_wall_time(jsons):     lowest_wall = None     for j in jsons:         <MASK>             lowest_wall = j[""wall_time""]         if lowest_wall > j[""wall_time""]:             lowest_wall = j[""wall_time""]     return lowest_wall",if lowest_wall is None :,if lowest_wall is None :,100.00000000000004,100.00000000000004,1.0
"def extract_wav_headers(data):     # def search_subchunk(data, subchunk_id):     pos = 12  # The size of the RIFF chunk descriptor     subchunks = []     while pos + 8 <= len(data) and len(subchunks) < 10:         subchunk_id = data[pos : pos + 4]         subchunk_size = struct.unpack_from(""<I"", data[pos + 4 : pos + 8])[0]         subchunks.append(WavSubChunk(subchunk_id, pos, subchunk_size))         <MASK>             # 'data' is the last subchunk             break         pos += subchunk_size + 8     return subchunks","if subchunk_id == b""data"" :",if subchunk_id == subchunk_id :,48.61555413051454,48.61555413051454,0.0
"def _any_targets_have_native_sources(self, targets):     # TODO(#5949): convert this to checking if the closure of python requirements has any     # platform-specific packages (maybe find the platforms there too?).     for tgt in targets:         for type_constraint, target_predicate in self._native_target_matchers.items():             <MASK>                 return True     return False",if type_constraint . satisfied_by ( tgt ) and target_predicate ( tgt ) :,if target_predicate ( tgt ) == tgt :,24.69258811787892,24.69258811787892,0.0
"def validate_memory(self, value):     for k, v in value.viewitems():         if v is None:  # use NoneType to unset a value             continue         <MASK>             raise serializers.ValidationError(""Process types can only contain [a-z]"")         if not re.match(MEMLIMIT_MATCH, str(v)):             raise serializers.ValidationError(                 ""Limit format: <number><unit>, where unit = B, K, M or G""             )     return value","if not re . match ( PROCTYPE_MATCH , k ) :","if k != ""a-z"" :",3.8902180856807296,3.8902180856807296,0.0
"def cart_number_checksum_validation(cls, number):     digits = []     even = False     if not number.isdigit():         return False     for digit in reversed(number):         digit = ord(digit) - ord(""0"")         if even:             digit *= 2             <MASK>                 digit = digit % 10 + digit // 10         digits.append(digit)         even = not even     return sum(digits) % 10 == 0 if digits else False",if digit >= 10 :,if digit < 10 :,24.736929544091932,24.736929544091932,0.0
"def transform(a, cmds):     buf = a.split(""\n"")     for cmd in cmds:         ctype, line, col, char = cmd         if ctype == ""D"":             <MASK>                 buf[line] = buf[line][:col] + buf[line][col + len(char) :]             else:                 buf[line] = buf[line] + buf[line + 1]                 del buf[line + 1]         elif ctype == ""I"":             buf[line] = buf[line][:col] + char + buf[line][col:]         buf = ""\n"".join(buf).split(""\n"")     return ""\n"".join(buf)","if char != ""\n"" :","if ctype == ""C"" :",14.575161396875705,14.575161396875705,0.0
"def get_partners(self) -> Dict[AbstractNode, Set[int]]:     partners = {}  # type: Dict[AbstractNode, Set[int]]     for edge in self.edges:         if edge.is_dangling():             raise ValueError(""Cannot contract copy tensor with dangling edges"")         if self._is_my_trace(edge):             continue         partner_node, shared_axis = self._get_partner(edge)         <MASK>             partners[partner_node] = set()         partners[partner_node].add(shared_axis)     return partners",if partner_node not in partners :,if partner_node not in partners :,100.00000000000004,100.00000000000004,1.0
"def _bind_interactive_rez(self):     if config.set_prompt and self.settings.prompt:         stored_prompt = os.getenv(""REZ_STORED_PROMPT_CMD"")         curr_prompt = stored_prompt or os.getenv(""PROMPT"", """")         <MASK>             self.setenv(""REZ_STORED_PROMPT_CMD"", curr_prompt)         new_prompt = ""%%REZ_ENV_PROMPT%%""         new_prompt = (             (new_prompt + "" %s"") if config.prefix_prompt else (""%s "" + new_prompt)         )         new_prompt = new_prompt % curr_prompt         self._addline(""set PROMPT=%s"" % new_prompt)",if not stored_prompt :,if curr_prompt :,34.98330125272253,34.98330125272253,0.0
"def __listingColumns(self):     columns = []     for name in self.__getColumns():         definition = column(name)         if not definition:             IECore.msg(                 IECore.Msg.Level.Error,                 ""GafferImageUI.CatalogueUI"",                 ""No column registered with name '%s'"" % name,             )             continue         <MASK>             c = GafferUI.PathListingWidget.IconColumn(definition.title(), """", name)         else:             c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name)         columns.append(c)     return columns","if isinstance ( definition , IconColumn ) :",if definition . icon :,7.715486568024961,7.715486568024961,0.0
"def _check_invalid_keys(self, section_name, section):     for key in section:         key_name = str(key)         valid_key_names = [s[0] for s in self.keys]         is_valid_key = key_name in valid_key_names         <MASK>             err_msg = (                 ""'{0}' is not a valid key name for '{1}'. Must "" ""be one of these: {2}""             ).format(key_name, section_name, "", "".join(valid_key_names))             raise InvalidConfig(err_msg)",if not is_valid_key :,if not is_valid_key :,100.00000000000004,100.00000000000004,1.0
"def _get_startup_packages(lib_path: Path, packages) -> Set[str]:     names = set()     for path in lib_path.iterdir():         name = path.name         if name == ""__pycache__"":             continue         if name.endswith("".py""):             names.add(name.split(""."")[0])         <MASK>             names.add(name)     if packages:         packages = {package.lower().replace(""-"", ""_"") for package in packages}         if len(names & packages) == len(packages):             return packages     return names","elif path . is_dir ( ) and ""."" not in name :","if name . startswith ( ""pycache"" ) :",3.7458052434944946,3.7458052434944946,0.0
"def sortkeypicker(keynames):     negate = set()     for i, k in enumerate(keynames):         <MASK>             keynames[i] = k[1:]             negate.add(k[1:])     def getit(adict):         composite = [adict[k] for k in keynames]         for i, (k, v) in enumerate(zip(keynames, composite)):             if k in negate:                 composite[i] = -v         return composite     return getit","if k [ : 1 ] == ""-"" :",if k [ 0 ] == - 1 :,23.801761257033814,23.801761257033814,0.0
"def iter_symbols(code):     """"""Yield names and strings used by `code` and its nested code objects""""""     for name in code.co_names:         yield name     for const in code.co_consts:         if isinstance(const, six.string_types):             yield const         <MASK>             for name in iter_symbols(const):                 yield name","elif isinstance ( const , CodeType ) :","if isinstance ( const , six . integer_types ) :",25.211936184349828,25.211936184349828,0.0
"def set_study_directions(     self, study_id: int, directions: Sequence[StudyDirection] ) -> None:     with self._lock:         <MASK>             current_directions = self._studies[study_id].directions             if directions == current_directions:                 return             elif (                 len(current_directions) == 1                 and current_directions[0] == StudyDirection.NOT_SET             ):                 self._studies[study_id].directions = list(directions)                 self._backend.set_study_directions(study_id, directions)                 return     self._backend.set_study_directions(study_id, directions)",if study_id in self . _studies :,if self . _studies [ study_id ] :,36.13284405728027,36.13284405728027,0.0
"def PreprocessConditionalStatement(self, IfList, ReplacedLine):     while self:         <MASK>             x = 1         elif not IfList:             if self <= 2:                 continue             RegionSizeGuid = 3             if not RegionSizeGuid:                 RegionLayoutLine = 5                 continue             RegionLayoutLine = self.CurrentLineNumber     return 1",if self . __Token :,if self <= 1 :,15.207218222740094,15.207218222740094,0.0
"def _check_blocking(self, current_time):     if self._switch_flag is False:         active_greenlet = self._active_greenlet         <MASK>             self._notify_greenlet_blocked(active_greenlet, current_time)     self._switch_flag = False",if active_greenlet is not None and active_greenlet != self . _hub :,if active_greenlet is not None :,24.909923021496894,24.909923021496894,0.0
"def detect(get_page):     retval = False     for vector in WAF_ATTACK_VECTORS:         page, headers, code = get_page(get=vector)         retval = (             re.search(r""BlockDos\.net"", headers.get(HTTP_HEADER.SERVER, """"), re.I)             is not None         )         <MASK>             break     return retval",if retval :,if retval :,100.00000000000004,0.0,1.0
"def _fastqc_data_section(self, section_name):     out = []     in_section = False     data_file = os.path.join(self._dir, ""fastqc_data.txt"")     if os.path.exists(data_file):         with open(data_file) as in_handle:             for line in in_handle:                 <MASK>                     in_section = True                 elif in_section:                     if line.startswith("">>END""):                         break                     out.append(line.rstrip(""\r\n""))     return out","if line . startswith ( "">>%s"" % section_name ) :",if line . startswith ( section_name ) :,39.01319655022955,39.01319655022955,0.0
"def shortcut(self, input, ch_out, stride, is_first, name):     ch_in = input.shape[1]     if ch_in != ch_out or stride != 1:         <MASK>             return self.conv_bn_layer(input, ch_out, 1, stride, name=name)         else:             return self.conv_bn_layer_new(input, ch_out, 1, stride, name=name)     elif is_first:         return self.conv_bn_layer(input, ch_out, 1, stride, name=name)     else:         return input",if is_first or stride == 1 :,if is_first :,26.013004751144457,26.013004751144457,0.0
"def get_value_from_string(self, string_value):     """"""Return internal representation starting from CFN/user-input value.""""""     param_value = self.get_default_value()     try:         if string_value is not None:             string_value = str(string_value).strip()             <MASK>                 param_value = int(string_value)     except ValueError:         self.pcluster_config.warn(             ""Unable to convert the value '{0}' to an Integer. ""             ""Using default value for parameter '{1}'"".format(string_value, self.key)         )     return param_value","if string_value != ""NONE"" :",if param_value is None :,10.175282441454787,10.175282441454787,0.0
"def get_running(workers):     running = []     for worker in workers:         current_test_name = worker.current_test_name         <MASK>             continue         dt = time.monotonic() - worker.start_time         if dt >= PROGRESS_MIN_TIME:             text = ""%s (%s)"" % (current_test_name, format_duration(dt))             running.append(text)     return running",if not current_test_name :,if current_test_name is None :,48.54917717073236,48.54917717073236,0.0
"def generate_data(self, request):     """"""Generate data for the widget.""""""     uptime = {}     cache_stats = get_cache_stats()     if cache_stats:         for hosts, stats in cache_stats:             if stats[""uptime""] > 86400:                 uptime[""value""] = stats[""uptime""] / 60 / 60 / 24                 uptime[""unit""] = _(""days"")             <MASK>                 uptime[""value""] = stats[""uptime""] / 60 / 60                 uptime[""unit""] = _(""hours"")             else:                 uptime[""value""] = stats[""uptime""] / 60                 uptime[""unit""] = _(""minutes"")     return {""cache_stats"": cache_stats, ""uptime"": uptime}","elif stats [ ""uptime"" ] > 3600 :","if hosts [ 0 ] == ""hours"" :",5.604233375480572,5.604233375480572,0.0
"def add_actors(self):     """"""Adds `self.actors` to the scene.""""""     if not self._actors_added:         self.reader.render_window = self.scene.render_window         self._update_reader()         self._actors_added = True         <MASK>             self._visible_changed(self.visible)         self.scene.render()",if not self . visible :,if self . visible :,57.89300674674101,57.89300674674101,0.0
"def _add_uniqu_suffix(self, titles):     counters = dict()     titles_with_suffix = []     for title in titles:         counters[title] = counters[title] + 1 if title in counters else 1         <MASK>             title = f""{title} ({counters[title]})""         titles_with_suffix.append(title)     return titles_with_suffix",if counters [ title ] > 1 :,if title in counters :,8.290829875388036,8.290829875388036,0.0
"def _verify_udf_resources(self, job, config):     udf_resources = config.get(""userDefinedFunctionResources"", ())     self.assertEqual(len(job.udf_resources), len(udf_resources))     for found, expected in zip(job.udf_resources, udf_resources):         <MASK>             self.assertEqual(found.udf_type, ""resourceUri"")             self.assertEqual(found.value, expected[""resourceUri""])         else:             self.assertEqual(found.udf_type, ""inlineCode"")             self.assertEqual(found.value, expected[""inlineCode""])","if ""resourceUri"" in expected :","if found . udf_type == ""resourceUri"" :",14.323145079400492,14.323145079400492,0.0
"def __init__(     self, layout, value=None, string=None, *, dtype: np.dtype = np.float64 ) -> None:     """"""Constructor.""""""     self.layout = layout     if value is None:         <MASK>             self.value = np.zeros((self.layout.gaDims,), dtype=dtype)         else:             self.value = layout.parse_multivector(string).value     else:         self.value = np.array(value)         if self.value.shape != (self.layout.gaDims,):             raise ValueError(                 ""value must be a sequence of length %s"" % self.layout.gaDims             )",if string is None :,if string is None :,100.00000000000004,100.00000000000004,1.0
"def read_file(filename, print_error=True):     """"""Returns the contents of a file.""""""     try:         for encoding in [""utf-8"", ""latin1""]:             try:                 with io.open(filename, encoding=encoding) as fp:                     return fp.read()             except UnicodeDecodeError:                 pass     except IOError as exception:         <MASK>             print(exception, file=sys.stderr)         return None",if print_error :,if print_error :,100.00000000000004,100.00000000000004,1.0
"def get_albums_for_iter(self, iter_):     obj = self.get_value(iter_)     if isinstance(obj, AlbumNode):         return {obj.album}     albums = set()     for child_iter, value in self.iterrows(iter_):         <MASK>             albums.add(value.album)         else:             albums.update(self.get_albums_for_iter(child_iter))     return albums","if isinstance ( value , AlbumNode ) :","if isinstance ( value , AlbumNode ) :",100.00000000000004,100.00000000000004,1.0
"def wait_til_ready(cls, connector=None):     if connector is None:         connector = cls.connector     while True:         now = time.time()         next_iteration = now // 1.0 + 1         <MASK>             break         else:             await cls._clock.run_til(next_iteration)         await asyncio.sleep(1.0)",if connector . ready :,if next_iteration > 1.0 :,7.809849842300637,7.809849842300637,0.0
"def remove_property(self, key):  # type: (str) -> None     with self.secure() as config:         keys = key.split(""."")         current_config = config         for i, key in enumerate(keys):             if key not in current_config:                 return             <MASK>                 del current_config[key]                 break             current_config = current_config[key]",if i == len ( keys ) - 1 :,if i == 0 :,23.350308364304226,23.350308364304226,0.0
"def get(self, hash160, default=None):     v = self.p2s_for_hash(hash160)     <MASK>         return v     if hash160 not in self._secret_exponent_cache:         v = self.path_for_hash160(hash160)         if v:             fingerprint, path = v             for key in self._secrets.get(fingerprint, []):                 subkey = key.subkey_for_path(path)                 self._add_key_to_cache(subkey)     return self._secret_exponent_cache.get(hash160, default)",if v :,if v :,100.00000000000004,0.0,1.0
"def fetch_all(self, api_client, fetchstatuslogger, q, targets):     self.fetchstatuslogger = fetchstatuslogger     if targets != None:         # Ensure targets is a tuple         <MASK>             targets = tuple(                 targets,             )         elif type(targets) != tuple:             targets = tuple(targets)     for target in targets:         self._fetch_targets(api_client, q, target)",if type ( targets ) != list and type ( targets ) != tuple :,if type ( targets ) == list :,22.472736255949545,22.472736255949545,0.0
"def dgl_mp_batchify_fn(data):     if isinstance(data[0], tuple):         data = zip(*data)         return [dgl_mp_batchify_fn(i) for i in data]     for dt in data:         <MASK>             if isinstance(dt, dgl.DGLGraph):                 return [d for d in data if isinstance(d, dgl.DGLGraph)]             elif isinstance(dt, nd.NDArray):                 pad = Pad(axis=(1, 2), num_shards=1, ret_length=False)                 data_list = [dt for dt in data if dt is not None]                 return pad(data_list)",if dt is not None :,if dt . shape == nd . NDArray :,9.287528999566801,9.287528999566801,0.0
"def capture_server(evt, buf, serv):     try:         serv.listen(5)         conn, addr = serv.accept()     except socket.timeout:         pass     else:         n = 200         while n > 0:             r, w, e = select.select([conn], [], [])             <MASK>                 data = conn.recv(10)                 # keep everything except for the newline terminator                 buf.write(data.replace(""\n"", """"))                 if ""\n"" in data:                     break             n -= 1             time.sleep(0.01)         conn.close()     finally:         serv.close()         evt.set()",if r :,if r == w :,17.965205598154213,17.965205598154213,0.0
"def elem():     if ints_only:         return random.randint(0, 10000000000)     else:         t = random.randint(0, 2)         if t == 0:             return random.randint(0, 10000000000)         elif t == 1:             return float(random.randint(0, 10000000000))         <MASK>             return strings[random.randint(0, len(strings) - 1)]         return random_string(random.randint(100, 1000))",elif strings is not None :,if t == 2 :,8.116697886877475,8.116697886877475,0.0
"def has_changed(self, initial, data):     if self.disabled:         return False     if initial is None:         initial = ["""" for x in range(0, len(data))]     else:         <MASK>             initial = self.widget.decompress(initial)     for field, initial, data in zip(self.fields, initial, data):         try:             initial = field.to_python(initial)         except ValidationError:             return True         if field.has_changed(initial, data):             return True     return False","if not isinstance ( initial , list ) :",if self . widget :,5.70796903405875,5.70796903405875,0.0
"def _load_testfile(filename, package, module_relative):     if module_relative:         package = _normalize_module(package, 3)         filename = _module_relative_path(package, filename)         <MASK>             if hasattr(package.__loader__, ""get_data""):                 file_contents = package.__loader__.get_data(filename)                 # get_data() opens files as 'rb', so one must do the equivalent                 # conversion as universal newlines would do.                 return file_contents.replace(os.linesep, ""\n""), filename     return open(filename).read(), filename","if hasattr ( package , ""__loader__"" ) :","if hasattr ( package , ""__loader__"" ) :",100.00000000000004,100.00000000000004,1.0
"def release(self):     tid = _thread.get_ident()     with self.lock:         if self.owner != tid:             raise RuntimeError(""cannot release un-acquired lock"")         assert self.count > 0         self.count -= 1         if self.count == 0:             self.owner = None             <MASK>                 self.waiters -= 1                 self.wakeup.release()",if self . waiters :,if self . waiters > 0 :,43.47208719449914,43.47208719449914,0.0
"def stage(     self, x, num_modules, num_blocks, channels, multi_scale_output=True, name=None ):     out = x     for i in range(num_modules):         <MASK>             out = self.high_resolution_module(                 out,                 num_blocks,                 channels,                 multi_scale_output=False,                 name=name + ""_"" + str(i + 1),             )         else:             out = self.high_resolution_module(                 out, num_blocks, channels, name=name + ""_"" + str(i + 1)             )     return out",if i == num_modules - 1 and multi_scale_output == False :,if multi_scale_output :,12.043498774973072,12.043498774973072,0.0
"def changeFrontAlteration(intV, alter):     # fati = front alteration transpose interval     fati = self.frontAlterationTransposeInterval     if fati:         newFati = interval.add([fati, intV])         self.frontAlterationTransposeInterval = newFati         self.frontAlterationAccidental.alter = (             self.frontAlterationAccidental.alter + alter         )         <MASK>             self.frontAlterationTransposeInterval = None             self.frontAlterationAccidental = None     else:         self.frontAlterationTransposeInterval = intV         self.frontAlterationAccidental = pitch.Accidental(alter)",if self . frontAlterationAccidental . alter == 0 :,if newFati == self . frontAlterationTransposeInterval :,13.462380890160203,13.462380890160203,0.0
"def set_to_train(self):     for T in self.trainable_attributes():         for k, v in T.items():             <MASK>                 c_f.set_requires_grad(v, requires_grad=False)                 v.eval()             else:                 v.train()     self.maybe_freeze_trunk_batchnorm()",if k in self . freeze_these :,"if k == ""requires_grad"" :",9.980099403873663,9.980099403873663,0.0
"def _migrate(self, sig=None, compact=True):     with self.lock:         sig = sig or self.sig         <MASK>             return         if sig in self.WORDS and len(self.WORDS[sig]) > 0:             PostingList.Append(                 self.session, sig, self.WORDS[sig], sig=sig, compact=compact             )             del self.WORDS[sig]",if sig in GPL_NEVER_MIGRATE :,if sig not in self .WORDS :,11.59119922599073,11.59119922599073,0.0
"def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs):     if self.prediction_bar is None:         <MASK>             self.prediction_bar = self.training_tracker.add_child(len(eval_dataloader))         else:             self.prediction_bar = NotebookProgressBar(len(eval_dataloader))         self.prediction_bar.update(1)     else:         self.prediction_bar.update(self.prediction_bar.value + 1)",if self . training_tracker is not None :,if self . training_tracker is not None :,100.00000000000004,100.00000000000004,1.0
"def show(self, indent=0):     """"""Pretty print this structure.""""""     if indent == 0:         print(""struct {}"".format(self.name))     for field in self.fields:         if field.offset is None:             offset = ""0x??""         else:             offset = ""0x{:02x}"".format(field.offset)         print(""{}+{} {} {}"".format("" "" * indent, offset, field.name, field.type))         <MASK>             field.type.show(indent + 1)","if isinstance ( field . type , Structure ) :",if field . type is not None :,18.190371142855746,18.190371142855746,0.0
"def __exit__(self, exc, value, tb):     for key in self.overrides.keys():         old_value = self.old[key]         <MASK>             delattr(self.instance, key)         else:             setattr(self.instance, key, old_value)     self.instance.save()",if old_value is NULL :,if old_value is None :,64.34588841607616,64.34588841607616,0.0
"def complete(self, block):     with self._condition:         <MASK>             return False         if self._complete():             self._calculate_state_root_if_not_already_done()             return True         if block:             self._condition.wait_for(self._complete)             self._calculate_state_root_if_not_already_done()             return True         return False",if not self . _final :,if not self . _condition . is_set ( ) :,30.26643726685862,30.26643726685862,0.0
"def parseArguments(self):     args = []     self.expect(""("")     if not self.match("")""):         while self.startIndex < self.length:             args.append(self.isolateCoverGrammar(self.parseAssignmentExpression))             <MASK>                 break             self.expectCommaSeparator()     self.expect("")"")     return args","if self . match ( "")"" ) :","if self . match ( "")"" ) :",100.00000000000004,100.00000000000004,1.0
"def isValidDateString(config_param_name, value, valid_value):     try:         if value == ""DD-MM-YYYY"":             return value         day, month, year = value.split(""-"")         <MASK>             raise DateStringValueError(config_param_name, value)         if int(month) < 1 or int(month) > 12:             raise DateStringValueError(config_param_name, value)         if int(year) < 1900 or int(year) > 2013:             raise DateStringValueError(config_param_name, value)         return value     except Exception:         raise DateStringValueError(config_param_name, value)",if int ( day ) < 1 or int ( day ) > 31 :,if day != 1 or day > 12 :,6.962210312500384,6.962210312500384,0.0
"def build_tree(path):     tree = Tree()     for basename, entry in trees[path].items():         <MASK>             mode = stat.S_IFDIR             sha = build_tree(pathjoin(path, basename))         else:             (mode, sha) = entry         tree.add(basename, mode, sha)     object_store.add_object(tree)     return tree.id","if isinstance ( entry , dict ) :",if entry is None :,7.715486568024961,7.715486568024961,0.0
"def get_quarantine_count(self):     """"""get obj/container/account quarantine counts""""""     qcounts = {""objects"": 0, ""containers"": 0, ""accounts"": 0}     qdir = ""quarantined""     for device in os.listdir(self.devices):         for qtype in qcounts:             qtgt = os.path.join(self.devices, device, qdir, qtype)             <MASK>                 linkcount = os.lstat(qtgt).st_nlink                 if linkcount > 2:                     qcounts[qtype] += linkcount - 2     return qcounts",if os . path . exists ( qtgt ) :,if os . path . isdir ( qtgt ) :,65.80370064762461,65.80370064762461,0.0
"def _is_static_shape(self, shape):     if shape is None or not isinstance(shape, list):         return False     for dim_value in shape:         <MASK>             return False         if dim_value < 0:             raise Exception(""Negative dimension is illegal: %d"" % dim_value)     return True","if not isinstance ( dim_value , int ) :",if dim_value > 1 :,15.685718045401451,15.685718045401451,0.0
"def BraceDetectAll(words):     # type: (List[compound_word]) -> List[word_t]     """"""Return a new list of words, possibly with BracedTree instances.""""""     out = []  # type: List[word_t]     for w in words:         # The shortest possible brace expansion is {,}.  This heuristic prevents         # a lot of garbage from being created, since otherwise nearly every word         # would be checked.  We could be even more precise but this is cheap.         if len(w.parts) >= 3:             brace_tree = _BraceDetect(w)             <MASK>                 out.append(brace_tree)                 continue         out.append(w)     return out",if brace_tree :,if len ( w . parts ) >= 2 :,4.456882760699063,4.456882760699063,0.0
"def __init__(original, self, *args, **kwargs):     data = args[0] if len(args) > 0 else kwargs.get(""data"")     if data is not None:         try:             <MASK>                 raise Exception(                     ""cannot gather example input when dataset is loaded from a file.""                 )             input_example_info = _InputExampleInfo(                 input_example=deepcopy(data[:INPUT_EXAMPLE_SAMPLE_ROWS])             )         except Exception as e:             input_example_info = _InputExampleInfo(error_msg=str(e))         setattr(self, ""input_example_info"", input_example_info)     original(self, *args, **kwargs)","if isinstance ( data , str ) :",if data [ INPUT_EXAMPLE_SAMPLE_ROWS ] is None :,3.737437943747671,3.737437943747671,0.0
"def setRow(self, row, vals):     if row > self.rowCount() - 1:         self.setRowCount(row + 1)     for col in range(len(vals)):         val = vals[col]         item = self.itemClass(val, row)         item.setEditable(self.editable)         sortMode = self.sortModes.get(col, None)         <MASK>             item.setSortMode(sortMode)         format = self._formats.get(col, self._formats[None])         item.setFormat(format)         self.items.append(item)         self.setItem(row, col, item)         item.setValue(val)  # Required--the text-change callback is invoked",if sortMode is not None :,if sortMode :,23.174952587773145,0.0,0.0
"def wakeUp(self):     """"""Write one byte to the pipe, and flush it.""""""     # We don't use fdesc.writeToFD since we need to distinguish     # between EINTR (try again) and EAGAIN (do nothing).     if self.o is not None:         try:             util.untilConcludes(os.write, self.o, b""x"")         except OSError as e:             # XXX There is no unit test for raising the exception             # for other errnos. See #4285.             <MASK>                 raise",if e . errno != errno . EAGAIN :,if e . errno == EINTR :,29.797147054518835,29.797147054518835,0.0
"def _setup(self, field_name, owner_model):     # Resolve possible name-based model references.     resolved_classes = []     for m in self.model_classes:         <MASK>             if m == owner_model.__name__:                 resolved_classes.append(owner_model)             else:                 raise Exception(                     ""PolyModelType: Unable to resolve model '{}'."".format(m)                 )         else:             resolved_classes.append(m)     self.model_classes = tuple(resolved_classes)     super(PolyModelType, self)._setup(field_name, owner_model)","if isinstance ( m , string_type ) :","if isinstance ( m , ( str , unicode ) ) :",36.462858619364674,36.462858619364674,0.0
"def _wrap_forwarded(self, key, value):     if isinstance(value, SourceCode) and value.late_binding:         # get cached return value if present         value_ = self._late_binding_returnvalues.get(key, KeyError)         if value_ is KeyError:             # evaluate the late-bound function             value_ = self._eval_late_binding(value)             schema = self.late_bind_schemas.get(key)             <MASK>                 value_ = schema.validate(value_)             # cache result of late bound func             self._late_binding_returnvalues[key] = value_         return value_     else:         return value",if schema is not None :,if schema is not None :,100.00000000000004,100.00000000000004,1.0
"def convert(self, ctx, argument):     arg = argument.replace(""0x"", """").lower()     if arg[0] == ""#"":         arg = arg[1:]     try:         value = int(arg, base=16)         <MASK>             raise BadColourArgument(arg)         return discord.Colour(value=value)     except ValueError:         arg = arg.replace("" "", ""_"")         method = getattr(discord.Colour, arg, None)         if arg.startswith(""from_"") or method is None or not inspect.ismethod(method):             raise BadColourArgument(arg)         return method()",if not ( 0 <= value <= 0xFFFFFF ) :,if value is None :,3.466791587270993,3.466791587270993,0.0
"def get_versions(*, all=False, quiet=None):     import bonobo     from bonobo.util.pkgs import bonobo_packages     yield _format_version(bonobo, quiet=quiet)     if all:         for name in sorted(bonobo_packages):             <MASK>                 try:                     mod = __import__(name.replace(""-"", ""_""))                     try:                         yield _format_version(mod, name=name, quiet=quiet)                     except Exception as exc:                         yield ""{} ({})"".format(name, exc)                 except ImportError as exc:                     yield ""{} is not importable ({})."".format(name, exc)","if name != ""bonobo"" :","if name . startswith ( ""bonobo-"" ) :",10.552670315936318,10.552670315936318,0.0
"def assertOperationsInjected(self, plan, **kwargs):     for migration, _backward in plan:         operations = iter(migration.operations)         for operation in operations:             <MASK>                 next_operation = next(operations)                 self.assertIsInstance(                     next_operation, contenttypes_management.RenameContentType                 )                 self.assertEqual(next_operation.app_label, migration.app_label)                 self.assertEqual(next_operation.old_model, operation.old_name_lower)                 self.assertEqual(next_operation.new_model, operation.new_name_lower)","if isinstance ( operation , migrations . RenameModel ) :",if next ( operation ) :,12.462989337200145,12.462989337200145,0.0
"def valid_localparts(strip_delimiters=False):     for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""):         # strip line, skip over empty lines         line = line.strip()         if line == """":             continue         # skip over comments or empty lines         match = COMMENT.match(line)         if match:             continue         # skip over localparts with delimiters         <MASK>             if "","" in line or "";"" in line:                 continue         yield line",if strip_delimiters :,if strip_delimiters :,100.00000000000004,100.00000000000004,1.0
"def read_lccn(line, is_marc8=False):     found = []     for k, v in get_raw_subfields(line, [""a""]):         lccn = v.strip()         if re_question.match(lccn):             continue         m = re_lccn.search(lccn)         if not m:             continue         # remove letters and bad chars         lccn = re_letters_and_bad.sub("""", m.group(1)).strip()         <MASK>             found.append(lccn)     return found",if lccn :,if not is_marc8 :,9.652434877402245,9.652434877402245,0.0
"def test_named_parameters_and_constraints(self):     likelihood = gpytorch.likelihoods.GaussianLikelihood()     model = ExactGPModel(None, None, likelihood)     for name, _param, constraint in model.named_parameters_and_constraints():         if name == ""likelihood.noise_covar.raw_noise"":             self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan)         elif name == ""mean_module.constant"":             self.assertIsNone(constraint)         elif name == ""covar_module.raw_outputscale"":             self.assertIsInstance(constraint, gpytorch.constraints.Positive)         <MASK>             self.assertIsInstance(constraint, gpytorch.constraints.Positive)","elif name == ""covar_module.base_kernel.raw_lengthscale"" :","if name == ""covar_module.raw_outputscale"" :",48.59223278648542,48.59223278648542,0.0
"def _cleanupSocket(self):     """"""Close the Connection's socket.""""""     try:         self._sock.shutdown(socket.SHUT_WR)     except:         return     try:         while True:             r, w, e = select.select([self._sock], [], [])             <MASK>                 break     except:         pass     self._sock.close()",if not r or not self . _sock . recv ( 1024 ) :,if r == w :,2.383515454163372,2.383515454163372,0.0
"def fadeIn(self, acts=None, t=None, duration=None):     """"""Gradually switch on the input list of meshes by increasing opacity.""""""     if self.bookingMode:         acts, t, duration, rng = self._parse(acts, t, duration)         for tt in rng:             alpha = linInterpolate(tt, [t, t + duration], [0, 1])             self.events.append((tt, self.fadeIn, acts, alpha))     else:         for a in self._performers:             <MASK>                 continue             a.alpha(self._inputvalues)     return self",if a . alpha ( ) >= self . _inputvalues :,if a . alpha is None :,18.448373350246094,18.448373350246094,0.0
"def get_config_updates_recursive(self):     config_updates = self.config_updates.copy()     for sr_path, subrunner in self.subrunners.items():         <MASK>             continue         update = subrunner.get_config_updates_recursive()         if update:             config_updates[rel_path(self.path, sr_path)] = update     return config_updates","if not is_prefix ( self . path , sr_path ) :",if subrunner is None :,1.9026155630072006,1.9026155630072006,0.0
"def setArgs(self, **kwargs):     """"""See GridSearchCostGamma""""""     for key, value in list(kwargs.items()):         if key in (""folds"", ""nfolds""):             self._n_folds = int(value)         <MASK>             self._validator_kwargs[""max_epochs""] = value         else:             GridSearchDOE.setArgs(self, **{key: value})","elif key in ( ""max_epochs"" ) :","if key in ( ""max_epochs"" , ""epochs"" ) :",60.260809785571396,60.260809785571396,0.0
"def _parse_composite_axis(composite_axis_name: str):     axes_names = [axis for axis in composite_axis_name.split("" "") if len(axis) > 0]     for axis in axes_names:         <MASK>             continue         assert ""a"" <= axis[0] <= ""z""         for letter in axis:             assert str.isdigit(letter) or ""a"" <= letter <= ""z""     return axes_names","if axis == ""_"" :",if not axis [ 0 ] :,7.492442692259767,7.492442692259767,0.0
"def visit_For(self, node, for_branch=""body"", **kwargs):     if for_branch == ""body"":         self.sym_visitor.visit(node.target, store_as_param=True)         branch = node.body     elif for_branch == ""else"":         branch = node.else_     elif for_branch == ""test"":         self.sym_visitor.visit(node.target, store_as_param=True)         <MASK>             self.sym_visitor.visit(node.test)         return     else:         raise RuntimeError(""Unknown for branch"")     for item in branch or ():         self.sym_visitor.visit(item)",if node . test is not None :,if node . test :,38.80684294761701,38.80684294761701,0.0
def contains_only_whitespace(node):     if is_tag(node):         <MASK>             if not any([unicode(s).strip() for s in node.contents]):                 return True     return False,if not any ( [ not is_text ( s ) for s in node . contents ] ) :,"if node . tag == ""whitespace"" :",3.322086503981984,3.322086503981984,0.0
"def dir_tag_click(event):     mouse_index = self.path_bar.index(""@%d,%d"" % (event.x, event.y))     lineno = int(float(mouse_index))     if lineno == 1:         self.request_focus_into("""")     else:         assert lineno == 2         dir_range = get_dir_range(event)         if dir_range:             _, end_index = dir_range             path = self.path_bar.get(""2.0"", end_index)             <MASK>                 path += ""\\""             self.request_focus_into(path)","if path . endswith ( "":"" ) :",if path :,6.108851178104657,0.0,0.0
"def validate_employee_id(self):     if self.employee:         sales_person = frappe.db.get_value(""Sales Person"", {""employee"": self.employee})         <MASK>             frappe.throw(                 _(""Another Sales Person {0} exists with the same Employee id"").format(                     sales_person                 )             )",if sales_person and sales_person != self . name :,if sales_person :,11.688396478408103,11.688396478408103,0.0
"def pytest_collection_modifyitems(items):     for item in items:         if item.nodeid.startswith(""tests/infer""):             if ""stage"" not in item.keywords:                 item.add_marker(pytest.mark.stage(""unit""))             <MASK>                 item.add_marker(pytest.mark.init(rng_seed=123))","if ""init"" not in item . keywords :","if ""init"" not in item . keywords :",100.00000000000004,100.00000000000004,1.0
"def poll(self, timeout):     if timeout < 0:         timeout = None  # kqueue behaviour     events = self._kqueue.control(None, KqueueLoop.MAX_EVENTS, timeout)     results = defaultdict(lambda: POLL_NULL)     for e in events:         fd = e.ident         if e.filter == select.KQ_FILTER_READ:             results[fd] |= POLL_IN         <MASK>             results[fd] |= POLL_OUT     return results.items()",elif e . filter == select . KQ_FILTER_WRITE :,if e . filter == select . KQ_FILTER_WRITE :,91.93227152249175,91.93227152249175,0.0
"def _read_dimensions(self, *dimnames, **kwargs):     path = kwargs.get(""path"", ""/"")     try:         <MASK>             return [self.rootgrp.dimensions[dname] for dname in dimnames]         group = self.path2group[path]         return [group.dimensions[dname] for dname in dimnames]     except KeyError:         raise self.Error(             ""In file %s:\nError while reading dimensions: `%s` with kwargs: `%s`""             % (self.path, dimnames, kwargs)         )","if path == ""/"" :",if path in self . rootgrp . dimensions :,10.552670315936318,10.552670315936318,0.0
"def spam_to_me(address):     sock = eventlet.connect(address)     while True:         try:             sock.sendall(b""hello world"")             # Arbitrary delay to not use all available CPU, keeps the test             # running quickly and reliably under a second             time.sleep(0.001)         except socket.error as e:             <MASK>                 return             raise",if get_errno ( e ) == errno . EPIPE :,if e . errno == socket . EINPROGRESS :,8.503662878579146,8.503662878579146,0.0
"def has_hash_of(self, destpath, code, package_level):     """"""Determine if a file has the hash of the code.""""""     if destpath is not None and os.path.isfile(destpath):         with univ_open(destpath, ""r"") as opened:             compiled = readfile(opened)         hashash = gethash(compiled)         <MASK>             return True     return False","if hashash is not None and hashash == self . comp . genhash ( code , package_level ) :",if hashash == code :,2.7629077445603603,2.7629077445603603,0.0
"def insert(self, index, item):     if len(self.lists) == 1:         self.lists[0].insert(index, item)         self._balance_list(0)     else:         list_idx, rel_idx = self._translate_index(index)         <MASK>             raise IndexError()         self.lists[list_idx].insert(rel_idx, item)         self._balance_list(list_idx)     return",if list_idx is None :,if list_idx == 0 :,36.55552228545123,36.55552228545123,0.0
"def _parse_class_simplified(symbol):     results = {}     name = symbol.name + ""(""     name += "", "".join([analyzer.expand_attribute(base) for base in symbol.bases])     name += "")""     for sym in symbol.body:         if isinstance(sym, ast.FunctionDef):             result = _parse_function_simplified(sym, symbol.name)             results.update(result)         <MASK>             result = _parse_class_simplified(sym)             results.update(result)     lineno = symbol.lineno     for decorator in symbol.decorator_list:         lineno += 1     results[lineno] = (name, ""c"")     return results","elif isinstance ( sym , ast . ClassDef ) :","if isinstance ( sym , ast . ClassDef ) :",88.01117367933934,88.01117367933934,0.0
"def append_vars(pairs, result):     for name, value in sorted(pairs.items()):         if isinstance(value, list):             value = ""[%s]"" % "","".join(value)         <MASK>             result.append(""%s:%s=%s"" % (package, name, value))         else:             result.append(""%s=%s"" % (name, value))",if package :,if package :,100.00000000000004,0.0,1.0
"def nextEditable(self):     """"""Moves focus of the cursor to the next editable window""""""     if self.currentEditable is None:         <MASK>             self._currentEditableRef = self._editableChildren[0]     else:         for ref in weakref.getweakrefs(self.currentEditable):             if ref in self._editableChildren:                 cei = self._editableChildren.index(ref)                 nei = cei + 1                 if nei >= len(self._editableChildren):                     nei = 0                 self._currentEditableRef = self._editableChildren[nei]     return self.currentEditable",if len ( self . _editableChildren ) :,if self . currentEditableRef == self . _editableRef :,15.851165692617148,15.851165692617148,0.0
"def everythingIsUnicode(d):     """"""Takes a dictionary, recursively verifies that every value is unicode""""""     for k, v in d.iteritems():         if isinstance(v, dict) and k != ""headers"":             if not everythingIsUnicode(v):                 return False         elif isinstance(v, list):             for i in v:                 if isinstance(i, dict) and not everythingIsUnicode(i):                     return False                 elif isinstance(i, _bytes):                     return False         <MASK>             return False     return True","elif isinstance ( v , _bytes ) :","if not isinstance ( i , _bytes ) :",45.180100180492246,45.180100180492246,0.0
"def is_valid(sample):     if sample is None:         return False     if isinstance(sample, tuple):         for s in sample:             if s is None:                 return False             <MASK>                 return False             elif isinstance(s, collections.abc.Sequence) and len(s) == 0:                 return False     return True","elif isinstance ( s , np . ndarray ) and s . size == 0 :","if not isinstance ( s , collections . abc . Sequence ) :",16.90643025058878,16.90643025058878,0.0
"def scan_resource_conf(self, conf):     if ""properties"" in conf:         if ""attributes"" in conf[""properties""]:             if ""exp"" in conf[""properties""][""attributes""]:                 <MASK>                     return CheckResult.PASSED     return CheckResult.FAILED","if conf [ ""properties"" ] [ ""attributes"" ] [ ""exp"" ] :","if ""exp"" in conf [ ""properties"" ] [ ""exp"" ] :",67.32677771875353,67.32677771875353,0.0
"def encode(self):     if self.expr in gpregs.expr:         self.value = gpregs.expr.index(self.expr)         self.parent.rot2.value = 0     elif isinstance(self.expr, ExprOp) and self.expr.op == allshifts[3]:         reg, value = self.expr.args         <MASK>             return False         self.value = gpregs.expr.index(reg)         if not isinstance(value, ExprInt):             return False         value = int(value)         if not value in [8, 16, 24]:             return False         self.parent.rot2.value = value // 8     return True",if reg not in gpregs . expr :,if reg not in gpregs . reg :,70.71067811865478,70.71067811865478,0.0
"def validate_transaction_reference(self):     bank_account = self.paid_to if self.payment_type == ""Receive"" else self.paid_from     bank_account_type = frappe.db.get_value(""Account"", bank_account, ""account_type"")     if bank_account_type == ""Bank"":         <MASK>             frappe.throw(                 _(""Reference No and Reference Date is mandatory for Bank transaction"")             )",if not self . reference_no or not self . reference_date :,"if bank_account_type == ""Reference"" :",3.7298577910273503,3.7298577910273503,0.0
"def monad(self):     if not self.cls_bl_idname:         return None     for monad in bpy.data.node_groups:         if hasattr(monad, ""cls_bl_idname""):             <MASK>                 return monad     return None",if monad . cls_bl_idname == self . cls_bl_idname :,if monad . cls_bl_idname == self . cls_bl_idname :,100.00000000000004,100.00000000000004,1.0
"def _create_mask(self, plen):     mask = []     for i in range(16):         if plen >= 8:             mask.append(0xFF)         <MASK>             mask.append(0xFF >> (8 - plen) << (8 - plen))         else:             mask.append(0x00)         plen -= 8     return mask",elif plen > 0 :,if i % 8 :,10.682175159905848,10.682175159905848,0.0
"def dataset_to_stream(dataset, input_name):     """"""Takes a tf.Dataset and creates a numpy stream of ready batches.""""""     # All input-pipeline processing should be on CPU.     for example in fastmath.dataset_as_numpy(dataset):         features = example[0]         inp, out = features[input_name], example[1]         mask = features[""mask""] if ""mask"" in features else None         # Some accelerators don't handle uint8 well, cast to int.         <MASK>             inp = inp.astype(np.int32)         if isinstance(out, np.uint8):             out = out.astype(np.int32)         yield (inp, out) if mask is None else (inp, out, mask)","if isinstance ( inp , np . uint8 ) :","if isinstance ( inp , np . uint8 ) :",100.00000000000004,100.00000000000004,1.0
"def _idle_redraw_cb(self):     assert self._idle_redraw_src_id is not None     queue = self._idle_redraw_queue     if len(queue) > 0:         bbox = queue.pop(0)         <MASK>             super(CanvasRenderer, self).queue_draw()         else:             super(CanvasRenderer, self).queue_draw_area(*bbox)     if len(queue) == 0:         self._idle_redraw_src_id = None         return False     return True",if bbox is None :,if bbox is None :,100.00000000000004,100.00000000000004,1.0
"def mutated(self, indiv):     """"""mutate some genes of the given individual""""""     res = indiv.copy()     # to avoid having a child identical to one of the currentpopulation'''     for i in range(self.numParameters):         <MASK>             if self.xBound is None:                 res[i] = indiv[i] + gauss(0, self.mutationStdDev)             else:                 res[i] = max(                     min(indiv[i] + gauss(0, self.mutationStdDev), self.maxs[i]),                     self.mins[i],                 )     return res",if random ( ) < self . mutationProb :,if self . xBound == self . xBound :,9.980099403873663,9.980099403873663,0.0
"def _justifyDrawParaLine(tx, offset, extraspace, words, last=0):     setXPos(tx, offset)     text = b"" "".join(words)     if last:         # last one, left align         tx._textOut(text, 1)     else:         nSpaces = len(words) - 1         <MASK>             tx.setWordSpace(extraspace / float(nSpaces))             tx._textOut(text, 1)             tx.setWordSpace(0)         else:             tx._textOut(text, 1)     setXPos(tx, -offset)     return offset",if nSpaces :,if extraspace :,34.66806371753173,0.0,0.0
"def _read_0(self, stream):     r = b""""     while True:         c = stream.read(2)         <MASK>             raise EOFError()         if c == b""\x00\x00"":             break         r += c     return r.decode(self.encoding)",if len ( c ) != 2 :,"if c == b""\x00"" :",5.934202609760488,5.934202609760488,0.0
"def run(self, app, editor, args):     line_nums = []     for cursor in editor.cursors:         <MASK>             line_nums.append(cursor.y)             data = editor.lines[cursor.y].get_data().upper()             editor.lines[cursor.y].set_data(data)",if cursor . y not in line_nums :,if cursor . y not in line_nums :,100.00000000000004,100.00000000000004,1.0
"def create_default_energy_point_rules():     for rule in get_default_energy_point_rules():         # check if any rule for ref. doctype exists         rule_exists = frappe.db.exists(             ""Energy Point Rule"", {""reference_doctype"": rule.get(""reference_doctype"")}         )         <MASK>             continue         doc = frappe.get_doc(rule)         doc.insert(ignore_permissions=True)",if rule_exists :,if not rule_exists :,53.7284965911771,53.7284965911771,0.0
"def __new__(cls, *nodes):     if not nodes:         raise TypeError(""DisjunctionNode() requires at least one node"")     elif len(nodes) == 1:         return nodes[0]     self = super(DisjunctionNode, cls).__new__(cls)     self.__nodes = []     # TODO: Remove duplicates?     for node in nodes:         if not isinstance(node, Node):             raise TypeError(                 ""DisjunctionNode() expects Node instances as arguments;""                 "" received a non-Node instance %r"" % node             )         <MASK>             self.__nodes.extend(node.__nodes)         else:             self.__nodes.append(node)     return self","if isinstance ( node , DisjunctionNode ) :",if node . __nodes :,7.492442692259767,7.492442692259767,0.0
def dfs(v: str) -> Iterator[Set[str]]:     index[v] = len(stack)     stack.append(v)     boundaries.append(index[v])     for w in edges[v]:         if w not in index:             yield from dfs(w)         <MASK>             while index[w] < boundaries[-1]:                 boundaries.pop()     if boundaries[-1] == index[v]:         boundaries.pop()         scc = set(stack[index[v] :])         del stack[index[v] :]         identified.update(scc)         yield scc,elif w not in identified :,if index [ w ] < boundaries [ 0 ] :,4.456882760699063,4.456882760699063,0.0
"def unpack_item_obj(map_uuid_global_id, misp_obj):     obj_meta = get_object_metadata(misp_obj)     obj_id = None     io_content = None     for attribute in misp_obj.attributes:         <MASK>             obj_id = attribute.value  # # TODO: sanitize             io_content = attribute.data  # # TODO: check if type == io     if obj_id and io_content:         res = Item.create_item(obj_id, obj_meta, io_content)         map_uuid_global_id[misp_obj.uuid] = get_global_id(""item"", obj_id)","if attribute . object_relation == ""raw-data"" :","if attribute . type == ""id"" :",24.437032551865375,24.437032551865375,0.0
"def parse(self, response):     soup = BeautifulSoup(response.content.decode(""utf-8"", ""ignore""), ""lxml"")     image_divs = soup.find_all(""div"", class_=""imgpt"")     pattern = re.compile(r""murl\"":\""(.*?)\.jpg"")     for div in image_divs:         href_str = html_parser.HTMLParser().unescape(div.a[""m""])         match = pattern.search(href_str)         <MASK>             name = match.group(1) if six.PY3 else match.group(1).encode(""utf-8"")             img_url = ""{}.jpg"".format(name)             yield dict(file_url=img_url)",if match :,if match :,100.00000000000004,0.0,1.0
"def filter_errors(self, errors: List[str]) -> List[str]:     real_errors: List[str] = list()     current_file = __file__     current_path = os.path.split(current_file)     for line in errors:         line = line.strip()         if not line:             continue         fn, lno, lvl, msg = self.parse_trace_line(line)         <MASK>             _path = os.path.split(fn)             if _path[-1] != current_path[-1]:                 continue         real_errors.append(line)     return real_errors",if fn is not None :,if lno != 0 :,9.652434877402245,9.652434877402245,0.0
"def decompileFormat1(self, reader, otFont):     self.classDefs = classDefs = []     startGlyphID = reader.readUShort()     glyphCount = reader.readUShort()     for i in range(glyphCount):         glyphName = otFont.getglyphName(startGlyphID + i)         classValue = reader.readUShort()         <MASK>             classDefs.append((glyphName, classValue))",if classValue :,if classValue != 0 :,17.965205598154213,17.965205598154213,0.0
"def compress(self, data_list):     if len(data_list) == 2:         value, lookup_expr = data_list         <MASK>             if lookup_expr not in EMPTY_VALUES:                 return Lookup(value=value, lookup_expr=lookup_expr)             else:                 raise forms.ValidationError(                     self.error_messages[""lookup_required""], code=""lookup_required""                 )     return None",if value not in EMPTY_VALUES :,if value not in EMPTY_VALUES :,100.00000000000004,100.00000000000004,1.0
"def open_compat(path, mode=""r""):     if mode in [""r"", ""rb""] and not os.path.exists(path):         raise FileNotFoundError(u'The file ""%s"" could not be found' % path)     if sys.version_info >= (3,):         encoding = ""utf-8""         errors = ""replace""         <MASK>             encoding = None             errors = None         return open(path, mode, encoding=encoding, errors=errors)     else:         return open(path, mode)","if mode in [ ""rb"" , ""wb"" , ""ab"" ] :",if os . path . isfile ( path ) :,2.4779853471705344,2.4779853471705344,0.0
"def filter_errors(self, errors: List[str]) -> List[str]:     real_errors: List[str] = list()     current_file = __file__     current_path = os.path.split(current_file)     for line in errors:         line = line.strip()         if not line:             continue         fn, lno, lvl, msg = self.parse_trace_line(line)         if fn is not None:             _path = os.path.split(fn)             <MASK>                 continue         real_errors.append(line)     return real_errors",if _path [ - 1 ] != current_path [ - 1 ] :,"if lno != 0 or lvl != 1 or msg != ""Error"" :",5.401157445454033,5.401157445454033,0.0
"def filter_by_level(record, level_per_module):     name = record[""name""]     level = 0     if name in level_per_module:         level = level_per_module[name]     elif name is not None:         lookup = """"         if """" in level_per_module:             level = level_per_module[""""]         for n in name.split("".""):             lookup += n             <MASK>                 level = level_per_module[lookup]             lookup += "".""     if level is False:         return False     return record[""level""].no >= level",if lookup in level_per_module :,if lookup in level_per_module :,100.00000000000004,100.00000000000004,1.0
"def CountButtons(self):     """"""Returns the number of visible buttons in the docked pane.""""""     n = 0     if self.HasCaption() or self.HasCaptionLeft():         if isinstance(wx.GetTopLevelParent(self.window), AuiFloatingFrame):             return 1         if self.HasCloseButton():             n += 1         <MASK>             n += 1         if self.HasMinimizeButton():             n += 1         if self.HasPinButton():             n += 1     return n",if self . HasMaximizeButton ( ) :,if self . HasCloseButton ( ) :,41.11336169005196,41.11336169005196,0.0
"def search(a, b, desired):     if a == b:         return a     if abs(b - a) < 0.005:         ca = count(a)         cb = count(b)         dista = abs(desired - ca)         distb = abs(desired - cb)         <MASK>             return a         else:             return b     m = (a + b) / 2.0     cm = count(m)     if desired < cm:         return search(m, b, desired)     else:         return search(a, m, desired)",if dista < distb :,if dista < distb :,100.00000000000004,100.00000000000004,1.0
"def force_ipv4(self, *args):     """"""only ipv4 localhost in /etc/hosts""""""     logg.debug(""checking /etc/hosts for '::1 localhost'"")     lines = []     for line in open(self.etc_hosts()):         <MASK>             newline = re.sub(""\\slocalhost\\s"", "" "", line)             if line != newline:                 logg.info(""/etc/hosts: '%s' => '%s'"", line.rstrip(), newline.rstrip())                 line = newline         lines.append(line)     f = open(self.etc_hosts(), ""w"")     for line in lines:         f.write(line)     f.close()","if ""::1"" in line :",if line :,8.525588607164655,0.0,0.0
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]:     yield ""Core"", ""0""     for _dir in data_manager.cog_data_path().iterdir():         fpath = _dir / ""settings.json""         <MASK>             continue         with fpath.open() as f:             try:                 data = json.load(f)             except json.JSONDecodeError:                 continue         if not isinstance(data, dict):             continue         cog_name = _dir.stem         for cog_id, inner in data.items():             if not isinstance(inner, dict):                 continue             yield cog_name, cog_id",if not fpath . exists ( ) :,if not fpath :,23.50540321304655,23.50540321304655,0.0
"def _get_dbutils():     try:         import IPython         ip_shell = IPython.get_ipython()         <MASK>             raise _NoDbutilsError         return ip_shell.ns_table[""user_global""][""dbutils""]     except ImportError:         raise _NoDbutilsError     except KeyError:         raise _NoDbutilsError",if ip_shell is None :,if ip_shell is None :,100.00000000000004,100.00000000000004,1.0
"def _bytecode_filenames(self, py_filenames):     bytecode_files = []     for py_file in py_filenames:         # Since build_py handles package data installation, the         # list of outputs can contain more than just .py files.         # Make sure we only report bytecode for the .py files.         ext = os.path.splitext(os.path.normcase(py_file))[1]         <MASK>             continue         if self.compile:             bytecode_files.append(py_file + ""c"")         if self.optimize > 0:             bytecode_files.append(py_file + ""o"")     return bytecode_files",if ext != PYTHON_SOURCE_EXTENSION :,"if ext != "".py"" :",28.24099048856542,28.24099048856542,0.0
"def compute_distances_mu(line, pts, result, gates, tolerance):     """"""calculate all distances with mathuutils""""""     line_origin = V(line[0])     line_end = V(line[-1])     local_result = [[], [], [], [], []]     for point in pts:         data = compute_distance(V(point), line_origin, line_end, tolerance)         for i, res in enumerate(local_result):             res.append(data[i])     for i, res in enumerate(result):         <MASK>             res.append(local_result[i])",if gates [ i ] :,if res [ 0 ] == gates :,7.129384882260374,7.129384882260374,0.0
"def _get_next_segment(self, segment_path, page_size, segment_cursor=None):     if segment_path:         <MASK>             return None         return Segment(self.client, segment_path, page_size, segment_cursor)     return None",if self . end_time and self . _is_later_than_end_time ( segment_path ) :,if not self . client :,0.8135814099134429,0.8135814099134429,0.0
"def _check_number_of_sessions():     nb_desktop_sessions = sessions.get_number_of_desktop_sessions(ignore_gdm=True)     if nb_desktop_sessions > 1:         print(             ""WARNING : There are %d other desktop sessions open. The GPU switch will not become effective until you have manually""             "" logged out from ALL desktop sessions.\n""             ""Continue ? (y/N)"" % (nb_desktop_sessions - 1)         )         confirmation = ask_confirmation()         <MASK>             sys.exit(0)",if not confirmation :,if confirmation :,45.13864405503391,0.0,0.0
"def delete_compute_environment(self, compute_environment_name):     if compute_environment_name is None:         raise InvalidParameterValueException(""Missing computeEnvironment parameter"")     compute_env = self.get_compute_environment(compute_environment_name)     if compute_env is not None:         # Pop ComputeEnvironment         self._compute_environments.pop(compute_env.arn)         # Delete ECS cluster         self.ecs_backend.delete_cluster(compute_env.ecs_name)         <MASK>             # Delete compute environment             instance_ids = [instance.id for instance in compute_env.instances]             self.ec2_backend.terminate_instances(instance_ids)","if compute_env . env_type == ""MANAGED"" :",if compute_env . instances is not None :,27.559110500755533,27.559110500755533,0.0
"def run(self):     results = {}     for func_name in [         # Execute every function starting with check_*         fn         for fn in self.check_functions         # if the user does not specify any name         if not self.args.get(""check"")         # of if specify the current function name         or self.args.get(""check"") == fn     ]:         function = getattr(self, func_name)         log.warn(function.__doc__)         result = function()         <MASK>             log.info(""\n"".join(result))             results.update({func_name: result})     return results",if result :,if result :,100.00000000000004,0.0,1.0
"def invalidate(self, layers=None):     if layers is None:         layers = Layer.AllLayers     if layers:         layers = set(layers)         self.invalidLayers.update(layers)         blockRenderers = [             br             for br in self.blockRenderers             <MASK>         ]         if len(blockRenderers) < len(self.blockRenderers):             self.forgetDisplayLists()         self.blockRenderers = blockRenderers         if self.renderer.showRedraw and Layer.Blocks in layers:             self.needsRedisplay = True",if br . layer is Layer . Blocks or br . layer not in layers,if br . isDisplayList ( ) :,7.796037894057233,7.796037894057233,0.0
"def get_library_dirs(platform, arch=None):     if platform == ""win32"":         jre_home = get_jre_home(platform)         jdk_home = JAVA_HOME         <MASK>             jre_home = jre_home.decode(""utf-8"")         return [join(jdk_home, ""lib""), join(jdk_home, ""bin"", ""server"")]     elif platform == ""android"":         return [""libs/{}"".format(arch)]     return []","if isinstance ( jre_home , bytes ) :",if arch :,3.361830360737634,0.0,0.0
"def save_plugin_options(self):     for name, option_widgets in self._plugin_option_widgets.items():         <MASK>             self.config[""plugins""][name] = {}         plugin_config = self.config[""plugins""][             name         ]  # use or instead of get incase the value is actually None         for option_name, option_widget in option_widgets.items():             plugin_config[option_name] = option_widget.option.get_widget_value(                 option_widget.widget             )","if name not in self . config [ ""plugins"" ] :","if name not in self . config [ ""plugins"" ] :",100.00000000000004,100.00000000000004,1.0
"def _select_block(str_in, start_tag, end_tag):     """"""Select first block delimited by start_tag and end_tag""""""     start_pos = str_in.find(start_tag)     if start_pos < 0:         raise ValueError(""start_tag not found"")     depth = 0     for pos in range(start_pos, len(str_in)):         if str_in[pos] == start_tag:             depth += 1         elif str_in[pos] == end_tag:             depth -= 1         <MASK>             break     sel = str_in[start_pos + 1 : pos]     return sel",if depth == 0 :,if depth > 0 :,24.736929544091932,24.736929544091932,0.0
"def _coerce_to_bool(self, node, var, true_val=True):     """"""Coerce the values in a variable to bools.""""""     bool_var = self.program.NewVariable()     for b in var.bindings:         v = b.data         if isinstance(v, mixin.PythonConstant) and isinstance(v.pyval, bool):             const = v.pyval is true_val         elif not compare.compatible_with(v, True):             const = not true_val         <MASK>             const = true_val         else:             const = None         bool_var.AddBinding(self.convert.bool_values[const], {b}, node)     return bool_var","elif not compare . compatible_with ( v , False ) :",if const is True :,2.156693969393992,2.156693969393992,0.0
def multiline_indentation(self):     if self._multiline_indentation is None:         offset = 0         <MASK>             offset = 2         indentation = make_indentation(3 * self.indent_size + offset)         self._multiline_indentation = indentation     if self.current_rule:         indent_extra = make_indentation(self.indent_size)         return self._multiline_indentation + indent_extra     return self._multiline_indentation,if self . show_aligned_keywords :,if self . current_rule :,20.873176328735713,20.873176328735713,0.0
"def __call__(self, event, data=None):     datatype, delta = event     self.midi_ctrl.delta += delta     if TIMING_CLOCK in datatype and not self.played:         self.midi_ctrl.pulse += 1         <MASK>             t_master = 60.0             self.midi_ctrl.bpm = round(60.0 / self.midi_ctrl.delta, 0)             self.midi_ctrl.pulse = 0             self.midi_ctrl.delta = 0.0",if self . midi_ctrl . pulse == self . midi_ctrl . ppqn :,if delta > t_master :,1.7955716566806617,1.7955716566806617,0.0
"def handle_sent(self, elt):     sent = []     for child in elt:         <MASK>             itm = self.handle_word(child)             if self._unit == ""word"":                 sent.extend(itm)             else:                 sent.append(itm)         else:             raise ValueError(""Unexpected element %s"" % child.tag)     return SemcorSentence(elt.attrib[""snum""], sent)","if child . tag in ( ""wf"" , ""punc"" ) :","if child . tag == ""sent"" :",18.32556812998321,18.32556812998321,0.0
"def _handle_def_errors(testdef):     # If the test generation had an error, raise     if testdef.error:         <MASK>             if isinstance(testdef.exception, Exception):                 raise testdef.exception             else:                 raise Exception(testdef.exception)         else:             raise Exception(""Test parse failure"")",if testdef . exception :,if testdef . exception :,100.00000000000004,100.00000000000004,1.0
"def _authorized_sid(self, jid, sid, ifrom, iq):     with self._preauthed_sids_lock:         <MASK>             del self._preauthed_sids[(jid, sid, ifrom)]             return True         return False","if ( jid , sid , ifrom ) in self . _preauthed_sids :",ifiq . is_authorized ( ) :,3.0379156085209207,3.0379156085209207,0.0
"def wait(self, timeout=None):     if self.returncode is None:         <MASK>             msecs = _subprocess.INFINITE         else:             msecs = max(0, int(timeout * 1000 + 0.5))         res = _subprocess.WaitForSingleObject(int(self._handle), msecs)         if res == _subprocess.WAIT_OBJECT_0:             code = _subprocess.GetExitCodeProcess(self._handle)             if code == TERMINATE:                 code = -signal.SIGTERM             self.returncode = code     return self.returncode",if timeout is None :,if timeout is None :,100.00000000000004,100.00000000000004,1.0
"def _gen_legal_y_s_t(self):     while True:         y = self._gen_random_scalar()         s = self.tec_arithmetic.mul(             scalar=y, a=self.tec_arithmetic.get_generator()         )  # S = yG         t = self._hash_tec_element(s)         <MASK>             # Both S and T are legal             LOGGER.info(""randomly generated y, S, T"")             return y, s, t",if self . tec_arithmetic . is_in_group ( s ) and type ( t ) != int :,if t is not None :,0.571493023920888,0.571493023920888,0.0
"def write_out():     while True:         <MASK>             time.sleep(0.1)             continue         data_str = self.instrument_queue.get()         data_str = data_str.splitlines()         tb.write("""")  # position cursor to end         for line in data_str:             tb.write(line)         tb.write(""\n"")",if self . instrument_queue . empty ( ) :,if not self . instrument_queue :,38.64911638244347,38.64911638244347,0.0
"def _parse_preamble(self):     """"""Parse metadata about query (PRIVATE).""""""     meta = {}     while self.line:         regx = re.search(_RE_QUERY, self.line)         if regx:             self.query_id = regx.group(1)         <MASK>             self.seq_len = int(self.line.strip().split()[1])         self.line = self.handle.readline().strip()     return meta","if self . line . startswith ( ""Match_columns"" ) :","if self . query_id == ""PRIVATE"" :",13.188274750399428,13.188274750399428,0.0
"def init_sequence(self, coll_name, seq_config):     if not isinstance(seq_config, list):         raise Exception('""sequence"" config must be a list')     handlers = []     for entry in seq_config:         <MASK>             raise Exception('""sequence"" entry must be a dict')         name = entry.get(""name"", """")         handler = self.load_coll(name, entry)         handlers.append(handler)     return HandlerSeq(handlers)","if not isinstance ( entry , dict ) :","if not isinstance ( entry , dict ) :",100.00000000000004,100.00000000000004,1.0
"def change_args_to_dict(string):     if string is None:         return None     ans = []     strings = string.split(""\n"")     ind = 1     start = 0     while ind <= len(strings):         if ind < len(strings) and strings[ind].startswith("" ""):             ind += 1         else:             <MASK>                 ans.append(""\n"".join(strings[start:ind]))             start = ind             ind += 1     d = {}     for line in ans:         if "":"" in line and len(line) > 0:             lines = line.split("":"")             d[lines[0]] = lines[1].strip()     return d",if start < ind :,if ind < len ( strings ) :,7.809849842300637,7.809849842300637,0.0
"def wait(self):     while True:         return_code = self._process.poll()         if return_code is not None:             line = self._process.stdout.readline().decode(""utf-8"")             <MASK>                 break             log.debug(line.strip(""\n""))     return True","if line == """" :",if line is None :,15.848738972120703,15.848738972120703,0.0
"def __getattr__(self, key):     for tag in self.tag.children:         <MASK>             continue         if ""name"" in tag.attrs and tag.attrs[""name""] in (key,):             from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation             return DOMImplementation.createHTMLElement(self.doc, tag)     raise AttributeError","if tag . name not in ( ""input"" , ) :",if not tag . attrs :,6.356491690403502,6.356491690403502,0.0
"def compare_hash(hash_of_gold, path_to_file):     with open(path_to_file, ""rb"") as f:         hash_of_file = hashlib.sha256(f.read()).hexdigest()         <MASK>             print(                 ""########## Hash sum of"",                 path_to_file,                 ""differs from the target, the topology will be deleted !!! ##########"",             )             shutil.rmtree(os.path.dirname(path_to_file))",if hash_of_file != hash_of_gold :,if hash_of_gold == hash_of_file :,71.38957847176475,71.38957847176475,0.0
def on_completed2():     doner[0] = True     if not qr:         <MASK>             observer.on_next(False)             observer.on_completed()         elif donel[0]:             observer.on_next(True)             observer.on_completed(),if len ( ql ) > 0 :,if donel [ 0 ] :,7.654112967106117,7.654112967106117,0.0
"def get_other(self, data, items):     is_tuple = False     if type(data) == tuple:         data = list(data)         is_tuple = True     if type(data) == list:         m_items = items.copy()         for idx, item in enumerate(items):             <MASK>                 m_items[idx] = len(data) - abs(item)         for i in sorted(set(m_items), reverse=True):             if i < len(data) and i > -1:                 del data[i]         if is_tuple:             return tuple(data)         else:             return data     else:         return None",if item < 0 :,if idx < len ( data ) :,7.267884212102741,7.267884212102741,0.0
"def _open_url(cls, url):     if config.browser:         cmd = [config.browser, url]         <MASK>             print(""running command: %s"" % "" "".join(cmd))         p = Popen(cmd)         p.communicate()     else:         if not config.quiet:             print(""opening URL in browser: %s"" % url)         webbrowser.open_new(url)",if not config . quiet :,if not config . quiet :,100.00000000000004,100.00000000000004,1.0
"def setLabel(self, s, protect=False):     """"""Set the label of the minibuffer.""""""     c, k, w = self.c, self, self.w     if w:         # Support for the curses gui.         if hasattr(g.app.gui, ""set_minibuffer_label""):             g.app.gui.set_minibuffer_label(c, s)         w.setAllText(s)         n = len(s)         w.setSelectionRange(n, n, insert=n)         <MASK>             k.mb_prefix = s",if protect :,if protect :,100.00000000000004,0.0,1.0
"def __init__(self, path):     self.symcaches = []     for path in path.split("";""):         if os.path.isdir(path):             self.symcaches.append(SymbolCache(dirname=path))             continue         <MASK>             import cobra             self.symcaches.append(cobra.CobraProxy(path))             continue","if path . startswith ( ""cobra://"" ) or path . startswith ( ""cobrassl://"" ) :",if os . path . isfile ( path ) :,3.1492572798507688,3.1492572798507688,0.0
"def init_params(net):     """"""Init layer parameters.""""""     for module in net.modules():         if isinstance(module, nn.Conv2d):             init.kaiming_normal(module.weight, mode=""fan_out"")             if module.bias:                 init.constant(module.bias, 0)         <MASK>             init.constant(module.weight, 1)             init.constant(module.bias, 0)         elif isinstance(module, nn.Linear):             init.normal(module.weight, std=1e-3)             if module.bias:                 init.constant(module.bias, 0)","elif isinstance ( module , nn . BatchNorm2d ) :",if module . weight :,5.171845311465849,5.171845311465849,0.0
"def _diff_dict(self, old, new):     diff = {}     removed = []     added = []     for key, value in old.items():         <MASK>             removed.append(key)         elif old[key] != new[key]:             # modified is indicated by a remove and add             removed.append(key)             added.append(key)     for key, value in new.items():         if key not in old:             added.append(key)     if removed:         diff[""removed""] = sorted(removed)     if added:         diff[""added""] = sorted(added)     return diff",if key not in new :,if key not in new :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, *args, **kwargs):     _kwargs = {         ""max_length"": 20,         ""widget"": forms.TextInput(attrs={""autocomplete"": ""off""}),         ""label"": _(""Card number""),     }     if ""types"" in kwargs:         self.accepted_cards = set(kwargs.pop(""types""))         difference = self.accepted_cards - VALID_CARDS         <MASK>             raise ImproperlyConfigured(                 ""The following accepted_cards are "" ""unknown: %s"" % difference             )     _kwargs.update(kwargs)     super().__init__(*args, **_kwargs)",if difference :,if difference != VALID_CARDS :,12.22307556087252,12.22307556087252,0.0
"def dumps(self):     sections = []     for name, env_info in self._dependencies_.items():         sections.append(""[ENV_%s]"" % name)         for var, values in sorted(env_info.vars.items()):             tmp = ""%s="" % var             <MASK>                 tmp += ""[%s]"" % "","".join(['""%s""' % val for val in values])             else:                 tmp += ""%s"" % values             sections.append(tmp)     return ""\n"".join(sections)","if isinstance ( values , list ) :",if values :,7.49553326588684,0.0,0.0
"def air_quality(self):     aqi_data = self._get_aqi_data()     if aqi_data:         if aqi_data.get(""status"") == ""ok"":             aqi_data = self._organize(aqi_data)             aqi_data = self._manipulate(aqi_data)         <MASK>             self.py3.error(aqi_data.get(""data""))     return {         ""cached_until"": self.py3.time_in(self.cache_timeout),         ""full_text"": self.py3.safe_format(self.format, aqi_data),     }","elif aqi_data . get ( ""status"" ) == ""error"" :",if not aqi_data :,5.190782388638235,5.190782388638235,0.0
"def _blend(x, y):  # pylint: disable=invalid-name     """"""Implements the ""blend"" strategy for `deep_merge`.""""""     if isinstance(x, (dict, OrderedDict)):         if not isinstance(y, (dict, OrderedDict)):             return y         return _merge(x, y, recursion_func=_blend)     if isinstance(x, (list, tuple)):         <MASK>             return y         result = [_blend(*i) for i in zip(x, y)]         if len(x) > len(y):             result += x[len(y) :]         elif len(x) < len(y):             result += y[len(x) :]         return result     return y","if not isinstance ( y , ( list , tuple ) ) :","if not isinstance ( y , ( list , tuple ) ) :",100.00000000000004,100.00000000000004,1.0
"def _rate(cls, sample1, sample2):     ""Simple rate""     try:         interval = sample2[0] - sample1[0]         <MASK>             raise Infinity()         delta = sample2[1] - sample1[1]         if delta < 0:             raise UnknownValue()         return (sample2[0], delta / interval, sample2[2], sample2[3])     except Infinity:         raise     except UnknownValue:         raise     except Exception as e:         raise NaN(e)",if interval == 0 :,if interval < 0 :,24.736929544091932,24.736929544091932,0.0
"def wrapped_request_method(*args, **kwargs):     """"""Modifies HTTP headers to include a specified user-agent.""""""     if kwargs.get(""headers"") is not None:         <MASK>             if user_agent not in kwargs[""headers""][""user-agent""]:                 # Save the existing user-agent header and tack on our own.                 kwargs[""headers""][""user-agent""] = (                     f""{user_agent} "" f'{kwargs[""headers""][""user-agent""]}'                 )         else:             kwargs[""headers""][""user-agent""] = user_agent     else:         kwargs[""headers""] = {""user-agent"": user_agent}     return request_method(*args, **kwargs)","if kwargs [ ""headers"" ] . get ( ""user-agent"" ) :","if ""user-agent"" in kwargs [ ""headers"" ] :",43.48783281197403,43.48783281197403,0.0
"def remove_addons(auth, resource_object_list):     for config in AbstractNode.ADDONS_AVAILABLE:         try:             settings_model = config.node_settings         except LookupError:             settings_model = None         <MASK>             addon_list = settings_model.objects.filter(                 owner__in=resource_object_list, is_deleted=False             )             for addon in addon_list:                 addon.after_delete(auth.user)",if settings_model :,if settings_model is None :,43.47208719449914,43.47208719449914,0.0
"def Decorator(*args, **kwargs):     delay = 0.2     num_attempts = 15     cur_attempt = 0     while True:         try:             return f(*args, **kwargs)         except exceptions.WebDriverException as e:             logging.warning(""Selenium raised %s"", utils.SmartUnicode(e))             cur_attempt += 1             <MASK>                 raise             time.sleep(delay)",if cur_attempt == num_attempts :,if cur_attempt >= num_attempts :,65.80370064762461,65.80370064762461,0.0
"def _cleanup_parts_dir(parts_dir, local_plugins_dir, parts):     if os.path.exists(parts_dir):         logger.info(""Cleaning up parts directory"")         for subdirectory in os.listdir(parts_dir):             path = os.path.join(parts_dir, subdirectory)             <MASK>                 try:                     shutil.rmtree(path)                 except NotADirectoryError:                     os.remove(path)     for part in parts:         part.mark_cleaned(steps.BUILD)         part.mark_cleaned(steps.PULL)",if path != local_plugins_dir :,if path in local_plugins_dir :,59.11602603314155,59.11602603314155,0.0
"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]):     if node_pos[""reach_leaf_node""].all():         return node_pos     for t_idx, tree in enumerate(trees):         cur_node_idx = node_pos[""node_pos""][t_idx]         # reach leaf         <MASK>             continue         rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree(             tree, sample, cur_node_idx         )         if reach_leaf:             node_pos[""reach_leaf_node""][t_idx] = True         node_pos[""node_pos""][t_idx] = rs     return node_pos",if cur_node_idx == - 1 :,if cur_node_idx == 0 :,70.80735452207037,70.80735452207037,0.0
"def get_measurements(self, pipeline, object_name, category):     if self.get_categories(pipeline, object_name) == [category]:         results = []         if self.do_corr_and_slope:             if object_name == ""Image"":                 results += [""Correlation"", ""Slope""]             else:                 results += [""Correlation""]         if self.do_overlap:             results += [""Overlap"", ""K""]         <MASK>             results += [""Manders""]         if self.do_rwc:             results += [""RWC""]         if self.do_costes:             results += [""Costes""]         return results     return []",if self . do_manders :,if self . do_manders :,100.00000000000004,100.00000000000004,1.0
"def create_connection(self, infos, f2, laddr_infos, protocol):     for family in infos:         try:             <MASK>                 for laddr in laddr_infos:                     try:                         break                     except OSError:                         protocol = ""foo""                 else:                     continue         except OSError:             protocol = ""bar""         else:             break     else:         raise     return protocol",if f2 :,if family == f2 :,17.965205598154213,17.965205598154213,0.0
"def app_middleware(next, root, info, **kwargs):     app_auth_header = ""HTTP_AUTHORIZATION""     prefix = ""bearer""     request = info.context     if request.path == API_PATH:         if not hasattr(request, ""app""):             request.app = None             auth = request.META.get(app_auth_header, """").split()             if len(auth) == 2:                 auth_prefix, auth_token = auth                 <MASK>                     request.app = SimpleLazyObject(lambda: get_app(auth_token))     return next(root, info, **kwargs)",if auth_prefix . lower ( ) == prefix :,if auth_prefix == prefix :,41.938051117049184,41.938051117049184,0.0
"def when(self, matches, context):     ret = []     for episode in matches.named(""episode"", lambda match: len(match.initiator) == 1):         group = matches.markers.at_match(             episode, lambda marker: marker.name == ""group"", index=0         )         <MASK>             if not matches.range(                 *group.span, predicate=lambda match: match.name == ""title""             ):                 ret.append(episode)     return ret",if group :,if group :,100.00000000000004,0.0,1.0
def locate_via_pep514(spec):     with _PY_LOCK:         if not _PY_AVAILABLE:             from . import pep514             _PY_AVAILABLE.extend(pep514.discover_pythons())             _PY_AVAILABLE.append(CURRENT)     for cur_spec in _PY_AVAILABLE:         <MASK>             return cur_spec.path,if cur_spec . satisfies ( spec ) :,if cur_spec . path == spec :,42.7287006396234,42.7287006396234,0.0
"def setCorkImageDefault(self):     if settings.corkBackground[""image""] != """":         i = self.cmbCorkImage.findData(settings.corkBackground[""image""])         <MASK>             self.cmbCorkImage.setCurrentIndex(i)",if i != - 1 :,if i != - 1 :,100.00000000000004,100.00000000000004,1.0
"def _split_key(key):     if isinstance(key, util.string_types):         # coerce fooload('*') into ""default loader strategy""         if key == _WILDCARD_TOKEN:             return (_DEFAULT_TOKEN,)         # coerce fooload("".*"") into ""wildcard on default entity""         <MASK>             key = key[1:]         return key.split(""."")     else:         return (key,)","elif key . startswith ( ""."" + _WILDCARD_TOKEN ) :",if key [ 0 ] == _WILDCARD_TOKEN :,18.27249680232283,18.27249680232283,0.0
"def detach_volume(self, volume):     # We need to find the node using this volume     for node in self.list_nodes():         <MASK>             # This node has only one associated image. It is not the one we             # are after.             continue         for disk in node.image:             if disk.id == volume.id:                 # Node found. We can now detach the volume                 disk_id = disk.extra[""disk_id""]                 return self._do_detach_volume(node.id, disk_id)     return False",if type ( node . image ) is not list :,if node . image is None :,16.417223692914014,16.417223692914014,0.0
"def create(self, private=False):     try:         <MASK>             log.info(""Creating private channel %s."", self)             self._bot.api_call(                 ""conversations.create"", data={""name"": self.name, ""is_private"": True}             )         else:             log.info(""Creating channel %s."", self)             self._bot.api_call(""conversations.create"", data={""name"": self.name})     except SlackAPIResponseError as e:         if e.error == ""user_is_bot"":             raise RoomError(f""Unable to create channel. {USER_IS_BOT_HELPTEXT}"")         else:             raise RoomError(e)",if private :,if private :,100.00000000000004,0.0,1.0
"def test_dataset_has_valid_etag(self, dataset_name):     py_script_path = list(filter(lambda x: x, dataset_name.split(""/"")))[-1] + "".py""     dataset_url = hf_bucket_url(dataset_name, filename=py_script_path, dataset=True)     etag = None     try:         response = requests.head(             dataset_url, allow_redirects=True, proxies=None, timeout=10         )         <MASK>             etag = response.headers.get(""Etag"")     except (EnvironmentError, requests.exceptions.Timeout):         pass     self.assertIsNotNone(etag)",if response . status_code == 200 :,if response . status_code == 200 :,100.00000000000004,100.00000000000004,1.0
"def set_dir_modes(self, dirname, mode):     if not self.is_chmod_supported():         return     for dirpath, dirnames, fnames in os.walk(dirname):         if os.path.islink(dirpath):             continue         log.info(""changing mode of %s to %o"", dirpath, mode)         <MASK>             os.chmod(dirpath, mode)",if not self . dry_run :,if mode in dirnames :,6.9717291216921975,6.9717291216921975,0.0
"def _clean(self):     logger.info(""Cleaning up..."")     if self._process is not None:         <MASK>             for _ in range(3):                 self._process.terminate()                 time.sleep(0.5)                 if self._process.poll() is not None:                     break             else:                 self._process.kill()                 self._process.wait()                 logger.error(""KILLED"")     if os.path.exists(self._tmp_dir):         shutil.rmtree(self._tmp_dir)     self._process = None     self._ws = None     logger.info(""Cleanup complete"")",if self . _process . poll ( ) is None :,if self . _process . wait ( ) :,49.02608580435959,49.02608580435959,0.0
"def iter_chars_to_words(self, chars):     current_word = []     for char in chars:         if not self.keep_blank_chars and char[""text""].isspace():             <MASK>                 yield current_word                 current_word = []         elif current_word and self.char_begins_new_word(current_word, char):             yield current_word             current_word = [char]         else:             current_word.append(char)     if current_word:         yield current_word",if current_word :,"if current_word and char [ ""text"" ] == "" "" :",16.188613565728215,16.188613565728215,0.0
"def _lookup(components, specs, provided, name, i, l):     if i < l:         for spec in specs[i].__sro__:             comps = components.get(spec)             if comps:                 r = _lookup(comps, specs, provided, name, i + 1, l)                 <MASK>                     return r     else:         for iface in provided:             comps = components.get(iface)             if comps:                 r = comps.get(name)                 if r is not None:                     return r     return None",if r is not None :,if r is not None :,100.00000000000004,100.00000000000004,1.0
"def run(cmd, task=None):     process = subprocess.Popen(         cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True     )     output_lines = []     while True:         line = process.stdout.readline()         <MASK>             break         line = line.decode(""utf-8"")         output_lines += [line]         logger.info(line.rstrip(""\n""))     process.stdout.close()     exit_code = process.wait()     if exit_code:         output = """".join(output_lines)         raise subprocess.CalledProcessError(exit_code, cmd, output=output)",if not line :,if not line :,100.00000000000004,100.00000000000004,1.0
"def process_response(self, request, response):     if (         response.status_code == 404         and request.path_info.endswith(""/"")         and not is_valid_path(request.path_info)         and is_valid_path(request.path_info[:-1])     ):         # Use request.path because we munged app/locale in path_info.         newurl = request.path[:-1]         <MASK>             with safe_query_string(request):                 newurl += ""?"" + request.META[""QUERY_STRING""]         return HttpResponsePermanentRedirect(newurl)     return response",if request . GET :,"if request . META [ ""QUERY_STRING"" ] :",13.545994273378144,13.545994273378144,0.0
"def dependencies(self):     deps = []     midx = None     if self.ref is not None:         query = TypeQuery(self.ref)         super = query.execute(self.schema)         if super is None:             log.debug(self.schema)             raise TypeNotFound(self.ref)         <MASK>             deps.append(super)             midx = 0     return (midx, deps)",if not super . builtin ( ) :,if midx == self . max_deps :,5.522397783539471,5.522397783539471,0.0
"def _get_vtkjs(self):     if self._vtkjs is None and self.object is not None:         if isinstance(self.object, string_types) and self.object.endswith("".vtkjs""):             <MASK>                 with open(self.object, ""rb"") as f:                     vtkjs = f.read()             else:                 data_url = urlopen(self.object)                 vtkjs = data_url.read()         elif hasattr(self.object, ""read""):             vtkjs = self.object.read()         self._vtkjs = vtkjs     return self._vtkjs",if isfile ( self . object ) :,"if self . object . startswith ( ""vtkjs"" ) :",17.242221289766636,17.242221289766636,0.0
"def _save(self):     fd, tempname = tempfile.mkstemp()     fd = os.fdopen(fd, ""w"")     json.dump(self._cache, fd, indent=2, separators=("","", "": ""))     fd.close()     # Silently ignore errors     try:         <MASK>             os.makedirs(os.path.dirname(self.filename))         shutil.move(tempname, self.filename)     except (IOError, OSError):         os.remove(tempname)",if not os . path . exists ( os . path . dirname ( self . filename ) ) :,if os . path . isdir ( self . filename ) :,29.64215118800291,29.64215118800291,0.0
"def refiner_configs(self):     rv = {}     for refiner in refiner_manager:         <MASK>             rv[refiner.name] = {k: v for k, v in self.config.items(refiner.name)}     return rv",if self . config . has_section ( refiner . name ) :,if self . config :,11.688396478408103,11.688396478408103,0.0
"def com_slice(self, primary, node, assigning):     # short_slice:  [lower_bound] "":"" [upper_bound]     lower = upper = None     if len(node.children) == 2:         <MASK>             upper = self.com_node(node.children[1])         else:             lower = self.com_node(node.children[0])     elif len(node.children) == 3:         lower = self.com_node(node.children[0])         upper = self.com_node(node.children[2])     return Slice(primary, assigning, lower, upper, lineno=extractLineNo(node))",if node . children [ 0 ] . type == token . COLON :,if len ( node . children ) == 1 :,13.264496309003723,13.264496309003723,0.0
"def close(self, *args, **kwargs):     super(mytqdm, self).close(*args, **kwargs)     # If it was not run in a notebook, sp is not assigned, check for it     if hasattr(self, ""sp""):         # Try to detect if there was an error or KeyboardInterrupt         # in manual mode: if n < total, things probably got wrong         if self.total and self.n < self.total:             self.sp(bar_style=""danger"")         else:             <MASK>                 self.sp(bar_style=""success"")             else:                 self.sp(close=True)",if self . leave :,if self . n > 0 :,26.269098944241588,26.269098944241588,0.0
"def test_alloc(self):     b = bytearray()     alloc = b.__alloc__()     self.assertTrue(alloc >= 0)     seq = [alloc]     for i in range(100):         b += b""x""         alloc = b.__alloc__()         self.assertTrue(alloc >= len(b))         <MASK>             seq.append(alloc)",if alloc not in seq :,if alloc >= len ( seq ) :,11.339582221952005,11.339582221952005,0.0
"def flush_file(self, key, f):     f.flush()     <MASK>         f.compress = zlib.compressobj(             9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0         )     if len(self.files) > self.MAX_OPEN_FILES:         if self.compress:             open_files = sum(1 for f in self.files.values() if f.fileobj is not None)             if open_files > self.MAX_OPEN_FILES:                 f.fileobj.close()                 f.fileobj = None         else:             f.close()             self.files.pop(key)",if self . compress :,if self . compress :,100.00000000000004,100.00000000000004,1.0
"def _run(self):     # Low-level run method to do the actual scheduling loop.     self.running = True     while self.running:         try:             self.sched.run()         except Exception as x:             logging.error(                 ""Error during scheduler execution: %s"" % str(x), exc_info=True             )         # queue is empty; sleep a short while before checking again         <MASK>             time.sleep(5)",if self . running :,if self . queue is None :,26.269098944241588,26.269098944241588,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             self.set_app_id(d.getPrefixedString())             continue         <MASK>             self.set_max_rows(d.getVarInt32())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 16 :,if tt == 1 :,53.7284965911771,53.7284965911771,0.0
"def check(dbdef):     ""drop script must clear the database""     for version in dbdef:         connector = MemConnector().bound(None)         create(dbdef, version, connector)         drop(dbdef, version, connector)         remaining = connector.execute(             ""SELECT * FROM sqlite_master WHERE name NOT LIKE 'sqlite_%'""         ).fetchall()         <MASK>             yield ""{0}:drop.sql"".format(version), remaining",if remaining :,if remaining :,100.00000000000004,0.0,1.0
"def test_open_overwrite_offset_size(self, sftp):     """"""Test writing data at a specific offset""""""     f = None     try:         self._create_file(""file"", ""xxxxyyyy"")         f = yield from sftp.open(""file"", ""r+"")         yield from f.write(""zz"", 3)         yield from f.close()         with open(""file"") as localf:             self.assertEqual(localf.read(), ""xxxzzyyy"")     finally:         <MASK>  # pragma: no branch             yield from f.close()         remove(""file"")",if f :,if f :,100.00000000000004,0.0,1.0
"def pump():     import sys as _sys     while self.countdown_active():         if not (self.connected(""send"") and other.connected(""recv"")):             break         try:             data = other.recv(timeout=0.05)         except EOFError:             break         <MASK>             return         if not data:             continue         try:             self.send(data)         except EOFError:             break         if not _sys:             return     self.shutdown(""send"")     other.shutdown(""recv"")",if not _sys :,if not _sys :,100.00000000000004,100.00000000000004,1.0
"def parse_results(cwd):     optimal_dd = None     optimal_measure = numpy.inf     for tup in tools.find_conf_files(cwd):         dd = tup[1]         if ""results.train_y_misclass"" in dd:             <MASK>                 optimal_measure = dd[""results.train_y_misclass""]                 optimal_dd = dd     print(""Optimal results.train_y_misclass:"", str(optimal_measure))     for key, value in optimal_dd.items():         if ""hyper_parameters"" in key:             print(key + "": "" + str(value))","if dd [ ""results.train_y_misclass"" ] < optimal_measure :","if ""results.train_y_misclass"" in dd :",47.3930294822252,47.3930294822252,0.0
"def valid(self):     valid = True     <MASK>         return valid     else:         try:             with io.open(self.pathfile, ""w"", encoding=""utf-8"") as f:                 f.close()  # do nothing         except OSError:             valid = False         if os.path.exists(self.pathfile):             os.remove(self.pathfile)         return valid",if os . path . exists ( self . pathfile ) :,if self . pathfile :,11.141275535087015,11.141275535087015,0.0
"def __getitem__(self, key):     try:         value = self.cache[key]     except KeyError:         f = BytesIO(self.dict[key.encode(self.keyencoding)])         value = Unpickler(f).load()         <MASK>             self.cache[key] = value     return value",if self . writeback :,"if not isinstance ( value , Unpickler ) :",5.669791110976001,5.669791110976001,0.0
"def hasMenu(cls, callingWindow, mainItem, selection, *fullContexts):     for i, fullContext in enumerate(fullContexts):         srcContext = fullContext[0]         for menuHandler in cls.menus:             m = menuHandler()             <MASK>                 return True         return False","if m . _baseDisplay ( callingWindow , srcContext , mainItem , selection ) :",if m . isMenu ( ) :,10.2601628763616,10.2601628763616,0.0
"def lr_read_tables(module=tab_module, optimize=0):     global _lr_action, _lr_goto, _lr_productions, _lr_method     try:         exec(""import %s as parsetab"" % module)         global parsetab  # declare the name of the imported module         <MASK>             _lr_action = parsetab._lr_action             _lr_goto = parsetab._lr_goto             _lr_productions = parsetab._lr_productions             _lr_method = parsetab._lr_method             return 1         else:             return 0     except (ImportError, AttributeError):         return 0",if ( optimize ) or ( Signature . digest ( ) == parsetab . _lr_signature ) :,if optimize :,0.09836934532021606,0.0,0.0
"def _Determine_Do(self):     if sys.platform.startswith(""win""):         self.applicable = 1         for opt, optarg in self.chosenOptions:             if opt == ""--moz-tools"":                 self.value = os.path.abspath(os.path.normpath(optarg))                 break         else:             <MASK>                 self.value = os.environ[self.name]             else:                 self.value = None     else:         self.applicable = 0     self.determined = 1",if os . environ . has_key ( self . name ) :,"if opt == ""--moz-tools"" :",3.102160927976006,3.102160927976006,0.0
"def parse_chunked(self, unreader):     (size, rest) = self.parse_chunk_size(unreader)     while size > 0:         while size > len(rest):             size -= len(rest)             yield rest             rest = unreader.read()             if not rest:                 raise NoMoreData()         yield rest[:size]         # Remove \r\n after chunk         rest = rest[size:]         while len(rest) < 2:             rest += unreader.read()         <MASK>             raise ChunkMissingTerminator(rest[:2])         (size, rest) = self.parse_chunk_size(unreader, data=rest[2:])","if rest [ : 2 ] != b""\r\n"" :",if len ( rest ) < 2 :,2.8730831956184355,2.8730831956184355,0.0
"def _scroll_down(self, cli):     ""Scroll window down.""     info = self.render_info     if self.vertical_scroll < info.content_height - info.window_height:         <MASK>             self.content.move_cursor_down(cli)         self.vertical_scroll += 1",if info . cursor_position . y <= info . configured_scroll_offsets . top :,if self . content :,0.699933150083178,0.699933150083178,0.0
"def _add_defaults_data_files(self):     # getting distribution.data_files     if self.distribution.has_data_files():         for item in self.distribution.data_files:             if isinstance(item, str):                 # plain file                 item = convert_path(item)                 if os.path.isfile(item):                     self.filelist.append(item)             else:                 # a (dirname, filenames) tuple                 dirname, filenames = item                 for f in filenames:                     f = convert_path(f)                     <MASK>                         self.filelist.append(f)",if os . path . isfile ( f ) :,if os . path . isdir ( item ) :,46.713797772819994,46.713797772819994,0.0
"def list_stuff(self, upto=10, start_after=-1):     for i in range(upto):         <MASK>             continue         if i == 2 and self.count < 1:             self.count += 1             raise TemporaryProblem         if i == 7 and self.count < 4:             self.count += 1             raise TemporaryProblem         yield i",if i <= start_after :,if start_after > self . count :,20.164945583740657,20.164945583740657,0.0
"def is_open(self):     if self.signup_code:         return True     else:         <MASK>             if self.messages.get(""invalid_signup_code""):                 messages.add_message(                     self.request,                     self.messages[""invalid_signup_code""][""level""],                     self.messages[""invalid_signup_code""][""text""].format(                         **{                             ""code"": self.get_code(),                         }                     ),                 )     return settings.ACCOUNT_OPEN_SIGNUP",if self . signup_code_present :,"if self . request . method == ""POST"" :",13.545994273378144,13.545994273378144,0.0
"def on_delete_from_disk(self, widget, data=None):     model, iter = self.get_selection().get_selected()     if iter:         path = model.get_value(iter, COLUMN_PATH)         <MASK>             ErrorDialog(_(""Can't delete system item from disk."")).launch()         else:             os.remove(path)     self.update_items()",if self . is_defaultitem ( path ) :,if not path :,4.690733795095046,4.690733795095046,0.0
"def get_detections_for_batch(self, images):     images = images[..., ::-1]     detected_faces = self.face_detector.detect_from_batch(images.copy())     results = []     for i, d in enumerate(detected_faces):         <MASK>             results.append(None)             continue         d = d[0]         d = np.clip(d, 0, None)         x1, y1, x2, y2 = map(int, d[:-1])         results.append((x1, y1, x2, y2))     return results",if len ( d ) == 0 :,if i == 0 :,32.58798048281462,32.58798048281462,0.0
def on_update(self):     #     # Calculate maximum # of planes per well     #     self.max_per_well = 0     for pd in list(self.plate_well_site.values()):         for wd in list(pd.values()):             nplanes = sum([len(x) for x in list(wd.values())])             <MASK>                 self.max_per_well = nplanes     for registrant in self.registrants:         registrant(),if nplanes > self . max_per_well :,if nplanes > self . max_per_well :,100.00000000000004,100.00000000000004,1.0
"def is_writable(self, path):     result = False     while not result:         if os.path.exists(path):             result = os.access(path, os.W_OK)             break         parent = os.path.dirname(path)         <MASK>             break         path = parent     return result",if parent == path :,if parent == path :,100.00000000000004,100.00000000000004,1.0
"def _check_seed(self, seed):     if seed is not None:         <MASK>             self._raise_error(                 ""The random number generator seed value, seed, should be integer type or None.""             )         if seed < 0:             self._raise_error(                 ""The random number generator seed value, seed, should be non-negative integer or None.""             )",if type ( seed ) != int :,if seed . type != int :,35.091197742655815,35.091197742655815,0.0
"def write(self, x):     # try to use backslash and surrogate escape strategies before failing     self._errors = ""backslashescape"" if self.encoding != ""mbcs"" else ""surrogateescape""     try:         return io.TextIOWrapper.write(self, to_text(x, errors=self._errors))     except UnicodeDecodeError:         <MASK>             self._errors = ""surrogateescape""         else:             self._errors = ""replace""         return io.TextIOWrapper.write(self, to_text(x, errors=self._errors))","if self . _errors != ""surrogateescape"" :","if self . encoding != ""mbcs"" :",27.007097700389536,27.007097700389536,0.0
"def post(self, request, *args, **kwargs):     validated_session = []     for session_id in request.data:         session = get_object_or_none(Session, id=session_id)         <MASK>             validated_session.append(session_id)             self.model.objects.create(                 name=""kill_session"",                 args=session.id,                 terminal=session.terminal,             )     return Response({""ok"": validated_session})",if session and not session . is_finished :,if session is not None :,10.480083056996865,10.480083056996865,0.0
"def _has_list_or_dict_var_value_before(self, arg_index):     for idx, value in enumerate(self.args):         <MASK>             return False         if variablematcher.is_list_variable(             value         ) and not variablematcher.is_list_variable_subitem(value):             return True         if robotapi.is_dict_var(value) and not variablematcher.is_dict_var_access(             value         ):             return True     return False",if idx > arg_index :,if idx == arg_index :,41.11336169005198,41.11336169005198,0.0
"def test_return_correct_type(self):     for proto in protocols:         # Protocol 0 supports only ASCII strings.         <MASK>             self._check_return_correct_type(""abc"", 0)         else:             for obj in [b""abc\n"", ""abc\n"", -1, -1.1 * 0.1, str]:                 self._check_return_correct_type(obj, proto)",if proto == 0 :,"if proto == ""0"" :",38.260294162784454,38.260294162784454,0.0
"def backward_impl(self, inputs, outputs, prop_down, accum):     # inputs: [inputs_fwd_graph] + [inputs_bwd_graph] or     # [inputs_fwd_graph] + [outputs_fwd_graph] + [inputs_bwd_graph]     # Args     axis = self.forward_func.info.args[""axis""]     # Compute     ## w.r.t. dy     if prop_down[-1]:         g_dy = inputs[-1].grad         g_dy_ = F.stack(*[o.grad for o in outputs], axis=axis)         <MASK>             g_dy += g_dy_         else:             g_dy.copy_from(g_dy_)",if accum [ - 1 ] :,if prop_down [ 0 ] :,13.134549472120788,13.134549472120788,0.0
"def remove(self, url):     try:         i = self.items.index(url)     except (ValueError, IndexError):         pass     else:         was_selected = i in self.selectedindices()         self.list.delete(i)         del self.items[i]         if not self.items:             self.mp.hidepanel(self.name)         elif was_selected:             <MASK>                 i = len(self.items) - 1             self.list.select_set(i)",if i >= len ( self . items ) :,if i == 0 :,8.389861810900507,8.389861810900507,0.0
"def prepend(self, value):     """"""prepend value to nodes""""""     root, root_text = self._get_root(value)     for i, tag in enumerate(self):         if not tag.text:             tag.text = """"         if len(root) > 0:             root[-1].tail = tag.text             tag.text = root_text         else:             tag.text = root_text + tag.text         <MASK>             root = deepcopy(list(root))         tag[:0] = root         root = tag[: len(root)]     return self",if i > 0 :,if i == 0 :,22.957488466614336,22.957488466614336,0.0
"def _get_tracks_compositors_list():     tracks_list = []     tracks = current_sequence().tracks     compositors = current_sequence().compositors     for track_index in range(1, len(tracks) - 1):         track_compositors = []         for j in range(0, len(compositors)):             comp = compositors[j]             <MASK>                 track_compositors.append(comp)         tracks_list.append(track_compositors)     return tracks_list",if comp . transition . b_track == track_index :,if comp . track_index == track_index :,47.26710158823674,47.26710158823674,0.0
"def __getattr__(self, name):     if name in self._sections:         return ""\n"".join(self._sections[name])     else:         <MASK>             return """"         else:             raise ConanException(""ConfigParser: Unrecognized field '%s'"" % name)",if self . _allowed_fields and name in self . _allowed_fields :,"if name == ""text"" :",2.3595365419339505,2.3595365419339505,0.0
"def get_first_param_index(self, group_id, param_group, partition_id):     for index, param in enumerate(param_group):         param_id = self.get_param_id(param)         <MASK>             return index     return None",if partition_id in self . param_to_partition_ids [ group_id ] [ param_id ] :,if param_id == group_id :,6.298514559701538,6.298514559701538,0.0
"def handle_uv_sockets(self, context):     u_socket = self.inputs[""U""]     v_socket = self.inputs[""V""]     if self.cast_mode == ""Sphere"":         u_socket.hide_safe = True         v_socket.hide_safe = True     elif self.cast_mode in [""Cylinder"", ""Prism""]:         v_socket.hide_safe = True         <MASK>             u_socket.hide_safe = False     else:         if u_socket.hide_safe:             u_socket.hide_safe = False         if v_socket.hide_safe:             v_socket.hide_safe = False",if u_socket . hide_safe :,if u_socket . hide_safe :,100.00000000000004,100.00000000000004,1.0
"def _scrub_generated_timestamps(self, target_workdir):     """"""Remove the first line of comment from each file if it contains a timestamp.""""""     for root, _, filenames in safe_walk(target_workdir):         for filename in filenames:             source = os.path.join(root, filename)             with open(source, ""r"") as f:                 lines = f.readlines()             <MASK>                 return             with open(source, ""w"") as f:                 if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]):                     f.write(lines[0])                 for line in lines[1:]:                     f.write(line)",if len ( lines ) < 1 :,if not lines :,7.733712583165139,7.733712583165139,0.0
"def inner(request, *args, **kwargs):     page = request.current_page     if page:         if page.login_required and not request.user.is_authenticated:             return redirect_to_login(                 urlquote(request.get_full_path()), settings.LOGIN_URL             )         site = get_current_site()         <MASK>             return _handle_no_page(request)     return func(request, *args, **kwargs)","if not user_can_view_page ( request . user , page , site ) :",if site . is_active :,1.7685724681566029,1.7685724681566029,0.0
"def flush(self, *args, **kwargs):     with self._lock:         self._last_updated = time.time()         try:             <MASK>                 self._locked_flush_without_tempfile()             else:                 mailbox.mbox.flush(self, *args, **kwargs)         except OSError:             if ""_create_temporary"" in traceback.format_exc():                 self._locked_flush_without_tempfile()             else:                 raise         self._last_updated = time.time()","if kwargs . get ( ""in_place"" , False ) :","if ""_create_temporary"" in traceback . format_exc ( ) :",8.47178590796544,8.47178590796544,0.0
"def sanitize_event_keys(kwargs, valid_keys):     # Sanity check: Don't honor keys that we don't recognize.     for key in list(kwargs.keys()):         if key not in valid_keys:             kwargs.pop(key)     # Truncate certain values over 1k     for key in [""play"", ""role"", ""task"", ""playbook""]:         <MASK>             if len(kwargs[""event_data""][key]) > 1024:                 kwargs[""event_data""][key] = Truncator(kwargs[""event_data""][key]).chars(                     1024                 )","if isinstance ( kwargs . get ( ""event_data"" , { } ) . get ( key ) , str ) :","if kwargs [ ""event_data"" ] [ key ] :",13.307513114640143,13.307513114640143,0.0
"def parse_auth(val):     if val is not None:         authtype, params = val.split("" "", 1)         <MASK>             if authtype == ""Basic"" and '""' not in params:                 # this is the ""Authentication: Basic XXXXX=="" case                 pass             else:                 params = parse_auth_params(params)         return authtype, params     return val",if authtype in known_auth_schemes :,"if authtype == ""Basic"" :",10.786826322527466,10.786826322527466,0.0
"def _memoized(*args):     now = time.time()     try:         value, last_update = self.cache[args]         age = now - last_update         if self._call_count > self.ctl or age > self.ttl:             self._call_count = 0             raise AttributeError         if self.ctl:             self._call_count += 1         return value     except (KeyError, AttributeError):         value = func(*args)         <MASK>             self.cache[args] = (value, now)         return value     except TypeError:         return func(*args)",if value :,if age > self . ttl :,7.809849842300637,7.809849842300637,0.0
"def _get_md_bg_color_down(self):     t = self.theme_cls     c = self.md_bg_color  # Default to no change on touch     # Material design specifies using darker hue when on Dark theme     if t.theme_style == ""Dark"":         if self.md_bg_color == t.primary_color:             c = t.primary_dark         <MASK>             c = t.accent_dark     return c",elif self . md_bg_color == t . accent_color :,"if t . theme_style == ""Touch"" :",7.595456438315031,7.595456438315031,0.0
def _init_table_h():     _table_h = []     for i in range(256):         part_l = i         part_h = 0         for j in range(8):             rflag = part_l & 1             part_l >>= 1             <MASK>                 part_l |= 1 << 31             part_h >>= 1             if rflag:                 part_h ^= 0xD8000000         _table_h.append(part_h)     return _table_h,if part_h & 1 :,if rflag :,9.138402379955025,0.0,0.0
"def migrate_Stats(self):     for old_obj in self.session_old.query(self.model_from[""Stats""]):         if not old_obj.summary:             self.entries_count[""Stats""] -= 1             continue         new_obj = self.model_to[""Stats""]()         for key in new_obj.__table__.columns._data.keys():             <MASK>                 continue             setattr(new_obj, key, getattr(old_obj, key))         self.session_new.add(new_obj)",if key not in old_obj . __table__ . columns :,if key not in old_obj . summary :,42.94683145077766,42.94683145077766,0.0
"def get_in_turn_repetition(pred, is_cn=False):     """"""Get in-turn repetition.""""""     if len(pred) == 0:         return 1.0     if isinstance(pred[0], str):         pred = [tok.lower() for tok in pred]         if is_cn:             pred = """".join(pred)     tri_grams = set()     for i in range(len(pred) - 2):         tri_gram = tuple(pred[i : i + 3])         <MASK>             return 1.0         tri_grams.add(tri_gram)     return 0.0",if tri_gram in tri_grams :,if tri_gram not in tri_grams :,65.80370064762461,65.80370064762461,0.0
"def translate():     assert Lex.next() is AttributeList     reader.read()  # Discard attribute list from reader.     attrs = {}     d = AttributeList.match.groupdict()     for k, v in d.items():         if v is not None:             <MASK>                 v = subs_attrs(v)                 if v:                     parse_attributes(v, attrs)             else:                 AttributeList.attrs[k] = v     AttributeList.subs(attrs)     AttributeList.attrs.update(attrs)","if k == ""attrlist"" :","if k == ""subs"" :",59.4603557501361,59.4603557501361,0.0
"def _parse(self, engine):     """"""Parse the layer.""""""     if isinstance(self.args, dict):         if ""axis"" in self.args:             self.axis = engine.evaluate(self.args[""axis""], recursive=True)             <MASK>                 raise ParsingError('""axis"" must be an integer.')         if ""momentum"" in self.args:             self.momentum = engine.evaluate(self.args[""momentum""], recursive=True)             if not isinstance(self.momentum, (int, float)):                 raise ParsingError('""momentum"" must be numeric.')","if not isinstance ( self . axis , int ) :","if not isinstance ( self . axis , ( int , float ) ) :",53.2800971987552,53.2800971987552,0.0
"def __getattr__(self, attrname):     if attrname in (""visamp"", ""visamperr"", ""visphi"", ""visphierr""):         return ma.masked_array(self.__dict__[""_"" + attrname], mask=self.flag)     elif attrname in (""cflux"", ""cfluxerr""):         <MASK>             return ma.masked_array(self.__dict__[""_"" + attrname], mask=self.flag)         else:             return None     else:         raise AttributeError(attrname)","if self . __dict__ [ ""_"" + attrname ] != None :","if attrname in ( ""cflux"" , ""cfluxerr"" ) :",2.9115521924905376,2.9115521924905376,0.0
"def draw(self, context):     layout = self.layout     presets.draw_presets_ops(layout, context=context)     for category in presets.get_category_names():         <MASK>             if category in preset_category_menus:                 class_name = preset_category_menus[category].__name__                 layout.menu(class_name)",if category in preset_category_menus :,if category in presets . get_category_menus ( ) :,28.917849332325716,28.917849332325716,0.0
"def __setitem__(self, key, value):     if isinstance(value, (tuple, list)):         info, reference = value         if info not in self._reverse_infos:             self._reverse_infos[info] = len(self._infos)             self._infos.append(info)         <MASK>             self._reverse_references[reference] = len(self._references)             self._references.append(reference)         self._trails[key] = ""%d,%d"" % (             self._reverse_infos[info],             self._reverse_references[reference],         )     else:         raise Exception(""unsupported type '%s'"" % type(value))",if reference not in self . _reverse_references :,if reference not in self . _reverse_references :,100.00000000000004,100.00000000000004,1.0
"def format_bpe_text(symbols, delimiter=b""@@""):     """"""Convert a sequence of bpe words into sentence.""""""     words = []     word = b""""     if isinstance(symbols, str):         symbols = symbols.encode()     delimiter_len = len(delimiter)     for symbol in symbols:         <MASK>             word += symbol[:-delimiter_len]         else:  # end of a word             word += symbol             words.append(word)             word = b""""     return b"" "".join(words)",if len ( symbol ) >= delimiter_len and symbol [ - delimiter_len : ] == delimiter :,if len ( symbol ) > delimiter_len :,21.410896227989312,21.410896227989312,0.0
"def output_type(data, request, response):     accept = request.accept     if accept in ("""", ""*"", ""/""):         handler = default or handlers and next(iter(handlers.values()))     else:         handler = default         accepted = [accept_quality(accept_type) for accept_type in accept.split("","")]         accepted.sort(key=itemgetter(0))         for _quality, accepted_content_type in reversed(accepted):             <MASK>                 handler = handlers[accepted_content_type]                 break     if not handler:         raise falcon.HTTPNotAcceptable(error)     response.content_type = handler.content_type     return handler(data, request=request, response=response)",if accepted_content_type in handlers :,"if _quality == ""none"" :",6.27465531099474,6.27465531099474,0.0
"def _render_raw_list(bytes_items):     flatten_items = []     for item in bytes_items:         if item is None:             flatten_items.append(b"""")         elif isinstance(item, bytes):             flatten_items.append(item)         <MASK>             flatten_items.append(str(item).encode())         elif isinstance(item, list):             flatten_items.append(_render_raw_list(item))     return b""\n"".join(flatten_items)","elif isinstance ( item , int ) :","if isinstance ( item , bytes ) :",41.11336169005198,41.11336169005198,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             self.set_mime_type(d.getVarInt32())             continue         if tt == 16:             self.set_quality(d.getVarInt32())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 8 :,if tt == 8 :,100.00000000000004,100.00000000000004,1.0
"def delete(self, waiters):     # Delete flow.     msgs = self.ofctl.get_all_flow(waiters)     for msg in msgs:         for stats in msg.body:             vlan_id = VlanRouter._cookie_to_id(REST_VLANID, stats.cookie)             <MASK>                 self.ofctl.delete_flow(stats)     assert len(self.packet_buffer) == 0",if vlan_id == self . vlan_id :,if vlan_id == self . vlan_id :,100.00000000000004,100.00000000000004,1.0
def missing_push_allowance(push_allowances: List[PushAllowance]) -> bool:     for push_allowance in push_allowances:         # a null databaseId indicates this is not a GitHub App.         if push_allowance.actor.databaseId is None:             continue         <MASK>             return False     return True,if str ( push_allowance . actor . databaseId ) == str ( app_config . GITHUB_APP_ID ) :,"if push_allowance . actor . databaseId == ""GitHub App"" :",22.364623267658008,22.364623267658008,0.0
"def _cluster_page(self, htmlpage):     template_cluster, preferred = _CLUSTER_NA, None     if self.clustering:         self.clustering.add_page(htmlpage)         <MASK>             clt = self.clustering.classify(htmlpage)             if clt != -1:                 template_cluster = preferred = self.template_names[clt]             else:                 template_cluster = _CLUSTER_OUTLIER     return template_cluster, preferred",if self . clustering . is_fit :,if self . template_names :,20.873176328735713,20.873176328735713,0.0
"def readlines(self, size=-1):     if self._nbr == self._size:         return []     # leave all additional logic to our readline method, we just check the size     out = []     nbr = 0     while True:         line = self.readline()         if not line:             break         out.append(line)         if size > -1:             nbr += len(line)             <MASK>                 break         # END handle size constraint     # END readline loop     return out",if nbr > size :,if nbr > self . _size :,25.848657697858535,25.848657697858535,0.0
"def post_mortem(t=None):     # handling the default     <MASK>         # sys.exc_info() returns (type, value, traceback) if an exception is         # being handled, otherwise it returns None         t = sys.exc_info()[2]         if t is None:             raise ValueError(                 ""A valid traceback must be passed if no exception is being handled.""             )     p = BPdb()     p.reset()     p.interaction(None, t)",if t is None :,if t is None :,100.00000000000004,100.00000000000004,1.0
"def fixup(m):     txt = m.group(0)     if txt[:2] == ""&#"":         # character reference         try:             <MASK>                 return unichr(int(txt[3:-1], 16))             else:                 return unichr(int(txt[2:-1]))         except ValueError:             pass     else:         # named entity         try:             txt = unichr(htmlentitydefs.name2codepoint[txt[1:-1]])         except KeyError:             pass     return txt  # leave as is","if txt [ : 3 ] == ""&#x"" :","if txt [ 3 ] == ""&#x"" :",78.81929718099911,78.81929718099911,0.0
"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]:     argstr += "",""     args = []     kwargs = {}     for item in _converter_args_re.finditer(argstr):         value = item.group(""stringval"")         <MASK>             value = item.group(""value"")         value = _pythonize(value)         if not item.group(""name""):             args.append(value)         else:             name = item.group(""name"")             kwargs[name] = value     return tuple(args), kwargs",if value is None :,"if not item . group ( ""stringval"" ) :",4.456882760699063,4.456882760699063,0.0
"def IT(cpu):     cc = cpu.instruction.cc     true_case = cpu._evaluate_conditional(cc)     # this is incredibly hacky--how else does capstone expose this?     # TODO: find a better way than string parsing the mnemonic -GR, 2017-07-13     for c in cpu.instruction.mnemonic[1:]:         <MASK>             cpu._it_conditional.append(true_case)         elif c == ""e"":             cpu._it_conditional.append(not true_case)","if c == ""t"" :","if c == ""g"" :",59.4603557501361,59.4603557501361,0.0
"def flatten(self):     # this is similar to fill_messages except it uses a list instead     # of a queue to place the messages in.     result = []     channel = await self.messageable._get_channel()     self.channel = channel     while self._get_retrieve():         data = await self._retrieve_messages(self.retrieve)         <MASK>             self.limit = 0  # terminate the infinite loop         if self.reverse:             data = reversed(data)         if self._filter:             data = filter(self._filter, data)         for element in data:             result.append(self.state.create_message(channel=channel, data=element))     return result",if len ( data ) < 100 :,if not data :,7.733712583165139,7.733712583165139,0.0
"def _get_beta_accumulators(self):     with tf.init_scope():         <MASK>             graph = None         else:             graph = tf.get_default_graph()         return (             self._get_non_slot_variable(""beta1_power"", graph=graph),             self._get_non_slot_variable(""beta2_power"", graph=graph),         )",if tf . executing_eagerly ( ) :,if self . _is_default_graph :,5.934202609760488,5.934202609760488,0.0
"def prefixed(self, prefix: _StrType) -> typing.Iterator[""Env""]:     """"""Context manager for parsing envvars with a common prefix.""""""     try:         old_prefix = self._prefix         <MASK>             self._prefix = prefix         else:             self._prefix = f""{old_prefix}{prefix}""         yield self     finally:         # explicitly reset the stored prefix on completion and exceptions         self._prefix = None     self._prefix = old_prefix",if old_prefix is None :,if prefix is not None :,20.547995616750768,20.547995616750768,0.0
"def decode_content(self):     """"""Return the best possible representation of the response body.""""""     ct = self.headers.get(""content-type"")     if ct:         ct, options = parse_options_header(ct)         charset = options.get(""charset"")         if ct in JSON_CONTENT_TYPES:             return self.json(charset)         <MASK>             return self.text(charset)         elif ct == FORM_URL_ENCODED:             return parse_qsl(self.content.decode(charset), keep_blank_values=True)     return self.content","elif ct . startswith ( ""text/"" ) :",if ct == TEXT_ENCODED :,4.513617516969122,4.513617516969122,0.0
"def test_incrementaldecoder(self):     UTF8Writer = codecs.getwriter(""utf-8"")     for sizehint in [None, -1] + list(range(1, 33)) + [64, 128, 256, 512, 1024]:         istream = BytesIO(self.tstring[0])         ostream = UTF8Writer(BytesIO())         decoder = self.incrementaldecoder()         while 1:             data = istream.read(sizehint)             <MASK>                 break             else:                 u = decoder.decode(data)                 ostream.write(u)         self.assertEqual(ostream.getvalue(), self.tstring[1])",if not data :,if not data :,100.00000000000004,100.00000000000004,1.0
"def delete_all(path):     ppath = os.getcwd()     os.chdir(path)     for fn in glob.glob(""*""):         fn_full = os.path.join(path, fn)         <MASK>             delete_all(fn_full)         elif fn.endswith("".png""):             os.remove(fn_full)         elif fn.endswith("".md""):             os.remove(fn_full)         elif DELETE_ALL_OLD:             os.remove(fn_full)     os.chdir(ppath)     os.rmdir(path)",if os . path . isdir ( fn ) :,"if fn . endswith ( "".png"" ) :",10.252286118120933,10.252286118120933,0.0
"def _delete_reason(self):     for i in range(_lib.X509_REVOKED_get_ext_count(self._revoked)):         ext = _lib.X509_REVOKED_get_ext(self._revoked, i)         obj = _lib.X509_EXTENSION_get_object(ext)         <MASK>             _lib.X509_EXTENSION_free(ext)             _lib.X509_REVOKED_delete_ext(self._revoked, i)             break",if _lib . OBJ_obj2nid ( obj ) == _lib . NID_crl_reason :,if obj is None :,0.5730567950718444,0.5730567950718444,0.0
"def hexcmp(x, y):     try:         a = int(x, 16)         b = int(y, 16)         if a < b:             return -1         <MASK>             return 1         return 0     except:         return cmp(x, y)",if a > b :,if a > b :,100.00000000000004,100.00000000000004,1.0
"def get_indentation_count(view, start):     indent_count = 0     i = start - 1     while i > 0:         ch = view.substr(i)         scope = view.scope_name(i)         # Skip preprocessors, strings, characaters and comments         if ""string.quoted"" in scope or ""comment"" in scope or ""preprocessor"" in scope:             extent = view.extract_scope(i)             i = extent.a - 1             continue         else:             i -= 1         <MASK>             indent_count -= 1         elif ch == ""{"":             indent_count += 1     return indent_count","if ch == ""}"" :","if ch == ""{"" :",59.4603557501361,59.4603557501361,0.0
"def set(self, name, value, ex=None, px=None, nx=False, xx=False):     if (         (not nx and not xx)         or (nx and self._db.get(name, None) is None)         or (xx and not self._db.get(name, None) is None)     ):         if ex > 0:             self._db.expire(name, datetime.now() + timedelta(seconds=ex))         <MASK>             self._db.expire(name, datetime.now() + timedelta(milliseconds=px))         self._db[name] = str(value)         return True     else:         return None",elif px > 0 :,if px > 0 :,66.87403049764218,66.87403049764218,0.0
"def _get_between(content, start, end=None):     should_yield = False     for line in content.split(""\n""):         if start in line:             should_yield = True             continue         if end and end in line:             return         <MASK>             yield line.strip().split("" "")[0]",if should_yield and line :,if should_yield :,47.39878501170795,47.39878501170795,0.0
"def iter_event_handlers(     self,     resource: resources_.Resource,     event: bodies.RawEvent, ) -> Iterator[handlers.ResourceWatchingHandler]:     warnings.warn(         ""SimpleRegistry.iter_event_handlers() is deprecated; use ""         ""ResourceWatchingRegistry.iter_handlers()."",         DeprecationWarning,     )     cause = _create_watching_cause(resource, event)     for handler in self._handlers:         <MASK>             pass         elif registries.match(handler=handler, cause=cause, ignore_fields=True):             yield handler","if not isinstance ( handler , handlers . ResourceWatchingHandler ) :",if handler is None :,4.234348806659263,4.234348806659263,0.0
"def __enter__(self):     if log_timer:         <MASK>             self.logger.debug(""%s starting"" % self.name)         else:             print((""[%s starting]..."" % self.name))         self.tstart = time.time()",if self . logger :,if self . tstart :,42.72870063962342,42.72870063962342,0.0
"def _handle_errors(errors):     """"""Log out and possibly reraise errors during import.""""""     if not errors:         return     log_all = True  # pylint: disable=unused-variable     err_msg = ""T2T: skipped importing {num_missing} data_generators modules.""     print(err_msg.format(num_missing=len(errors)))     for module, err in errors:         err_str = str(err)         <MASK>             print(""Did not import module: %s; Cause: %s"" % (module, err_str))         if not _is_import_err_msg(err_str, module):             print(""From module %s"" % module)             raise err",if log_all :,if not log_all :,53.7284965911771,53.7284965911771,0.0
"def _ungroup(sequence, groups=None):     for v in sequence:         if isinstance(v, (list, tuple)):             <MASK>                 groups.append(list(_ungroup(v, groups=None)))             for v in _ungroup(v, groups):                 yield v         else:             yield v",if groups is not None :,if groups :,23.174952587773145,0.0,0.0
def run(self):     while not self.completed:         if self.block:             time.sleep(self.period)         else:             self._completed.wait(self.period)         self.counter += 1         try:             self.callback(self.counter)         except Exception:             self.stop()         if self.timeout is not None:             dt = time.time() - self._start_time             <MASK>                 self.stop()         if self.counter == self.count:             self.stop(),if dt > self . timeout :,if dt > self . timeout :,100.00000000000004,100.00000000000004,1.0
"def dont_let_stderr_buffer():     while True:         line = context.daemon.stderr.readline()         <MASK>             return         if DEAD_DEPLOYD_WORKER_MESSAGE.encode(""utf-8"") in line:             context.num_workers_crashed += 1         print(f""deployd stderr: {line}"")",if not line :,if not line :,100.00000000000004,100.00000000000004,1.0
"def mergeHiLo(self, x_stats):     """"""Merge the highs and lows of another accumulator into myself.""""""     if x_stats.firsttime is not None:         if self.firsttime is None or x_stats.firsttime < self.firsttime:             self.firsttime = x_stats.firsttime             self.first = x_stats.first     if x_stats.lasttime is not None:         <MASK>             self.lasttime = x_stats.lasttime             self.last = x_stats.last",if self . lasttime is None or x_stats . lasttime >= self . lasttime :,if self . lasttime is None :,17.469470584451173,17.469470584451173,0.0
"def test_rlimit_get(self):     import resource     p = psutil.Process(os.getpid())     names = [x for x in dir(psutil) if x.startswith(""RLIMIT"")]     assert names     for name in names:         value = getattr(psutil, name)         self.assertGreaterEqual(value, 0)         <MASK>             self.assertEqual(value, getattr(resource, name))             self.assertEqual(p.rlimit(value), resource.getrlimit(value))         else:             ret = p.rlimit(value)             self.assertEqual(len(ret), 2)             self.assertGreaterEqual(ret[0], -1)             self.assertGreaterEqual(ret[1], -1)",if name in dir ( resource ) :,"if hasattr ( resource , name ) :",17.286039232097043,17.286039232097043,0.0
"def _calculate_writes_for_built_in_indices(self, entity):     writes = 0     for prop_name in entity.keys():         if not prop_name in entity.unindexed_properties():             prop_vals = entity[prop_name]             <MASK>                 num_prop_vals = len(prop_vals)             else:                 num_prop_vals = 1             writes += 2 * num_prop_vals     return writes","if isinstance ( prop_vals , ( list ) ) :",if len ( prop_vals ) > 0 :,24.736929544091943,24.736929544091943,0.0
"def check_value_check(self, x_data, t_data, use_cudnn):     x = chainer.Variable(x_data)     t = chainer.Variable(t_data)     with chainer.using_config(""use_cudnn"", use_cudnn):         <MASK>             # Check if it throws nothing             functions.softmax_cross_entropy(                 x, t, enable_double_backprop=self.enable_double_backprop             )         else:             with self.assertRaises(ValueError):                 functions.softmax_cross_entropy(                     x, t, enable_double_backprop=self.enable_double_backprop                 )",if self . valid :,if self . assertRaises ( ValueError ) :,22.089591134157878,22.089591134157878,0.0
"def get_note_title_file(note):     mo = note_title_re.match(note.get(""content"", """"))     if mo:         fn = mo.groups()[0]         fn = fn.replace("" "", ""_"")         fn = fn.replace(""/"", ""_"")         <MASK>             return """"         if isinstance(fn, str):             fn = unicode(fn, ""utf-8"")         else:             fn = unicode(fn)         if note_markdown(note):             fn += "".mkdn""         else:             fn += "".txt""         return fn     else:         return """"",if not fn :,if not fn :,100.00000000000004,100.00000000000004,1.0
"def _parseparam(s):     plist = []     while s[:1] == "";"":         s = s[1:]         end = s.find("";"")         while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2:             end = s.find("";"", end + 1)         if end < 0:             end = len(s)         f = s[:end]         <MASK>             i = f.index(""="")             f = f[:i].strip().lower() + ""="" + f[i + 1 :].strip()         plist.append(f.strip())         s = s[end:]     return plist","if ""="" in f :","if f [ 0 ] == "";"" :",9.864703138979419,9.864703138979419,0.0
"def doDir(elem):     for child in elem.childNodes:         if not isinstance(child, minidom.Element):             continue         if child.tagName == ""Directory"":             doDir(child)         elif child.tagName == ""Component"":             for grandchild in child.childNodes:                 if not isinstance(grandchild, minidom.Element):                     continue                 <MASK>                     continue                 files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))","if grandchild . tagName != ""File"" :","if grandchild . tagName == ""File"" :",65.80370064762461,65.80370064762461,0.0
"def date_to_format(value, target_format):     """"""Convert date to specified format""""""     if target_format == str:         <MASK>             ret = value.strftime(""%d/%m/%y"")         elif isinstance(value, datetime.datetime):             ret = value.strftime(""%d/%m/%y"")         elif isinstance(value, datetime.time):             ret = value.strftime(""%H:%M:%S"")     else:         ret = value     return ret","if isinstance ( value , datetime . date ) :","if isinstance ( value , datetime . date ) :",100.00000000000004,100.00000000000004,1.0
"def __listingColumns(self):     columns = []     for name in self.__getColumns():         definition = column(name)         <MASK>             IECore.msg(                 IECore.Msg.Level.Error,                 ""GafferImageUI.CatalogueUI"",                 ""No column registered with name '%s'"" % name,             )             continue         if isinstance(definition, IconColumn):             c = GafferUI.PathListingWidget.IconColumn(definition.title(), """", name)         else:             c = GafferUI.PathListingWidget.StandardColumn(definition.title(), name)         columns.append(c)     return columns",if not definition :,if not definition :,100.00000000000004,100.00000000000004,1.0
"def metrics_to_scalars(self, metrics):     new_metrics = {}     for k, v in metrics.items():         <MASK>             v = v.item()         if isinstance(v, dict):             v = self.metrics_to_scalars(v)         new_metrics[k] = v     return new_metrics","if isinstance ( v , torch . Tensor ) :","if isinstance ( v , list ) :",46.307771619910305,46.307771619910305,0.0
"def start(self, connection):     try:         if self.client_name:             creds = gssapi.Credentials(name=gssapi.Name(self.client_name))         else:             creds = None         hostname = self.get_hostname(connection)         name = gssapi.Name(             b""@"".join([self.service, hostname]), gssapi.NameType.hostbased_service         )         context = gssapi.SecurityContext(name=name, creds=creds)         return context.step(None)     except gssapi.raw.misc.GSSError:         <MASK>             return NotImplemented         else:             raise",if self . fail_soft :,if self . service is None :,26.269098944241588,26.269098944241588,0.0
"def nanmax(self, axis=None, dtype=None, keepdims=None):     ret = self._reduction(         ""nanmax"", axis=axis, dtype=dtype, keepdims=keepdims, todense=True     )     if not issparse(ret):         <MASK>             return ret         xps = get_sparse_module(self.spmatrix)         ret = SparseNDArray(xps.csr_matrix(ret))         return ret     return ret",if get_array_module ( ret ) . isscalar ( ret ) :,if self . spmatrix is None :,2.75631563063758,2.75631563063758,0.0
"def utterance_to_sample(query_data, tagging_scheme, language):     tokens, tags = [], []     current_length = 0     for chunk in query_data:         chunk_tokens = tokenize(chunk[TEXT], language)         tokens += [             Token(t.value, current_length + t.start, current_length + t.end)             for t in chunk_tokens         ]         current_length += len(chunk[TEXT])         <MASK>             tags += negative_tagging(len(chunk_tokens))         else:             tags += positive_tagging(                 tagging_scheme, chunk[SLOT_NAME], len(chunk_tokens)             )     return {TOKENS: tokens, TAGS: tags}",if SLOT_NAME not in chunk :,"if tagging_scheme == ""negative"" :",5.522397783539471,5.522397783539471,0.0
"def use_index(     self, term: Union[str, Index], *terms: Union[str, Index] ) -> ""QueryBuilder"":     for t in (term, *terms):         if isinstance(t, Index):             self._use_indexes.append(t)         <MASK>             self._use_indexes.append(Index(t))","elif isinstance ( t , str ) :","if isinstance ( t , ( str , str ) ) :",32.091389827940986,32.091389827940986,0.0
"def reconfigServiceWithBuildbotConfig(self, new_config):     if new_config.manhole != self.manhole:         if self.manhole:             yield self.manhole.disownServiceParent()             self.manhole = None         <MASK>             self.manhole = new_config.manhole             yield self.manhole.setServiceParent(self)     # chain up     yield service.ReconfigurableServiceMixin.reconfigServiceWithBuildbotConfig(         self, new_config     )",if new_config . manhole :,if new_config . manhole :,100.00000000000004,100.00000000000004,1.0
"def cleanup_folder(target_folder):     for file in os.listdir(target_folder):         file_path = os.path.join(target_folder, file)         try:             <MASK>                 os.remove(file_path)         except Exception as e:             logging.error(e)",if os . path . isfile ( file_path ) :,if os . path . isfile ( file_path ) :,100.00000000000004,100.00000000000004,1.0
"def to_key(literal_or_identifier):     """"""returns string representation of this object""""""     if literal_or_identifier[""type""] == ""Identifier"":         return literal_or_identifier[""name""]     elif literal_or_identifier[""type""] == ""Literal"":         k = literal_or_identifier[""value""]         <MASK>             return unicode(float_repr(k))         elif ""regex"" in literal_or_identifier:             return compose_regex(k)         elif isinstance(k, bool):             return ""true"" if k else ""false""         elif k is None:             return ""null""         else:             return unicode(k)","if isinstance ( k , float ) :","if ""float"" in literal_or_identifier :",4.9323515694897075,4.9323515694897075,0.0
"def decompile(decompiler):     for pos, next_pos, opname, arg in decompiler.instructions:         if pos in decompiler.targets:             decompiler.process_target(pos)         method = getattr(decompiler, opname, None)         <MASK>             throw(DecompileError(""Unsupported operation: %s"" % opname))         decompiler.pos = pos         decompiler.next_pos = next_pos         x = method(*arg)         if x is not None:             decompiler.stack.append(x)",if method is None :,if not method :,16.37226966703825,16.37226966703825,0.0
"def shutdown(self, timeout, callback=None):     logger.debug(""background worker got shutdown request"")     with self._lock:         if self.is_alive:             self._queue.put_nowait(_TERMINATOR)             <MASK>                 self._wait_shutdown(timeout, callback)         self._thread = None         self._thread_for_pid = None     logger.debug(""background worker shut down"")",if timeout > 0.0 :,if callback :,17.799177396293473,0.0,0.0
"def getDOMImplementation(features=None):     if features:         <MASK>             features = domreg._parse_feature_string(features)         for f, v in features:             if not Document.implementation.hasFeature(f, v):                 return None     return Document.implementation","if isinstance ( features , str ) :",if not features :,7.733712583165139,7.733712583165139,0.0
"def validate_subevent(self, subevent):     if self.context[""event""].has_subevents:         <MASK>             raise ValidationError(""You need to set a subevent."")         if subevent.event != self.context[""event""]:             raise ValidationError(                 ""The specified subevent does not belong to this event.""             )     elif subevent:         raise ValidationError(""You cannot set a subevent for this event."")     return subevent",if not subevent :,if not subevent :,100.00000000000004,100.00000000000004,1.0
"def einsum(job_id, idx, einsum_expr, data_list):     _, all_parties = session_init(job_id, idx)     with SPDZ():         <MASK>             x = FixedPointTensor.from_source(""x"", data_list[0])             y = FixedPointTensor.from_source(""y"", all_parties[1])         else:             x = FixedPointTensor.from_source(""x"", all_parties[0])             y = FixedPointTensor.from_source(""y"", data_list[1])         return x.einsum(y, einsum_expr).get()",if idx == 0 :,if all_parties :,10.400597689005304,10.400597689005304,0.0
"def slowSorted(qq):     ""Reference sort peformed by insertion using only <""     rr = list()     for q in qq:         i = 0         for i in range(len(rr)):             <MASK>                 rr.insert(i, q)                 break         else:             rr.append(q)     return rr",if q < rr [ i ] :,if i < len ( rr ) :,8.25791079503452,8.25791079503452,0.0
"def _format_entry(entry, src):     if entry:         result = []         for x in entry.split("",""):             x = x.strip()             if os.path.exists(os.path.join(src, x)):                 result.append(relpath(os.path.join(src, x), src))             <MASK>                 result.append(relpath(os.path.abspath(x), src))             else:                 raise RuntimeError(""No entry script %s found"" % x)         return "","".join(result)",elif os . path . exists ( x ) :,if os . path . isdir ( x ) :,52.53819788848316,52.53819788848316,0.0
"def reloadCols(self):     self.columns = []     for i, (name, fmt, *shape) in enumerate(self.npy.dtype.descr):         if shape:             t = anytype         elif ""M"" in fmt:             self.addColumn(Column(name, type=date, getter=lambda c, r, i=i: str(r[i])))             continue         elif ""i"" in fmt:             t = int         <MASK>             t = float         else:             t = anytype         self.addColumn(ColumnItem(name, i, type=t))","elif ""f"" in fmt :","if ""f"" in fmt :",80.91067115702207,80.91067115702207,0.0
"def tool_lineages(self, trans):     rval = []     for id, tool in self.app.toolbox.tools():         <MASK>             lineage_dict = tool.lineage.to_dict()         else:             lineage_dict = None         entry = dict(id=id, lineage=lineage_dict)         rval.append(entry)     return rval","if hasattr ( tool , ""lineage"" ) :",if tool . lineage :,5.557509463743763,5.557509463743763,0.0
"def item(self, tensor):     numel = 0     if len(tensor.shape) > 0:         numel = fct.reduce(op.mul, tensor.shape)         <MASK>             raise ValueError(                 f""expected tensor with one element, "" f""got {tensor.shape}""             )     if numel == 1:         return tensor[0]     return tensor",if numel != 1 :,if numel != 1 :,100.00000000000004,100.00000000000004,1.0
"def get_host_metadata(self):     meta = {}     if self.agent_url:         try:             resp = requests.get(                 self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1             ).json()             if ""Version"" in resp:                 match = AGENT_VERSION_EXP.search(resp.get(""Version""))                 <MASK>                     meta[""ecs_version""] = match.group(1)         except Exception as e:             self.log.debug(""Error getting ECS version: %s"" % str(e))     return meta",if match is not None and len ( match . groups ( ) ) == 1 :,if match :,0.42446406286118865,0.0,0.0
"def generate():     for leaf in u.leaves:         if isinstance(leaf, Integer):             val = leaf.get_int_value()             if val in (0, 1):                 yield val             else:                 raise _NoBoolVector         elif isinstance(leaf, Symbol):             <MASK>                 yield 1             elif leaf == SymbolFalse:                 yield 0             else:                 raise _NoBoolVector         else:             raise _NoBoolVector",if leaf == SymbolTrue :,if leaf == SymbolTrue :,100.00000000000004,100.00000000000004,1.0
"def _test_set_metadata(self, metadata, mask=None):     header = ofproto.OXM_OF_METADATA     match = OFPMatch()     if mask is None:         match.set_metadata(metadata)     else:         <MASK>             header = ofproto.OXM_OF_METADATA_W         match.set_metadata_masked(metadata, mask)         metadata &= mask     self._test_serialize_and_parser(match, header, metadata, mask)",if ( mask + 1 ) >> 64 != 1 :,if mask is not None :,3.3264637832151163,3.3264637832151163,0.0
"def pixbufrenderer(self, column, crp, model, it):     tok = model.get_value(it, 0)     if tok.type == ""class"":         icon = ""class""     else:         if tok.visibility == ""private"":             icon = ""method_priv""         <MASK>             icon = ""method_prot""         else:             icon = ""method""     crp.set_property(""pixbuf"", imagelibrary.pixbufs[icon])","elif tok . visibility == ""protected"" :","if tok . visibility == ""prot"" :",58.14307369682194,58.14307369682194,0.0
"def path_sum2(root, s):     if root is None:         return []     res = []     stack = [(root, [root.val])]     while stack:         node, ls = stack.pop()         if node.left is None and node.right is None and sum(ls) == s:             res.append(ls)         <MASK>             stack.append((node.left, ls + [node.left.val]))         if node.right is not None:             stack.append((node.right, ls + [node.right.val]))     return res",if node . left is not None :,if node . left is not None :,100.00000000000004,100.00000000000004,1.0
"def clear_slot(self, slot_id, trigger_changed):     if self.slots[slot_id] is not None:         old_resource_id = self.slots[slot_id].resource_id         <MASK>             del self.sell_list[old_resource_id]         else:             del self.buy_list[old_resource_id]     self.slots[slot_id] = None     if trigger_changed:         self._changed()",if self . slots [ slot_id ] . selling :,if old_resource_id in self . sell_list :,9.669265690880861,9.669265690880861,0.0
"def OnRightUp(self, event):     self.HandleMouseEvent(event)     self.Unbind(wx.EVT_RIGHT_UP, handler=self.OnRightUp)     self.Unbind(wx.EVT_MOUSE_CAPTURE_LOST, handler=self.OnRightUp)     self._right = False     if not self._left:         self.Unbind(wx.EVT_MOTION, handler=self.OnMotion)         self.SendChangeEvent()         self.SetToolTip(wx.ToolTip(self._tooltip))         <MASK>             self.ReleaseMouse()",if self . HasCapture ( ) :,if self . _left :,27.482545710800192,27.482545710800192,0.0
"def __init__(self, *args, **kwargs):     for arg in args:         for k, v in arg.items():             <MASK>                 arg[k] = AttrDict(v)             else:                 arg[k] = v     super(AttrDict, self).__init__(*args, **kwargs)","if isinstance ( v , dict ) :","if isinstance ( v , AttrDict ) :",59.4603557501361,59.4603557501361,0.0
"def _toplevelTryFunc(func, *args, status=status, **kwargs):     with ThreadProfiler(threading.current_thread()) as prof:         t = threading.current_thread()         t.name = func.__name__         try:             t.status = func(*args, **kwargs)         except EscapeException as e:  # user aborted             t.status = ""aborted by user""             if status:                 status(""%s aborted"" % t.name, priority=2)         except Exception as e:             t.exception = e             t.status = ""exception""             vd.exceptionCaught(e)         <MASK>             t.sheet.currentThreads.remove(t)",if t . sheet :,"if t . status == ""aborted"" :",16.784459625186194,16.784459625186194,0.0
"def comboSelectionChanged(self, index):     text = self.comboBox.cb.itemText(index)     for i in range(self.labelList.count()):         if text == """":             self.labelList.item(i).setCheckState(2)         <MASK>             self.labelList.item(i).setCheckState(0)         else:             self.labelList.item(i).setCheckState(2)",elif text != self . labelList . item ( i ) . text ( ) :,"if text == ""0"" :",2.3595365419339505,2.3595365419339505,0.0
"def __attempt_add_to_linked_match(     self, input_name, hdca, collection_type_description, subcollection_type ):     structure = get_structure(         hdca, collection_type_description, leaf_subcollection_type=subcollection_type     )     if not self.linked_structure:         self.linked_structure = structure         self.collections[input_name] = hdca         self.subcollection_types[input_name] = subcollection_type     else:         <MASK>             raise exceptions.MessageException(CANNOT_MATCH_ERROR_MESSAGE)         self.collections[input_name] = hdca         self.subcollection_types[input_name] = subcollection_type",if not self . linked_structure . can_match ( structure ) :,if not structure . match ( input_name ) :,10.732392989213341,10.732392989213341,0.0
"def _wait_for_bot_presense(self, online):     for _ in range(10):         time.sleep(2)         <MASK>             break         if not online and not self._is_testbot_online():             break     else:         raise AssertionError(             ""test bot is still {}"".format(""offline"" if online else ""online"")         )",if online and self . _is_testbot_online ( ) :,if self . _is_testbot_offline ( ) :,54.96509142160034,54.96509142160034,0.0
"def find(self, path):     if os.path.isfile(path) or os.path.islink(path):         self.num_files = self.num_files + 1         <MASK>             self.files.append(path)     elif os.path.isdir(path):         for content in os.listdir(path):             file = os.path.join(path, content)             if os.path.isfile(file) or os.path.islink(file):                 self.num_files = self.num_files + 1                 if self.match_function(file):                     self.files.append(file)             else:                 self.find(file)",if self . match_function ( path ) :,if self . match_function ( path ) :,100.00000000000004,100.00000000000004,1.0
"def optimize(self, graph: Graph):     MAX_TEXTURE_SIZE = config.WEBGL_MAX_TEXTURE_SIZE     flag_changed = False     for v in traverse.listup_variables(graph):         if not Placeholder.check_resolved(v.size):             continue         height, width = TextureShape.get(v)         <MASK>             continue         if not v.has_attribute(SplitTarget):             flag_changed = True             v.attributes.add(SplitTarget())     return graph, flag_changed",if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE :,if height > MAX_TEXTURE_SIZE :,21.74757062134855,21.74757062134855,0.0
"def brightness_func(args):     device = _get_device_from_filter(args)     if args.set is None:         # Get brightness         if args.raw:             print(str(device.brightness))         else:             print(""Brightness: {0}%"".format(device.brightness))     else:         brightness_value = float(_clamp_u8(args.set))         <MASK>             print(""Setting brightness to {0}%"".format(brightness_value))         device.brightness = brightness_value",if not args . raw :,if args . raw :,57.89300674674101,57.89300674674101,0.0
"def _setup(self, field_name, owner_model):     # Resolve possible name-based model reference.     if not self.model_class:         <MASK>             self.model_class = owner_model         else:             raise Exception(                 ""ModelType: Unable to resolve model '{}'."".format(self.model_name)             )     super(ModelType, self)._setup(field_name, owner_model)",if self . model_name == owner_model . __name__ :,if owner_model :,3.355687704487231,3.355687704487231,0.0
"def build_json_schema_object(cls, parent_builder=None):     builder = builders.ObjectBuilder(cls, parent_builder)     if builder.count_type(builder.type) > 1:         return builder     for _, name, field in cls.iterate_with_name():         if isinstance(field, fields.EmbeddedField):             builder.add_field(name, field, _parse_embedded(field, builder))         <MASK>             builder.add_field(name, field, _parse_list(field, builder))         else:             builder.add_field(name, field, _create_primitive_field_schema(field))     return builder","elif isinstance ( field , fields . ListField ) :","if isinstance ( field , fields . ListField ) :",88.01117367933934,88.01117367933934,0.0
"def filter_module(mod, type_req=None, subclass_req=None):     for name in dir(mod):         val = getattr(mod, name)         if type_req is not None and not isinstance(val, type_req):             continue         <MASK>             continue         yield name, val","if subclass_req is not None and not issubclass ( val , subclass_req ) :",if subclass_req is not None :,24.909923021496894,24.909923021496894,0.0
"def get_icon(self):     if self.icon is not None:         # Load it from an absolute filename         if os.path.exists(self.icon):             try:                 return GdkPixbuf.Pixbuf.new_from_file_at_size(self.icon, 24, 24)             except GObject.GError as ge:                 pass         # Load it from the current icon theme         (icon_name, extension) = os.path.splitext(os.path.basename(self.icon))         theme = Gtk.IconTheme()         <MASK>             return theme.load_icon(icon_name, 24, 0)",if theme . has_icon ( icon_name ) :,"if extension == ""png"" :",3.983253478176822,3.983253478176822,0.0
"def sysctlTestAndSet(name, limit):     ""Helper function to set sysctl limits""     # convert non-directory names into directory names     if ""/"" not in name:         name = ""/proc/sys/"" + name.replace(""."", ""/"")     # read limit     with open(name, ""r"") as readFile:         oldLimit = readFile.readline()         if isinstance(limit, int):             # compare integer limits before overriding             <MASK>                 with open(name, ""w"") as writeFile:                     writeFile.write(""%d"" % limit)         else:             # overwrite non-integer limits             with open(name, ""w"") as writeFile:                 writeFile.write(limit)",if int ( oldLimit ) < limit :,if oldLimit != limit :,13.83254362586636,13.83254362586636,0.0
"def _wait_for_bot_presense(self, online):     for _ in range(10):         time.sleep(2)         if online and self._is_testbot_online():             break         <MASK>             break     else:         raise AssertionError(             ""test bot is still {}"".format(""offline"" if online else ""online"")         )",if not online and not self . _is_testbot_online ( ) :,if self . _is_testbot_offline ( ) :,46.52694539532656,46.52694539532656,0.0
"def handle(self, context, sign, *args):     if context.rounding in (ROUND_HALF_UP, ROUND_HALF_EVEN, ROUND_HALF_DOWN, ROUND_UP):         return Infsign[sign]     if sign == 0:         if context.rounding == ROUND_CEILING:             return Infsign[sign]         return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))     if sign == 1:         <MASK>             return Infsign[sign]         return Decimal((sign, (9,) * context.prec, context.Emax - context.prec + 1))",if context . rounding == ROUND_FLOOR :,if context . rounding == ROUND_CEILING :,78.25422900366438,78.25422900366438,0.0
"def _get_item_columns_panel(items, rows):     hbox = Gtk.HBox(False, 4)     n_item = 0     col_items = 0     vbox = Gtk.VBox()     hbox.pack_start(vbox, False, False, 0)     while n_item < len(items):         item = items[n_item]         vbox.pack_start(item, False, False, 0)         n_item += 1         col_items += 1         <MASK>             vbox = Gtk.VBox()             hbox.pack_start(vbox, False, False, 0)             col_items = 0     return hbox",if col_items > rows :,if col_items > rows :,100.00000000000004,100.00000000000004,1.0
"def _changed(self):     if self.gtk_range.get_sensitive():         <MASK>             self.timer.cancel()         self.timer = _Timer(0.5, lambda: GLib.idle_add(self._write))         self.timer.start()",if self . timer :,if self . timer . is_alive ( ) :,24.808415001701817,24.808415001701817,0.0
"def unlock_graph(result, callback, interval=1, propagate=False, max_retries=None):     if result.ready():         second_level_res = result.get()         <MASK>             with allow_join_result():                 signature(callback).delay(                     list(joinall(second_level_res, propagate=propagate))                 )     else:         unlock_graph.retry(countdown=interval, max_retries=max_retries)",if second_level_res . ready ( ) :,if second_level_res :,47.486944442513455,47.486944442513455,0.0
"def update(self, other=None, /, **kwargs):     if self._pending_removals:         self._commit_removals()     d = self.data     if other is not None:         <MASK>             other = dict(other)         for key, o in other.items():             d[key] = KeyedRef(o, self._remove, key)     for key, o in kwargs.items():         d[key] = KeyedRef(o, self._remove, key)","if not hasattr ( other , ""items"" ) :","if isinstance ( other , dict ) :",18.594002123233256,18.594002123233256,0.0
"def default(self, o):     try:         if type(o) == datetime.datetime:             return str(o)         else:             # remove unwanted attributes from the provider object during conversion to json             if hasattr(o, ""profile""):                 del o.profile             if hasattr(o, ""credentials""):                 del o.credentials             if hasattr(o, ""metadata_path""):                 del o.metadata_path             <MASK>                 del o.services_config             return vars(o)     except Exception as e:         return str(o)","if hasattr ( o , ""services_config"" ) :","if hasattr ( o , ""services_config"" ) :",100.00000000000004,100.00000000000004,1.0
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True):     try:         return self._read(count, timeout)     except usb.USBError as e:         <MASK>             log.info(                 ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s""                 % (e.errno, e.strerror, e.message, repr(e))             )         if ignore_timeouts and is_timeout(e):             return []         if ignore_non_errors and is_noerr(e):             return []         raise",if DEBUG_COMM :,if e . errno :,12.703318703865365,12.703318703865365,0.0
def heal(self):     if not self.doctors:         return     proc_ids = self._get_process_ids()     for proc_id in proc_ids:         # get proc every time for latest state         proc = PipelineProcess.objects.get(id=proc_id)         <MASK>             continue         for dr in self.doctors:             if dr.confirm(proc):                 dr.cure(proc)                 break,if not proc . is_alive or proc . is_frozen :,if not proc :,5.2447643832804935,5.2447643832804935,0.0
"def to_value(self, value):     # Tip: 'value' is the object returned by     #      taiga.projects.history.models.HistoryEntry.values_diff()     ret = {}     for key, val in value.items():         <MASK>             ret[key] = val         elif key == ""points"":             ret[key] = {k: {""from"": v[0], ""to"": v[1]} for k, v in val.items()}         else:             ret[key] = {""from"": val[0], ""to"": val[1]}     return ret","if key in [ ""attachments"" , ""custom_attributes"" , ""description_diff"" ] :","if key == ""points"" :",2.7347280853315854,2.7347280853315854,0.0
"def default_generator(     self, dataset, epochs=1, mode=""fit"", deterministic=True, pad_batches=True ):     for epoch in range(epochs):         for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(             batch_size=self.batch_size,             deterministic=deterministic,             pad_batches=pad_batches,         ):             <MASK>                 dropout = np.array(0.0)             else:                 dropout = np.array(1.0)             yield ([X_b, dropout], [y_b], [w_b])","if mode == ""predict"" :","if mode == ""fit"" :",59.4603557501361,59.4603557501361,0.0
"def _cygwin_hack_find_addresses(target):     addresses = []     for h in [         target,         ""localhost"",         ""127.0.0.1"",     ]:         try:             addr = get_local_ip_for(h)             <MASK>                 addresses.append(addr)         except socket.gaierror:             pass     return defer.succeed(addresses)",if addr not in addresses :,if addr :,23.174952587773145,0.0,0.0
"def _get_notify(self, action_node):     if action_node.name not in self._skip_notify_tasks:         if action_node.notify:             task_notify = NotificationsHelper.to_model(action_node.notify)             return task_notify         <MASK>             return self._chain_notify     return None",elif self . _chain_notify :,if action_node . chain_notify :,33.03164318013809,33.03164318013809,0.0
"def filterTokenLocation():     i = None     entry = None     token = None     tokens = []     i = 0     while 1:         <MASK>             break         entry = extra.tokens[i]         token = jsdict(             {                 ""type"": entry.type,                 ""value"": entry.value,             }         )         if extra.range:             token.range = entry.range         if extra.loc:             token.loc = entry.loc         tokens.append(token)         i += 1     extra.tokens = tokens",if not ( i < len ( extra . tokens ) ) :,if i >= extra . tokens . count ( ) :,16.40212036255558,16.40212036255558,0.0
"def read(self, size=-1):     buf = bytearray()     while size != 0 and self.cursor < self.maxpos:         if not self.in_current_block(self.cursor):             self.seek_to_block(self.cursor)         part = self.current_stream.read(size)         <MASK>             if len(part) == 0:                 raise EOFError()             size -= len(part)         self.cursor += len(part)         buf += part     return bytes(buf)",if size > 0 :,if part is None :,12.703318703865365,12.703318703865365,0.0
"def get_properties_from_model(model_class):     """"""Show properties from a model""""""     properties = []     attr_names = [name for (name, value) in inspect.getmembers(model_class, isprop)]     for attr_name in attr_names:         <MASK>             attr_names.remove(attr_name)         else:             properties.append(                 dict(label=attr_name, name=attr_name.strip(""_"").replace(""_"", "" ""))             )     return sorted(properties, key=lambda k: k[""label""])","if attr_name . endswith ( ""pk"" ) :","if attr_name in [ ""label"" , ""name"" ] :",18.92240568795936,18.92240568795936,0.0
"def __getitem__(self, name, set=set, getattr=getattr, id=id):     visited = set()     mydict = self.basedict     while 1:         value = mydict[name]         if value is not None:             return value         myid = id(mydict)         assert myid not in visited         visited.add(myid)         mydict = mydict.Parent         <MASK>             return",if mydict is None :,if myid in visited :,12.703318703865365,12.703318703865365,0.0
"def multicolumn(self, list, format, cols=4):     """"""Format a list of items into a multi-column list.""""""     result = """"     rows = (len(list) + cols - 1) // cols     for col in range(cols):         result = result + '<td width=""%d%%"" valign=top>' % (100 // cols)         for i in range(rows * col, rows * col + rows):             <MASK>                 result = result + format(list[i]) + ""<br>\n""         result = result + ""</td>""     return '<table width=""100%%"" summary=""list""><tr>%s</tr></table>' % result",if i < len ( list ) :,if i % 2 :,12.975849993980741,12.975849993980741,0.0
"def format_exc(exc=None):     """"""Return exc (or sys.exc_info if None), formatted.""""""     try:         <MASK>             exc = _exc_info()         if exc == (None, None, None):             return """"         import traceback         return """".join(traceback.format_exception(*exc))     finally:         del exc",if exc is None :,if exc is None :,100.00000000000004,100.00000000000004,1.0
"def assert_counts(res, lang, files, blank, comment, code):     for line in res:         fields = line.split()         if len(fields) >= 5:             <MASK>                 self.assertEqual(files, int(fields[1]))                 self.assertEqual(blank, int(fields[2]))                 self.assertEqual(comment, int(fields[3]))                 self.assertEqual(code, int(fields[4]))                 return     self.fail(""Found no output line for {}"".format(lang))",if fields [ 0 ] == lang :,if not fields :,6.023021415818187,6.023021415818187,0.0
"def __iter__(self):     for name, value in self.__class__.__dict__.items():         <MASK>             continue         if isinstance(value, flag_value):             yield (name, self._has_flag(value.flag))","if isinstance ( value , alias_flag_value ) :",if not value :,2.845073863275343,2.845073863275343,0.0
"def optimize_models(args, use_cuda, models):     """"""Optimize ensemble for generation""""""     for model in models:         model.make_generation_fast_(             beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,             need_attn=args.print_alignment,         )         if args.fp16:             model.half()         <MASK>             model.cuda()",if use_cuda :,if args . use_cuda :,43.47208719449914,43.47208719449914,0.0
"def convertstore(self, mydict):     targetheader = self.mypofile.header()     targetheader.addnote(""extracted from web2py"", ""developer"")     for source_str in mydict.keys():         target_str = mydict[source_str]         if target_str == source_str:             # a convention with new (untranslated) web2py files             target_str = u""""         <MASK>             # an older convention             target_str = u""""         pounit = self.convertunit(source_str, target_str)         self.mypofile.addunit(pounit)     return self.mypofile","elif target_str . startswith ( u""*** "" ) :",if target_str == source_str :,10.180289369384242,10.180289369384242,0.0
"def __sparse_values_set(instances, static_col_indexes: list):     tmp_result = {idx: set() for idx in static_col_indexes}     for _, instance in instances:         data_generator = instance.features.get_all_data()         for idx, value in data_generator:             <MASK>                 continue             tmp_result[idx].add(value)     result = [tmp_result[x] for x in static_col_indexes]     return result",if idx not in tmp_result :,if idx not in tmp_result :,100.00000000000004,100.00000000000004,1.0
def puts(self):     <MASK>         self.lazy_init_lock_.acquire()         try:             if self.puts_ is None:                 self.puts_ = PutRequest()         finally:             self.lazy_init_lock_.release()     return self.puts_,if self . puts_ is None :,if self . puts_ is None :,100.00000000000004,100.00000000000004,1.0
"def run(self, args, **kwargs):     if args.resource_ref or args.policy_type:         filters = {}         if args.resource_ref:             filters[""resource_ref""] = args.resource_ref         <MASK>             filters[""policy_type""] = args.policy_type         filters.update(**kwargs)         return self.manager.query(**filters)     else:         return self.manager.get_all(**kwargs)",if args . policy_type :,if args . policy_type :,100.00000000000004,100.00000000000004,1.0
"def Get_Gene(self, id):     """"""Retreive the gene name (GN).""""""     entry = self.Get(id)     if not entry:         return None     GN = """"     for line in string.split(entry, ""\n""):         if line[0:5] == ""GN   "":             GN = string.strip(line[5:])             <MASK>                 GN = GN[0:-1]             return GN         if line[0:2] == ""//"":             break     return GN","if GN [ - 1 ] == ""."" :","if line [ 0 : - 1 ] == ""GN"" :",42.61082723917018,42.61082723917018,0.0
"def processMovie(self, atom):     for field in atom:         <MASK>             self.processTrack(field[""track""])         if ""movie_hdr"" in field:             self.processMovieHeader(field[""movie_hdr""])","if ""track"" in field :","if ""track"" in field :",100.00000000000004,100.00000000000004,1.0
"def get_next_video_frame(self, skip_empty_frame=True):     if not self.video_format:         return     while True:         # We skip video packets which are not video frames         # This happens in mkv files for the first few frames.         video_packet = self._get_video_packet()         <MASK>             self._decode_video_packet(video_packet)         if video_packet.image is not None or not skip_empty_frame:             break     if _debug:         print(""Returning"", video_packet)     return video_packet.image",if video_packet . image == 0 :,if video_packet is not None :,28.46946938149361,28.46946938149361,0.0
"def get_devices(display=None):     base = ""/dev/input""     for filename in os.listdir(base):         if filename.startswith(""event""):             path = os.path.join(base, filename)             <MASK>                 continue             try:                 _devices[path] = EvdevDevice(display, path)             except OSError:                 pass     return list(_devices.values())",if path in _devices :,if not path :,12.750736437345598,12.750736437345598,0.0
"def _ensure_header_written(self, datasize):     if not self._headerwritten:         if not self._nchannels:             raise Error(""# channels not specified"")         <MASK>             raise Error(""sample width not specified"")         if not self._framerate:             raise Error(""sampling rate not specified"")         self._write_header(datasize)",if not self . _sampwidth :,if not self . _samplewidth :,64.34588841607616,64.34588841607616,0.0
"def process(self, fuzzresult):     base_url = urljoin(fuzzresult.url, "".."")     for line in fuzzresult.history.content.splitlines():         record = line.split(""/"")         if len(record) == 6 and record[1]:             self.queue_url(urljoin(base_url, record[1]))             # Directory             <MASK>                 self.queue_url(urljoin(base_url, record[1]))                 self.queue_url(urljoin(base_url, ""%s/CVS/Entries"" % (record[1])))","if record [ 0 ] == ""D"" :",if len ( record ) == 2 :,9.600960275119885,9.600960275119885,0.0
"def tearDown(self):     """"""Shutdown the UDP server.""""""     try:         <MASK>             self.server.stop(2.0)         if self.sock_hdlr:             self.root_logger.removeHandler(self.sock_hdlr)             self.sock_hdlr.close()     finally:         BaseTest.tearDown(self)",if self . server :,if self . server :,100.00000000000004,100.00000000000004,1.0
"def get_backend(find_library=None):     try:         global _lib, _ctx         <MASK>             _lib = _load_library(find_library)             _setup_prototypes(_lib)             _ctx = _Context()         _logger.warning(             ""OpenUSB backend deprecated (https://github.com/pyusb/pyusb/issues/284)""         )         return _OpenUSB()     except usb.libloader.LibraryException:         # exception already logged (if any)         _logger.error(""Error loading OpenUSB backend"", exc_info=False)         return None     except Exception:         _logger.error(""Error loading OpenUSB backend"", exc_info=True)         return None",if _lib is None :,if find_library :,11.51015341649912,11.51015341649912,0.0
"def __init__(self, event, event_info, fields=[]):     _wmi_object.__init__(self, event, fields=fields)     _set(self, ""event_type"", None)     _set(self, ""timestamp"", None)     _set(self, ""previous"", None)     if event_info:         event_type = self.event_type_re.match(event_info.Path_.Class).group(1).lower()         _set(self, ""event_type"", event_type)         if hasattr(event_info, ""TIME_CREATED""):             _set(self, ""timestamp"", from_1601(event_info.TIME_CREATED))         <MASK>             _set(self, ""previous"", event_info.PreviousInstance)","if hasattr ( event_info , ""PreviousInstance"" ) :","if hasattr ( event_info , ""PreviousInstance"" ) :",100.00000000000004,100.00000000000004,1.0
"def _getListNextPackagesReadyToBuild():     for pkg in Scheduler.listOfPackagesToBuild:         <MASK>             continue         if constants.rpmCheck or Scheduler._checkNextPackageIsReadyToBuild(pkg):             Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg))             Scheduler.logger.debug(""Adding "" + pkg + "" to the schedule list"")",if pkg in Scheduler . listOfPackagesCurrentlyBuilding :,if pkg not in Scheduler . listOfPackagesNextToBuild :,27.054113452696992,27.054113452696992,0.0
"def process_all(self, lines, times=1):     gap = False     for _ in range(times):         for line in lines:             <MASK>                 self.write("""")             self.process(line)             if not is_command(line):                 gap = True     return 0",if gap :,if gap :,100.00000000000004,0.0,1.0
"def diff(old, new, display=True):     """"""Nice colored diff implementation""""""     if not isinstance(old, list):         old = decolorize(str(old)).splitlines()     if not isinstance(new, list):         new = decolorize(str(new)).splitlines()     line_types = {"" "": ""%Reset"", ""-"": ""%Red"", ""+"": ""%Green"", ""?"": ""%Pink""}     if display:         for line in difflib.Differ().compare(old, new):             <MASK>                 continue             print(colorize(line_types[line[0]], line))     return old != new","if line . startswith ( ""?"" ) :",if not line :,4.690733795095046,4.690733795095046,0.0
"def get_limit(self, request):     if self.limit_query_param:         try:             limit = int(request.query_params[self.limit_query_param])             if limit < 0:                 raise ValueError()             # Enforce maximum page size, if defined             if settings.MAX_PAGE_SIZE:                 <MASK>                     return settings.MAX_PAGE_SIZE                 else:                     return min(limit, settings.MAX_PAGE_SIZE)             return limit         except (KeyError, ValueError):             pass     return self.default_limit",if limit == 0 :,if limit > settings . MAX_PAGE_SIZE :,8.29519350710986,8.29519350710986,0.0
"def slice_fill(self, slice_):     ""Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true""     if isinstance(self.indexes, int):         new_slice_ = [0]         offset = 0     else:         new_slice_ = [slice_[0]]         offset = 1     for i in range(1, len(self.nums)):         if self.squeeze_dims[i]:             new_slice_.append(0)         <MASK>             new_slice_.append(slice_[offset])             offset += 1     new_slice_ += slice_[offset:]     return new_slice_",elif offset < len ( slice_ ) :,if slice_ [ i ] :,10.923299908191149,10.923299908191149,0.0
"def wrapper(*args, **kw):     instance = args[0]     try:         <MASK>             ret_dict = instance._create_ret_object(                 instance.FAILURE, None, True, instance.MUST_JSON             )             instance.logger.error(instance.MUST_JSON)             return jsonify(ret_dict), 400     except BadRequest:         ret_dict = instance._create_ret_object(             instance.FAILURE, None, True, instance.MUST_JSON         )         instance.logger.error(instance.MUST_JSON)         return jsonify(ret_dict), 400     instance.logger.debug(""JSON is valid"")     return f(*args, **kw)",if request . get_json ( ) is None :,if instance . FAILURE :,4.234348806659263,4.234348806659263,0.0
"def add_css(self, data):     if data:         for medium, paths in data.items():             for path in paths:                 <MASK>                     self._css.setdefault(medium, []).append(path)",if not self . _css . get ( medium ) or path not in self . _css [ medium ] :,if medium in self . _css :,8.916529139038362,8.916529139038362,0.0
"def mangle_template(template: str, template_vars: Set[str]) -> str:     if TEMPLATE_PREFIX in template or TEMPLATE_SUFFIX in template:         raise Exception(""Cannot parse a template containing reserved strings"")     for var in template_vars:         original = f""{{{var}}}""         <MASK>             raise Exception(                 f'Template string is missing a reference to ""{var}"" referred to in kwargs'             )         template = template.replace(original, mangled_name(var))     return template",if original not in template :,if var not in kwargs :,19.304869754804482,19.304869754804482,0.0
"def filterSimilarKeywords(keyword, kwdsIterator):     """"""Return a sorted list of keywords similar to the one given.""""""     seenDict = {}     kwdSndx = soundex(keyword.encode(""ascii"", ""ignore""))     matches = []     matchesappend = matches.append     checkContained = False     if len(keyword) > 4:         checkContained = True     for movieID, key in kwdsIterator:         if key in seenDict:             continue         seenDict[key] = None         if checkContained and keyword in key:             matchesappend(key)             continue         <MASK>             matchesappend(key)     return _sortKeywords(keyword, matches)","if kwdSndx == soundex ( key . encode ( ""ascii"" , ""ignore"" ) ) :",if kwdSndx == kwdSndx :,5.210158044842422,5.210158044842422,0.0
"def GetInfo(self):     for k, v in sorted(self.memory_parameters.items()):         <MASK>             continue         if not v:             continue         print(""%s: \t%#08x (%s)"" % (k, v, v))     print(""Memory ranges:"")     print(""Start\t\tEnd\t\tLength"")     for start, length in self.runs:         print(""0x%X\t\t0x%X\t\t0x%X"" % (start, start + length, length))","if k . startswith ( ""Pad"" ) :",if not k :,4.690733795095046,4.690733795095046,0.0
"def Children(self):     """"""Returns a list of all of this object's owned (strong) children.""""""     children = []     for property, attributes in self._schema.iteritems():         (is_list, property_type, is_strong) = attributes[0:3]         if is_strong and property in self._properties:             <MASK>                 children.append(self._properties[property])             else:                 children.extend(self._properties[property])     return children",if not is_list :,if is_list :,57.89300674674101,57.89300674674101,0.0
"def normalize_res_identifier(self, emu, cw, val):     mask = (16 ** (emu.get_ptr_size() // 2) - 1) << 16     if val & mask:  # not an INTRESOURCE         name = emu.read_mem_string(val, cw)         <MASK>             try:                 name = int(name[1:])             except Exception:                 return 0     else:         name = val     return name","if name [ 0 ] == ""#"" :",if name [ 0 ] == 0 :,60.10525952194528,60.10525952194528,0.0
"def _optimize(self, solutions):     best_a = None     best_silhouette = None     best_k = None     for a, silhouette, k in solutions():         if best_silhouette is None:             pass         <MASK>             break         best_silhouette = silhouette         best_a = a         best_k = k     return best_a, best_silhouette, best_k",elif silhouette <= best_silhouette :,if silhouette is None :,6.9717291216921975,6.9717291216921975,0.0
"def find_commit_type(sha):     try:         o = obj_store[sha]     except KeyError:         <MASK>             raise     else:         if isinstance(o, Commit):             commits.add(sha)         elif isinstance(o, Tag):             tags.add(sha)             commits.add(o.object[1])         else:             raise KeyError(""Not a commit or a tag: %s"" % sha)",if not ignore_unknown :,if o is None :,10.400597689005304,10.400597689005304,0.0
"def on_search_entry_keypress(self, widget, event):     key = Gdk.keyval_name(event.keyval)     if key == ""Escape"":         self.hide_search_box()     elif key == ""Return"":         # Combine with Shift?         <MASK>             self.search_prev = False             self.do_search(None)         else:             self.search_prev = True",if event . state & Gdk . ModifierType . SHIFT_MASK :,if self . search_prev :,3.9413751108533592,3.9413751108533592,0.0
"def process_webhook_prop(namespace):     if not isinstance(namespace.webhook_properties, list):         return     result = {}     for each in namespace.webhook_properties:         <MASK>             if ""="" in each:                 key, value = each.split(""="", 1)             else:                 key, value = each, """"             result[key] = value     namespace.webhook_properties = result",if each :,"if ""="" in each :",14.535768424205482,14.535768424205482,0.0
"def run(self):     global WAITING_BEFORE_START     time.sleep(WAITING_BEFORE_START)     while self.keep_alive:         path_id, module, resolve = self.queue_receive.get()         <MASK>             continue         self.lock.acquire()         self.modules[path_id] = module         self.lock.release()         if resolve:             resolution = self._resolve_with_other_modules(resolve)             self._relations[path_id] = []             for package in resolution:                 self._relations[path_id].append(resolution[package])             self.queue_send.put((path_id, module, False, resolution))",if path_id is None :,if not path_id :,29.05925408079185,29.05925408079185,0.0
"def _get_download_link(self, url, download_type=""torrent""):     links = {         ""torrent"": """",         ""magnet"": """",     }     try:         data = self.session.get(url).text         with bs4_parser(data) as html:             downloads = html.find(""div"", {""class"": ""download""})             if downloads:                 for download in downloads.findAll(""a""):                     link = download[""href""]                     <MASK>                         links[""magnet""] = link                     else:                         links[""torrent""] = urljoin(self.urls[""base_url""], link)     except Exception:         pass     return links[download_type]","if link . startswith ( ""magnet"" ) :",if link is not None :,9.22364410103253,9.22364410103253,0.0
"def _parse_fields(cls, read):     read = unicode_to_str(read)     if type(read) is not str:         _wrong_type_for_arg(read, ""str"", ""read"")     fields = {}     while read and read[0] != "";"":         <MASK>             DeserializeError(read, ""does not separate fields with commas"")         read = read[1:]         key, _type, value, read = cls._parse_field(read)         fields[key] = (_type, value)     if read:         # read[0] == ';'         read = read[1:]     return fields, read","if read and read [ 0 ] != "","" :","if read [ 0 ] != "";"" :",55.89668689756618,55.89668689756618,0.0
"def _convertDict(self, d):     r = {}     for k, v in d.items():         <MASK>             v = str(v, ""utf-8"")         elif isinstance(v, list) or isinstance(v, tuple):             v = self._convertList(v)         elif isinstance(v, dict):             v = self._convertDict(v)         if isinstance(k, bytes):             k = str(k, ""utf-8"")         r[k] = v     return r","if isinstance ( v , bytes ) :","if isinstance ( v , bytes ) :",100.00000000000004,100.00000000000004,1.0
"def wrapper(filename):     mtime = getmtime(filename)     with lock:         if filename in cache:             old_mtime, result = cache.pop(filename)             <MASK>                 # Move to the end                 cache[filename] = old_mtime, result                 return result     result = function(filename)     with lock:         cache[filename] = mtime, result  # at the end         if len(cache) > max_size:             cache.popitem(last=False)     return result",if old_mtime == mtime :,if mtime != old_mtime :,28.117066259517458,28.117066259517458,0.0
def isFinished(self):     # returns true if episode timesteps has reached episode length and resets the task     if self.count > self.epiLen:         self.res()         return True     else:         <MASK>             self.pertGlasPos(0)         if self.count == self.epiLen / 2 + 1:             self.env.reset()             self.pertGlasPos(1)         self.count += 1         return False,if self . count == 1 :,if self . count == self . episodes :,53.7284965911771,53.7284965911771,0.0
"def _check_vulnerabilities(self, processed_analysis):     matched_vulnerabilities = list()     for vulnerability in self._rule_base_vulnerabilities:         <MASK>             vulnerability_data = vulnerability.get_dict()             name = vulnerability_data.pop(""short_name"")             matched_vulnerabilities.append((name, vulnerability_data))     return matched_vulnerabilities","if evaluate ( processed_analysis , vulnerability . rule ) :",if vulnerability . get_short_name ( ) in processed_analysis . rules :,12.39899236095509,12.39899236095509,0.0
"def _table_reprfunc(self, row, col, val):     if self._table.column_names[col].endswith(""Size""):         <MASK>             return ""  %s"" % val         elif val < 1024 ** 2:             return ""  %.1f KB"" % (val / 1024.0 ** 1)         elif val < 1024 ** 3:             return ""  %.1f MB"" % (val / 1024.0 ** 2)         else:             return ""  %.1f GB"" % (val / 1024.0 ** 3)     if col in (0, """"):         return str(val)     else:         return ""  %s"" % val","if isinstance ( val , compat . string_types ) :",if val < 1024 ** 1 :,4.408194605881708,4.408194605881708,0.0
"def serve_until_stopped(self) -> None:     while True:         rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout)         if rd:             self.handle_request()         <MASK>             break",if self . event is not None and self . event . is_set ( ) :,if ex :,0.23359157588964669,0.0,0.0
"def resize(self, *e):     bold = (""helvetica"", -self._size.get(), ""bold"")     helv = (""helvetica"", -self._size.get())     xspace = self._size.get()     yspace = self._size.get()     for widget in self._widgets:         widget[""node_font""] = bold         widget[""leaf_font""] = helv         widget[""xspace""] = xspace         widget[""yspace""] = yspace         if self._size.get() < 20:             widget[""line_width""] = 1         <MASK>             widget[""line_width""] = 2         else:             widget[""line_width""] = 3     self._layout()",elif self . _size . get ( ) < 30 :,if self . _size . get ( ) < 10 :,72.92571723872932,72.92571723872932,0.0
"def __assertTilesChangedInRegion(self, t1, t2, region):     for tileOriginTuple in t1.keys():         tileOrigin = imath.V2i(*tileOriginTuple)         tileRegion = imath.Box2i(             tileOrigin, tileOrigin + imath.V2i(GafferImage.ImagePlug.tileSize())         )         <MASK>             self.assertNotEqual(t1[tileOriginTuple], t2[tileOriginTuple])         else:             self.assertEqual(t1[tileOriginTuple], t2[tileOriginTuple])","if GafferImage . BufferAlgo . intersects ( tileRegion , region ) :",if region . is_in_region ( tileRegion ) :,11.016798394984653,11.016798394984653,0.0
"def grouped_by_prefix(args, prefixes):     """"""Group behave args by (directory) scope into multiple test-runs.""""""     group_args = []     current_scope = None     for arg in args.strip().split():         assert not arg.startswith(""-""), ""REQUIRE: arg, not options""         scope = select_prefix_for(arg, prefixes)         if scope != current_scope:             <MASK>                 # -- DETECTED GROUP-END:                 yield "" "".join(group_args)                 group_args = []             current_scope = scope         group_args.append(arg)     if group_args:         yield "" "".join(group_args)",if group_args :,if group_args :,100.00000000000004,100.00000000000004,1.0
"def __print__(self, defaults=False):     if defaults:         print_func = str     else:         print_func = repr     pieces = []     default_values = self.__defaults__     for k in self.__fields__:         value = getattr(self, k)         <MASK>             continue         if isinstance(value, basestring):             print_func = repr  # keep quotes around strings         pieces.append(""%s=%s"" % (k, print_func(value)))     if pieces or self.__base__:         return ""%s(%s)"" % (self.__class__.__name__, "", "".join(pieces))     else:         return """"",if not defaults and value == default_values [ k ] :,if not value :,3.118555560555009,3.118555560555009,0.0
"def setInnerHTML(self, html):     log.HTMLClassifier.classify(         log.ThugLogging.url if log.ThugOpts.local else log.last_url, html     )     self.tag.clear()     for node in bs4.BeautifulSoup(html, ""html.parser"").contents:         self.tag.append(node)         name = getattr(node, ""name"", None)         <MASK>             continue         handler = getattr(log.DFT, ""handle_%s"" % (name,), None)         if handler:             handler(node)",if name is None :,if name is None :,100.00000000000004,100.00000000000004,1.0
"def createFields(self):     yield Enum(Bits(self, ""class"", 2), self.CLASS_DESC)     yield Enum(Bit(self, ""form""), self.FORM_DESC)     if self[""class""].value == 0:         yield Enum(Bits(self, ""type"", 5), self.TYPE_DESC)     else:         yield Bits(self, ""type"", 5)     yield ASNInteger(self, ""size"", ""Size in bytes"")     size = self[""size""].value     if size:         <MASK>             for field in self._handler(self, size):                 yield field         else:             yield RawBytes(self, ""raw"", size)",if self . _handler :,if self . _handler :,100.00000000000004,100.00000000000004,1.0
"def _process_service_request(self, pkttype, pktid, packet):     """"""Process a service request""""""     # pylint: disable=unused-argument     service = packet.get_string()     packet.check_end()     if service == self._next_service:         self.logger.debug2(""Accepting request for service %s"", service)         self._next_service = None         self.send_packet(MSG_SERVICE_ACCEPT, String(service))         <MASK>  # pragma: no branch             self._auth_in_progress = True             self._send_deferred_packets()     else:         raise DisconnectError(             DISC_SERVICE_NOT_AVAILABLE, ""Unexpected service request received""         )",if self . is_server ( ) and service == _USERAUTH_SERVICE :,if self . _auth_in_progress :,9.586514611843183,9.586514611843183,0.0
"def _read_fixed_body(     self, content_length: int, delegate: httputil.HTTPMessageDelegate ) -> None:     while content_length > 0:         body = await self.stream.read_bytes(             min(self.params.chunk_size, content_length), partial=True         )         content_length -= len(body)         <MASK>             with _ExceptionLoggingContext(app_log):                 ret = delegate.data_received(body)                 if ret is not None:                     await ret",if not self . _write_finished or self . is_client :,if body is not None :,2.5612540390806937,2.5612540390806937,0.0
"def wait_for_child(pid, timeout=1.0):     deadline = mitogen.core.now() + timeout     while timeout < mitogen.core.now():         try:             target_pid, status = os.waitpid(pid, os.WNOHANG)             <MASK>                 return         except OSError:             e = sys.exc_info()[1]             if e.args[0] == errno.ECHILD:                 return         time.sleep(0.05)     assert False, ""wait_for_child() timed out""",if target_pid == pid :,if target_pid == 0 :,70.71067811865478,70.71067811865478,0.0
"def execute(cls, ctx, op: ""DataFrameGroupByAgg""):     try:         pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na)         if op.stage == OperandStage.map:             cls._execute_map(ctx, op)         <MASK>             cls._execute_combine(ctx, op)         elif op.stage == OperandStage.agg:             cls._execute_agg(ctx, op)         else:  # pragma: no cover             raise ValueError(""Aggregation operand not executable"")     finally:         pd.reset_option(""mode.use_inf_as_na"")",elif op . stage == OperandStage . combine :,if op . stage == OperandStage . combine :,88.01117367933934,88.01117367933934,0.0
def cut(sentence):     sentence = strdecode(sentence)     blocks = re_han.split(sentence)     for blk in blocks:         if re_han.match(blk):             for word in __cut(blk):                 <MASK>                     yield word                 else:                     for c in word:                         yield c         else:             tmp = re_skip.split(blk)             for x in tmp:                 if x:                     yield x,if word not in Force_Split_Words :,if word :,6.108851178104657,0.0,0.0
"def _iter_tags(self, type=None):     """"""Yield all raw tags (limit to |type| if specified)""""""     for n in itertools.count():         tag = self._get_tag(n)         <MASK>             yield tag         if tag[""d_tag""] == ""DT_NULL"":             break","if type is None or tag [ ""d_tag"" ] == type :","if tag [ ""d_tag"" ] == type :",68.41262339661338,68.41262339661338,0.0
"def reverse_search_history(self, searchfor, startpos=None):     if startpos is None:         startpos = self.history_cursor     if _ignore_leading_spaces:         res = [             (idx, line.lstrip())             for idx, line in enumerate(self.history[startpos:0:-1])             if line.lstrip().startswith(searchfor.lstrip())         ]     else:         res = [             (idx, line)             for idx, line in enumerate(self.history[startpos:0:-1])             <MASK>         ]     if res:         self.history_cursor -= res[0][0]         return res[0][1].get_line_text()     return """"",if line . startswith ( searchfor ),if line . strip ( ) == searchfor :,18.575057999133595,18.575057999133595,0.0
"def value_to_db_datetime(self, value):     if value is None:         return None     # Oracle doesn't support tz-aware datetimes     if timezone.is_aware(value):         <MASK>             value = value.astimezone(timezone.utc).replace(tzinfo=None)         else:             raise ValueError(                 ""Oracle backend does not support timezone-aware datetimes when USE_TZ is False.""             )     return unicode(value)",if settings . USE_TZ :,if USE_TZ :,47.39878501170795,47.39878501170795,0.0
"def _sniff(filename, oxlitype):     try:         with open(filename, ""rb"") as fileobj:             header = fileobj.read(4)             if header == b""OXLI"":                 fileobj.read(1)  # skip the version number                 ftype = fileobj.read(1)                 <MASK>                     return True         return False     except OSError:         return False",if binascii . hexlify ( ftype ) == oxlitype :,if ftype == oxlitype :,24.439253249722206,24.439253249722206,0.0
"def unget(self, char):     # Only one character is allowed to be ungotten at once - it must     # be consumed again before any further call to unget     if char is not EOF:         <MASK>             # unget is called quite rarely, so it's a good idea to do             # more work here if it saves a bit of work in the frequently             # called char and charsUntil.             # So, just prepend the ungotten character onto the current             # chunk:             self.chunk = char + self.chunk             self.chunkSize += 1         else:             self.chunkOffset -= 1             assert self.chunk[self.chunkOffset] == char",if self . chunkOffset == 0 :,if self . chunkOffset == self . chunkSize :,53.7284965911771,53.7284965911771,0.0
"def scan(rule, extensions, paths, ignore_paths=None):     """"""The libsast scan.""""""     try:         options = {             ""match_rules"": rule,             ""match_extensions"": extensions,             ""ignore_paths"": ignore_paths,             ""show_progress"": False,         }         scanner = Scanner(options, paths)         res = scanner.scan()         <MASK>             return format_findings(res[""pattern_matcher""], paths[0])     except Exception:         logger.exception(""libsast scan"")     return {}",if res :,if res :,100.00000000000004,0.0,1.0
"def _getPatternTemplate(pattern, key=None):     if key is None:         key = pattern         <MASK>             key = pattern.upper()     template = DD_patternCache.get(key)     if not template:         if key in (""EPOCH"", ""{^LN-BEG}EPOCH"", ""^EPOCH""):             template = DateEpoch(lineBeginOnly=(key != ""EPOCH""))         elif key in (""TAI64N"", ""{^LN-BEG}TAI64N"", ""^TAI64N""):             template = DateTai64n(wordBegin=(""start"" if key != ""TAI64N"" else False))         else:             template = DatePatternRegex(pattern)     DD_patternCache.set(key, template)     return template","if ""%"" not in pattern :","if key == ""EPOCH"" :",7.809849842300637,7.809849842300637,0.0
"def _forward_response(self, src, dst):     """"""Forward an SCP response between two remote SCP servers""""""     # pylint: disable=no-self-use     try:         exc = yield from src.await_response()         <MASK>             dst.send_error(exc)             return exc         else:             dst.send_ok()             return None     except OSError as exc:         return exc",if exc :,if exc :,100.00000000000004,0.0,1.0
"def _maybe_signal_recovery_end() -> None:     if self.in_recovery and not self.active_remaining_total():         # apply anything stuck in the buffers         self.flush_buffers()         self._set_recovery_ended()         <MASK>             self._actives_span.set_tag(""Actives-Ready"", True)         self.signal_recovery_end.set()",if self . _actives_span is not None :,if self . in_recovery :,15.685718045401451,15.685718045401451,0.0
"def main():     tmpdir = None     try:         # Create a temporary working directory         tmpdir = tempfile.mkdtemp()         # Unpack the zipfile into the temporary directory         pip_zip = os.path.join(tmpdir, ""pip.zip"")         with open(pip_zip, ""wb"") as fp:             fp.write(b85decode(DATA.replace(b""\n"", b"""")))         # Add the zipfile to sys.path so that we can import it         sys.path.insert(0, pip_zip)         # Run the bootstrap         bootstrap(tmpdir=tmpdir)     finally:         # Clean up our temporary working directory         <MASK>             shutil.rmtree(tmpdir, ignore_errors=True)",if tmpdir :,if tmpdir :,100.00000000000004,0.0,1.0
"def __init__(self, api_version_str):     try:         self.latest = self.preview = False         self.yyyy = self.mm = self.dd = None         if api_version_str == ""latest"":             self.latest = True         else:             <MASK>                 self.preview = True             parts = api_version_str.split(""-"")             self.yyyy = int(parts[0])             self.mm = int(parts[1])             self.dd = int(parts[2])     except (ValueError, TypeError):         raise ValueError(             ""The API version {} is not in a "" ""supported format"".format(api_version_str)         )","if ""preview"" in api_version_str :","if api_version_str == ""preview"" :",44.833867003844574,44.833867003844574,0.0
"def _merge(self, items, map_id, dep_id, use_disk, meminfo, mem_limit):     combined = self.combined     merge_combiner = self.aggregator.mergeCombiners     for k, v in items:         o = combined.get(k)         combined[k] = merge_combiner(o, v) if o is not None else v         <MASK>             mem_limit = self._rotate()",if use_disk and meminfo . rss > mem_limit :,if mem_limit is not None :,12.502047063713437,12.502047063713437,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             self.set_value(d.getVarInt32())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 8 :,if tt == 1 :,53.7284965911771,53.7284965911771,0.0
"def nice(deltat):     # singular,plural     times = _(         ""second,seconds:minute,minutes:hour,hours:day,days:week,weeks:month,months:year,years""     ).split("":"")     d = abs(int(deltat))     for div, time in zip((60, 60, 24, 7, 4, 12, 100), times):         <MASK>             return ""%s%i %s"" % (deltat < 0 and ""-"" or """", d, time.split("","")[d != 1])         d /= div",if d < div * 5 :,if div == 1 :,9.042266054940777,9.042266054940777,0.0
"def after_get_object(self, event, view_kwargs):     if event and event.state == ""draft"":         <MASK>             raise ObjectNotFound({""parameter"": ""{id}""}, ""Event: not found"")","if not is_logged_in ( ) or not has_access ( ""is_coorganizer"" , event_id = event . id ) :",if not event . id :,0.7282945743438883,0.7282945743438883,0.0
def daemonize_if_required(self):     if self.options.daemon:         <MASK>             # Stop the logging queue listener for the current process             # We'll restart it once forked             log.shutdown_multiprocessing_logging_listener(daemonizing=True)         # Late import so logging works correctly         salt.utils.process.daemonize()     # Setup the multiprocessing log queue listener if enabled     self._setup_mp_logging_listener(),if self . _setup_mp_logging_listener_ is True :,if self . options . daemonizing :,8.377387908310832,8.377387908310832,0.0
"def iter_modules(self, by_clients=False, clients_filter=None):     """"""iterate over all modules""""""     clients = None     if by_clients:         clients = self.get_clients(clients_filter)         if not clients:             return     self._refresh_modules()     for module_name in self.modules:         try:             module = self.get_module(module_name)         except PupyModuleDisabled:             continue         if clients is not None:             for client in clients:                 <MASK>                     yield module                     break         else:             yield module",if module . is_compatible_with ( client ) :,if client . id == module_name :,5.376829790324733,5.376829790324733,0.0
"def _incremental_avg_dp(self, avg, new_el, idx):     for attr in [""coarse_segm"", ""fine_segm"", ""u"", ""v""]:         setattr(             avg, attr, (getattr(avg, attr) * idx + getattr(new_el, attr)) / (idx + 1)         )         <MASK>             # Deletion of the > 0 index intermediary values to prevent GPU OOM             setattr(new_el, attr, None)     return avg",if idx :,if idx > 0 :,23.643540225079384,23.643540225079384,0.0
"def run(self, paths=[]):     collapsed = False     for item in SideBarSelection(paths).getSelectedDirectories():         for view in item.views():             <MASK>                 Window().focus_view(view)                 self.collapse_sidebar_folder()                 collapsed = True             view.close()",if not collapsed :,if collapsed :,45.13864405503391,0.0,0.0
"def test_reductions(expr, rdd):     result = compute(expr, rdd)     expected = compute(expr, data)     if not result == expected:         print(result)         print(expected)         <MASK>             assert abs(result - expected) < 0.001         else:             assert result == expected","if isinstance ( result , float ) :",if result < expected :,7.715486568024961,7.715486568024961,0.0
"def deltask(task, d):     if task[:3] != ""do_"":         task = ""do_"" + task     bbtasks = d.getVar(""__BBTASKS"", False) or []     if task in bbtasks:         bbtasks.remove(task)         d.delVarFlag(task, ""task"")         d.setVar(""__BBTASKS"", bbtasks)     d.delVarFlag(task, ""deps"")     for bbtask in d.getVar(""__BBTASKS"", False) or []:         deps = d.getVarFlag(bbtask, ""deps"", False) or []         <MASK>             deps.remove(task)             d.setVarFlag(bbtask, ""deps"", deps)",if task in deps :,if bbtask in deps :,42.72870063962342,42.72870063962342,0.0
"def _apply_weightnorm(self, list_layers):     """"""Try apply weightnorm for all layer in list_layers.""""""     for i in range(len(list_layers)):         try:             layer_name = list_layers[i].name.lower()             <MASK>                 list_layers[i] = WeightNormalization(list_layers[i])         except Exception:             pass","if ""conv1d"" in layer_name or ""dense"" in layer_name :",if layer_name == self . layer_name :,20.89946492122517,20.89946492122517,0.0
"def __init__(self, execution_context, aggregate_operators):     super(_QueryExecutionAggregateEndpointComponent, self).__init__(execution_context)     self._local_aggregators = []     self._results = None     self._result_index = 0     for operator in aggregate_operators:         if operator == ""Average"":             self._local_aggregators.append(_AverageAggregator())         elif operator == ""Count"":             self._local_aggregators.append(_CountAggregator())         <MASK>             self._local_aggregators.append(_MaxAggregator())         elif operator == ""Min"":             self._local_aggregators.append(_MinAggregator())         elif operator == ""Sum"":             self._local_aggregators.append(_SumAggregator())","elif operator == ""Max"" :","if operator == ""Max"" :",84.08964152537145,84.08964152537145,0.0
"def _conv_layer(self, sess, bottom, name, trainable=True, padding=""SAME"", relu=True):     with tf.variable_scope(name) as scope:         filt = self._get_conv_filter(sess, name, trainable=trainable)         conv_biases = self._get_bias(sess, name, trainable=trainable)         conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=padding)         bias = tf.nn.bias_add(conv, conv_biases)         <MASK>             bias = tf.nn.relu(bias)         return bias",if relu :,if relu :,100.00000000000004,0.0,1.0
"def get_partners(self) -> Dict[AbstractNode, Set[int]]:     partners = {}  # type: Dict[AbstractNode, Set[int]]     for edge in self.edges:         if edge.is_dangling():             raise ValueError(""Cannot contract copy tensor with dangling edges"")         <MASK>             continue         partner_node, shared_axis = self._get_partner(edge)         if partner_node not in partners:             partners[partner_node] = set()         partners[partner_node].add(shared_axis)     return partners",if self . _is_my_trace ( edge ) :,if edge . is_directed ( ) :,10.282347627191612,10.282347627191612,0.0
"def close(self):     with self._lock:         """"""Close this _MultiFileWatcher object forever.""""""         <MASK>             self._folder_handlers = {}             LOGGER.debug(                 ""Stopping observer thread even though there is a non-zero ""                 ""number of event observers!""             )         else:             LOGGER.debug(""Stopping observer thread"")         self._observer.stop()         self._observer.join(timeout=5)",if len ( self . _folder_handlers ) != 0 :,if self . _folder_handlers :,34.53521209125189,34.53521209125189,0.0
"def comboSelectionChanged(self, index):     text = self.comboBox.cb.itemText(index)     for i in range(self.labelList.count()):         <MASK>             self.labelList.item(i).setCheckState(2)         elif text != self.labelList.item(i).text():             self.labelList.item(i).setCheckState(0)         else:             self.labelList.item(i).setCheckState(2)","if text == """" :",if text == self . labelList . item ( i ) :,20.448007360218387,20.448007360218387,0.0
"def _get_messages(self):     r = []     try:         self._connect()         self._login()         for message in self._fetch():             <MASK>                 r.append(message)         self._connection.expunge()         self._connection.close()         self._connection.logout()     except MailFetcherError as e:         self.log(""error"", str(e))     return r",if message :,if message . message_id == self . _message_id :,5.816635421147513,5.816635421147513,0.0
"def get_current_user(self):     try:         <MASK>             return config.get(""json_authentication_override"")         tkn_header = self.request.headers[""authorization""]     except KeyError:         raise WebAuthNError(reason=""Missing Authorization Header"")     else:         tkn_str = tkn_header.split("" "")[-1]     try:         tkn = self.jwt_validator(tkn_str)     except AuthenticationError as e:         raise WebAuthNError(reason=e.message)     else:         return tkn","if config . get ( ""development"" ) and config . get ( ""json_authentication_override"" ) :","if config . get ( ""json_authentication_override"" ) :",52.578802442578,52.578802442578,0.0
def _get_data(self):     formdata = self._formdata     if formdata:         data = []         # TODO: Optimize?         for item in formdata:             model = self.loader.get_one(item) if item else None             <MASK>                 data.append(model)             else:                 self._invalid_formdata = True         self._set_data(data)     return self._data,if model :,if model :,100.00000000000004,0.0,1.0
"def _getSubstrings(self, va, size, ltyp):     # rip through the desired memory range to populate any substrings     subs = set()     end = va + size     for offs in range(va, end, 1):         loc = self.getLocation(offs, range=True)         if loc and loc[L_LTYPE] == LOC_STRING and loc[L_VA] > va:             subs.add((loc[L_VA], loc[L_SIZE]))             <MASK>                 subs = subs.union(set(loc[L_TINFO]))     return list(subs)",if loc [ L_TINFO ] :,if loc [ L_TINFO ] == ltyp :,59.00468726392806,59.00468726392806,0.0
def monad(self):     if not self.cls_bl_idname:         return None     for monad in bpy.data.node_groups:         <MASK>             if monad.cls_bl_idname == self.cls_bl_idname:                 return monad     return None,"if hasattr ( monad , ""cls_bl_idname"" ) :",if monad . cls_bl_idname is not None :,30.181358455294284,30.181358455294284,0.0
"def _set_peer_statuses(self):     """"""Set peer statuses.""""""     cutoff = time.time() - STALE_SECS     for peer in self.peers:         if peer.bad:             peer.status = PEER_BAD         <MASK>             peer.status = PEER_GOOD         elif peer.last_good:             peer.status = PEER_STALE         else:             peer.status = PEER_NEVER",elif peer . last_good > cutoff :,if cutoff > peer . last_good :,50.197242487957936,50.197242487957936,0.0
"def title_by_index(self, trans, index, context):     d_type = self.get_datatype(trans, context)     for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()):         if i == index:             rval = composite_name             <MASK>                 rval = ""{} ({})"".format(rval, composite_file.description)             if composite_file.optional:                 rval = ""%s [optional]"" % rval             return rval     if index < self.get_file_count(trans, context):         return ""Extra primary file""     return None",if composite_file . description :,if composite_file . description :,100.00000000000004,100.00000000000004,1.0
"def testUiViewServerDump_windowIntM1(self):     device = None     try:         device = MockDevice(version=15, startviewserver=True)         vc = ViewClient(device, device.serialno, adb=TRUE, autodump=False)         vc.dump(window=-1)         vc.findViewByIdOrRaise(""id/home"")     finally:         <MASK>             device.shutdownMockViewServer()",if device :,if device . is_running ( ) :,10.552670315936318,10.552670315936318,0.0
"def _convertDict(self, d):     r = {}     for k, v in d.items():         if isinstance(v, bytes):             v = str(v, ""utf-8"")         elif isinstance(v, list) or isinstance(v, tuple):             v = self._convertList(v)         <MASK>             v = self._convertDict(v)         if isinstance(k, bytes):             k = str(k, ""utf-8"")         r[k] = v     return r","elif isinstance ( v , dict ) :","if isinstance ( v , dict ) :",84.08964152537145,84.08964152537145,0.0
"def _testSendmsgTimeout(self):     try:         self.cli_sock.settimeout(0.03)         try:             while True:                 self.sendmsgToServer([b""a"" * 512])         except socket.timeout:             pass         except OSError as exc:             <MASK>                 raise             # bpo-33937 the test randomly fails on Travis CI with             # ""OSError: [Errno 12] Cannot allocate memory""         else:             self.fail(""socket.timeout not raised"")     finally:         self.misc_event.set()",if exc . errno != errno . ENOMEM :,if exc . errno == 12 :,29.797147054518835,29.797147054518835,0.0
"def addError(self, test, err):     if err[0] is SkipTest:         if self.showAll:             self.stream.writeln(str(err[1]))         <MASK>             self.stream.write(""s"")             self.stream.flush()         return     _org_AddError(self, test, err)",elif self . dots :,if self . showAll :,23.643540225079384,23.643540225079384,0.0
"def mouse_down(self, event):     if event.button == 1:         if self.scrolling:             p = event.local             if self.scroll_up_rect().collidepoint(p):                 self.scroll_up()                 return             <MASK>                 self.scroll_down()                 return     if event.button == 4:         self.scroll_up()     if event.button == 5:         self.scroll_down()     GridView.mouse_down(self, event)",elif self . scroll_down_rect ( ) . collidepoint ( p ) :,if event . button == 3 :,2.4159653599212296,2.4159653599212296,0.0
"def find_file_copyright_notices(fname):     ret = set()     f = open(fname)     lines = f.readlines()     for l in lines[:80]:  # hmmm, assume copyright to be in first 80 lines         idx = l.lower().find(""copyright"")         if idx < 0:             continue         copyright = l[idx + 9 :].strip()         <MASK>             continue         copyright = sanitise(copyright)         # hmm, do a quick check to see if there's a year,         # if not, skip it         if not copyright.find(""200"") >= 0 and not copyright.find(""199"") >= 0:             continue         ret.add(copyright)     return ret",if not copyright :,if not copyright :,100.00000000000004,100.00000000000004,1.0
"def get_selectable_values(self, request):     shop = lfs.core.utils.get_default_shop(request)     countries = []     for country in shop.shipping_countries.all():         <MASK>             selected = True         else:             selected = False         countries.append(             {                 ""id"": country.id,                 ""name"": country.name,                 ""selected"": selected,             }         )     return countries",if country in self . value . all ( ) :,if country . selected :,7.652332131360532,7.652332131360532,0.0
"def _addItemToLayout(self, sample, label):     col = self.layout.columnCount()     row = self.layout.rowCount()     if row:         row -= 1     nCol = self.columnCount * 2     # FIRST ROW FULL     if col == nCol:         for col in range(0, nCol, 2):             # FIND RIGHT COLUMN             if not self.layout.itemAt(row, col):                 break         <MASK>             # MAKE NEW ROW             col = 0             row += 1     self.layout.addItem(sample, row, col)     self.layout.addItem(label, row, col + 1)",if col + 2 == nCol :,if col == nCol :,43.29820146406896,43.29820146406896,0.0
def contains_only_whitespace(node):     if is_tag(node):         if not any([not is_text(s) for s in node.contents]):             <MASK>                 return True     return False,if not any ( [ unicode ( s ) . strip ( ) for s in node . contents ] ) :,if not node . tag :,1.6866821919248658,1.6866821919248658,0.0
"def tokenize_generator(cw):     ret = []     done = {}     for op in ops:         ch = op.symbol[0]         <MASK>             continue         sops = start_symbols[ch]         cw.write(""case '%s':"" % ch)         for t in gen_tests(sops, 1):             cw.write(t)         done[ch] = True     return ret",if ch in done :,if ch in done :,100.00000000000004,100.00000000000004,1.0
"def _convertNbCharsInNbBits(self, nbChars):     nbMinBit = None     nbMaxBit = None     if nbChars is not None:         <MASK>             nbMinBit = nbChars * 8             nbMaxBit = nbMinBit         else:             if nbChars[0] is not None:                 nbMinBit = nbChars[0] * 8             if nbChars[1] is not None:                 nbMaxBit = nbChars[1] * 8     return (nbMinBit, nbMaxBit)","if isinstance ( nbChars , int ) :",if nbChars [ 0 ] is not None :,6.27465531099474,6.27465531099474,0.0
"def init(self, *args, **kwargs):     if ""_state"" not in kwargs:         state = {}         # Older versions have the _state entries as individual kwargs         for arg in (""children"", ""windowState"", ""detachedPanels""):             if arg in kwargs:                 state[arg] = kwargs[arg]                 del kwargs[arg]         <MASK>             kwargs[""_state""] = state     originalInit(self, *args, **kwargs)",if state :,"if ""_state"" in kwargs :",7.267884212102741,7.267884212102741,0.0
"def spm_decode(tokens: List[str]) -> List[str]:     words = []     pieces: List[str] = []     for t in tokens:         <MASK>             if len(pieces) > 0:                 words.append("""".join(pieces))             pieces = [t[1:]]         else:             pieces.append(t)     if len(pieces) > 0:         words.append("""".join(pieces))     return words",if t [ 0 ] == DecodeMixin . spm_bos_token :,"if t [ 0 ] == ""word"" :",41.016750098594244,41.016750098594244,0.0
"def _compare_dirs(self, dir1: str, dir2: str) -> List[str]:     # check that dir1 and dir2 are equivalent,     # return the diff     diff = []  # type: List[str]     for root, dirs, files in os.walk(dir1):         for file_ in files:             path = os.path.join(root, file_)             target_path = os.path.join(dir2, os.path.split(path)[-1])             <MASK>                 diff.append(file_)     return diff",if not os . path . exists ( target_path ) :,if target_path != dir2 :,12.502047063713437,12.502047063713437,0.0
"def credentials(self):     """"""The session credentials as a dict""""""     creds = {}     if self._creds:         <MASK>  # pragma: no branch             creds[""aws_access_key_id""] = self._creds.access_key         if self._creds.secret_key:  # pragma: no branch             creds[""aws_secret_access_key""] = self._creds.secret_key         if self._creds.token:             creds[""aws_session_token""] = self._creds.token     if self._session.region_name:         creds[""aws_region""] = self._session.region_name     if self.requester_pays:         creds[""aws_request_payer""] = ""requester""     return creds",if self . _creds . access_key :,if self . _creds . access_key :,100.00000000000004,100.00000000000004,1.0
"def got_arbiter_module_type_defined(self, mod_type):     for a in self.arbiters:         # Do like the linkify will do after....         for m in getattr(a, ""modules"", []):             # So look at what the arbiter try to call as module             m = m.strip()             # Ok, now look in modules...             for mod in self.modules:                 # try to see if this module is the good type                 if getattr(mod, ""module_type"", """").strip() == mod_type.strip():                     # if so, the good name?                     <MASK>                         return True     return False","if getattr ( mod , ""module_name"" , """" ) . strip ( ) == m :","if m == ""arbiter"" :",2.6251815872142057,2.6251815872142057,0.0
"def find_file_at_path_with_indexes(self, path, url):     if url.endswith(""/""):         path = os.path.join(path, self.index_file)         return self.get_static_file(path, url)     elif url.endswith(""/"" + self.index_file):         if os.path.isfile(path):             return self.redirect(url, url[: -len(self.index_file)])     else:         try:             return self.get_static_file(path, url)         except IsDirectoryError:             <MASK>                 return self.redirect(url, url + ""/"")     raise MissingFileError(path)","if os . path . isfile ( os . path . join ( path , self . index_file ) ) :",if os . path . isdir ( path ) :,13.722896630276955,13.722896630276955,0.0
def _use_full_params(self) -> None:     for p in self.params:         if not p._is_sharded:             <MASK>                 assert p._fp16_shard.storage().size() != 0                 p.data = p._fp16_shard         else:             assert p._full_param_padded.storage().size() != 0             p.data = p._full_param_padded[: p._orig_size.numel()].view(p._orig_size),if self . mixed_precision :,if p . _fp16_shard :,7.809849842300637,7.809849842300637,0.0
"def _attrdata(self, cont, name, *val):     if not name:         return None, False     if isinstance(name, Mapping):         if val:             raise TypeError(""Cannot set a value to %s"" % name)         return name, True     else:         if val:             <MASK>                 return {name: val[0]}, True             else:                 raise TypeError(""Too may arguments"")         else:             cont = self._extra.get(cont)             return cont.get(name) if cont else None, False",if len ( val ) == 1 :,if len ( val ) == 1 :,100.00000000000004,100.00000000000004,1.0
"def evaluate(env, net, device=""cpu""):     obs = env.reset()     reward = 0.0     steps = 0     while True:         obs_v = ptan.agent.default_states_preprocessor([obs]).to(device)         action_v = net(obs_v)         action = action_v.data.cpu().numpy()[0]         obs, r, done, _ = env.step(action)         reward += r         steps += 1         <MASK>             break     return reward, steps",if done :,if done :,100.00000000000004,0.0,1.0
"def convert_html_js_files(app: Sphinx, config: Config) -> None:     """"""This converts string styled html_js_files to tuple styled one.""""""     html_js_files = []  # type: List[Tuple[str, Dict]]     for entry in config.html_js_files:         <MASK>             html_js_files.append((entry, {}))         else:             try:                 filename, attrs = entry                 html_js_files.append((filename, attrs))             except Exception:                 logger.warning(__(""invalid js_file: %r, ignored""), entry)                 continue     config.html_js_files = html_js_files  # type: ignore","if isinstance ( entry , str ) :","if entry . startswith ( ""html"" ) :",10.552670315936318,10.552670315936318,0.0
"def _check_duplications(self, regs):     """"""n^2 loop which verifies that each reg exists only once.""""""     for reg in regs:         count = 0         for r in regs:             <MASK>                 count += 1         if count > 1:             genutil.die(""reg %s defined more than once"" % reg)",if reg == r :,if r == reg :,21.3643503198117,21.3643503198117,0.0
"def PyJsHoisted_vault_(key, forget, this, arguments, var=var):     var = Scope(         {u""this"": this, u""forget"": forget, u""key"": key, u""arguments"": arguments}, var     )     var.registers([u""forget"", u""key""])     if PyJsStrictEq(var.get(u""key""), var.get(u""passkey"")):         return (             var.put(u""secret"", var.get(u""null""))             <MASK>             else (                 var.get(u""secret"")                 or var.put(u""secret"", var.get(u""secretCreatorFn"")(var.get(u""object"")))             )         )","if var . get ( u""forget"" )","if var . get ( u""passkey"" ) :",63.15552371794039,63.15552371794039,0.0
"def sort_nested_dictionary_lists(d):     for k, v in d.items():         if isinstance(v, list):             for i in range(0, len(v)):                 if isinstance(v[i], dict):                     v[i] = await sort_nested_dictionary_lists(v[i])                 d[k] = sorted(v)         <MASK>             d[k] = await sort_nested_dictionary_lists(v)     return d","if isinstance ( v , dict ) :","if isinstance ( v , list ) :",59.4603557501361,59.4603557501361,0.0
"def transceiver(self, data):     out = []     for t in range(8):         if data[t] == 0:             continue         value = data[t]         for b in range(8):             if value & 0x80:                 <MASK>                     out.append(""(unknown)"")                 else:                     out.append(TRANSCEIVER[t][b])             value <<= 1     self.annotate(""Transceiver compliance"", "", "".join(out))",if len ( TRANSCEIVER [ t ] ) < b + 1 :,if value & 0x80 :,2.564755813286796,2.564755813286796,0.0
"def process_string(self, remove_repetitions, sequence):     string = """"     for i, char in enumerate(sequence):         if char != self.int_to_char[self.blank_index]:             # if this char is a repetition and remove_repetitions=true,             # skip.             <MASK>                 pass             elif char == self.labels[self.space_index]:                 string += "" ""             else:                 string = string + char     return string",if remove_repetitions and i != 0 and char == sequence [ i - 1 ] :,if i == self . space_index :,4.063153221069824,4.063153221069824,0.0
"def clean(self):     username = self.cleaned_data.get(""username"")     password = self.cleaned_data.get(""password"")     if username and password:         self.user_cache = authenticate(username=username, password=password)         <MASK>             raise forms.ValidationError(self.error_messages[""invalid_login""])         elif not self.user_cache.is_active:             raise forms.ValidationError(self.error_messages[""inactive""])     self.check_for_test_cookie()     return self.cleaned_data",if self . user_cache is None :,if not self . user_cache . is_valid :,35.65506208559251,35.65506208559251,0.0
"def is_listening_for_message(conversation_id: Text, endpoint: EndpointConfig) -> bool:     """"""Check if the conversation is in need for a user message.""""""     tracker = await retrieve_tracker(endpoint, conversation_id, EventVerbosity.APPLIED)     for i, e in enumerate(reversed(tracker.get(""events"", []))):         if e.get(""event"") == UserUttered.type_name:             return False         <MASK>             return e.get(""name"") == ACTION_LISTEN_NAME     return False","elif e . get ( ""event"" ) == ActionExecuted . type_name :",if i == 0 :,2.8722725093023906,2.8722725093023906,0.0
"def getReferences(view, name=""""):     """"""Find all reference definitions.""""""     # returns {name -> Region}     refs = []     name = re.escape(name)     if name == """":         refs.extend(view.find_all(r""(?<=^\[)([^\]]+)(?=\]:)"", 0))     else:         refs.extend(view.find_all(r""(?<=^\[)(%s)(?=\]:)"" % name, 0))     regions = refs     ids = {}     for reg in regions:         name = view.substr(reg).strip()         key = name.lower()         <MASK>             ids[key].regions.append(reg)         else:             ids[key] = Obj(regions=[reg], label=name)     return ids",if key in ids :,if name in ids :,42.72870063962342,42.72870063962342,0.0
"def _get_header(self, requester, header_name):     hits = sum([header_name in headers for _, headers in requester.requests])     self.assertEquals(hits, 2 if self.revs_enabled else 1)     for url, headers in requester.requests:         <MASK>             if self.revs_enabled:                 self.assertTrue(url.endswith(""/latest""), msg=url)             else:                 self.assertTrue(url.endswith(""/download_urls""), msg=url)             return headers.get(header_name)",if header_name in headers :,"if url . startswith ( ""https://"" ) :",3.673526562988939,3.673526562988939,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             self.set_shuffle_name(d.getPrefixedString())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 10 :,if tt == 0 :,53.7284965911771,53.7284965911771,0.0
"def make_release_tree(self, base_dir, files):     """"""Make the release tree.""""""     self.mkpath(base_dir)     create_tree(base_dir, files, dry_run=self.dry_run)     if not files:         self.log.warning(""no files to distribute -- empty manifest?"")     else:         self.log.info(""copying files to %s..."", base_dir)     for filename in files:         <MASK>             self.log.warning(""'%s' not a regular file -- skipping"", filename)         else:             dest = os.path.join(base_dir, filename)             self.copy_file(filename, dest)     self.distribution.metadata.write_pkg_info(base_dir)",if not os . path . isfile ( filename ) :,if not os . path . isfile ( filename ) :,100.00000000000004,100.00000000000004,1.0
"def _parse_names_set(feature_names):     """"""Helping function of `_parse_feature_names` that parses a set of feature names.""""""     feature_collection = OrderedDict()     for feature_name in feature_names:         <MASK>             feature_collection[feature_name] = ...         else:             raise ValueError(""Failed to parse {}, expected string"".format(feature_name))     return feature_collection","if isinstance ( feature_name , str ) :","if isinstance ( feature_name , str ) :",100.00000000000004,100.00000000000004,1.0
"def get_connection(self, url, proxies=None):     with self.pools.lock:         pool = self.pools.get(url)         <MASK>             return pool         pool = NpipeHTTPConnectionPool(             self.npipe_path, self.timeout, maxsize=self.max_pool_size         )         self.pools[url] = pool     return pool",if pool :,if pool :,100.00000000000004,0.0,1.0
"def _parse_dimensions(dimensions):     arrays = []     names = []     for key in dimensions:         values = [v[""name""] for v in key[""values""]]         role = key.get(""role"", None)         <MASK>             values = [_fix_quarter_values(v) for v in values]             values = pd.DatetimeIndex(values)         arrays.append(values)         names.append(key[""name""])     midx = pd.MultiIndex.from_product(arrays, names=names)     if len(arrays) == 1 and isinstance(midx, pd.MultiIndex):         # Fix for pandas >= 0.21         midx = midx.levels[0]     return midx","if role in ( ""time"" , ""TIME_PERIOD"" ) :","if role == ""quarter"" :",5.789419402078114,5.789419402078114,0.0
"def _add_trials(self, name, spec):     """"""Add trial by invoking TrialRunner.""""""     resource = {}     resource[""trials""] = []     trial_generator = BasicVariantGenerator()     trial_generator.add_configurations({name: spec})     while not trial_generator.is_finished():         trial = trial_generator.next_trial()         <MASK>             break         runner.add_trial(trial)         resource[""trials""].append(self._trial_info(trial))     return resource",if not trial :,if not trial :,100.00000000000004,100.00000000000004,1.0
"def _retrieve_key(self):     url = ""http://www.canadapost.ca/cpo/mc/personal/postalcode/fpc.jsf""     text = """"     try:         r = requests.get(url, timeout=self.timeout, proxies=self.proxies)         text = r.text     except:         self.error = ""ERROR - URL Connection""     if text:         expression = r""'(....-....-....-....)';""         pattern = re.compile(expression)         match = pattern.search(text)         <MASK>             self.key = match.group(1)             return self.key         else:             self.error = ""ERROR - No API Key""",if match :,if match :,100.00000000000004,0.0,1.0
"def test_net(net, env, count=10, device=""cpu""):     rewards = 0.0     steps = 0     for _ in range(count):         obs = env.reset()         while True:             obs_v = ptan.agent.float32_preprocessor([obs]).to(device)             mu_v = net(obs_v)[0]             action = mu_v.squeeze(dim=0).data.cpu().numpy()             action = np.clip(action, -1, 1)             obs, reward, done, _ = env.step(action)             rewards += reward             steps += 1             <MASK>                 break     return rewards / count, steps / count",if done :,if done :,100.00000000000004,0.0,1.0
"def compile(self, filename, obfuscate=False, raw=False, magic=""\x00"" * 8):     body = marshal.dumps(compile(self.visit(self._source_ast), filename, ""exec""))     if obfuscate:         body_len = len(body)         offset = 0 if raw else 8         output = bytearray(body_len + 8)         for i, x in enumerate(body):             output[i + offset] = ord(x) ^ ((2 ** ((65535 - i) % 65535)) % 251)         <MASK>             for i in xrange(8):                 output[i] = 0         return output     elif raw:         return body     else:         return magic + body",if raw :,"if magic == ""0x00"" :",6.567274736060395,6.567274736060395,0.0
"def _map_saslprep(s):     """"""Map stringprep table B.1 to nothing and C.1.2 to ASCII space""""""     r = []     for c in s:         if stringprep.in_table_c12(c):             r.append("" "")         <MASK>             r.append(c)     return """".join(r)",elif not stringprep . in_table_b1 ( c ) :,if stringprep . in_table_b12 ( c ) :,58.33510584342546,58.33510584342546,0.0
"def ensemble(self, pairs, other_preds):     """"""Ensemble the dict with statistical model predictions.""""""     lemmas = []     assert len(pairs) == len(other_preds)     for p, pred in zip(pairs, other_preds):         w, pos = p         if (w, pos) in self.composite_dict:             lemma = self.composite_dict[(w, pos)]         <MASK>             lemma = self.word_dict[w]         else:             lemma = pred         if lemma is None:             lemma = w         lemmas.append(lemma)     return lemmas",elif w in self . word_dict :,if lemma is None :,4.79981069911921,4.79981069911921,0.0
"def quiet_f(*args):     vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)}     value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation)     if expect_list:         <MASK>             value = [extract_pyreal(item) for item in value.leaves]             if any(item is None for item in value):                 return None             return value         else:             return None     else:         value = extract_pyreal(value)         if value is None or isinf(value) or isnan(value):             return None         return value","if value . has_form ( ""List"" , None ) :",if value . leaves :,7.063006710882745,7.063006710882745,0.0
"def _copy_package_apps(     local_bin_dir: Path, app_paths: List[Path], suffix: str = """" ) -> None:     for src_unresolved in app_paths:         src = src_unresolved.resolve()         app = src.name         dest = Path(local_bin_dir / add_suffix(app, suffix))         if not dest.parent.is_dir():             mkdir(dest.parent)         <MASK>             logger.warning(f""{hazard}  Overwriting file {str(dest)} with {str(src)}"")             dest.unlink()         if src.exists():             shutil.copy(src, dest)",if dest . exists ( ) :,if src . exists ( ) :,64.34588841607616,64.34588841607616,0.0
"def assert_readback(vehicle, values):     i = 10     while i > 0:         time.sleep(0.1)         i -= 0.1         for k, v in values.items():             <MASK>                 continue         break     if i <= 0:         raise Exception(""Did not match in channels readback %s"" % values)",if vehicle . channels [ k ] != v :,if v != vehicle :,9.284908374907788,9.284908374907788,0.0
"def _get_linode_client(self):     api_key = self.credentials.conf(""key"")     api_version = self.credentials.conf(""version"")     if api_version == """":         api_version = None     if not api_version:         api_version = 3         # Match for v4 api key         regex_v4 = re.compile(""^[0-9a-f]{64}$"")         regex_match = regex_v4.match(api_key)         <MASK>             api_version = 4     else:         api_version = int(api_version)     return _LinodeLexiconClient(api_key, api_version)",if regex_match :,if not regex_match :,53.7284965911771,53.7284965911771,0.0
"def mergeHiLo(self, x_stats):     """"""Merge the highs and lows of another accumulator into myself.""""""     if x_stats.firsttime is not None:         <MASK>             self.firsttime = x_stats.firsttime             self.first = x_stats.first     if x_stats.lasttime is not None:         if self.lasttime is None or x_stats.lasttime >= self.lasttime:             self.lasttime = x_stats.lasttime             self.last = x_stats.last",if self . firsttime is None or x_stats . firsttime < self . firsttime :,if self . firsttime >= self . firsttime :,26.089696959105602,26.089696959105602,0.0
"def _check_good_input(self, X, y=None):     if isinstance(X, dict):         lengths = [len(X1) for X1 in X.values()]         <MASK>             raise ValueError(""Not all values of X are of equal length."")         x_len = lengths[0]     else:         x_len = len(X)     if y is not None:         if len(y) != x_len:             raise ValueError(""X and y are not of equal length."")     if self.regression and y is not None and y.ndim == 1:         y = y.reshape(-1, 1)     return X, y",if len ( set ( lengths ) ) > 1 :,if len ( lengths ) != 1 :,26.264048972269727,26.264048972269727,0.0
"def set(self, obj, **kwargs):     """"""Check for missing event functions and substitute these with""""""     """"""the ignore method""""""     ignore = getattr(self, ""ignore"")     for k, v in kwargs.iteritems():         setattr(self, k, getattr(obj, v))         if k in self.combinations:             for k1 in self.combinations[k]:                 <MASK>                     setattr(self, k1, ignore)","if not hasattr ( self , k1 ) :",if k1 not in ignore :,6.962210312500384,6.962210312500384,0.0
"def _parse_list(self, tokens):     # Process left to right, allow descending in sub lists     assert tokens[0] in (""["", ""("")     delim = ""]"" if tokens.pop(0) == ""["" else "")""     expr = ExpressionList()     while tokens and tokens[0] != delim:         item = self._parse(tokens)         <MASK>             if tokens.pop(0) != "","":                 raise ExpressionSyntaxError('Expected: "",""')         expr.append(item)     if not tokens or tokens[0] != delim:         raise ExpressionSyntaxError('Missing: ""%s""' % delim)     else:         tokens.pop(0)     return expr",if tokens and tokens [ 0 ] != delim :,if item :,2.4088567143060917,0.0,0.0
"def param_value(self):     # This is part of the ""handle quoted extended parameters"" hack.     for token in self:         if token.token_type == ""value"":             return token.stripped_value         <MASK>             for token in token:                 if token.token_type == ""bare-quoted-string"":                     for token in token:                         if token.token_type == ""value"":                             return token.stripped_value     return """"","if token . token_type == ""quoted-string"" :","if token . token_type == ""bare-quoted-string"" :",76.91605673134588,76.91605673134588,0.0
"def paragraph_is_fully_commented(lines, comment, main_language):     """"""Is the paragraph fully commented?""""""     for i, line in enumerate(lines):         <MASK>             if line[len(comment) :].lstrip().startswith(comment):                 continue             if is_magic(line, main_language):                 return False             continue         return i > 0 and _BLANK_LINE.match(line)     return True",if line . startswith ( comment ) :,if i == 0 :,6.916271812933183,6.916271812933183,0.0
"def lots_connected_to_existing_roads(model):     set = []     for h in model.HarvestCells:         for (i, j) in model.ExistingRoads:             <MASK>                 if h not in set:                     set.append(h)     return set",if ( i in model . COriginNodeForCell [ h ] ) or ( j in model . COriginNodeForCell [ h ] ) :,if i == j :,0.6751392346890166,0.6751392346890166,0.0
"def detect(get_page):     retval = False     for vector in WAF_ATTACK_VECTORS:         page, headers, code = get_page(get=vector)         retval = (             re.search(                 r""\Abarra_counter_session="",                 headers.get(HTTP_HEADER.SET_COOKIE, """"),                 re.I,             )             is not None         )         retval |= (             re.search(                 r""(\A|\b)barracuda_"", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I             )             is not None         )         <MASK>             break     return retval",if retval :,if code == 0 :,9.652434877402245,9.652434877402245,0.0
"def test_files(self):     # get names of files to test     dist_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)     names = []     for d in self.test_directories:         test_dir = os.path.join(dist_dir, d)         for n in os.listdir(test_dir):             <MASK>                 names.append(os.path.join(test_dir, n))     for filename in names:         if test_support.verbose:             print(""Testing %s"" % filename)         source = read_pyfile(filename)         self.check_roundtrip(source)","if n . endswith ( "".py"" ) and not n . startswith ( ""bad"" ) :",if n != os . path . basename ( n ) :,5.615262138421592,5.615262138421592,0.0
"def test_calibrate_target(create_target):     mod, params = testing.synthetic.get_workload()     dataset = get_calibration_dataset(mod, ""data"")     with relay.quantize.qconfig(calibrate_mode=""kl_divergence""):         <MASK>             with tvm.target.Target(""llvm""):                 relay.quantize.quantize(mod, params, dataset)         else:             # current_target = None             relay.quantize.quantize(mod, params, dataset)",if create_target :,if create_target :,100.00000000000004,100.00000000000004,1.0
"def _cleanSubmodule(self, _=None):     rc = RC_SUCCESS     if self.submodules:         command = [             ""submodule"",             ""foreach"",             ""--recursive"",             ""git"",             ""clean"",             ""-f"",             ""-f"",             ""-d"",         ]         <MASK>             command.append(""-x"")         rc = yield self._dovccmd(command)     defer.returnValue(rc)","if self . mode == ""full"" and self . method == ""fresh"" :",if self . submodules :,2.5983349617896914,2.5983349617896914,0.0
"def screen_length_to_bytes_count(string, screen_length_limit, encoding):     bytes_count = 0     screen_length = 0     for unicode_char in string:         screen_length += screen_len(unicode_char)         char_bytes_count = len(unicode_char.encode(encoding))         bytes_count += char_bytes_count         <MASK>             bytes_count -= char_bytes_count             break     return bytes_count",if screen_length > screen_length_limit :,if screen_length_limit > bytes_count :,51.099558112917826,51.099558112917826,0.0
"def tamper(payload, **kwargs):     junk_chars = ""!#$%&()*~+-_.,:;?@[/|\]^`""     retval = """"     for i, char in enumerate(payload, start=1):         amount = random.randint(10, 15)         if char == "">"":             retval += "">""             for _ in range(amount):                 retval += random.choice(junk_chars)         <MASK>             retval += ""<""             for _ in range(amount):                 retval += random.choice(junk_chars)         elif char == "" "":             for _ in range(amount):                 retval += random.choice(junk_chars)         else:             retval += char     return retval","elif char == ""<"" :","if char == ""<"" :",84.08964152537145,84.08964152537145,0.0
"def test_parse(self):     correct = 0     for example in EXAMPLES:         try:             schema.parse(example.schema_string)             if example.valid:                 correct += 1             else:                 self.fail(""Invalid schema was parsed: "" + example.schema_string)         except:             <MASK>                 correct += 1             else:                 self.fail(""Valid schema failed to parse: "" + example.schema_string)     fail_msg = ""Parse behavior correct on %d out of %d schemas."" % (         correct,         len(EXAMPLES),     )     self.assertEqual(correct, len(EXAMPLES), fail_msg)",if not example . valid :,if example . error :,20.80119537801062,20.80119537801062,0.0
"def _on_change(self):     changed = False     self.save()     for key, value in self.data.items():         <MASK>             if value:                 changed = True                 break         if isinstance(value, int):             if value != 1:                 changed = True                 break         elif value is None:             continue         elif len(value) != 0:             changed = True             break     self._reset_button.disabled = not changed","if isinstance ( value , bool ) :","if key == ""id"" :",6.567274736060395,6.567274736060395,0.0
"def normalize(d: Dict[Any, Any]) -> Dict[str, Any]:     first_exception = None     for normalizer in normalizers:         try:             normalized = normalizer(d)         except KeyError as e:             <MASK>                 first_exception = e         else:             return normalized     assert first_exception is not None     raise first_exception",if not first_exception :,if e . args :,10.400597689005304,10.400597689005304,0.0
"def gather_callback_args(self, obj, callbacks):     session = sa.orm.object_session(obj)     for callback in callbacks:         backref = callback.backref         root_objs = getdotattr(obj, backref) if backref else obj         if root_objs:             if not isinstance(root_objs, Iterable):                 root_objs = [root_objs]             with session.no_autoflush:                 for root_obj in root_objs:                     <MASK>                         args = self.get_callback_args(root_obj, callback)                         if args:                             yield args",if root_obj :,if root_obj . id == self . id :,22.416933501922287,22.416933501922287,0.0
"def test_opdm_to_oqdm(self):     for file in filter(lambda x: x.endswith("".hdf5""), os.listdir(DATA_DIRECTORY)):         molecule = MolecularData(filename=os.path.join(DATA_DIRECTORY, file))         <MASK>             test_oqdm = map_one_pdm_to_one_hole_dm(molecule.fci_one_rdm)             true_oqdm = numpy.eye(molecule.n_qubits) - molecule.fci_one_rdm             assert numpy.allclose(test_oqdm, true_oqdm)",if molecule . fci_one_rdm is not None :,if molecule . n_qubits > 0 :,14.448814886766836,14.448814886766836,0.0
"def emitSubDomainData(self, subDomainData, event):     self.emitRawRirData(subDomainData, event)     for subDomainElem in subDomainData:         if self.checkForStop():             return None         subDomain = subDomainElem.get(""subdomain"", """").strip()         <MASK>             self.emitHostname(subDomain, event)",if subDomain :,if subDomain :,100.00000000000004,0.0,1.0
"def download_cve(     download_path: str, years: Optional[List[int]] = None, update: bool = False ):     if update:         process_url(CVE_URL.format(""modified""), download_path)     else:         all_cve_urls = get_cve_links(CVE_URL, years)         <MASK>             raise CveLookupException(""Error: No CVE links found"")         for url in all_cve_urls:             process_url(url, download_path)",if not all_cve_urls :,if not all_cve_urls :,100.00000000000004,100.00000000000004,1.0
"def is_special(s, i, directive):     """"""Return True if the body text contains the @ directive.""""""     # j = skip_line(s,i) ; trace(s[i:j],':',directive)     assert directive and directive[0] == ""@""     # 10/23/02: all directives except @others must start the line.     skip_flag = directive in (""@others"", ""@all"")     while i < len(s):         if match_word(s, i, directive):             return True, i         else:             i = skip_line(s, i)             <MASK>                 i = skip_ws(s, i)     return False, -1",if skip_flag :,if skip_flag :,100.00000000000004,100.00000000000004,1.0
"def run_async(self, nuke_cursors):     # type: (bool) -> None     interface_type = self.view.settings().get(""git_savvy.interface"")     for cls in subclasses:         if cls.interface_type == interface_type:             vid = self.view.id()             interface = interfaces.get(vid, None)             <MASK>                 interface = interfaces[vid] = cls(view=self.view)             interface.render(nuke_cursors=nuke_cursors)  # type: ignore[union-attr]             break",if not interface :,if interface is None :,14.058533129758727,14.058533129758727,0.0
"def scan_resource_conf(self, conf):     if ""properties"" in conf:         <MASK>             if str(conf[""properties""][""sslEnforcement""]).lower() == ""enabled"":                 return CheckResult.PASSED     return CheckResult.FAILED","if ""sslEnforcement"" in conf [ ""properties"" ] :","if conf [ ""properties"" ] [ ""sslEnforcement"" ] :",55.12003357447271,55.12003357447271,0.0
"def do_shorts(     opts: List[Tuple[str, str]], optstring: str, shortopts: str, args: List[str] ) -> Tuple[List[Tuple[str, str]], List[str]]:     while optstring != """":         opt, optstring = optstring[0], optstring[1:]         if short_has_arg(opt, shortopts):             if optstring == """":                 <MASK>                     raise GetoptError(""option -%s requires argument"" % opt, opt)                 optstring, args = args[0], args[1:]             optarg, optstring = optstring, """"         else:             optarg = """"         opts.append((""-"" + opt, optarg))     return opts, args",if not args :,"if optarg == """" :",7.809849842300637,7.809849842300637,0.0
"def release(self):     tid = _thread.get_ident()     with self.lock:         <MASK>             raise RuntimeError(""cannot release un-acquired lock"")         assert self.count > 0         self.count -= 1         if self.count == 0:             self.owner = None             if self.waiters:                 self.waiters -= 1                 self.wakeup.release()",if self . owner != tid :,if tid != self . owner :,29.071536848410968,29.071536848410968,0.0
"def _summarize_kraken(fn):     """"""get the value at species level""""""     kraken = {}     list_sp, list_value = [], []     with open(fn) as handle:         for line in handle:             cols = line.strip().split(""\t"")             sp = cols[5].strip()             <MASK>                 list_sp.append(sp)                 list_value.append(cols[0])     kraken = {""kraken_sp"": list_sp, ""kraken_value"": list_value}     return kraken","if len ( sp . split ( "" "" ) ) > 1 and not sp . startswith ( ""cellular"" ) :",if sp :,0.03618805978480172,0.0,0.0
"def _sync_remote_run(remote_run):     assert remote_run.remote     remote_name = remote_run.remote.name     pull_args = click_util.Args(remote=remote_name, delete=False)     try:         remote_impl_support.pull_runs([remote_run], pull_args)     except Exception as e:         <MASK>             log.exception(""pull %s from %s"", remote_run.id, remote_name)         else:             log.error(""error pulling %s from %s: %s"", remote_run.id, remote_name, e)",if log . getEffectiveLevel ( ) <= logging . DEBUG :,if e . status == 404 :,4.736913377107212,4.736913377107212,0.0
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x):     sign = None     subseq = []     for i in seq:         ki = key(i)         <MASK>             subseq.append(i)             if ki != 0:                 sign = ki / abs(ki)         else:             subseq.append(i)             if sign * ki < -slop:                 sign = ki / abs(ki)                 yield subseq                 subseq = [i]     if subseq:         yield subseq",if sign is None :,if sign == 0 :,17.965205598154213,17.965205598154213,0.0
"def import_til(self):     log(""Importing type libraries..."")     cur = self.db_cursor()     sql = ""select name from diff.program_data where type = 'til'""     cur.execute(sql)     for row in cur.fetchall():         til = row[""name""]         <MASK>             til = til.decode(""utf-8"")         try:             add_default_til(til)         except:             log(""Error loading til %s: %s"" % (row[""name""], str(sys.exc_info()[1])))     cur.close()     auto_wait()",if type ( til ) is bytes :,if til :,7.49553326588684,0.0,0.0
"def getBranches(self):     returned = []     for git_branch_line in self._executeGitCommandAssertSuccess(""branch"").stdout:         <MASK>             git_branch_line = git_branch_line[1:]         git_branch_line = git_branch_line.strip()         if BRANCH_ALIAS_MARKER in git_branch_line:             alias_name, aliased = git_branch_line.split(BRANCH_ALIAS_MARKER)             returned.append(branch.LocalBranchAlias(self, alias_name, aliased))         else:             returned.append(branch.LocalBranch(self, git_branch_line))     return returned","if git_branch_line . startswith ( ""*"" ) :",if git_branch_line :,30.93485033266056,30.93485033266056,0.0
"def add_include_dirs(self, args):     ids = []     for a in args:         # FIXME same hack, forcibly unpack from holder.         <MASK>             a = a.includedirs         if not isinstance(a, IncludeDirs):             raise InvalidArguments(                 ""Include directory to be added is not an include directory object.""             )         ids.append(a)     self.include_dirs += ids","if hasattr ( a , ""includedirs"" ) :",if a . includeirs :,5.171845311465849,5.171845311465849,0.0
"def _serialize_feature(self, feature):     name = feature.unique_name()     <MASK>         self._features_dict[feature.unique_name()] = feature.to_dictionary()         for dependency in feature.get_dependencies(deep=True):             name = dependency.unique_name()             if name not in self._features_dict:                 self._features_dict[name] = dependency.to_dictionary()",if name not in self . _features_dict :,if name not in self . _features_dict :,100.00000000000004,100.00000000000004,1.0
"def generate_io(chart_type, race_configs, environment):     # output JSON structures     structures = []     for race_config in race_configs:         <MASK>             title = chart_type.format_title(                 environment,                 race_config.track,                 es_license=race_config.es_license,                 suffix=""%s-io"" % race_config.label,             )             structures.append(chart_type.io(title, environment, race_config))     return structures","if ""io"" in race_config . charts :",if race_config . label :,25.694343649393552,25.694343649393552,0.0
"def format_partition(partition, partition_schema):     tokens = []     if isinstance(partition, dict):         for name in partition_schema:             <MASK>                 tok = _format_partition_kv(                     name, partition[name], partition_schema[name]                 )             else:                 # dynamic partitioning                 tok = name             tokens.append(tok)     else:         for name, value in zip(partition_schema, partition):             tok = _format_partition_kv(name, value, partition_schema[name])             tokens.append(tok)     return ""PARTITION ({})"".format("", "".join(tokens))",if name in partition :,if name in partition :,100.00000000000004,100.00000000000004,1.0
"def to_dict(self, validate=True, ignore=(), context=None):     context = context or {}     condition = getattr(self, ""condition"", Undefined)     copy = self  # don't copy unless we need to     if condition is not Undefined:         if isinstance(condition, core.SchemaBase):             pass         <MASK>             kwds = parse_shorthand(condition[""field""], context.get(""data"", None))             copy = self.copy(deep=[""condition""])             copy.condition.update(kwds)     return super(ValueChannelMixin, copy).to_dict(         validate=validate, ignore=ignore, context=context     )","elif ""field"" in condition and ""type"" not in condition :","if ""field"" in context :",15.992487143668557,15.992487143668557,0.0
"def _checkForCommand(self):     prompt = b""cftp> ""     if self._expectingCommand and self._lineBuffer == prompt:         buf = b""\n"".join(self._linesReceived)         <MASK>             buf = buf[len(prompt) :]         self.clearBuffer()         d, self._expectingCommand = self._expectingCommand, None         d.callback(buf)",if buf . startswith ( prompt ) :,if self . _lineBuffer == prompt :,6.742555929751843,6.742555929751843,0.0
"def schedule_logger(job_id=None, delete=False):     if not job_id:         return getLogger(""fate_flow_schedule"")     else:         if delete:             with LoggerFactory.lock:                 try:                     for key in LoggerFactory.schedule_logger_dict.keys():                         if job_id in key:                             del LoggerFactory.schedule_logger_dict[key]                 except:                     pass             return True         key = job_id + ""schedule""         <MASK>             return LoggerFactory.schedule_logger_dict[key]         return LoggerFactory.get_schedule_logger(job_id)",if key in LoggerFactory . schedule_logger_dict :,if key in LoggerFactory . schedule_logger_dict :,100.00000000000004,100.00000000000004,1.0
"def halfMultipartScore(nzb_name):     try:         wrong_found = 0         for nr in [1, 2, 3, 4, 5, ""i"", ""ii"", ""iii"", ""iv"", ""v"", ""a"", ""b"", ""c"", ""d"", ""e""]:             for wrong in [""cd"", ""part"", ""dis"", ""disc"", ""dvd""]:                 if ""%s%s"" % (wrong, nr) in nzb_name.lower():                     wrong_found += 1         <MASK>             return -30         return 0     except:         log.error(""Failed doing halfMultipartScore: %s"", traceback.format_exc())     return 0",if wrong_found == 1 :,if wrong_found == 0 :,70.71067811865478,70.71067811865478,0.0
"def parse_converter_args(argstr: str) -> t.Tuple[t.Tuple, t.Dict[str, t.Any]]:     argstr += "",""     args = []     kwargs = {}     for item in _converter_args_re.finditer(argstr):         value = item.group(""stringval"")         if value is None:             value = item.group(""value"")         value = _pythonize(value)         <MASK>             args.append(value)         else:             name = item.group(""name"")             kwargs[name] = value     return tuple(args), kwargs","if not item . group ( ""name"" ) :","if item . group ( ""args"" ) :",48.959148327580515,48.959148327580515,0.0
"def leaves(self, unique=True):     """"""Get the leaves of the tree starting at this root.""""""     if not self.children:         return [self]     else:         res = list()         for child in self.children:             for sub_child in child.leaves(unique=unique):                 <MASK>                     res.append(sub_child)         return res",if not unique or sub_child not in res :,if sub_child . parent == self :,16.058516370438436,16.058516370438436,0.0
"def to_tree(self, tagname=None, idx=None, namespace=None):     axIds = set((ax.axId for ax in self._axes))     for chart in self._charts:         for id, axis in chart._axes.items():             <MASK>                 setattr(self, axis.tagname, axis)                 axIds.add(id)     return super(PlotArea, self).to_tree(tagname)",if id not in axIds :,if id not in axIds :,100.00000000000004,100.00000000000004,1.0
"def update_neighbor(neigh_ip_address, changes):     rets = []     for k, v in changes.items():         if k == neighbors.MULTI_EXIT_DISC:             rets.append(_update_med(neigh_ip_address, v))         if k == neighbors.ENABLED:             rets.append(update_neighbor_enabled(neigh_ip_address, v))         <MASK>             rets.append(_update_connect_mode(neigh_ip_address, v))     return all(rets)",if k == neighbors . CONNECT_MODE :,if k == neighbors . CONNECT_MODE :,100.00000000000004,100.00000000000004,1.0
"def close_all_connections():     global _managers, _lock, _in_use, _timer     _lock.acquire()     try:         <MASK>             _timer.cancel()             _timer = None         for domain, managers in _managers.items():             for manager in managers:                 manager.close()         _managers = {}     finally:         _lock.release()",if _timer :,if _timer :,100.00000000000004,100.00000000000004,1.0
"def _instrument_model(self, model):     for key, value in list(         model.__dict__.items()     ):  # avoid ""dictionary keys changed during iteration""         if isinstance(value, tf.keras.layers.Layer):             new_layer = self._instrument(value)             if new_layer is not value:                 setattr(model, key, new_layer)         elif isinstance(value, list):             for i, item in enumerate(value):                 <MASK>                     value[i] = self._instrument(item)     return model","if isinstance ( item , tf . keras . layers . Layer ) :","if isinstance ( item , tf . keras . layers . Layer ) :",100.00000000000004,100.00000000000004,1.0
"def target_glob(tgt, hosts):     ret = {}     for host in hosts:         if fnmatch.fnmatch(tgt, host):             ret[host] = copy.deepcopy(__opts__.get(""roster_defaults"", {}))             ret[host].update({""host"": host})             <MASK>                 ret[host].update({""user"": __opts__[""ssh_user""]})     return ret","if __opts__ . get ( ""ssh_user"" ) :","if __opts__ [ ""ssh_user"" ] :",54.59709700160347,54.59709700160347,0.0
"def write(self, data):     if mock_target._mirror_on_stderr:         if self._write_line:             sys.stderr.write(fn + "": "")         <MASK>             sys.stderr.write(data.decode(""utf8""))         else:             sys.stderr.write(data)         if (data[-1]) == ""\n"":             self._write_line = True         else:             self._write_line = False     super(Buffer, self).write(data)",if bytes :,if data . decode :,12.703318703865365,12.703318703865365,0.0
"def task_thread():     while not task_queue.empty():         host, port, username, password = task_queue.get()         logger.info(             ""try burst {}:{} use username:{} password:{}"".format(                 host, port, username, password             )         )         <MASK>             with task_queue.mutex:                 task_queue.queue.clear()             result_queue.put((username, password))","if telnet_login ( host , port , username , password ) :",if host and port :,2.497149970415641,2.497149970415641,0.0
"def _format_results(name, ppl, scores, metrics):     """"""Format results.""""""     result_str = """"     if ppl:         result_str = ""%s ppl %.2f"" % (name, ppl)     if scores:         for metric in metrics:             <MASK>                 result_str += "", %s %s %.1f"" % (name, metric, scores[metric])             else:                 result_str = ""%s %s %.1f"" % (name, metric, scores[metric])     return result_str",if result_str :,if metric in scores :,12.703318703865365,12.703318703865365,0.0
"def info_query(self, query):     """"""Send a query which only returns 1 row""""""     self._cmysql.query(query)     first_row = ()     if self._cmysql.have_result_set:         first_row = self._cmysql.fetch_row()         <MASK>             self._cmysql.free_result()             raise errors.InterfaceError(""Query should not return more than 1 row"")     self._cmysql.free_result()     return first_row",if self . _cmysql . fetch_row ( ) :,if first_row > 1 :,7.64649370538093,7.64649370538093,0.0
"def reset_class(self):     for f in self.fields_order:         <MASK>             f.value = int(f.strbits, 2)         elif ""default_val"" in f.kargs:             f.value = int(f.kargs[""default_val""], 2)         else:             f.value = None         if f.fname:             setattr(self, f.fname, f)",if f . strbits and isbin ( f . strbits ) :,"if ""strbits"" in f . kargs :",8.591316733350183,8.591316733350183,0.0
"def _walk_map_list(self, access_func):     seen = []     cur = self     while cur:         <MASK>             break         yield cur         seen.append(cur.obj_offset)         # check for signs of infinite looping         if len(seen) > 1024:             break         cur = access_func(cur)",if cur . obj_offset in seen :,if cur . obj_offset not in seen :,65.80370064762461,65.80370064762461,0.0
def bgdel():     q = bgdelq     while True:         name = q.get()         while os.path.exists(name):             try:                 <MASK>                     os.remove(name)                 else:                     shutil.rmtree(name)             except:                 pass             if os.path.exists(name):                 time.sleep(0.1),if os . path . isfile ( name ) :,if os . path . isdir ( name ) :,65.80370064762461,65.80370064762461,0.0
"def _find_all_variables(transfer_variable):     d = {}     for _k, _v in transfer_variable.__dict__.items():         if isinstance(_v, Variable):             d[_v._name] = _v         <MASK>             d.update(_find_all_variables(_v))     return d","elif isinstance ( _v , BaseTransferVariables ) :",if _v . _name in d :,10.552670315936318,10.552670315936318,0.0
"def set_val():     idx = 0     for idx in range(0, len(model)):         row = model[idx]         <MASK>             break         if idx == len(os_widget.get_model()) - 1:             idx = -1     os_widget.set_active(idx)     if idx == -1:         os_widget.set_active(0)     if idx >= 0:         return row[1]     if self.show_all_os:         return None",if value and row [ 0 ] == value :,if row is None :,4.234348806659263,4.234348806659263,0.0
"def _make_cache_key(group, window, rate, value, methods):     count, period = _split_rate(rate)     safe_rate = ""%d/%ds"" % (count, period)     parts = [group, safe_rate, value, str(window)]     if methods is not None:         if methods == ALL:             methods = """"         <MASK>             methods = """".join(sorted([m.upper() for m in methods]))         parts.append(methods)     prefix = getattr(settings, ""RATELIMIT_CACHE_PREFIX"", ""rl:"")     return prefix + hashlib.md5(u"""".join(parts).encode(""utf-8"")).hexdigest()","elif isinstance ( methods , ( list , tuple ) ) :","if methods in [ ""all"" , ""all"" , ""all"" ] :",3.2342452920962157,3.2342452920962157,0.0
"def findfiles(path):     files = []     for name in os.listdir(path):         # ignore hidden files/dirs and other unwanted files         <MASK>             continue         pathname = os.path.join(path, name)         st = os.lstat(pathname)         mode = st.st_mode         if stat.S_ISDIR(mode):             files.extend(findfiles(pathname))         elif stat.S_ISREG(mode):             files.append((pathname, name, st))     return files","if name . startswith ( ""."" ) or name == ""lastsnap.jpg"" :","if name in [ ""hidden"" , ""file"" , ""file"" ] :",5.56175040497305,5.56175040497305,0.0
"def __getitem__(self, key):     if isinstance(key, str_types):         keys = self.get_keys()         <MASK>             raise KeyError(' ""{0}"" is an invalid key'.format(key))         else:             return self[keys.index(key)]     else:         return list.__getitem__(self, key)",if key not in keys :,if keys . index ( key ) < 0 :,5.934202609760488,5.934202609760488,0.0
"def test_assert_set_equal(estimate: tp.Iterable[int], message: str) -> None:     reference = {1, 2, 3}     try:         testing.assert_set_equal(estimate, reference)     except AssertionError as error:         if not message:             raise AssertionError(                 ""An error has been raised while it should not.""             ) from error         np.testing.assert_equal(error.args[0].split(""\n"")[1:], message)     else:         <MASK>             raise AssertionError(""An error should have been raised."")",if message :,if not error . args [ 0 ] :,5.669791110976001,5.669791110976001,0.0
"def get_directory_info(prefix, pth, recursive):     res = []     directory = os.listdir(pth)     directory.sort()     for p in directory:         <MASK>             subp = os.path.join(pth, p)             p = os.path.join(prefix, p)             if recursive and os.path.isdir(subp):                 res.append([p, get_directory_info(prefix, subp, 1)])             else:                 res.append([p, None])     return res","if p [ 0 ] != ""."" :",if os . path . isdir ( p ) :,5.369488567517933,5.369488567517933,0.0
"def check(self, runner, script, info):     if isinstance(info, ast.FunctionDef):         for arg in info.args.args:             <MASK>                 if arg.id in script.modelVars:                     self.problem(                         ""Function {0} may shadow model variable {1}"".format(                             info.name, arg.id                         ),                         lineno=info.lineno,                     )","if isinstance ( arg , ast . Name ) :",if arg . id in script . modelVars :,6.033504141761816,6.033504141761816,0.0
"def db_lookup(field, key, publish_year=None):     sql = ""select sum(ebook_count) as num from subjects where field=$field and key=$key""     if publish_year:         <MASK>             sql += "" and publish_year between $y1 and $y2""             (y1, y2) = publish_year         else:             sql += "" and publish_year=$publish_year""     return list(ebook_count_db.query(sql, vars=locals()))[0].num","if isinstance ( publish_year , ( tuple , list ) ) :",if publish_year :,7.468220329575271,7.468220329575271,0.0
"def put(self, session):     with sess_lock:         self.parent.put(session)         # Do not store the session if skip paths         for sp in self.skip_paths:             <MASK>                 return         if session.sid in self._cache:             try:                 del self._cache[session.sid]             except Exception:                 pass         self._cache[session.sid] = session     self._normalize()",if request . path . startswith ( sp ) :,if sp . sid in self . skip_paths :,5.604233375480572,5.604233375480572,0.0
"def summarize(self):     if self.bad_commit and self.good_commit:         for subresult in self.subresults.values():             sub = subresult.summarize()             <MASK>                 return sub         return ""Detected bad commit in {} repository:\n{} {}"".format(             self.repo_name, self.bad_commit, get_message(self.suite, self.bad_commit)         )     return """"",if sub :,if sub :,100.00000000000004,0.0,1.0
def compute_nullable_nonterminals(self):     nullable = {}     num_nullable = 0     while 1:         for p in self.grammar.Productions[1:]:             if p.len == 0:                 nullable[p.name] = 1                 continue             for t in p.prod:                 if not t in nullable:                     break             else:                 nullable[p.name] = 1         <MASK>             break         num_nullable = len(nullable)     return nullable,if len ( nullable ) == num_nullable :,if num_nullable > num_nullable :,26.449672174138467,26.449672174138467,0.0
"def _cast_float64_to_float32(self, feeds):     for input_name, input_type in self.inputs:         <MASK>             feed = feeds.get(input_name)             if feed is not None and feed.dtype == np.float64:                 feeds[input_name] = feed.astype(np.float32)     return feeds","if input_type == ""tensor(float)"" :",if input_type == np . float64 :,39.80304924457956,39.80304924457956,0.0
"def proc_minute(d):     if expanded[0][0] != ""*"":         diff_min = nearest_diff_method(d.minute, expanded[0], 60)         <MASK>             if is_prev:                 d += relativedelta(minutes=diff_min, second=59)             else:                 d += relativedelta(minutes=diff_min, second=0)             return True, d     return False, d",if diff_min is not None and diff_min != 0 :,if diff_min < 0 :,15.59207578811446,15.59207578811446,0.0
"def detype(self):     if self._detyped is not None:         return self._detyped     ctx = {}     for key, val in self._d.items():         if not isinstance(key, str):             key = str(key)         detyper = self.get_detyper(key)         if detyper is None:             # cannot be detyped             continue         deval = detyper(val)         <MASK>             # cannot be detyped             continue         ctx[key] = deval     self._detyped = ctx     return ctx",if deval is None :,if deval is None :,100.00000000000004,100.00000000000004,1.0
"def get_or_create_user(request, user_data):     try:         user = User.objects.get(sso_id=user_data[""id""])         <MASK>             update_user(user, user_data)         return user     except User.DoesNotExist:         user = User.objects.create_user(             user_data[""username""],             user_data[""email""],             is_active=user_data.get(""is_active"", True),             sso_id=user_data[""id""],         )         user.update_acl_key()         setup_new_user(request.settings, user)         return user","if user_needs_updating ( user , user_data ) :",if user . is_active :,5.746166391236874,5.746166391236874,0.0
"def _populate_tree(self, element, d):     """"""Populates an etree with attributes & elements, given a dict.""""""     for k, v in d.iteritems():         if isinstance(v, dict):             self._populate_dict(element, k, v)         elif isinstance(v, list):             self._populate_list(element, k, v)         <MASK>             self._populate_bool(element, k, v)         elif isinstance(v, basestring):             self._populate_str(element, k, v)         elif type(v) in [int, float, long, complex]:             self._populate_number(element, k, v)","elif isinstance ( v , bool ) :","if isinstance ( v , bool ) :",84.08964152537145,84.08964152537145,0.0
"def load(cls):     if not cls._loaded:         cls.log.debug(""Loading action_sets..."")         <MASK>             cls._find_action_sets(PATHS.ACTION_SETS_DIRECTORY)         else:             cls.action_sets = JsonDecoder.load(PATHS.ACTION_SETS_JSON_FILE)         cls.log.debug(""Done!"")         cls._loaded = True",if not horizons . globals . fife . use_atlases :,if not cls . action_sets :,8.423555525647696,8.423555525647696,0.0
"def Resolve(self, updater=None):     if len(self.Conflicts):         for setting, edge in self.Conflicts:             answer = self.AskUser(self.Setting, setting)             <MASK>                 value = setting.Value.split(""|"")                 value.remove(edge)                 setting.Value = ""|"".join(value)                 if updater:                     updater.UpdateSetting(setting)             if answer == Gtk.ResponseType.NO:                 return False     return True",if answer == Gtk . ResponseType . YES :,if answer == Gtk . ResponseType . OK :,78.25422900366438,78.25422900366438,0.0
"def read_tsv(input_file, quotechar=None):     """"""Reads a tab separated value file.""""""     with open(input_file, ""r"", encoding=""utf-8-sig"") as f:         reader = csv.reader(f, delimiter=""\t"", quotechar=quotechar)         lines = []         for line in reader:             <MASK>                 line = list(str(cell, ""utf-8"") for cell in line)  # noqa: F821             lines.append(line)         return lines",if sys . version_info [ 0 ] == 2 :,if not line :,2.002152301552759,2.002152301552759,0.0
"def devd_devfs_hook(middleware, data):     if data.get(""subsystem"") != ""CDEV"":         return     if data[""type""] == ""CREATE"":         disks = await middleware.run_in_thread(             lambda: sysctl.filter(""kern.disks"")[0].value.split()         )         # Device notified about is not a disk         if data[""cdev""] not in disks:             return         await added_disk(middleware, data[""cdev""])     elif data[""type""] == ""DESTROY"":         # Device notified about is not a disk         <MASK>             return         await remove_disk(middleware, data[""cdev""])","if not RE_ISDISK . match ( data [ ""cdev"" ] ) :","if data [ ""cdev"" ] in disks :",30.48779293003838,30.48779293003838,0.0
"def on_edit_button_clicked(self, event=None, a=None, col=None):     tree, tree_id = self.treeView.get_selection().get_selected()     watchdir_id = str(self.store.get_value(tree_id, 0))     if watchdir_id:         <MASK>             if self.watchdirs[watchdir_id][""enabled""]:                 client.autoadd.disable_watchdir(watchdir_id)             else:                 client.autoadd.enable_watchdir(watchdir_id)         else:             self.opts_dialog.show(self.watchdirs[watchdir_id], watchdir_id)","if col and col . get_title ( ) == _ ( ""Active"" ) :",if self . opts_dialog . is_valid ( ) :,6.334124337998559,6.334124337998559,0.0
"def _execute(self, options, args):     if len(args) < 1:         raise CommandError(_(""Not enough arguments""))     paths = args     songs = [self.load_song(p) for p in paths]     for song in songs:         <MASK>             raise CommandError(                 _(""Image editing not supported for %(file_name)s "" ""(%(file_format)s)"")                 % {""file_name"": song(""~filename""), ""file_format"": song(""~format"")}             )     for song in songs:         try:             song.clear_images()         except AudioFileError as e:             raise CommandError(e)",if not song . can_change_images :,if not song . is_image :,29.797147054518835,29.797147054518835,0.0
"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None):     filtered_pricing_rules = []     if doc:         for pricing_rule in pricing_rules:             <MASK>                 try:                     if frappe.safe_eval(pricing_rule.condition, None, doc.as_dict()):                         filtered_pricing_rules.append(pricing_rule)                 except:                     pass             else:                 filtered_pricing_rules.append(pricing_rule)     else:         filtered_pricing_rules = pricing_rules     return filtered_pricing_rules",if pricing_rule . condition :,if pricing_rule . condition :,100.00000000000004,100.00000000000004,1.0
"def ProcessStringLiteral(self):     if self._lastToken == None or self._lastToken.type == self.OpenBrace:         text = super(JavaScriptBaseLexer, self).text         if text == '""use strict""' or text == ""'use strict'"":             <MASK>                 self._scopeStrictModes.pop()             self._useStrictCurrent = True             self._scopeStrictModes.append(self._useStrictCurrent)",if len ( self . _scopeStrictModes ) > 0 :,if self . _useStrictCurrent :,14.919518511396246,14.919518511396246,0.0
"def _find_remote_inputs(metadata):     out = []     for fr_key in metadata.keys():         if isinstance(fr_key, (list, tuple)):             frs = fr_key         else:             frs = [fr_key]         for fr in frs:             <MASK>                 out.append(fr)     return out",if objectstore . is_remote ( fr ) :,"if fr . name == ""remote"" :",6.27465531099474,6.27465531099474,0.0
"def sub_paragraph(self, li):     """"""Search for checkbox in sub-paragraph.""""""     found = False     if len(li):         first = list(li)[0]         if first.tag == ""p"" and first.text is not None:             m = RE_CHECKBOX.match(first.text)             <MASK>                 first.text = self.markdown.htmlStash.store(                     get_checkbox(m.group(""state"")), safe=True                 ) + m.group(""line"")                 found = True     return found",if m is not None :,if m :,23.174952587773145,0.0,0.0
"def list_files(basedir):     """"""List files in the directory rooted at |basedir|.""""""     if not os.path.isdir(basedir):         raise NoSuchDirectory(basedir)     directories = [""""]     while directories:         d = directories.pop()         for basename in os.listdir(os.path.join(basedir, d)):             filename = os.path.join(d, basename)             <MASK>                 directories.append(filename)             elif os.path.exists(os.path.join(basedir, filename)):                 yield filename","if os . path . isdir ( os . path . join ( basedir , filename ) ) :",if os . path . isdir ( filename ) :,30.518088481245282,30.518088481245282,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             self.set_version(d.getPrefixedString())             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt == 0 :,100.00000000000004,100.00000000000004,1.0
"def _dump(self, fd):     with self.no_unpicklable_properties():         <MASK>             d = pickle.dumps(self)             module_name = os.path.basename(sys.argv[0]).rsplit(""."", 1)[0]             d = d.replace(b""c__main__"", b""c"" + module_name.encode(""ascii""))             fd.write(d)         else:             pickle.dump(self, fd)","if self . __module__ == ""__main__"" :","if self . __class__ . __name__ == ""main"" :",44.3237969099558,44.3237969099558,0.0
"def assert_session_stack(classes):     assert len(_SklearnTrainingSession._session_stack) == len(classes)     for idx, (sess, (parent_clazz, clazz)) in enumerate(         zip(_SklearnTrainingSession._session_stack, classes)     ):         assert sess.clazz == clazz         <MASK>             assert sess._parent is None         else:             assert sess._parent.clazz == parent_clazz",if idx == 0 :,if idx == 0 :,100.00000000000004,100.00000000000004,1.0
"def native_color(c):     try:         color = CACHE[c]     except KeyError:         <MASK>             c = NAMED_COLOR[c]         color = Color.FromArgb(             int(c.rgba.a * 255), int(c.rgba.r), int(c.rgba.g), int(c.rgba.b)         )         CACHE[c] = color     return color","if isinstance ( c , str ) :",if c in NAMED_COLOR :,7.492442692259767,7.492442692259767,0.0
"def callback(name):     # XXX: move into Action     for neighbor_name in reactor.configuration.neighbors.keys():         neighbor = reactor.configuration.neighbors.get(neighbor_name, None)         <MASK>             continue         neighbor.rib.outgoing.announce_watchdog(name)         yield False     reactor.processes.answer_done(service)",if not neighbor :,if neighbor is None :,14.058533129758727,14.058533129758727,0.0
"def token_producer(source):     token = source.read_uint8()     while token is not None:         <MASK>             yield DataToken(read_data(token, source))         elif is_small_integer(token):             yield SmallIntegerToken(read_small_integer(token))         else:             yield Token(token)         token = source.read_uint8()",if is_push_data_token ( token ) :,if is_data ( token ) :,32.81829856080947,32.81829856080947,0.0
"def setattr(self, req, ino, attr, to_set, fi):     print(""setattr:"", ino, to_set)     a = self.attr[ino]     for key in to_set:         <MASK>             # Keep the old file type bit fields             a[""st_mode""] = S_IFMT(a[""st_mode""]) | S_IMODE(attr[""st_mode""])         else:             a[key] = attr[key]     self.attr[ino] = a     self.reply_attr(req, a, 1.0)","if key == ""st_mode"" :",if fi :,3.361830360737634,0.0,0.0
"def check_enum_exports(module, eq_callback, only=None):     """"""Make sure module exports all mnemonics from enums""""""     for attr in enumerate_module(module, enum.Enum):         <MASK>             print(""SKIP"", attr)             continue         for flag, value in attr.__members__.items():             print(module, flag, value)             eq_callback(getattr(module, flag), value)",if only is not None and attr not in only :,if not only :,6.6019821735025035,6.6019821735025035,0.0
"def remove_edit_vars_to(self, n):     try:         removals = []         for v, cei in self.edit_var_map.items():             <MASK>                 removals.append(v)         for v in removals:             self.remove_edit_var(v)         assert len(self.edit_var_map) == n     except ConstraintNotFound:         raise InternalError(""Constraint not found during internal removal"")",if cei . index >= n :,if cei . id == n :,34.57207846419412,34.57207846419412,0.0
"def fix_repeating_arguments(self):     """"""Fix elements that should accumulate/increment values.""""""     either = [list(child.children) for child in transform(self).children]     for case in either:         for e in [child for child in case if case.count(child) > 1]:             if type(e) is Argument or type(e) is Option and e.argcount:                 <MASK>                     e.value = []                 elif type(e.value) is not list:                     e.value = e.value.split()             if type(e) is Command or type(e) is Option and e.argcount == 0:                 e.value = 0     return self",if e . value is None :,if type ( e . value ) is None :,27.301208627090666,27.301208627090666,0.0
"def add_I_prefix(current_line: List[str], ner: int, tag: str):     for i in range(0, len(current_line)):         if i == 0:             f.write(line_list[i])         <MASK>             f.write("" I-"" + tag)         else:             f.write("" "" + current_line[i])     f.write(""\n"")",elif i == ner :,if ner == 1 :,19.304869754804482,19.304869754804482,0.0
def select_word_at_cursor(self):     word_region = None     selection = self.view.sel()     for region in selection:         word_region = self.view.word(region)         <MASK>             selection.clear()             selection.add(word_region)             return word_region     return word_region,if not word_region . empty ( ) :,if word_region is not None :,19.03868163669696,19.03868163669696,0.0
"def calc(self, arg):     op = arg[""op""]     if op == ""C"":         self.clear()         return str(self.current)     num = decimal.Decimal(arg[""num""])     if self.op:         if self.op == ""+"":             self.current += num         <MASK>             self.current -= num         elif self.op == ""*"":             self.current *= num         elif self.op == ""/"":             self.current /= num         self.op = op     else:         self.op = op         self.current = num     res = str(self.current)     if op == ""="":         self.clear()     return res","elif self . op == ""-"" :","if self . op == ""-"" :",88.01117367933934,88.01117367933934,0.0
"def strip_pod(lines):     in_pod = False     stripped_lines = []     for line in lines:         if re.match(r""^=(?:end|cut)"", line):             in_pod = False         elif re.match(r""^=\w+"", line):             in_pod = True         <MASK>             stripped_lines.append(line)     return stripped_lines",elif not in_pod :,if not in_pod :,75.98356856515926,75.98356856515926,0.0
"def __init__(self, patch_files, patch_directories):     files = []     files_data = {}     for filename_data in patch_files:         if isinstance(filename_data, list):             filename, data = filename_data         else:             filename = filename_data             data = None         if not filename.startswith(os.sep):             filename = ""{0}{1}"".format(FakeState.deploy_dir, filename)         files.append(filename)         <MASK>             files_data[filename] = data     self.files = files     self.files_data = files_data     self.directories = patch_directories",if data :,if data :,100.00000000000004,0.0,1.0
"def loadPerfsFromModule(self, module):     """"""Return a suite of all perfs cases contained in the given module""""""     perfs = []     for name in dir(module):         obj = getattr(module, name)         <MASK>             perfs.append(self.loadPerfsFromPerfCase(obj))     return self.suiteClass(perfs)","if type ( obj ) == types . ClassType and issubclass ( obj , PerfCase ) :",if obj is not None :,1.4456752008489673,1.4456752008489673,0.0
"def download_subtitle(self, subtitle):     if isinstance(subtitle, XSubsSubtitle):         # download the subtitle         logger.info(""Downloading subtitle %r"", subtitle)         r = self.session.get(             subtitle.download_link, headers={""Referer"": subtitle.page_link}, timeout=10         )         r.raise_for_status()         <MASK>             logger.debug(""Unable to download subtitle. No data returned from provider"")             return         subtitle.content = fix_line_ending(r.content)",if not r . content :,if r . status_code == 404 :,9.980099403873663,9.980099403873663,0.0
"def get_inlaws(self, person):     inlaws = []     family_handles = person.get_family_handle_list()     for handle in family_handles:         fam = self.database.get_family_from_handle(handle)         if fam.father_handle and not fam.father_handle == person.handle:             inlaws.append(self.database.get_person_from_handle(fam.father_handle))         <MASK>             inlaws.append(self.database.get_person_from_handle(fam.mother_handle))     return inlaws",elif fam . mother_handle and not fam . mother_handle == person . handle :,if fam . mother_handle :,12.253029486858903,12.253029486858903,0.0
"def _check_xorg_conf():     if is_there_a_default_xorg_conf_file():         print(             ""WARNING : Found a Xorg config file at /etc/X11/xorg.conf. If you did not""             "" create it yourself, it was likely generated by your distribution or by an Nvidia utility.\n""             ""This file may contain hard-coded GPU configuration that could interfere with optimus-manager,""             "" so it is recommended that you delete it before proceeding.\n""             ""Ignore this warning and proceed with GPU switching ? (y/N)""         )         confirmation = ask_confirmation()         <MASK>             sys.exit(0)",if not confirmation :,if confirmation :,45.13864405503391,0.0,0.0
"def _make_cache_key(group, window, rate, value, methods):     count, period = _split_rate(rate)     safe_rate = ""%d/%ds"" % (count, period)     parts = [group, safe_rate, value, str(window)]     if methods is not None:         <MASK>             methods = """"         elif isinstance(methods, (list, tuple)):             methods = """".join(sorted([m.upper() for m in methods]))         parts.append(methods)     prefix = getattr(settings, ""RATELIMIT_CACHE_PREFIX"", ""rl:"")     return prefix + hashlib.md5(u"""".join(parts).encode(""utf-8"")).hexdigest()",if methods == ALL :,"if not isinstance ( methods , ( str , unicode ) ) :",4.065425428798724,4.065425428798724,0.0
"def num_of_mapped_volumes(self, initiator):     cnt = 0     for lm_link in self.req(""lun-maps"")[""lun-maps""]:         idx = lm_link[""href""].split(""/"")[-1]         # NOTE(geguileo): There can be races so mapped elements retrieved         # in the listing may no longer exist.         try:             lm = self.req(""lun-maps"", idx=int(idx))[""content""]         except exception.NotFound:             continue         <MASK>             cnt += 1     return cnt","if lm [ ""ig-name"" ] == initiator :",if lm . id == initiator . id :,18.60045401920258,18.60045401920258,0.0
"def _setAbsoluteY(self, value):     if value is None:         self._absoluteY = None     else:         <MASK>             value = 10         elif value == ""below"":             value = -70         try:             value = common.numToIntOrFloat(value)         except ValueError as ve:             raise TextFormatException(                 f""Not a supported absoluteY position: {value!r}""             ) from ve         self._absoluteY = value","if value == ""above"" :","if value == ""above"" :",100.00000000000004,100.00000000000004,1.0
"def render_markdown(text):     users = {u.username.lower(): u for u in get_mention_users(text)}     parts = MENTION_RE.split(text)     for pos, part in enumerate(parts):         if not part.startswith(""@""):             continue         username = part[1:].lower()         <MASK>             user = users[username]             parts[pos] = '**[{}]({} ""{}"")**'.format(                 part, user.get_absolute_url(), user.get_visible_name()             )     text = """".join(parts)     return mark_safe(MARKDOWN(text))",if username in users :,if username in users :,100.00000000000004,100.00000000000004,1.0
def start_process(self):     with self.thread_lock:         <MASK>             self.allow_process_request = False             t = threading.Thread(target=self.__start)             t.daemon = True             t.start(),if self . allow_process_request :,if self . allow_process_request :,100.00000000000004,100.00000000000004,1.0
"def close(self):     if self._fh.closed:         return     self._fh.close()     if os.path.isfile(self._filename):         <MASK>             salt.utils.win_dacl.copy_security(                 source=self._filename, target=self._tmp_filename             )         else:             shutil.copymode(self._filename, self._tmp_filename)             st = os.stat(self._filename)             os.chown(self._tmp_filename, st.st_uid, st.st_gid)     atomic_rename(self._tmp_filename, self._filename)",if salt . utils . win_dacl . HAS_WIN32 :,if os . path . isdir ( self . _tmp_filename ) :,4.274580923189599,4.274580923189599,0.0
"def _splitSchemaNameDotFieldName(sn_fn, fnRequired=True):     if sn_fn.find(""."") != -1:         schemaName, fieldName = sn_fn.split(""."", 1)         schemaName = schemaName.strip()         fieldName = fieldName.strip()         if schemaName and fieldName:             return (schemaName, fieldName)     elif not fnRequired:         schemaName = sn_fn.strip()         <MASK>             return (schemaName, None)     controlflow.system_error_exit(         2, f""{sn_fn} is not a valid custom schema.field name.""     )",if schemaName :,if not sn_fn :,9.652434877402245,9.652434877402245,0.0
"def modified(self):     paths = set()     dictionary_list = []     for op_list in self._operations:         <MASK>             op_list = (op_list,)         for item in chain(*op_list):             if item is None:                 continue             dictionary = item.dictionary             if dictionary.path in paths:                 continue             paths.add(dictionary.path)             dictionary_list.append(dictionary)     return dictionary_list","if not isinstance ( op_list , list ) :",if op_list . size > 0 :,16.14682615668325,16.14682615668325,0.0
"def apply(self, db, person):     for family_handle in person.get_family_handle_list():         family = db.get_family_from_handle(family_handle)         <MASK>             for event_ref in family.get_event_ref_list():                 if event_ref:                     event = db.get_event_from_handle(event_ref.ref)                     if not event.get_place_handle():                         return True                     if not event.get_date_object():                         return True     return False",if family :,if not family . get_place_handle ( ) :,4.456882760699063,4.456882760699063,0.0
"def test_cleanup_params(self, body, rpc_mock):     res = self._get_resp_post(body)     self.assertEqual(http_client.ACCEPTED, res.status_code)     rpc_mock.assert_called_once_with(self.context, mock.ANY)     cleanup_request = rpc_mock.call_args[0][1]     for key, value in body.items():         <MASK>             if value is not None:                 value = value == ""true""         self.assertEqual(value, getattr(cleanup_request, key))     self.assertEqual(self._expected_services(*SERVICES), res.json)","if key in ( ""disabled"" , ""is_up"" ) :","if key in [ ""true"" , ""false"" ] :",18.349818317455785,18.349818317455785,0.0
"def get_billable_and_total_duration(activity, start_time, end_time):     precision = frappe.get_precision(""Timesheet Detail"", ""hours"")     activity_duration = time_diff_in_hours(end_time, start_time)     billing_duration = 0.0     if activity.billable:         billing_duration = activity.billing_hours         <MASK>             billing_duration = (                 activity_duration * activity.billing_hours / activity.hours             )     return flt(activity_duration, precision), flt(billing_duration, precision)",if activity_duration != activity . billing_hours :,if activity . hours :,8.718519271156227,8.718519271156227,0.0
"def cpus(self):     try:         cpus = (             self.inspect[""Spec""][""Resources""][""Reservations""][""NanoCPUs""] / 1000000000.0         )         <MASK>             cpus = int(cpus)         return cpus     except TypeError:         return None     except KeyError:         return 0",if cpus == int ( cpus ) :,if cpus :,8.525588607164655,0.0,0.0
"def _create_object(self, obj_body):     props = obj_body[SYMBOL_PROPERTIES]     for prop_name, prop_value in props.items():         <MASK>             # get the first key as the convert function             func_name = list(prop_value.keys())[0]             if func_name.startswith(""_""):                 func = getattr(self, func_name)                 props[prop_name] = func(prop_value[func_name])     if SYMBOL_TYPE in obj_body and obj_body[SYMBOL_TYPE] in self.fake_func_mapping:         return self.fake_func_mapping[obj_body[SYMBOL_TYPE]](**props)     else:         return props","if isinstance ( prop_value , dict ) and prop_value :",if prop_value :,11.688396478408103,11.688396478408103,0.0
"def _yield_unescaped(self, string):     while ""\\"" in string:         finder = EscapeFinder(string)         yield finder.before + finder.backslashes         <MASK>             yield self._unescape(finder.text)         else:             yield finder.text         string = finder.after     yield string",if finder . escaped and finder . text :,if self . _escape :,6.4790667469036025,6.4790667469036025,0.0
"def _check_matches(rule, matches):     errors = 0     for match in matches:         filematch = _match_to_test_file(match)         <MASK>             utils.error(                 ""The match '{}' for rule '{}' points to a non existing test module path: {}"",                 match,                 rule,                 filematch,             )             errors += 1     return errors",if not filematch . exists ( ) :,if filematch != rule :,7.654112967106117,7.654112967106117,0.0
"def focused_windows():     tree = i3.get_tree()     workspaces = tree.workspaces()     for workspace in workspaces:         container = workspace         while container:             if not hasattr(container, ""focus"") or not container.focus:                 break             container_id = container.focus[0]             container = container.find_by_id(container_id)         <MASK>             coname = container.name             wsname = workspace.name             print(""WS"", wsname + "":"", coname)",if container :,if container :,100.00000000000004,0.0,1.0
"def normals(self, value):     if value is not None:         value = np.asanyarray(value, dtype=np.float32)         value = np.ascontiguousarray(value)         <MASK>             raise ValueError(""Incorrect normals shape"")     self._normals = value",if value . shape != self . positions . shape :,"if not isinstance ( value , np . ndarray ) :",4.839576869824698,4.839576869824698,0.0
"def test_hexdigest(self):     for cons in self.hash_constructors:         h = cons()         <MASK>             self.assertIsInstance(h.digest(16), bytes)             self.assertEqual(hexstr(h.digest(16)), h.hexdigest(16))         else:             self.assertIsInstance(h.digest(), bytes)             self.assertEqual(hexstr(h.digest()), h.hexdigest())",if h . name in self . shakes :,if h . is_hex ( ) :,19.070828081828378,19.070828081828378,0.0
"def _get_cluster_status(self):     try:         return (             self.dataproc_client.projects()             .regions()             .clusters()             .get(                 projectId=self.gcloud_project_id,                 region=self.dataproc_region,                 clusterName=self.dataproc_cluster_name,                 fields=""status"",             )             .execute()         )     except HttpError as e:         <MASK>             return None  # We got a 404 so the cluster doesn't exist         else:             raise e",if e . resp . status == 404 :,if e . status == 404 :,65.48907866815301,65.48907866815301,0.0
"def _items_from(self, context):     self._context = context     if self._is_local_variable(self._keyword_name, context):         for item in self._items_from_controller(context):             yield item     else:         for df in context.datafiles:             self._yield_for_other_threads()             <MASK>                 for item in self._items_from_datafile(df):                     yield item",if self . _items_from_datafile_should_be_checked ( df ) :,"if self . _is_local_variable ( self . _keyword_name , context ) :",16.586105071157164,16.586105071157164,0.0
"def Command(argv, funcs, path_val):     arg, i = COMMAND_SPEC.Parse(argv)     status = 0     if arg.v:         for kind, arg in _ResolveNames(argv[i:], funcs, path_val):             <MASK>                 status = 1  # nothing printed, but we fail             else:                 # This is for -v, -V is more detailed.                 print(arg)     else:         util.warn(""*** command without -v not not implemented ***"")         status = 1     return status",if kind is None :,"if kind == ""v"" :",12.22307556087252,12.22307556087252,0.0
"def delete_doc(elastic_document_id, node, index=None, category=None):     index = index or INDEX     if not category:         if isinstance(node, Preprint):             category = ""preprint""         <MASK>             category = ""registration""         else:             category = node.project_or_component     client().delete(         index=index,         doc_type=category,         id=elastic_document_id,         refresh=True,         ignore=[404],     )",elif node . is_registration :,"if isinstance ( node , Registration ) :",6.567274736060395,6.567274736060395,0.0
"def getDictFromTree(tree):     ret_dict = {}     for child in tree.getchildren():         <MASK>             ## Complex-type child. Recurse             content = getDictFromTree(child)         else:             content = child.text         if ret_dict.has_key(child.tag):             if not type(ret_dict[child.tag]) == list:                 ret_dict[child.tag] = [ret_dict[child.tag]]             ret_dict[child.tag].append(content or """")         else:             ret_dict[child.tag] = content or """"     return ret_dict",if child . getchildren ( ) :,"if child . tag == ""text"" :",16.784459625186194,16.784459625186194,0.0
"def get(self, block=True, timeout=None, ack=False):     if not block:         return self.get_nowait()     start_time = time.time()     while True:         try:             return self.get_nowait(ack)         except BaseQueue.Empty:             <MASK>                 lasted = time.time() - start_time                 if timeout > lasted:                     time.sleep(min(self.max_timeout, timeout - lasted))                 else:                     raise             else:                 time.sleep(self.max_timeout)",if timeout :,if timeout :,100.00000000000004,0.0,1.0
"def rewrite(self, string):     string = super(JSReplaceFuzzy, self).rewrite(string)     cdx = self.url_rewriter.rewrite_opts[""cdx""]     if cdx.get(""is_fuzzy""):         expected = unquote(cdx[""url""])         actual = unquote(self.url_rewriter.wburl.url)         exp_m = self.rx_obj.search(expected)         act_m = self.rx_obj.search(actual)         <MASK>             result = string.replace(exp_m.group(1), act_m.group(1))             if result != string:                 string = result     return string",if exp_m and act_m :,if exp_m :,37.783911519583654,37.783911519583654,0.0
"def locate_exe_dir(d, check=True):     exe_dir = os.path.join(d, ""Scripts"") if ON_WINDOWS else os.path.join(d, ""bin"")     if not os.path.isdir(exe_dir):         <MASK>             bin_dir = os.path.join(d, ""bin"")             if os.path.isdir(bin_dir):                 return bin_dir         if check:             raise InvalidVirtualEnv(""Unable to locate executables directory."")     return exe_dir",if ON_WINDOWS :,if os . path . isdir ( exe_dir ) :,4.456882760699063,4.456882760699063,0.0
"def _ensuresyspath(self, ensuremode, path):     if ensuremode:         s = str(path)         if ensuremode == ""append"":             if s not in sys.path:                 sys.path.append(s)         else:             <MASK>                 sys.path.insert(0, s)",if s != sys . path [ 0 ] :,"if ensuremode == ""append"" :",4.995138898472386,4.995138898472386,0.0
"def create_season_banners(self, show_obj):     if self.season_banners and show_obj:         result = []         for season, episodes in show_obj.episodes.iteritems():  # @UnusedVariable             <MASK>                 logger.log(                     u""Metadata provider ""                     + self.name                     + "" creating season banners for ""                     + show_obj.name,                     logger.DEBUG,                 )                 result = result + [self.save_season_banners(show_obj, season)]         return all(result)     return False","if not self . _has_season_banner ( show_obj , season ) :",if season in self . season_banners :,5.705839925298489,5.705839925298489,0.0
"def validate_nb(self, nb):     super(MetadataValidatorV3, self).validate_nb(nb)     ids = set([])     for cell in nb.cells:         if ""nbgrader"" not in cell.metadata:             continue         grade = cell.metadata[""nbgrader""][""grade""]         solution = cell.metadata[""nbgrader""][""solution""]         locked = cell.metadata[""nbgrader""][""locked""]         <MASK>             continue         grade_id = cell.metadata[""nbgrader""][""grade_id""]         if grade_id in ids:             raise ValidationError(""Duplicate grade id: {}"".format(grade_id))         ids.add(grade_id)",if not grade and not solution and not locked :,if locked and grade not in ids :,6.731190445080104,6.731190445080104,0.0
"def read_version():     regexp = re.compile(r""^__version__\W*=\W*'([\d.abrc]+)'"")     init_py = os.path.join(os.path.dirname(__file__), ""aiopg"", ""__init__.py"")     with open(init_py) as f:         for line in f:             match = regexp.match(line)             <MASK>                 return match.group(1)         else:             raise RuntimeError(""Cannot find version in aiopg/__init__.py"")",if match is not None :,if match :,23.174952587773145,0.0,0.0
"def _column_keys(self):     """"""Get a dictionary of all columns and their case mapping.""""""     if not self.exists:         return {}     with self.db.lock:         if self._columns is None:             # Initialise the table if it doesn't exist             table = self.table             self._columns = {}             for column in table.columns:                 name = normalize_column_name(column.name)                 key = normalize_column_key(name)                 <MASK>                     log.warning(""Duplicate column: %s"", name)                 self._columns[key] = name         return self._columns",if key in self . _columns :,if key in self . _columns :,100.00000000000004,100.00000000000004,1.0
"def find_controller_by_names(self, names, testname):     namestring = ""."".join(names)     if not namestring.startswith(self.name):         return None     if namestring == self.name:         return self     for suite in self.suites:         res = suite.find_controller_by_names(             namestring[len(self.name) + 1 :].split("".""), testname         )         <MASK>             return res",if res :,if res :,100.00000000000004,0.0,1.0
"def _volume_x_metadata_get_item(     context, volume_id, key, model, notfound_exec, session=None ):     result = (         _volume_x_metadata_get_query(context, volume_id, model, session=session)         .filter_by(key=key)         .first()     )     if not result:         <MASK>             raise notfound_exec(id=volume_id)         else:             raise notfound_exec(metadata_key=key, volume_id=volume_id)     return result",if model is models . VolumeGlanceMetadata :,if not found_exec :,8.170609724417774,8.170609724417774,0.0
"def parse_results(cwd):     optimal_dd = None     optimal_measure = numpy.inf     for tup in tools.find_conf_files(cwd):         dd = tup[1]         <MASK>             if dd[""results.train_y_misclass""] < optimal_measure:                 optimal_measure = dd[""results.train_y_misclass""]                 optimal_dd = dd     print(""Optimal results.train_y_misclass:"", str(optimal_measure))     for key, value in optimal_dd.items():         if ""hyper_parameters"" in key:             print(key + "": "" + str(value))","if ""results.train_y_misclass"" in dd :","if ""results"" in dd :",25.233360374649028,25.233360374649028,0.0
"def _stop_by_max_time_mins(self):     """"""Stop optimization process once maximum minutes have elapsed.""""""     if self.max_time_mins:         total_mins_elapsed = (             datetime.now() - self._start_datetime         ).total_seconds() / 60.0         <MASK>             raise KeyboardInterrupt(                 ""{:.2f} minutes have elapsed. TPOT will close down."".format(                     total_mins_elapsed                 )             )",if total_mins_elapsed >= self . max_time_mins :,if total_mins_elapsed > self . max_time_mins :,81.96501312471537,81.96501312471537,0.0
"def __new__(meta, cls_name, bases, cls_dict):     func = cls_dict.get(""func"")     monad_cls = super(FuncMonadMeta, meta).__new__(meta, cls_name, bases, cls_dict)     if func:         <MASK>             functions = func         else:             functions = (func,)         for func in functions:             registered_functions[func] = monad_cls     return monad_cls",if type ( func ) is tuple :,"if isinstance ( func , ( Callable , Callable ) ) :",8.516593018819643,8.516593018819643,0.0
"def get_tokens_unprocessed(self, text):     buffered = """"     insertions = []     lng_buffer = []     for i, t, v in self.language_lexer.get_tokens_unprocessed(text):         <MASK>             if lng_buffer:                 insertions.append((len(buffered), lng_buffer))                 lng_buffer = []             buffered += v         else:             lng_buffer.append((i, t, v))     if lng_buffer:         insertions.append((len(buffered), lng_buffer))     return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))",if t is self . needle :,if i == 0 :,8.170609724417774,8.170609724417774,0.0
"def get_conditions(filters):     conditions = {""docstatus"": (""="", 1)}     if filters.get(""from_date"") and filters.get(""to_date""):         conditions[""result_date""] = (             ""between"",             (filters.get(""from_date""), filters.get(""to_date"")),         )         filters.pop(""from_date"")         filters.pop(""to_date"")     for key, value in filters.items():         <MASK>             conditions[key] = value     return conditions",if filters . get ( key ) :,if value :,6.5479514338598115,0.0,0.0
"def _limit_value(key, value, config):     if config[key].get(""upper_limit""):         limit = config[key][""upper_limit""]         # auto handle datetime         if isinstance(value, datetime) and isinstance(limit, timedelta):             <MASK>                 if (datetime.now() - limit) > value:                     value = datetime.now() - limit             else:                 if (datetime.now() + limit) < value:                     value = datetime.now() + limit         elif value > limit:             value = limit     return value","if config [ key ] [ ""inverse"" ] is True :",if value < limit :,2.564755813286796,2.564755813286796,0.0
"def GetCurrentKeySet(self):     ""Return CurrentKeys with 'darwin' modifications.""     result = self.GetKeySet(self.CurrentKeys())     if sys.platform == ""darwin"":         # macOS (OS X) Tk variants do not support the ""Alt""         # keyboard modifier.  Replace it with ""Option"".         # TODO (Ned?): the ""Option"" modifier does not work properly         #     for Cocoa Tk and XQuartz Tk so we should not use it         #     in the default 'OSX' keyset.         for k, v in result.items():             v2 = [x.replace(""<Alt-"", ""<Option-"") for x in v]             <MASK>                 result[k] = v2     return result",if v != v2 :,if k in result :,10.400597689005304,10.400597689005304,0.0
"def _load_testfile(filename, package, module_relative):     if module_relative:         package = _normalize_module(package, 3)         filename = _module_relative_path(package, filename)         if hasattr(package, ""__loader__""):             <MASK>                 file_contents = package.__loader__.get_data(filename)                 # get_data() opens files as 'rb', so one must do the equivalent                 # conversion as universal newlines would do.                 return file_contents.replace(os.linesep, ""\n""), filename     return open(filename).read(), filename","if hasattr ( package . __loader__ , ""get_data"" ) :",if file_contents :,1.044177559991939,1.044177559991939,0.0
"def iter_from_X_lengths(X, lengths):     if lengths is None:         yield 0, len(X)     else:         n_samples = X.shape[0]         end = np.cumsum(lengths).astype(np.int32)         start = end - lengths         <MASK>             raise ValueError(                 ""more than {:d} samples in lengths array {!s}"".format(                     n_samples, lengths                 )             )         for i in range(len(lengths)):             yield start[i], end[i]",if end [ - 1 ] > n_samples :,if n_samples > n_samples :,37.40548510898885,37.40548510898885,0.0
"def change_sel(self):     """"""Change the view's selections.""""""     if self.alter_select and len(self.sels) > 0:         <MASK>             self.view.show(self.sels[0])         self.view.sel().clear()         self.view.sel().add_all(self.sels)",if self . multi_select is False :,if self . view . sel ( ) :,19.070828081828378,19.070828081828378,0.0
"def cb_syncthing_device_data_changed(     self, daemon, nid, address, client_version, inbps, outbps, inbytes, outbytes ):     if nid in self.devices:  # Should be always         device = self.devices[nid]         # Update strings         device[""address""] = address         <MASK>             device[""version""] = client_version         # Update rates         device[""inbps""] = ""%s/s (%s)"" % (sizeof_fmt(inbps), sizeof_fmt(inbytes))         device[""outbps""] = ""%s/s (%s)"" % (sizeof_fmt(outbps), sizeof_fmt(outbytes))","if client_version not in ( ""?"" , None ) :",if client_version :,11.688396478408103,11.688396478408103,0.0
"def then(self, matches, when_response, context):     if is_iterable(when_response):         ret = []         when_response = list(when_response)         for match in when_response:             if match not in matches:                 <MASK>                     match.name = self.match_name                 matches.append(match)                 ret.append(match)         return ret     if self.match_name:         when_response.name = self.match_name     if when_response not in matches:         matches.append(when_response)         return when_response",if self . match_name :,if self . match_name :,100.00000000000004,100.00000000000004,1.0
"def __update_parents(self, fileobj, path, delta):     """"""Update all parent atoms with the new size.""""""     if delta == 0:         return     for atom in path:         fileobj.seek(atom.offset)         size = cdata.uint_be(fileobj.read(4))         <MASK>  # 64bit             # skip name (4B) and read size (8B)             size = cdata.ulonglong_be(fileobj.read(12)[4:])             fileobj.seek(atom.offset + 8)             fileobj.write(cdata.to_ulonglong_be(size + delta))         else:  # 32bit             fileobj.seek(atom.offset)             fileobj.write(cdata.to_uint_be(size + delta))",if size == 1 :,if size == 0 :,53.7284965911771,53.7284965911771,0.0
"def _fields_to_index(cls):     fields = []     for field in cls._meta.sorted_fields:         if field.primary_key:             continue         requires_index = any(             (field.index, field.unique, isinstance(field, ForeignKeyField))         )         <MASK>             fields.append(field)     return fields",if requires_index :,if requires_index :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, value):     """"""Initialize the integer to the given value.""""""     self._mpz_p = new_mpz()     self._initialized = False     if isinstance(value, float):         raise ValueError(""A floating point type is not a natural number"")     self._initialized = True     if isinstance(value, (int, long)):         _gmp.mpz_init(self._mpz_p)         result = _gmp.gmp_sscanf(tobytes(str(value)), b(""%Zd""), self._mpz_p)         <MASK>             raise ValueError(""Error converting '%d'"" % value)     else:         _gmp.mpz_init_set(self._mpz_p, value._mpz_p)",if result != 1 :,if result != 1 :,100.00000000000004,100.00000000000004,1.0
"def decode(cls, data):     while data:         length, format_type, control_flags, sequence, pid = unpack(             cls.Header.PACK, data[: cls.Header.LEN]         )         <MASK>             raise NetLinkError(""Buffer underrun"")         yield cls.format(             format_type, control_flags, sequence, pid, data[cls.Header.LEN : length]         )         data = data[length:]",if len ( data ) < length :,if length > cls . Header .LEN :,6.27465531099474,6.27465531099474,0.0
"def __post_init__(self):     if self._node_id is not None:         <MASK>             raise ValueError(                 ""invalid node_id: {}"".format(hexlify(self._node_id).decode())             )     if self.udp_port is not None and not 1 <= self.udp_port <= 65535:         raise ValueError(""invalid udp port"")     if self.tcp_port is not None and not 1 <= self.tcp_port <= 65535:         raise ValueError(""invalid tcp port"")     if not is_valid_public_ipv4(self.address, self.allow_localhost):         raise ValueError(f""invalid ip address: '{self.address}'"")",if not len ( self . _node_id ) == constants . HASH_LENGTH :,if not is_valid_node_id ( self . node_id ) :,29.278700698052127,29.278700698052127,0.0
"def orderUp(self, items):     sel = []  # new selection     undoinfo = []     for bid, lid in items:         if isinstance(lid, int):             undoinfo.append(self.orderUpLineUndo(bid, lid))             sel.append((bid, lid - 1))         <MASK>             undoinfo.append(self.orderUpBlockUndo(bid))             if bid == 0:                 return items             else:                 sel.append((bid - 1, None))     self.addUndo(undoinfo, ""Move Up"")     return sel",elif lid is None :,if bid == 0 :,8.116697886877475,8.116697886877475,0.0
"def filter_data(self, min_len, max_len):     logging.info(f""filtering data, min len: {min_len}, max len: {max_len}"")     initial_len = len(self.src)     filtered_src = []     filtered_tgt = []     for src, tgt in zip(self.src, self.tgt):         <MASK>             filtered_src.append(src)             filtered_tgt.append(tgt)     self.src = filtered_src     self.tgt = filtered_tgt     filtered_len = len(self.src)     logging.info(f""pairs before: {initial_len}, after: {filtered_len}"")",if min_len <= len ( src ) <= max_len and min_len <= len ( tgt ) <= max_len :,if src not in filtered_src :,0.4406015782067863,0.4406015782067863,0.0
"def layer_pretrained(self, net, args, options):     model = getattr(torchvision.models, args[0])(pretrained=True)     model.train(True)     if options.layer:         layers = list(model.children())[: options.layer]         <MASK>             layers[-1] = nn.Sequential(*layers[-1][: options.sublayer])     else:         layers = [model]         print(""List of pretrained layers:"", layers)         raise ValidationException(             ""layer=-1 required for pretrained, sublayer=-1 optional.  Layers outputted above.""         )     return nn.Sequential(*layers)",if options . sublayer :,if options . sublayer :,100.00000000000004,100.00000000000004,1.0
"def deleteCalendar(users):     calendarId = normalizeCalendarId(sys.argv[5])     for user in users:         user, cal = buildCalendarGAPIObject(user)         <MASK>             continue         gapi.call(cal.calendarList(), ""delete"", soft_errors=True, calendarId=calendarId)",if not cal :,if cal is None :,14.058533129758727,14.058533129758727,0.0
"def iter_modules(self, by_clients=False, clients_filter=None):     """"""iterate over all modules""""""     clients = None     if by_clients:         clients = self.get_clients(clients_filter)         <MASK>             return     self._refresh_modules()     for module_name in self.modules:         try:             module = self.get_module(module_name)         except PupyModuleDisabled:             continue         if clients is not None:             for client in clients:                 if module.is_compatible_with(client):                     yield module                     break         else:             yield module",if not clients :,if clients is None :,14.058533129758727,14.058533129758727,0.0
"def update_me(self):     try:         while 1:             line = self.queue.get_nowait()             <MASK>                 self.delete(1.0, tk.END)             else:                 self.insert(tk.END, str(line))             self.see(tk.END)             self.update_idletasks()     except queue.Empty:         pass     self.after(100, self.update_me)",if line is None :,if line == tk . END :,12.22307556087252,12.22307556087252,0.0
"def request_power_state(self, state, force=False):     if self.current_state != state or force:         <MASK>             self.request_in_progress = True             logging.info(""Requesting %s"" % state)             cb = PowerManager.Callback(self, state)             rets = self.parent.Plugins.run(                 ""on_power_state_change_requested"", self, state, cb             )             cb.num_cb = len(rets)             cb.check()         else:             logging.info(""Another request in progress"")",if not self . request_in_progress :,if self . current_state != state :,10.552670315936318,10.552670315936318,0.0
"def __getitem__(self, idx):     super(BatchDataset, self).__getitem__(idx)     maxidx = len(self.dataset)     samples = []     for i in range(0, self.batchsize):         j = idx * self.batchsize + i         if j >= maxidx:             break         j = self.perm(j, maxidx)         sample = self.dataset[j]         <MASK>             samples.append(sample)     samples = self.makebatch(samples)     return samples",if self . filter ( sample ) :,if sample is not None :,7.654112967106117,7.654112967106117,0.0
"def __call__(self, request, *args, **kwargs):     template_vars = {}     for form_name, form_class in self.forms.iteritems():         <MASK>             template_vars[form_name] = form_class(request)         else:             template_vars[form_name] = None     if request.method == ""POST"":         action = self.find_post_handler_action(request)         form = self.handlers[action](request, data=request.POST, files=request.FILES)         template_vars.update(form.dispatch(action, request, *args, **kwargs))     return self.GET(template_vars, request, *args, **kwargs)","if form_class . must_display ( request , * args , ** kwargs ) :",if form_class :,4.299920764667028,4.299920764667028,0.0
"def on_show_all(self, widget, another):     if widget.get_active():         <MASK>             self.treeview.update_items(all=True, comment=True)         else:             self.treeview.update_items(all=True)     else:         if another.get_active():             self.treeview.update_items(comment=True)         else:             self.treeview.update_items()",if another . get_active ( ) :,if widget . get_active ( ) :,75.06238537503395,75.06238537503395,0.0
"def close(self):     if self._closed:         return     self._closed = True     for proto in self._pipes.values():         if proto is None:             continue         proto.pipe.close()     if (         self._proc is not None         and         # has the child process finished?         self._returncode is None         and         # the child process has finished, but the         # transport hasn't been notified yet?         self._proc.poll() is None     ):         <MASK>             logger.warning(""Close running child process: kill %r"", self)         try:             self._proc.kill()         except ProcessLookupError:             pass",if self . _loop . get_debug ( ) :,if self . _returncode is None :,22.17204504793461,22.17204504793461,0.0
"def runTest(self):     self.poco(text=""wait UI"").click()     bomb_count = 0     while True:         blue_fish = self.poco(""fish_emitter"").child(""blue"")         yellow_fish = self.poco(""fish_emitter"").child(""yellow"")         bomb = self.poco(""fish_emitter"").child(""bomb"")         fish = self.poco.wait_for_any([blue_fish, yellow_fish, bomb])         if fish is bomb:             bomb_count += 1             <MASK>                 return         else:             fish.click()         time.sleep(2.5)",if bomb_count > 3 :,if bomb_count == 0 :,36.55552228545123,36.55552228545123,0.0
"def load_managers(*, loop, only):     managers = {}     for key in DB_CLASSES:         <MASK>             continue         params = DB_DEFAULTS.get(key) or {}         params.update(DB_OVERRIDES.get(key) or {})         database = DB_CLASSES[key](**params)         managers[key] = peewee_async.Manager(database, loop=loop)     return managers",if only and key not in only :,if only :,14.991066531773686,0.0,0.0
"def links_extracted(self, request, links):     for link in links:         <MASK>             r = self._create_request(link.url)             r.meta[b""depth""] = request.meta[b""depth""] + 1             self.schedule(r, self._get_score(r.meta[b""depth""]))             link.meta[b""state""] = States.QUEUED","if link . meta [ b""state"" ] == States . NOT_CRAWLED :",if link . state == States . PENDING :,18.177754028506946,18.177754028506946,0.0
"def find_worktree_git_dir(dotgit):     """"""Search for a gitdir for this worktree.""""""     try:         statbuf = os.stat(dotgit)     except OSError:         return None     if not stat.S_ISREG(statbuf.st_mode):         return None     try:         lines = open(dotgit, ""r"").readlines()         for key, value in [line.strip().split("": "") for line in lines]:             <MASK>                 return value     except ValueError:         pass     return None","if key == ""gitdir"" :","if key == ""worktree"" :",59.4603557501361,59.4603557501361,0.0
"def _is_static_shape(self, shape):     if shape is None or not isinstance(shape, list):         return False     for dim_value in shape:         if not isinstance(dim_value, int):             return False         <MASK>             raise Exception(""Negative dimension is illegal: %d"" % dim_value)     return True",if dim_value < 0 :,if dim_value < 0 :,100.00000000000004,100.00000000000004,1.0
"def init_logger():     configured_loggers = [log_config.get(""root"", {})] + [         logger for logger in log_config.get(""loggers"", {}).values()     ]     used_handlers = {         handler for log in configured_loggers for handler in log.get(""handlers"", [])     }     for handler_id, handler in list(log_config[""handlers""].items()):         if handler_id not in used_handlers:             del log_config[""handlers""][handler_id]         <MASK>             filename = handler[""filename""]             logfile_path = Path(filename).expanduser().resolve()             handler[""filename""] = str(logfile_path)     logging.config.dictConfig(log_config)","elif ""filename"" in handler . keys ( ) :","if handler [ ""filename"" ] :",16.0529461904344,16.0529461904344,0.0
"def __call__(self):     dmin, dmax = self.viewlim_to_dt()     ymin = self.base.le(dmin.year)     ymax = self.base.ge(dmax.year)     ticks = [dmin.replace(year=ymin, **self.replaced)]     while 1:         dt = ticks[-1]         <MASK>             return date2num(ticks)         year = dt.year + self.base.get_base()         ticks.append(dt.replace(year=year, **self.replaced))",if dt . year >= ymax :,if dt . year == ymin :,38.260294162784454,38.260294162784454,0.0
"def taiga(request, trigger_id, key):     signature = request.META.get(""HTTP_X_TAIGA_WEBHOOK_SIGNATURE"")     # check that the data are ok with the provided signature     if verify_signature(request._request.body, key, signature):         data = data_filter(trigger_id, **request.data)         status = save_data(trigger_id, data)         return (             Response({""message"": ""Success""})             <MASK>             else Response({""message"": ""Failed!""})         )     Response({""message"": ""Bad request""})",if status,if status == 200 :,16.233395773754953,16.233395773754953,0.0
"def ParseResponses(     self,     knowledge_base: rdf_client.KnowledgeBase,     responses: Iterable[rdfvalue.RDFValue], ) -> Iterator[rdf_client.User]:     for response in responses:         <MASK>             raise TypeError(f""Unexpected response type: `{type(response)}`"")         # TODO: `st_mode` has to be an `int`, not `StatMode`.         if stat.S_ISDIR(int(response.st_mode)):             homedir = response.pathspec.path             username = os.path.basename(homedir)             if username not in self._ignore_users:                 yield rdf_client.User(username=username, homedir=homedir)","if not isinstance ( response , rdf_client_fs . StatEntry ) :","if not isinstance ( response , rdfvalue . RDFValue ) :",37.178099888227045,37.178099888227045,0.0
"def _iter_lines(path=path, response=response, max_next=options.http_max_next):     path.responses = []     n = 0     while response:         path.responses.append(response)         yield from response.iter_lines(decode_unicode=True)         src = response.links.get(""next"", {}).get(""url"", None)         if not src:             break         n += 1         <MASK>             vd.warning(f""stopping at max {max_next} pages"")             break         vd.status(f""fetching next page from {src}"")         response = requests.get(src, stream=True)",if n > max_next :,if n >= max_next :,50.000000000000014,50.000000000000014,0.0
"def __enter__(self):     """"""Open a file and read it.""""""     if self.code is None:         LOGGER.info(""File is reading: %s"", self.path)         <MASK>             self._file = open(self.path, encoding=""utf-8"")         else:             self._file = open(self.path, ""rU"")         self.code = self._file.read()     return self","if sys . version_info >= ( 3 , ) :","if self . code == ""utf-8"" :",4.396165418527572,4.396165418527572,0.0
"def facts_for_oauthclients(self, namespace):     """"""Gathers facts for oauthclients used with logging""""""     self.default_keys_for(""oauthclients"")     a_list = self.oc_command(         ""get"", ""oauthclients"", namespace=namespace, add_options=[""-l"", LOGGING_SELECTOR]     )     if len(a_list[""items""]) == 0:         return     for item in a_list[""items""]:         name = item[""metadata""][""name""]         comp = self.comp(name)         <MASK>             result = dict(redirectURIs=item[""redirectURIs""])             self.add_facts_for(comp, ""oauthclients"", name, result)",if comp is not None :,if comp is not None :,100.00000000000004,100.00000000000004,1.0
"def get(self, k):     with self._lock:         <MASK>             self._data1[k] = self._data2[k]             del self._data2[k]     return self._data1.get(k)",if k not in self . _data1 and k in self . _data2 :,if self . _data2 :,15.020723831494387,15.020723831494387,0.0
"def _parseparam(s):     plist = []     while s[:1] == "";"":         s = s[1:]         end = s.find("";"")         while end > 0 and (s.count('""', 0, end) - s.count('\\""', 0, end)) % 2:             end = s.find("";"", end + 1)         <MASK>             end = len(s)         f = s[:end]         if ""="" in f:             i = f.index(""="")             f = f[:i].strip().lower() + ""="" + f[i + 1 :].strip()         plist.append(f.strip())         s = s[end:]     return plist",if end < 0 :,if end < 0 :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, **params):     if ""length"" in params:         <MASK>             raise ValueError(""Supply either length or start and end to Player not both"")         params[""start""] = 0         params[""end""] = params.pop(""length"") - 1     elif params.get(""start"", 0) > 0 and not ""value"" in params:         params[""value""] = params[""start""]     super(Player, self).__init__(**params)","if ""start"" in params or ""end"" in params :","if ""start"" in params :",42.43728456769501,42.43728456769501,0.0
"def libcxx_define(settings):     compiler = _base_compiler(settings)     libcxx = settings.get_safe(""compiler.libcxx"")     if not compiler or not libcxx:         return """"     if str(compiler) in GCC_LIKE:         if str(libcxx) == ""libstdc++"":             return ""_GLIBCXX_USE_CXX11_ABI=0""         <MASK>             return ""_GLIBCXX_USE_CXX11_ABI=1""     return """"","elif str ( libcxx ) == ""libstdc++11"" :","if str ( libcxx ) == ""libstdc++"" :",73.24967962619755,73.24967962619755,0.0
"def _get_sort_map(tags):     """"""See TAG_TO_SORT""""""     tts = {}     for name, tag in tags.items():         if tag.has_sort:             if tag.user:                 tts[name] = ""%ssort"" % name             <MASK>                 tts[""~%s"" % name] = ""~%ssort"" % name     return tts",if tag . internal :,if tag . user :,42.72870063962342,42.72870063962342,0.0
"def quiet_f(*args):     vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)}     value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation)     if expect_list:         if value.has_form(""List"", None):             value = [extract_pyreal(item) for item in value.leaves]             if any(item is None for item in value):                 return None             return value         else:             return None     else:         value = extract_pyreal(value)         <MASK>             return None         return value",if value is None or isinf ( value ) or isnan ( value ) :,if not value :,1.3439177330889347,1.3439177330889347,0.0
"def on_action_chosen(self, id, action, mark_changed=True):     before = self.set_action(self.current, id, action)     if mark_changed:         <MASK>             # TODO: Maybe better comparison             self.undo.append(UndoRedo(id, before, action))             self.builder.get_object(""btUndo"").set_sensitive(True)         self.on_profile_modified()     else:         self.on_profile_modified(update_ui=False)     return before",if before . to_string ( ) != action . to_string ( ) :,if self . current . id == id :,2.8193843710818896,2.8193843710818896,0.0
"def setUp(self):     super(OperaterTest, self).setUp()     if is_cli:         import clr         self.load_iron_python_test()         <MASK>             clr.AddReference(""System.Drawing.Primitives"")         else:             clr.AddReference(""System.Drawing"")",if is_netcoreapp :,if self . is_cli :,15.619699684601283,15.619699684601283,0.0
"def field_to_field_type(field):     field_type = field[""type""]     if isinstance(field_type, dict):         field_type = field_type[""type""]     if isinstance(field_type, list):         field_type_length = len(field_type)         if field_type_length == 0:             raise Exception(""Zero-length type list encountered, invalid CWL?"")         <MASK>             field_type = field_type[0]     return field_type",elif len ( field_type ) == 1 :,if field_type_length == 1 :,34.3763879968285,34.3763879968285,0.0
"def _flatten(*args):     ahs = set()     if len(args) > 0:         for item in args:             <MASK>                 ahs.add(item)             elif type(item) in (list, tuple, dict, set):                 for ah in item:                     if type(ah) is not ActionHandle:  # pragma:nocover                         raise ActionManagerError(""Bad argument type %s"" % str(ah))                     ahs.add(ah)             else:  # pragma:nocover                 raise ActionManagerError(""Bad argument type %s"" % str(item))     return ahs",if type ( item ) is ActionHandle :,if type ( item ) is ActionHandle :,100.00000000000004,100.00000000000004,1.0
"def _Determine_Do(self):     self.applicable = 1     configTokens = black.configure.items[""configTokens""].Get()     buildFlavour = black.configure.items[""buildFlavour""].Get()     if buildFlavour == ""full"":         self.value = False     else:         self.value = True     for opt, optarg in self.chosenOptions:         <MASK>             if not self.value:                 configTokens.append(""tests"")             self.value = True         elif opt == ""--without-tests"":             if self.value:                 configTokens.append(""notests"")             self.value = False     self.determined = 1","if opt == ""--with-tests"" :","if optarg == ""--tests"" :",27.054113452696992,27.054113452696992,0.0
"def title_by_index(self, trans, index, context):     d_type = self.get_datatype(trans, context)     for i, (composite_name, composite_file) in enumerate(d_type.writable_files.items()):         <MASK>             rval = composite_name             if composite_file.description:                 rval = ""{} ({})"".format(rval, composite_file.description)             if composite_file.optional:                 rval = ""%s [optional]"" % rval             return rval     if index < self.get_file_count(trans, context):         return ""Extra primary file""     return None",if i == index :,if i == index :,100.00000000000004,100.00000000000004,1.0
"def func(x, y):     try:         <MASK>             z = x + 2 * math.sin(y)             return z ** 2         elif x == y:             return 4         else:             return 2 ** 3     except ValueError:         foo = 0         for i in range(4):             foo += i         return foo     except TypeError:         return 42     else:         return 33     finally:         print(""finished"")",if x > y :,if x < y :,30.213753973567677,30.213753973567677,0.0
"def test_suite():     suite = unittest.TestSuite()     for fn in os.listdir(here):         <MASK>             modname = ""distutils.tests."" + fn[:-3]             __import__(modname)             module = sys.modules[modname]             suite.addTest(module.test_suite())     return suite","if fn . startswith ( ""test"" ) and fn . endswith ( "".py"" ) :","if fn [ - 3 ] == ""distutils.tests"" :",4.869426103311578,4.869426103311578,0.0
"def check_stack_names(self, frame, expected):     names = []     while frame:         name = frame.f_code.co_name         # Stop checking frames when we get to our test helper.         <MASK>             break         names.append(name)         frame = frame.f_back     self.assertEqual(names, expected)","if name . startswith ( ""check_"" ) or name . startswith ( ""call_"" ) :",if name not in expected :,1.4746738768025476,1.4746738768025476,0.0
"def leave(self, reason=None):     try:         if self.id.startswith(""C""):             log.info(""Leaving channel %s (%s)"", self, self.id)             self._bot.api_call(""conversations.leave"", data={""channel"": self.id})         else:             log.info(""Leaving group %s (%s)"", self, self.id)             self._bot.api_call(""conversations.leave"", data={""channel"": self.id})     except SlackAPIResponseError as e:         <MASK>             raise RoomError(f""Unable to leave channel. {USER_IS_BOT_HELPTEXT}"")         else:             raise RoomError(e)     self._id = None","if e . error == ""user_is_bot"" :",if e . status == 404 :,12.779458309114789,12.779458309114789,0.0
"def ident(self):     value = self._ident     if value is False:         value = None         # XXX: how will this interact with orig_prefix ?         #      not exposing attrs for now if orig_prefix is set.         <MASK>             wrapped = self.wrapped             ident = getattr(wrapped, ""ident"", None)             if ident is not None:                 value = self._wrap_hash(ident)         self._ident = value     return value",if not self . orig_prefix :,if value is None :,6.9717291216921975,6.9717291216921975,0.0
"def is_ac_power_connected():     for power_source_path in Path(""/sys/class/power_supply/"").iterdir():         try:             with open(power_source_path / ""type"", ""r"") as f:                 <MASK>                     continue             with open(power_source_path / ""online"", ""r"") as f:                 if f.read(1) == ""1"":                     return True         except IOError:             continue     return False","if f . read ( ) . strip ( ) != ""Mains"" :","if f . read ( 1 ) == ""1"" :",30.215132342213096,30.215132342213096,0.0
"def _get_pending_by_app_token(self, app_token):     result = []     with self._pending_lock:         self._remove_stale_pending()         for data in self._pending_decisions:             <MASK>                 result.append(data)     return result",if data . app_token == app_token :,"if data [ ""app_token"" ] == app_token :",44.80304273880272,44.80304273880272,0.0
"def do_create(specific_tables=None, base=Base):     engine = get_engine()     try:         <MASK>             logger.info(                 ""Initializing only a subset of tables as requested: {}"".format(                     specific_tables                 )             )             base.metadata.create_all(engine, tables=specific_tables)         else:             base.metadata.create_all(engine)     except Exception as err:         raise Exception(""could not create/re-create DB tables - exception: "" + str(err))",if specific_tables :,if specific_tables :,100.00000000000004,100.00000000000004,1.0
"def __setitem__(self, ndx, val):     #     # Get the expression data object     #     exprdata = None     if ndx in self._data:         exprdata = self._data[ndx]     else:         _ndx = normalize_index(ndx)         <MASK>             exprdata = self._data[_ndx]     if exprdata is None:         raise KeyError(             ""Cannot set the value of Expression '%s' with ""             ""invalid index '%s'"" % (self.cname(True), str(ndx))         )     #     # Set the value     #     exprdata.set_value(val)",if _ndx in self . _data :,if _ndx in self . _data :,100.00000000000004,100.00000000000004,1.0
"def write(self, *bits):     for bit in bits:         <MASK>             self.bytestream.append(0)         byte = self.bytestream[self.bytenum]         if self.bitnum == 8:             if self.bytenum == len(self.bytestream) - 1:                 byte = 0                 self.bytestream += bytes([byte])             self.bytenum += 1             self.bitnum = 0         mask = 2 ** self.bitnum         if bit:             byte |= mask         else:             byte &= ~mask         self.bytestream[self.bytenum] = byte         self.bitnum += 1",if not self . bytestream :,if bit == 0 :,9.652434877402245,9.652434877402245,0.0
"def terminate_subprocess(proc, timeout=0.1, log=None):     if proc.poll() is None:         <MASK>             log.info(""Sending SIGTERM to %r"", proc)         proc.terminate()         timeout_time = time.time() + timeout         while proc.poll() is None and time.time() < timeout_time:             time.sleep(0.02)         if proc.poll() is None:             if log:                 log.info(""Sending SIGKILL to %r"", proc)             proc.kill()     return proc.returncode",if log :,if log :,100.00000000000004,0.0,1.0
"def mkpanel(color, rows, cols, tly, tlx):     win = curses.newwin(rows, cols, tly, tlx)     pan = panel.new_panel(win)     if curses.has_colors():         <MASK>             fg = curses.COLOR_WHITE         else:             fg = curses.COLOR_BLACK         bg = color         curses.init_pair(color, fg, bg)         win.bkgdset(ord("" ""), curses.color_pair(color))     else:         win.bkgdset(ord("" ""), curses.A_BOLD)     return pan",if color == curses . COLOR_BLUE :,if color == curses . A_BOLD :,55.55238068023578,55.55238068023578,0.0
"def all_words(filename):     start_char = True     for c in characters(filename):         if start_char == True:             word = """"             <MASK>                 # We found the start of a word                 word = c.lower()                 start_char = False             else:                 pass         else:             if c.isalnum():                 word += c.lower()             else:                 # We found end of word, emit it                 start_char = True                 yield word",if c . isalnum ( ) :,if c . isalnum ( ) :,100.00000000000004,100.00000000000004,1.0
"def get_tf_weights_as_numpy(path=""./ckpt/aeslc/model.ckpt-32000"") -> Dict:     init_vars = tf.train.list_variables(path)     tf_weights = {}     ignore_name = [""Adafactor"", ""global_step""]     for name, shape in tqdm(init_vars, desc=""converting tf checkpoint to dict""):         skip_key = any([pat in name for pat in ignore_name])         <MASK>             continue         array = tf.train.load_variable(path, name)         tf_weights[name] = array     return tf_weights",if skip_key :,if skip_key :,100.00000000000004,100.00000000000004,1.0
"def app(scope, receive, send):     while True:         message = await receive()         if message[""type""] == ""websocket.connect"":             await send({""type"": ""websocket.accept""})         <MASK>             pass         elif message[""type""] == ""websocket.disconnect"":             break","elif message [ ""type"" ] == ""websocket.receive"" :","if not message [ ""type"" ] :",31.342758852754873,31.342758852754873,0.0
"def autoload(self):     if self._app.config.THEME == ""auto"":         if sys.platform == ""darwin"":             <MASK>                 theme = DARK             else:                 theme = LIGHT         else:             theme = self.guess_system_theme()             if theme == Dark:                 theme = MacOSDark     else:  # user settings have highest priority         theme = self._app.config.THEME     self.load_theme(theme)",if get_osx_theme ( ) == 1 :,if theme == LIGHT :,7.509307647752128,7.509307647752128,0.0
"def example_reading_spec(self):     data_fields = {""targets"": tf.VarLenFeature(tf.int64)}     <MASK>         data_fields[""inputs""] = tf.VarLenFeature(tf.int64)     if self.packed_length:         if self.has_inputs:             data_fields[""inputs_segmentation""] = tf.VarLenFeature(tf.int64)             data_fields[""inputs_position""] = tf.VarLenFeature(tf.int64)         data_fields[""targets_segmentation""] = tf.VarLenFeature(tf.int64)         data_fields[""targets_position""] = tf.VarLenFeature(tf.int64)     data_items_to_decoders = None     return (data_fields, data_items_to_decoders)",if self . has_inputs :,if self . has_targets :,64.34588841607616,64.34588841607616,0.0
"def _prepare_travel_graph(self):     for op in self.op_dict.values():         op.const = False         if op.node.op in [""Const"", ""Placeholder""]:             op.resolved = True             <MASK>                 op.const = True         else:             op.resolved = False","if op . node . op == ""Const"" :","if op . node . op in [ ""Const"" , ""Placeholder"" ] :",36.821398145189974,36.821398145189974,0.0
"def get_filestream_file_items(self):     data = {}     fs_file_updates = self.get_filestream_file_updates()     for k, v in six.iteritems(fs_file_updates):         l = []         for d in v:             offset = d.get(""offset"")             content = d.get(""content"")             assert offset is not None             assert content is not None             assert offset == 0 or offset == len(l), (k, v, l, d)             <MASK>                 l = []             l.extend(map(json.loads, content))         data[k] = l     return data",if not offset :,if not l :,35.35533905932737,35.35533905932737,0.0
"def _rewrite_exprs(self, table, what):     from ibis.expr.analysis import substitute_parents     what = util.promote_list(what)     all_exprs = []     for expr in what:         <MASK>             all_exprs.extend(expr.exprs())         else:             bound_expr = ir.bind_expr(table, expr)             all_exprs.append(bound_expr)     return [substitute_parents(x, past_projection=False) for x in all_exprs]","if isinstance ( expr , ir . ExprList ) :","if isinstance ( expr , Expr ) :",46.307771619910305,46.307771619910305,0.0
"def _group_by_commit_and_time(self, hits):     result = {}     for hit in hits:         source_hit = hit[""_source""]         key = ""%s_%s"" % (source_hit[""commit_info""][""id""], source_hit[""datetime""])         benchmark = self._benchmark_from_es_record(source_hit)         <MASK>             result[key][""benchmarks""].append(benchmark)         else:             run_info = self._run_info_from_es_record(source_hit)             run_info[""benchmarks""] = [benchmark]             result[key] = run_info     return result",if key in result :,if benchmark :,17.799177396293473,0.0,0.0
"def _build_index(self):     self._index = {}     for start_char, sorted_offsets in self._offsets.items():         self._index[start_char] = {}         for i, offset in enumerate(sorted_offsets.get_offsets()):             identifier = sorted_offsets.get_identifier_by_offset(offset)             <MASK>                 self._index[start_char][identifier[0 : self.index_depth]] = i",if identifier [ 0 : self . index_depth ] not in self . _index [ start_char ] :,if identifier :,0.08017090575578772,0.0,0.0
"def scan_resource_conf(self, conf):     if ""properties"" in conf:         <MASK>             if ""exp"" in conf[""properties""][""attributes""]:                 if conf[""properties""][""attributes""][""exp""]:                     return CheckResult.PASSED     return CheckResult.FAILED","if ""attributes"" in conf [ ""properties"" ] :","if ""attributes"" in conf [ ""properties"" ] :",100.00000000000004,100.00000000000004,1.0
"def _PatchArtifact(self, artifact: rdf_artifacts.Artifact) -> rdf_artifacts.Artifact:     """"""Patches artifact to not contain byte-string source attributes.""""""     patched = False     for source in artifact.sources:         attributes = source.attributes.ToDict()         unicode_attributes = compatibility.UnicodeJson(attributes)         <MASK>             source.attributes = unicode_attributes             patched = True     if patched:         self.DeleteArtifact(str(artifact.name))         self.WriteArtifact(artifact)     return artifact",if attributes != unicode_attributes :,if unicode_attributes :,38.80684294761701,38.80684294761701,0.0
"def edit_file(self, filename):     import subprocess     editor = self.get_editor()     if self.env:         environ = os.environ.copy()         environ.update(self.env)     else:         environ = None     try:         c = subprocess.Popen('%s ""%s""' % (editor, filename), env=environ, shell=True)         exit_code = c.wait()         <MASK>             raise ClickException(""%s: Editing failed!"" % editor)     except OSError as e:         raise ClickException(""%s: Editing failed: %s"" % (editor, e))",if exit_code != 0 :,if exit_code != 0 :,100.00000000000004,100.00000000000004,1.0
"def findControlPointsInMesh(glyph, va, subsegments):     controlPointIndices = np.zeros((len(va), 1))     index = 0     for i, c in enumerate(subsegments):         segmentCount = len(glyph.contours[i].segments) - 1         for j, s in enumerate(c):             <MASK>                 if glyph.contours[i].segments[j].type == ""line"":                     controlPointIndices[index] = 1             index += s[1]     return controlPointIndices",if j < segmentCount :,if s [ 0 ] == segmentCount :,10.552670315936318,10.552670315936318,0.0
"def to_representation(self, value):     old_social_string_fields = [""twitter"", ""github"", ""linkedIn""]     request = self.context.get(""request"")     show_old_format = (         request         and is_deprecated(request.version, self.min_version)         and request.method == ""GET""     )     if show_old_format:         social = value.copy()         for key in old_social_string_fields:             <MASK>                 social[key] = value[key][0]             elif social.get(key) == []:                 social[key] = """"         value = social     return super(SocialField, self).to_representation(value)",if social . get ( key ) :,"if social . get ( key ) == [ ""twitter"" , ""github"" ] :",31.872714733206724,31.872714733206724,0.0
"def iter_raw_frames(path, packet_sizes, ctx):     with open(path, ""rb"") as f:         for i, size in enumerate(packet_sizes):             packet = Packet(size)             read_size = f.readinto(packet)             assert size             assert read_size == size             if not read_size:                 break             for frame in ctx.decode(packet):                 yield frame         while True:             try:                 frames = ctx.decode(None)             except EOFError:                 break             for frame in frames:                 yield frame             <MASK>                 break",if not frames :,if frames is None :,14.058533129758727,14.058533129758727,0.0
"def get_shadows_zip(filename):     import zipfile     shadow_pkgs = set()     with zipfile.ZipFile(filename) as lib_zip:         already_test = []         for fname in lib_zip.namelist():             pname, fname = os.path.split(fname)             if fname or (pname and fname):                 continue             if pname not in already_test and ""/"" not in pname:                 already_test.append(pname)                 <MASK>                     shadow_pkgs.add(pname)     return shadow_pkgs",if is_shadowing ( pname ) :,if not os . path . isdir ( fname ) :,8.913765521398126,8.913765521398126,0.0
"def metrics_to_scalars(self, metrics):     new_metrics = {}     for k, v in metrics.items():         if isinstance(v, torch.Tensor):             v = v.item()         <MASK>             v = self.metrics_to_scalars(v)         new_metrics[k] = v     return new_metrics","if isinstance ( v , dict ) :","if isinstance ( v , torch . Scalar ) :",45.180100180492246,45.180100180492246,0.0
"def insert_resets(f):     newsync = dict()     for k, v in f.sync.items():         <MASK>             newsync[k] = insert_reset(ResetSignal(k), v)         else:             newsync[k] = v     f.sync = newsync",if f . clock_domains [ k ] . rst is not None :,if k in newsync :,1.9026155630072006,1.9026155630072006,0.0
"def get_attached_nodes(self, external_account):     for node in self.get_nodes_with_oauth_grants(external_account):         if node is None:             continue         node_settings = node.get_addon(self.oauth_provider.short_name)         <MASK>             continue         if node_settings.external_account == external_account:             yield node",if node_settings is None :,if node_settings is None :,100.00000000000004,100.00000000000004,1.0
"def visitIf(self, node, scope):     for test, body in node.tests:         if isinstance(test, ast.Const):             if type(test.value) in self._const_types:                 <MASK>                     continue         self.visit(test, scope)         self.visit(body, scope)     if node.else_:         self.visit(node.else_, scope)",if not test . value :,if not body :,21.444097124017667,21.444097124017667,0.0
"def flatten(self):     # this is similar to fill_messages except it uses a list instead     # of a queue to place the messages in.     result = []     channel = await self.messageable._get_channel()     self.channel = channel     while self._get_retrieve():         data = await self._retrieve_messages(self.retrieve)         if len(data) < 100:             self.limit = 0  # terminate the infinite loop         <MASK>             data = reversed(data)         if self._filter:             data = filter(self._filter, data)         for element in data:             result.append(self.state.create_message(channel=channel, data=element))     return result",if self . reverse :,if self . limit > 0 :,26.269098944241588,26.269098944241588,0.0
"def compute(self, x, y=None, targets=None):     if targets is None:         targets = self.out_params     in_params = list(self.in_x)     if len(in_params) == 1:         args = [x]     else:         args = list(zip(*x))     if y is None:         pipe = self.pipe     else:         pipe = self.train_pipe         <MASK>             args.append(y)         else:             args += list(zip(*y))         in_params += self.in_y     return self._compute(*args, pipe=pipe, param_names=in_params, targets=targets)",if len ( self . in_y ) == 1 :,if len ( y ) == 1 :,47.93941561908408,47.93941561908408,0.0
"def _import_top_module(self, name):     # scan sys.path looking for a location in the filesystem that contains     # the module, or an Importer object that can import the module.     for item in sys.path:         <MASK>             module = self.fs_imp.import_from_dir(item, name)         else:             module = item.import_top(name)         if module:             return module     return None","if isinstance ( item , _StringType ) :",if self . fs_imp :,6.495032985064742,6.495032985064742,0.0
"def __getitem__(self, key, _get_mode=False):     if not _get_mode:         if isinstance(key, (int, long)):             return self._list[key]         elif isinstance(key, slice):             return self.__class__(self._list[key])     ikey = key.lower()     for k, v in self._list:         <MASK>             return v     # micro optimization: if we are in get mode we will catch that     # exception one stack level down so we can raise a standard     # key error instead of our special one.     if _get_mode:         raise KeyError()     raise BadRequestKeyError(key)",if k . lower ( ) == ikey :,if ikey == k :,10.968823191200759,10.968823191200759,0.0
"def execute(self, arbiter, props):     watcher = self._get_watcher(arbiter, props.pop(""name""))     action = 0     for key, val in props.get(""options"", {}).items():         if key == ""hooks"":             new_action = 0             for name, _val in val.items():                 action = watcher.set_opt(""hooks.%s"" % name, _val)                 <MASK>                     new_action = 1         else:             new_action = watcher.set_opt(key, val)         if new_action == 1:             action = 1     # trigger needed action     return watcher.do_action(action)",if action == 1 :,if new_action == 0 :,23.356898886410015,23.356898886410015,0.0
"def OnBodyClick(self, event=None):     try:         c = self.c         p = c.currentPosition()         <MASK>             self.OnActivateBody(event=event)         g.doHook(""bodyclick2"", c=c, p=p, v=p, event=event)     except:         g.es_event_exception(""bodyclick"")","if not g . doHook ( ""bodyclick1"" , c = c , p = p , v = p , event = event ) :",if p . is_active ( ) :,1.6983645074511788,1.6983645074511788,0.0
"def _class_weights(spec: config.MetricsSpec) -> Optional[Dict[int, float]]:     """"""Returns class weights associated with AggregationOptions at offset.""""""     if spec.aggregate.HasField(""top_k_list""):         <MASK>             raise ValueError(                 ""class_weights are not supported when top_k_list used: ""                 ""spec={}"".format(spec)             )         return None     return dict(spec.aggregate.class_weights) or None",if spec . aggregate . class_weights :,"if spec . aggregate . HasField ( ""top_k_list"" ) :",24.903286388467727,24.903286388467727,0.0
"def _is_perf_file(file_path):     f = get_file(file_path)     for line in f:         if line[0] == ""#"":             continue         r = event_regexp.search(line)         <MASK>             f.close()             return True         f.close()         return False",if r :,if r :,100.00000000000004,0.0,1.0
"def _get_before_insertion_node(self):     if self._nodes_stack.is_empty():         return None     line = self._nodes_stack.parsed_until_line + 1     node = self._new_module.get_last_leaf()     while True:         parent = node.parent         <MASK>             assert node.end_pos[0] <= line             assert node.end_pos[1] == 0 or ""\n"" in self._prefix             return node         node = parent","if parent . type in ( ""suite"" , ""file_input"" ) :",if node . end_pos is not None :,2.946837805727687,2.946837805727687,0.0
"def PyJsHoisted_parseClassRanges_(this, arguments, var=var):     var = Scope({u""this"": this, u""arguments"": arguments}, var)     var.registers([u""res""])     pass     if var.get(u""current"")(Js(u""]"")):         return Js([])     else:         var.put(u""res"", var.get(u""parseNonemptyClassRanges"")())         <MASK>             var.get(u""bail"")(Js(u""nonEmptyClassRanges""))         return var.get(u""res"")","if var . get ( u""res"" ) . neg ( ) :","if var . get ( u""bail"" ) :",46.60402581158052,46.60402581158052,0.0
"def _recurse_children(self, offset):     """"""Recurses thorugh the available children""""""     while offset < self.obj_offset + self.Length:         item = obj.Object(""VerStruct"", offset=offset, vm=self.obj_vm, parent=self)         <MASK>             raise StopIteration(                 ""Could not recover a key for a child at offset {0}"".format(                     item.obj_offset                 )             )         yield item.get_key(), item.get_children()         offset = self.offset_pad(offset + item.Length)     raise StopIteration(""No children"")",if item . Length < 1 or item . get_key ( ) == None :,if not item . key :,2.762519901039491,2.762519901039491,0.0
"def _adapt_types(self, descr):     names = []     adapted_types = []     for col in descr:         names.append(col[0])         impala_typename = col[1]         typename = udf._impala_to_ibis_type[impala_typename.lower()]         <MASK>             precision, scale = col[4:6]             adapted_types.append(dt.Decimal(precision, scale))         else:             adapted_types.append(typename)     return names, adapted_types","if typename == ""decimal"" :","if isinstance ( typename , dt . Decimal ) :",5.522397783539471,5.522397783539471,0.0
"def sniff(self, filename):     try:         <MASK>             with tarfile.open(filename, ""r"") as temptar:                 for f in temptar:                     if not f.isfile():                         continue                     if f.name.endswith("".fast5""):                         return True                     else:                         return False     except Exception as e:         log.warning(""%s, sniff Exception: %s"", self, e)     return False",if filename and tarfile . is_tarfile ( filename ) :,if self . _is_file :,8.423555525647696,8.423555525647696,0.0
"def getValue(self):     if getattr(self.object, ""type"", """") != ""CURVE"":         return BezierSpline()     evaluatedObject = getEvaluatedID(self.object)     bSplines = evaluatedObject.data.splines     if len(bSplines) > 0:         spline = createSplineFromBlenderSpline(bSplines[0])         # Is None when the spline type is not supported.         if spline is not None:             <MASK>                 spline.transform(evaluatedObject.matrix_world)             return spline     return BezierSpline()",if self . useWorldSpace :,if self . matrix_world is not None :,16.784459625186194,16.784459625186194,0.0
"def escape(text, newline=False):     """"""Escape special html characters.""""""     if isinstance(text, str):         if ""&"" in text:             text = text.replace(""&"", ""&amp;"")         <MASK>             text = text.replace("">"", ""&gt;"")         if ""<"" in text:             text = text.replace(""<"", ""&lt;"")         if '""' in text:             text = text.replace('""', ""&quot;"")         if ""'"" in text:             text = text.replace(""'"", ""&quot;"")         if newline:             if ""\n"" in text:                 text = text.replace(""\n"", ""<br>"")     return text","if "">"" in text :","if "">"" in text :",100.00000000000004,100.00000000000004,1.0
"def _get_ilo_version(self):     try:         self._get_ilo2('<?xml version=""1.0""?><RIBCL VERSION=""2.0""></RIBCL>')     except ResponseError as e:         if hasattr(e, ""code""):             <MASK>                 return 3             if e.code == 501:                 return 1         raise     return 2",if e . code == 405 :,if e . code == 502 :,70.71067811865478,70.71067811865478,0.0
"def convert_path(ctx, tpath):     for points, code in tpath.iter_segments():         if code == Path.MOVETO:             ctx.move_to(*points)         elif code == Path.LINETO:             ctx.line_to(*points)         elif code == Path.CURVE3:             ctx.curve_to(                 points[0], points[1], points[0], points[1], points[2], points[3]             )         elif code == Path.CURVE4:             ctx.curve_to(*points)         <MASK>             ctx.close_path()",elif code == Path . CLOSEPOLY :,if code == Path . CLOSE :,54.10822690539397,54.10822690539397,0.0
"def called_by_shrinker():     frame = sys._getframe(0)     while frame:         fname = frame.f_globals.get(""__file__"", """")         <MASK>             return True         frame = frame.f_back     return False","if os . path . basename ( fname ) == ""shrinker.py"" :","if fname == ""shrinker"" :",14.342880529282967,14.342880529282967,0.0
"def _ensuresyspath(self, ensuremode, path):     if ensuremode:         s = str(path)         if ensuremode == ""append"":             <MASK>                 sys.path.append(s)         else:             if s != sys.path[0]:                 sys.path.insert(0, s)",if s not in sys . path :,if s != sys . path [ 0 ] :,18.36028134946796,18.36028134946796,0.0
"def get_instances(self, region: str, vpc: str):     try:         await self._cache_instances(region)         return [             instance             for instance in self._instances_cache[region]             <MASK>         ]     except Exception as e:         print_exception(f""Failed to get RDS instances: {e}"")         return []","if instance [ ""VpcId"" ] == vpc",if instance . vpc == vpc :,20.130954856436926,20.130954856436926,0.0
def get_and_set_all_disambiguation(self):     all_disambiguations = []     for page in self.pages:         if page.relations.disambiguation_links_norm is not None:             all_disambiguations.extend(page.relations.disambiguation_links_norm)         <MASK>             all_disambiguations.extend(page.relations.disambiguation_links)     return set(all_disambiguations),if page . relations . disambiguation_links is not None :,if page . relations . disambiguation_links is not None :,100.00000000000004,100.00000000000004,1.0
"def __str__(self, prefix="""", printElemNumber=0):     res = """"     cnt = 0     for e in self.options_:         elm = """"         <MASK>             elm = ""(%d)"" % cnt         res += prefix + (""options%s <\n"" % elm)         res += e.__str__(prefix + ""  "", printElemNumber)         res += prefix + "">\n""         cnt += 1     return res",if printElemNumber :,if cnt > 0 :,12.703318703865365,12.703318703865365,0.0
"def pre_save_task(self, task, credentials, verrors):     if task[""attributes""][""encryption""] not in (None, """", ""AES256""):         verrors.add(""encryption"", 'Encryption should be null or ""AES256""')     if not credentials[""attributes""].get(""skip_region"", False):         <MASK>             response = await self.middleware.run_in_thread(                 self._get_client(credentials).get_bucket_location,                 Bucket=task[""attributes""][""bucket""],             )             task[""attributes""][""region""] = response[""LocationConstraint""] or ""us-east-1""","if not credentials [ ""attributes"" ] . get ( ""region"" , """" ) . strip ( ) :",if response :,0.04411974971393308,0.0,0.0
"def get_best_config_reward(self):     """"""Returns the best configuration found so far, as well as the reward associated with this best config.""""""     with self.LOCK:         <MASK>             config_pkl = max(self._results, key=self._results.get)             return pickle.loads(config_pkl), self._results[config_pkl]         else:             return dict(), self._reward_while_pending()",if self . _results :,if self . _results :,100.00000000000004,100.00000000000004,1.0
"def parse_setup_cfg(self):     # type: () -> Dict[STRING_TYPE, Any]     if self.setup_cfg is not None and self.setup_cfg.exists():         contents = self.setup_cfg.read_text()         base_dir = self.setup_cfg.absolute().parent.as_posix()         try:             parsed = setuptools_parse_setup_cfg(self.setup_cfg.as_posix())         except Exception:             <MASK>                 contents = self.setup_cfg.read_bytes()             parsed = parse_setup_cfg(contents, base_dir)         if not parsed:             return {}         return parsed     return {}",if six . PY2 :,if contents :,17.799177396293473,0.0,0.0
"def readall(read_fn, sz):     buff = b""""     have = 0     while have < sz:         chunk = yield from read_fn(sz - have)         have += len(chunk)         buff += chunk         <MASK>             raise TTransportException(                 TTransportException.END_OF_FILE, ""End of file reading from transport""             )     return buff",if len ( chunk ) == 0 :,if have >= sz :,6.4790667469036025,6.4790667469036025,0.0
"def _get_use_previous(     f, ):  # TODO Sort and group features for DateOffset with two different temporal values     if isinstance(f, AggregationFeature) and f.use_previous is not None:         <MASK>             return ("""", -1)         else:             unit = list(f.use_previous.times.keys())[0]             value = f.use_previous.times[unit]             return (unit, value)     else:         return ("""", -1)",if len ( f . use_previous . times . keys ( ) ) > 1 :,"if unit == ""time"" :",1.881555714180042,1.881555714180042,0.0
"def istrue(self):     try:         return self._istrue()     except Exception:         self.exc = sys.exc_info()         <MASK>             msg = [                 "" "" * (self.exc[1].offset + 4) + ""^"",             ]             msg.append(""SyntaxError: invalid syntax"")         else:             msg = traceback.format_exception_only(*self.exc[:2])         pytest.fail(             ""Error evaluating %r expression\n""             ""    %s\n""             ""%s"" % (self.name, self.expr, ""\n"".join(msg)),             pytrace=False,         )","if isinstance ( self . exc [ 1 ] , SyntaxError ) :",if self . exc [ 1 ] . offset < 2 :,41.24914892312113,41.24914892312113,0.0
"def wait_for_crm_operation(operation, crm):     """"""Poll for cloud resource manager operation until finished.""""""     logger.info(         ""wait_for_crm_operation: ""         ""Waiting for operation {} to finish..."".format(operation)     )     for _ in range(MAX_POLLS):         result = crm.operations().get(name=operation[""name""]).execute()         <MASK>             raise Exception(result[""error""])         if ""done"" in result and result[""done""]:             logger.info(""wait_for_crm_operation: Operation done."")             break         time.sleep(POLL_INTERVAL)     return result","if ""error"" in result :","if ""error"" in result :",100.00000000000004,100.00000000000004,1.0
"def cb_blob_detail_from_elem_and_buf(self, elem, buf):     if elem.get(""lang"") != buf.lang:  # multi-lang doc         return ""%s Code in %s"" % (elem.get(""lang""), buf.path)     else:         dir, base = os.path.split(buf.path)         <MASK>             return ""%s (%s)"" % (base, dir)         else:             return base",if dir :,if dir :,100.00000000000004,0.0,1.0
"def removedir(self, path):     # type: (Text) -> None     _path = self.validatepath(path)     if _path == ""/"":         raise errors.RemoveRootError()     with ftp_errors(self, path):         try:             self.ftp.rmd(_encode(_path, self.ftp.encoding))         except error_perm as error:             code, _ = _parse_ftp_error(error)             <MASK>                 if self.isfile(path):                     raise errors.DirectoryExpected(path)                 if not self.isempty(path):                     raise errors.DirectoryNotEmpty(path)             raise  # pragma: no cover","if code == ""550"" :",if code != 0 :,13.83254362586636,13.83254362586636,0.0
"def p_clause(self, node, position):     if isinstance(node, Graph):         self.subjectDone(node)         <MASK>             self.write("" "")         self.write(""{"")         self.depth += 1         serializer = N3Serializer(node, parent=self)         serializer.serialize(self.stream)         self.depth -= 1         self.write(self.indent() + ""}"")         return True     else:         return False",if position is OBJECT :,"if position == ""clause"" :",12.22307556087252,12.22307556087252,0.0
"def get_default_shell_info(shell_name=None, settings=None):     if not shell_name:         settings = settings or load_settings(lazy=True)         shell_name = settings.get(""shell"")         if shell_name:             return shell_name, None         shell_path = os.environ.get(""SHELL"")         <MASK>             shell_name = basepath(shell_path)         else:             shell_name = DEFAULT_SHELL         return shell_name, shell_path     return shell_name, None",if shell_path :,if shell_path :,100.00000000000004,100.00000000000004,1.0
"def GetCategory(self, pidls):     ret = []     for pidl in pidls:         # Why don't we just get the size of the PIDL?         val = self.sf.GetDetailsEx(pidl, PKEY_Sample_AreaSize)         val = int(val)  # it probably came in a VT_BSTR variant         if val < 255 // 3:             cid = IDS_SMALL         <MASK>             cid = IDS_MEDIUM         else:             cid = IDS_LARGE         ret.append(cid)     return ret",elif val < 2 * 255 // 3 :,if val > 255 :,5.171845311465849,5.171845311465849,0.0
"def Tokenize(s):     # type: (str) -> Iterator[Token]     for item in TOKEN_RE.findall(s):         # The type checker can't know the true type of item!         item = cast(TupleStr4, item)         if item[0]:             typ = ""number""             val = item[0]         elif item[1]:             typ = ""name""             val = item[1]         elif item[2]:             typ = item[2]             val = item[2]         <MASK>             typ = item[3]             val = item[3]         yield Token(typ, val)",elif item [ 3 ] :,if item [ 3 ] :,75.98356856515926,75.98356856515926,0.0
"def add_package_declarations(generated_root_path):     file_names = os.listdir(generated_root_path)     for file_name in file_names:         <MASK>             continue         full_name = os.path.join(generated_root_path, file_name)         add_package(full_name)","if not file_name . endswith ( "".java"" ) :",if not os . path . isfile ( file_name ) :,17.108853604910248,17.108853604910248,0.0
"def _call_with_retry(out, retry, retry_wait, method, *args, **kwargs):     for counter in range(retry + 1):         try:             return method(*args, **kwargs)         except (             NotFoundException,             ForbiddenException,             AuthenticationException,             RequestErrorException,         ):             raise         except ConanException as exc:             <MASK>                 raise             else:                 if out:                     out.error(exc)                     out.info(""Waiting %d seconds to retry..."" % retry_wait)                 time.sleep(retry_wait)",if counter == retry :,if counter == retry_wait :,54.10822690539397,54.10822690539397,0.0
"def to_wburl_str(     url, type=BaseWbUrl.LATEST_REPLAY, mod="""", timestamp="""", end_timestamp="""" ):     if WbUrl.is_query_type(type):         tsmod = """"         <MASK>             tsmod += mod + ""/""         tsmod += timestamp         tsmod += ""*""         tsmod += end_timestamp         tsmod += ""/"" + url         if type == BaseWbUrl.URL_QUERY:             tsmod += ""*""         return tsmod     else:         tsmod = timestamp + mod         if len(tsmod) > 0:             return tsmod + ""/"" + url         else:             return url",if mod :,if len ( tsmod ) > 0 :,6.567274736060395,6.567274736060395,0.0
"def _configured_ploidy(items):     ploidies = collections.defaultdict(set)     for data in items:         ploidy = dd.get_ploidy(data)         <MASK>             for k, v in ploidy.items():                 ploidies[k].add(v)         else:             ploidies[""default""].add(ploidy)     out = {}     for k, vs in ploidies.items():         assert len(vs) == 1, ""Multiple ploidies set for group calling: %s %s"" % (             k,             list(vs),         )         out[k] = vs.pop()     return out","if isinstance ( ploidy , dict ) :",if ploidy :,7.49553326588684,0.0,0.0
"def removeUser(self, username):     hideFromOSD = not constants.SHOW_DIFFERENT_ROOM_OSD     if username in self._users:         user = self._users[username]         <MASK>             if self.isRoomSame(user.room):                 hideFromOSD = not constants.SHOW_SAME_ROOM_OSD     if username in self._users:         self._users.pop(username)         message = getMessage(""left-notification"").format(username)         self.ui.showMessage(message, hideFromOSD)         self._client.lastLeftTime = time.time()         self._client.lastLeftUser = username     self.userListChange()",if user . room :,if user . room :,100.00000000000004,100.00000000000004,1.0
"def _thd_cleanup_instance(self):     container_name = self.getContainerName()     instances = self.client.containers(all=1, filters=dict(name=container_name))     for instance in instances:         # hyper filtering will match 'hyper12"" if you search for 'hyper1' !         <MASK>             continue         try:             self.client.remove_container(instance[""Id""], v=True, force=True)         except NotFound:             pass  # that's a race condition         except docker.errors.APIError as e:             if ""Conflict operation on container"" not in str(e):                 raise","if """" . join ( instance [ ""Names"" ] ) . strip ( ""/"" ) != container_name :","if instance [ ""Id"" ] != ""hyper12"" :",6.961164030105808,6.961164030105808,0.0
"def handle_ctcp(self, conn, evt):     args = evt.arguments()     source = evt.source().split(""!"")[0]     if args:         if args[0] == ""VERSION"":             conn.ctcp_reply(source, ""VERSION "" + BOT_VERSION)         <MASK>             conn.ctcp_reply(source, ""PING"")         elif args[0] == ""CLIENTINFO"":             conn.ctcp_reply(source, ""CLIENTINFO PING VERSION CLIENTINFO"")","elif args [ 0 ] == ""PING"" :","if args [ 0 ] == ""PING"" :",89.31539818068698,89.31539818068698,0.0
"def new_func(self, *args, **kwargs):     obj = self.obj_ref()     attr = self.attr     if obj is not None:         args = tuple(TrackedValue.make(obj, attr, arg) for arg in args)         <MASK>             kwargs = {                 key: TrackedValue.make(obj, attr, value)                 for key, value in iteritems(kwargs)             }     result = func(self, *args, **kwargs)     self._changed_()     return result",if kwargs :,if obj is not None :,9.652434877402245,9.652434877402245,0.0
"def add_doc(target, variables, body_lines):     if isinstance(target, ast.Name):         # if it is a variable name add it to the doc         name = target.id         <MASK>             doc = find_doc_for(target, body_lines)             if doc is not None:                 variables[name] = doc     elif isinstance(target, ast.Tuple):         # if it is a tuple then iterate the elements         # this can happen like this:         # a, b = 1, 2         for e in target.elts:             add_doc(e, variables, body_lines)",if name not in variables :,if name in variables :,40.93653765389909,40.93653765389909,0.0
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout):     try:         if tp == ""write"":             out.write(msg)         elif tp == ""flush"":             out.flush()         <MASK>             out.write(msg)             out.flush()         elif tp == ""print"":             print(msg, file=out)         else:             raise ValueError(""Unsupported type: "" + tp)     except IOError as e:         logger.critical(""{}: {}"".format(type(e).__name__, ucd(e)))         pass","elif tp == ""write_flush"" :","if tp == ""write"" :",46.307771619910305,46.307771619910305,0.0
"def get_files(d):     res = []     for p in glob.glob(os.path.join(d, ""*"")):         if not p:             continue         (pth, fname) = os.path.split(p)         <MASK>             continue         if os.path.islink(p):             continue         if os.path.isdir(p):             res += get_dir(p)         else:             res.append(p)     return res",if skip_file ( fname ) :,if not os . path . isfile ( p ) :,8.913765521398126,8.913765521398126,0.0
"def _list_outputs(self):     outputs = super(VolSymm, self)._list_outputs()     # Have to manually check for the grid files.     if os.path.exists(outputs[""trans_file""]):         <MASK>             outputs[""output_grid""] = re.sub(                 "".(nlxfm|xfm)$"", ""_grid_0.mnc"", outputs[""trans_file""]             )     return outputs","if ""grid"" in open ( outputs [ ""trans_file"" ] , ""r"" ) . read ( ) :","if not outputs [ ""output_grid"" ] :",6.115027287597785,6.115027287597785,0.0
"def _set_texture(self, texture):     if texture.id is not self._texture.id:         self._group = SpriteGroup(             texture, self._group.blend_src, self._group.blend_dest, self._group.parent         )         <MASK>             self._vertex_list.tex_coords[:] = texture.tex_coords         else:             self._vertex_list.delete()             self._texture = texture             self._create_vertex_list()     else:         self._vertex_list.tex_coords[:] = texture.tex_coords     self._texture = texture",if self . _batch is None :,if self . _texture . id == self . _texture . id :,16.188613565728215,16.188613565728215,0.0
"def got_result(result):     deployment = self.persistence_service.get()     for node in deployment.nodes:         <MASK>             dataset_ids = [                 (m.dataset.deleted, m.dataset.dataset_id)                 for m in node.manifestations.values()             ]             self.assertIn((True, expected_dataset_id), dataset_ids)             break     else:         self.fail(""Node not found. {}"".format(node.uuid))","if same_node ( node , origin ) :",if node . dataset_id in result :,6.033504141761816,6.033504141761816,0.0
"def check_result(result, func, arguments):     if check_warning(result) and (result.value != ReturnCode.WARN_NODATA):         log.warning(UcanWarning(result, func, arguments))     elif check_error(result):         <MASK>             raise UcanCmdError(result, func, arguments)         else:             raise UcanError(result, func, arguments)     return result",if check_error_cmd ( result ) :,if result . value != ReturnCode . ERROR :,5.522397783539471,5.522397783539471,0.0
"def _compress_and_sort_bdg_files(out_dir, data):     for fn in glob.glob(os.path.join(out_dir, ""*bdg"")):         out_file = fn + "".gz""         <MASK>             continue         bedtools = config_utils.get_program(""bedtools"", data)         with file_transaction(out_file) as tx_out_file:             cmd = f""sort -k1,1 -k2,2n {fn} | bgzip -c > {tx_out_file}""             message = f""Compressing and sorting {fn}.""             do.run(cmd, message)",if utils . file_exists ( out_file ) :,if not data :,2.570814443273602,2.570814443273602,0.0
"def kill_members(members, sig, hosts=nodes):     for member in sorted(members):         try:             if ha_tools_debug:                 print(""killing %s"" % member)             proc = hosts[member][""proc""]             # Not sure if cygwin makes sense here...             <MASK>                 os.kill(proc.pid, signal.CTRL_C_EVENT)             else:                 os.kill(proc.pid, sig)         except OSError:             if ha_tools_debug:                 print(""%s already dead?"" % member)","if sys . platform in ( ""win32"" , ""cygwin"" ) :",if proc . signal == signal . SIGINT :,3.3495035708457803,3.3495035708457803,0.0
"def get_top_level_stats(self):     for func, (cc, nc, tt, ct, callers) in self.stats.items():         self.total_calls += nc         self.prim_calls += cc         self.total_tt += tt         <MASK>             self.top_level[func] = None         if len(func_std_string(func)) > self.max_name_len:             self.max_name_len = len(func_std_string(func))","if ( ""jprofile"" , 0 , ""profiler"" ) in callers :",if func in self . top_level :,3.221515452693472,3.221515452693472,0.0
"def __str__(self):     """"""Only keeps the True values.""""""     result = [""SlicingSpec(""]     if self.entire_dataset:         result.append("" Entire dataset,"")     if self.by_class:         if isinstance(self.by_class, Iterable):             result.append("" Into classes %s,"" % self.by_class)         <MASK>             result.append("" Up to class %d,"" % self.by_class)         else:             result.append("" By classes,"")     if self.by_percentiles:         result.append("" By percentiles,"")     if self.by_classification_correctness:         result.append("" By classification correctness,"")     result.append("")"")     return ""\n"".join(result)","elif isinstance ( self . by_class , int ) :",if self . by_class :,31.499993000872454,31.499993000872454,0.0
"def save_params(self):     if self._save_controller:         if not os.path.exists(self._save_controller):             os.makedirs(self._save_controller)         output_dir = self._save_controller     else:         <MASK>             os.makedirs(""./.rlnas_controller"")         output_dir = ""./.rlnas_controller""     with open(os.path.join(output_dir, ""rlnas.params""), ""wb"") as f:         pickle.dump(self._params_dict, f)     _logger.debug(""Save params done"")","if not os . path . exists ( ""./.rlnas_controller"" ) :",if not os . path . exists ( self . _save_controller ) :,46.98609056567354,46.98609056567354,0.0
"def unexport(self, pin):     with self._lock:         self._pin_refs[pin] -= 1         <MASK>             with io.open(self.path(""unexport""), ""wb"") as f:                 f.write(str(pin).encode(""ascii""))",if self . _pin_refs [ pin ] == 0 :,if pin == self . _pin_refs [ pin ] :,67.25080050576685,67.25080050576685,0.0
"def emit(self, type, info=None):     # Overload emit() to send events to the proxy object at the other end     ev = super().emit(type, info)     if self._has_proxy is True and self._session.status > 0:         # implicit: and self._disposed is False:         if type in self.__proxy_properties__:             self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])         <MASK>             self._session.send_command(""INVOKE"", self._id, ""_emit_at_proxy"", [ev])",elif type in self . __event_types_at_proxy :,if type in self . __proxy_properties__ :,39.740876265191105,39.740876265191105,0.0
"def __call__(self, params):     all_errs = {}     for handler in self.handlers:         out_headers, res, errs = handler(params)         all_errs.update(errs)         <MASK>             return out_headers, res, all_errs     return None, None, all_errs",if res is not None :,if out_headers and res :,8.643019616048525,8.643019616048525,0.0
"def await_test_end(self):     iterations = 0     while True:         <MASK>             self.log.debug(""Await: iteration limit reached"")             return         status = self.master.get_status()         if status.get(""status"") == ""ENDED"":             return         iterations += 1         time.sleep(1.0)",if iterations > 100 :,if iterations >= self . max_iterations :,16.784459625186194,16.784459625186194,0.0
"def _load(self, path: str):     ds = DataSet()     with open(path, ""r"", encoding=""utf-8"") as f:         for line in f:             line = line.strip()             <MASK>                 parts = line.split(""\t"")                 raw_words1 = parts[1]                 raw_words2 = parts[2]                 target = parts[0]                 if raw_words1 and raw_words2 and target:                     ds.append(                         Instance(                             raw_words1=raw_words1, raw_words2=raw_words2, target=target                         )                     )     return ds",if line :,if len ( line ) > 3 :,7.267884212102741,7.267884212102741,0.0
"def avatar_delete(event_id, speaker_id):     if request.method == ""DELETE"":         speaker = (             DataGetter.get_speakers(event_id)             .filter_by(user_id=login.current_user.id, id=speaker_id)             .first()         )         <MASK>             speaker.photo = """"             speaker.small = """"             speaker.thumbnail = """"             speaker.icon = """"             save_to_db(speaker)             return jsonify({""status"": ""ok""})         else:             abort(403)",if speaker :,if speaker :,100.00000000000004,0.0,1.0
"def getline(filename, lineno, *args, **kwargs):     line = py2exe_getline(filename, lineno, *args, **kwargs)     if not line:         try:             with open(filename, ""rb"") as f:                 for i, line in enumerate(f):                     line = line.decode(""utf-8"")                     <MASK>                         break                 else:                     line = """"         except (IOError, OSError):             line = """"     return line",if lineno == i + 1 :,if i == 0 :,14.626134130279034,14.626134130279034,0.0
"def write(self, data):     if not isinstance(data, (bytes, bytearray, memoryview)):         raise TypeError(""data argument must be byte-ish (%r)"", type(data))     if not data:         return     if self._conn_lost:         <MASK>             logger.warning(""socket.send() raised exception."")         self._conn_lost += 1         return     if not self._buffer:         self._loop.add_writer(self._sock_fd, self._write_ready)     # Add it to the buffer.     self._buffer.extend(data)     self._maybe_pause_protocol()",if self . _conn_lost >= constants . LOG_THRESHOLD_FOR_CONNLOST_WRITES :,if self . _loop . is_running ( ) :,11.518272556484067,11.518272556484067,0.0
"def _get_x_for_y(self, xValue, x, y):     # print(""searching ""+x+"" with the value ""+str(xValue)+"" and want to give back ""+y)     if not self.xmlMap:         return 0     x_value = str(xValue)     for anime in self.xmlMap.findall(""anime""):         try:             <MASK>                 return int(anime.get(y, 0))         except ValueError as e:             continue     return 0","if anime . get ( x , False ) == x_value :","if anime . get ( x , 0 ) == x_value :",80.03203203845001,80.03203203845001,0.0
"def _RewriteModinfo(     self,     modinfo,     obj_kernel_version,     this_kernel_version,     info_strings=None,     to_remove=None, ):     new_modinfo = """"     for line in modinfo.split(""\x00""):         if not line:             continue         if to_remove and line.split(""="")[0] == to_remove:             continue         if info_strings is not None:             info_strings.add(line.split(""="")[0])         <MASK>             line = line.replace(obj_kernel_version, this_kernel_version)         new_modinfo += line + ""\x00""     return new_modinfo","if line . startswith ( ""vermagic"" ) :",if obj_kernel_version is not None :,4.990049701936832,4.990049701936832,0.0
"def _score(self, X, y):     for col in self.cols:         # Score the column         X[col] = X[col].map(self.mapping[col])         # Randomization is meaningful only for training data -> we do it only if y is present         <MASK>             random_state_generator = check_random_state(self.random_state)             X[col] = X[col] * random_state_generator.normal(                 1.0, self.sigma, X[col].shape[0]             )     return X",if self . randomized and y is not None :,if y is not None :,40.83056064145291,40.83056064145291,0.0
"def onMouseWheel(self, event):     if self.selectedHuman.isVisible():         zoomOut = event.wheelDelta > 0         <MASK>             zoomOut = not zoomOut         if event.x is not None:             self.modelCamera.mousePickHumanCenter(event.x, event.y)         if zoomOut:             self.zoomOut()         else:             self.zoomIn()","if self . getSetting ( ""invertMouseWheel"" ) :",if zoomOut :,3.361830360737634,0.0,0.0
"def prehook(self, emu, op, eip):     if op in self.badops:         emu.stopEmu()         raise v_exc.BadOpBytes(op.va)     if op.mnem in STOS:         if self.arch == ""i386"":             reg = emu.getRegister(envi.archs.i386.REG_EDI)         <MASK>             reg = emu.getRegister(envi.archs.amd64.REG_RDI)         if self.vw.isValidPointer(reg) and self.vw.getLocation(reg) is None:             self.vw.makePointer(reg, follow=True)","elif self . arch == ""amd64"" :","if self . arch == ""amd64"" :",88.01117367933934,88.01117367933934,0.0
"def callback(actions, form, tablename=None):     if actions:         if tablename and isinstance(actions, dict):             actions = actions.get(tablename, [])         <MASK>             actions = [actions]         [action(form) for action in actions]","if not isinstance ( actions , ( list , tuple ) ) :",if actions :,1.4157233641833757,0.0,0.0
"def FetchFn(bigger_than_3_only=None, less_than_7_only=None, even_only=None):     result = []     for i in range(10):         # This line introduces a bug.         if bigger_than_3_only and less_than_7_only and i == 4:             continue         if bigger_than_3_only and i <= 3:             continue         if less_than_7_only and i >= 7:             continue         <MASK>             continue         result.append(i)     return result",if even_only and i % 2 != 0 :,if even_only and i != 0 :,64.32188699036833,64.32188699036833,0.0
"def set_trial_values(self, trial_id: int, values: Sequence[float]) -> None:     with self._lock:         cached_trial = self._get_cached_trial(trial_id)         <MASK>             self._check_trial_is_updatable(cached_trial)             updates = self._get_updates(trial_id)             cached_trial.values = values             updates.values = values             return     self._backend._update_trial(trial_id, values=values)",if cached_trial is not None :,if cached_trial is not None :,100.00000000000004,100.00000000000004,1.0
"def _get_label_format(self, workunit):     for label, label_format in self.LABEL_FORMATTING.items():         if workunit.has_label(label):             return label_format     # Recursively look for a setting to suppress child label formatting.     if workunit.parent:         label_format = self._get_label_format(workunit.parent)         if label_format == LabelFormat.CHILD_DOT:             return LabelFormat.DOT         <MASK>             return LabelFormat.SUPPRESS     return LabelFormat.FULL",if label_format == LabelFormat . CHILD_SUPPRESS :,if label_format == LabelFormat .CHILD_SUPPRESS :,100.00000000000004,100.00000000000004,0.0
"def open_session(self, app, request):     sid = request.cookies.get(app.session_cookie_name)     if sid:         stored_session = self.cls.objects(sid=sid).first()         if stored_session:             expiration = stored_session.expiration             if not expiration.tzinfo:                 expiration = expiration.replace(tzinfo=utc)             <MASK>                 return MongoEngineSession(                     initial=stored_session.data, sid=stored_session.sid                 )     return MongoEngineSession(sid=str(uuid.uuid4()))",if expiration > datetime . datetime . utcnow ( ) . replace ( tzinfo = utc ) :,if expiration . is_valid ( ) :,6.075831217041841,6.075831217041841,0.0
"def _manage_torrent_cache(self):     """"""Carry tracker/peer/file lists over to new torrent list""""""     for torrent in self._torrent_cache:         new_torrent = rtorrentlib.common.find_torrent(torrent.info_hash, self.torrents)         <MASK>             new_torrent.files = torrent.files             new_torrent.peers = torrent.peers             new_torrent.trackers = torrent.trackers     self._torrent_cache = self.torrents",if new_torrent is not None :,if new_torrent :,38.80684294761701,38.80684294761701,0.0
"def _clean_regions(items, region):     """"""Intersect region with target file if it exists""""""     variant_regions = bedutils.population_variant_regions(items, merged=True)     with utils.tmpfile() as tx_out_file:         target = subset_variant_regions(variant_regions, region, tx_out_file, items)         if target:             <MASK>                 target = _load_regions(target)             else:                 target = [target]             return target","if isinstance ( target , six . string_types ) and os . path . isfile ( target ) :",if len ( target ) > 0 :,4.599246087297971,4.599246087297971,0.0
def _get_stdout(self):     while True:         BUFFER_SIZE = 1000         stdout_buffer = self.kernel.process.GetSTDOUT(BUFFER_SIZE)         <MASK>             break         yield stdout_buffer,if len ( stdout_buffer ) == 0 :,if stdout_buffer is None :,15.685718045401451,15.685718045401451,0.0
"def do_query(data, q):     ret = []     if not q:         return ret     qkey = q[0]     for key, value in iterate(data):         if len(q) == 1:             <MASK>                 ret.append(value)             elif is_iterable(value):                 ret.extend(do_query(value, q))         else:             if not is_iterable(value):                 continue             if key == qkey:                 ret.extend(do_query(value, q[1:]))             else:                 ret.extend(do_query(value, q))     return ret",if key == qkey :,if key == qkey :,100.00000000000004,100.00000000000004,1.0
"def test_expect_setecho_off(self):     """"""This tests that echo may be toggled off.""""""     p = pexpect.spawn(""cat"", echo=True, timeout=5)     try:         self._expect_echo_toggle(p)     except IOError:         <MASK>             if hasattr(unittest, ""SkipTest""):                 raise unittest.SkipTest(""Not supported on this platform."")             return ""skip""         raise","if sys . platform . lower ( ) . startswith ( ""sunos"" ) :",if not p . returncode :,2.017602272943647,2.017602272943647,0.0
"def _resolve_relative_config(dir, config):     # Some code shared between Notebook and NotebookInfo     # Resolve icon, can be relative     icon = config.get(""icon"")     if icon:         <MASK>             icon = File(icon)         else:             icon = dir.resolve_file(icon)     # Resolve document_root, can also be relative     document_root = config.get(""document_root"")     if document_root:         if zim.fs.isabs(document_root) or not dir:             document_root = Dir(document_root)         else:             document_root = dir.resolve_dir(document_root)     return icon, document_root",if zim . fs . isabs ( icon ) or not dir :,if zim . fs . isabs ( icon ) or not dir :,100.00000000000004,100.00000000000004,1.0
"def _providers(self, descriptor):     res = []     for _md in self.metadata.values():         for ent_id, ent_desc in _md.items():             <MASK>                 if ent_id in res:                     # print(""duplicated entity_id: %s"" % res)                     pass                 else:                     res.append(ent_id)     return res",if descriptor in ent_desc :,if ent_desc . entity_id == ent_desc . entity_id :,9.083627868206413,9.083627868206413,0.0
"def poll_ms(self, timeout=-1):     s = bytearray(self.evbuf)     if timeout >= 0:         deadline = utime.ticks_add(utime.ticks_ms(), timeout)     while True:         n = epoll_wait(self.epfd, s, 1, timeout)         if not os.check_error(n):             break         if timeout >= 0:             timeout = utime.ticks_diff(deadline, utime.ticks_ms())             <MASK>                 n = 0                 break     res = []     if n > 0:         vals = struct.unpack(epoll_event, s)         res.append((vals[1], vals[0]))     return res",if timeout < 0 :,if n == 0 :,17.965205598154213,17.965205598154213,0.0
"def banned():     if request.endpoint == ""views.themes"":         return     if authed():         user = get_current_user_attrs()         team = get_current_team_attrs()         if user and user.banned:             return (                 render_template(                     ""errors/403.html"", error=""You have been banned from this CTF""                 ),                 403,             )         <MASK>             return (                 render_template(                     ""errors/403.html"",                     error=""Your team has been banned from this CTF"",                 ),                 403,             )",if team and team . banned :,if team and team . banned :,100.00000000000004,100.00000000000004,1.0
"def _update_read(self):     """"""Update state when there is read event""""""     try:         msg = bytes(self._sock.recv(4096))         if msg:             self.on_message(msg)             return True         # normal close, remote is closed         self.close()     except socket.error as err:         <MASK>             pass         else:             self.on_error(err)     return False","if err . args [ 0 ] in ( errno . EAGAIN , errno . EWOULDBLOCK ) :",if err . errno == socket . ECONNREFUSED :,8.346311569168009,8.346311569168009,0.0
"def update_topic_attr_as_not(modeladmin, request, queryset, attr):     for topic in queryset:         if attr == ""sticky"":             topic.sticky = not topic.sticky         elif attr == ""closed"":             topic.closed = not topic.closed         <MASK>             topic.hidden = not topic.hidden         topic.save()","elif attr == ""hidden"" :","if attr == ""hidden"" :",84.08964152537145,84.08964152537145,0.0
"def Startprobe(self, q):     while not self.finished:         try:             sniff(iface=self.interface, count=10, prn=lambda x: q.put(x))         except:             pass         <MASK>             break",if self . finished :,if self . finished :,100.00000000000004,100.00000000000004,1.0
"def _maybe_female(self, path_elements, female, strict):     if female:         <MASK>             elements = path_elements + [""female""]             try:                 return self._get_file(elements, "".png"", strict=strict)             except ValueError:                 if strict:                     raise         elif strict:             raise ValueError(""Pokemon %s has no gender differences"" % self.species_id)     return self._get_file(path_elements, "".png"", strict=strict)",if self . has_gender_differences :,if self . species_id in path_elements :,16.59038701421971,16.59038701421971,0.0
"def change_args_to_dict(string):     if string is None:         return None     ans = []     strings = string.split(""\n"")     ind = 1     start = 0     while ind <= len(strings):         <MASK>             ind += 1         else:             if start < ind:                 ans.append(""\n"".join(strings[start:ind]))             start = ind             ind += 1     d = {}     for line in ans:         if "":"" in line and len(line) > 0:             lines = line.split("":"")             d[lines[0]] = lines[1].strip()     return d","if ind < len ( strings ) and strings [ ind ] . startswith ( "" "" ) :",if start > ind :,0.8549002800393298,0.8549002800393298,0.0
"def _send_with_auth(self, req_kwargs, desired_auth, rsession):     if desired_auth.oauth:         <MASK>             self._oauth_creds.refresh(httplib2.Http())         req_kwargs[""headers""] = req_kwargs.get(""headers"", {})         req_kwargs[""headers""][""Authorization""] = (             ""Bearer "" + self._oauth_creds.access_token         )     return rsession.request(**req_kwargs)",if self . _oauth_creds . access_token_expired :,if desired_auth . oauth :,3.612710856430898,3.612710856430898,0.0
"def parse_search_response(json_data):     """"""Construct response for any input""""""     if json_data is None:         return {""error"": ""Error parsing empty search engine response""}     try:         return json.loads(json_data)     except json.JSONDecodeError:         logger.exception(""Error parsing search engine response"")         m = re_pre.search(json_data)         <MASK>             return {""error"": ""Error parsing search engine response""}         error = web.htmlunquote(m.group(1))         solr_error = ""org.apache.lucene.queryParser.ParseException: ""         if error.startswith(solr_error):             error = error[len(solr_error) :]         return {""error"": error}",if m is None :,if not m :,16.37226966703825,16.37226966703825,0.0
"def wrapper(*args, **kws):     missing = []     saved = getattr(warnings, ""__warningregistry__"", missing).copy()     try:         return func(*args, **kws)     finally:         <MASK>             try:                 del warnings.__warningregistry__             except AttributeError:                 pass         else:             warnings.__warningregistry__ = saved",if saved is missing :,if saved is not saved :,32.46679154750991,32.46679154750991,0.0
"def parse_expression(self):     """"""Return string containing command to run.""""""     expression_el = self.root.find(""expression"")     if expression_el is not None:         expression_type = expression_el.get(""type"")         <MASK>             raise Exception(                 ""Unknown expression type [%s] encountered"" % expression_type             )         return expression_el.text     return None","if expression_type != ""ecma5.1"" :",if not expression_type :,17.625328548379716,17.625328548379716,0.0
"def test_geocode():     # look for tweets from New York ; the search radius is larger than NYC     # so hopefully we'll find one from New York in the first 500?     count = 0     found = False     for tweet in T.search(None, geocode=""40.7484,-73.9857,1mi""):         <MASK>             found = True             break         if count > 500:             break         count += 1     assert found","if ( tweet [ ""place"" ] or { } ) . get ( ""name"" ) == ""Manhattan"" :",if count == 0 :,0.8135814099134429,0.8135814099134429,0.0
"def __init__(self, name: Optional[str] = None, order: int = 0):     if name is None:         <MASK>             name = ""std_dev""         elif order == 1:             name = ""sample_std_dev""         else:             name = f""std_dev{order})""     super().__init__(name=name, order=order)     self.order = order",if order == 0 :,if order == 0 :,100.00000000000004,100.00000000000004,1.0
"def __cmp__(self, other):     if isinstance(other, date) or isinstance(other, datetime):         a = self._d.getTime()         b = other._d.getTime()         <MASK>             return -1         elif a == b:             return 0     else:         raise TypeError(""expected date or datetime object"")     return 1",if a < b :,if a > b :,30.213753973567677,30.213753973567677,0.0
"def run(self):     tid = self.ident     try:         with self._lock:             _GUIS[tid] = self             self._state(True)         self.new_mail_notifications(summarize=True)         loop_count = 0         while self._sock:             loop_count += 1             self._select_sleep(1)  # FIXME: Lengthen this when possible             self.change_state()             <MASK>                 # FIXME: This involves a fair number of set operations,                 #        should only do this after new mail has arrived.                 self.new_mail_notifications()     finally:         del _GUIS[tid]",if loop_count % 5 == 0 :,if loop_count > 0 :,31.850355294022695,31.850355294022695,0.0
"def __cache_dimension_masks(self, *args):     # cache masks for each feature map we'll need     if len(self.masks) == 0:         for m1 in args:             batch_size, emb_dim, h, w = m1.size()             # make mask             <MASK>                 mask = self.feat_size_w_mask(h, m1)                 self.masks[h] = mask",if h not in self . masks :,if batch_size == self . batch_size :,8.054496384843702,8.054496384843702,0.0
"def __call__(self, *flattened_representation):     unflattened_representation = []     for index, subtree in self.children:         <MASK>             unflattened_representation.append(flattened_representation[index])         else:             sub_representation = flattened_representation[index]             unflattened_representation.append(subtree(*sub_representation))     return self._cls(*unflattened_representation, **self._kwargs)",if subtree is None :,if subtree . is_unflattened ( ) :,11.339582221952005,11.339582221952005,0.0
"def click_outside(event):     if event not in d:         x, y, z = self.blockFaceUnderCursor[0]         if y == 0:             y = 64         y += 3         gotoPanel.X, gotoPanel.Y, gotoPanel.Z = x, y, z         <MASK>             d.dismiss(""Goto"")",if event . num_clicks == 2 :,if x == 0 :,9.911450612811139,9.911450612811139,0.0
"def get_mapped_input_keysequences(self, mode=""global"", prefix=u""""):     # get all bindings in this mode     globalmaps, modemaps = self.get_keybindings(mode)     candidates = list(globalmaps.keys()) + list(modemaps.keys())     if prefix is not None:         prefixes = prefix + "" ""         cand = [c for c in candidates if c.startswith(prefixes)]         <MASK>             candidates = cand + [prefix]         else:             candidates = cand     return candidates",if prefix in candidates :,if prefix in candidates :,100.00000000000004,100.00000000000004,1.0
"def _set_length(self, length):     with self._cond:         self._length = length         <MASK>             self._ready = True             self._cond.notify()             del self._cache[self._job]",if self . _index == self . _length :,if self . _job in self . _cache :,32.03558799120807,32.03558799120807,0.0
"def _pct_encoded_replace_unreserved(mo):     try:         i = int(mo.group(1), 16)         <MASK>             return chr(i)         else:             return mo.group().upper()     except ValueError:         return mo.group()",if _unreserved [ i ] :,if i > 0 :,9.423716574733431,9.423716574733431,0.0
"def is_open(self):     if self.signup_code:         return True     else:         if self.signup_code_present:             <MASK>                 messages.add_message(                     self.request,                     self.messages[""invalid_signup_code""][""level""],                     self.messages[""invalid_signup_code""][""text""].format(                         **{                             ""code"": self.get_code(),                         }                     ),                 )     return settings.ACCOUNT_OPEN_SIGNUP","if self . messages . get ( ""invalid_signup_code"" ) :",if self . signup_code_present :,14.735248042173795,14.735248042173795,0.0
"def _get_field_value(self, test, key, match):     if test.ver == ofproto_v1_0.OFP_VERSION:         members = inspect.getmembers(match)         for member in members:             <MASK>                 field_value = member[1]             elif member[0] == ""wildcards"":                 wildcards = member[1]         if key == ""nw_src"":             field_value = test.nw_src_to_str(wildcards, field_value)         elif key == ""nw_dst"":             field_value = test.nw_dst_to_str(wildcards, field_value)     else:         field_value = match[key]     return field_value",if member [ 0 ] == key :,"if member [ 0 ] == ""value"" :",59.00468726392806,59.00468726392806,0.0
"def move_sender_strings_to_sender_model(apps, schema_editor):     sender_model = apps.get_model(""documents"", ""Sender"")     document_model = apps.get_model(""documents"", ""Document"")     # Create the sender and log the relationship with the document     for document in document_model.objects.all():         <MASK>             (                 DOCUMENT_SENDER_MAP[document.pk],                 created,             ) = sender_model.objects.get_or_create(                 name=document.sender, defaults={""slug"": slugify(document.sender)}             )",if document . sender :,if document . sender :,100.00000000000004,100.00000000000004,1.0
"def compute_output_shape(self, input_shape):     if None not in input_shape[1:]:         <MASK>             total = np.prod(input_shape[2:4]) * self.num_anchors         else:             total = np.prod(input_shape[1:3]) * self.num_anchors         return (input_shape[0], total, 4)     else:         return (input_shape[0], None, 4)","if keras . backend . image_data_format ( ) == ""channels_first"" :",if self . num_anchors > 0 :,1.9861872553779454,1.9861872553779454,0.0
"def decompress(self, value):     if value:         if type(value) == PhoneNumber:             <MASK>                 return [                     ""+%d"" % value.country_code,                     national_significant_number(value),                 ]         else:             return value.split(""."")     return [None, """"]",if value . country_code and value . national_number :,if value . country_code :,35.685360466076496,35.685360466076496,0.0
"def ignore(self, other):     if isinstance(other, Suppress):         <MASK>             super(ParseElementEnhance, self).ignore(other)             if self.expr is not None:                 self.expr.ignore(self.ignoreExprs[-1])     else:         super(ParseElementEnhance, self).ignore(other)         if self.expr is not None:             self.expr.ignore(self.ignoreExprs[-1])     return self",if other not in self . ignoreExprs :,"if isinstance ( self , ParseElement ) :",7.267884212102741,7.267884212102741,0.0
"def mkdir(self, mode=0o777, parents=False, exist_ok=False):     if self._closed:         self._raise_closed()     if not parents:         try:             self._accessor.mkdir(self, mode)         except FileExistsError:             <MASK>                 raise     else:         try:             self._accessor.mkdir(self, mode)         except FileExistsError:             if not exist_ok or not self.is_dir():                 raise         except OSError as e:             if e.errno != ENOENT:                 raise             self.parent.mkdir(parents=True)             self._accessor.mkdir(self, mode)",if not exist_ok or not self . is_dir ( ) :,if not exist_ok :,17.744888507115053,17.744888507115053,0.0
"def _mark_lcs(mask, dirs, m, n):     while m != 0 and n != 0:         if dirs[m, n] == ""|"":             m -= 1             n -= 1             mask[m] = 1         elif dirs[m, n] == ""^"":             m -= 1         <MASK>             n -= 1         else:             raise UnboundLocalError(""Illegal move"")     return mask","elif dirs [ m , n ] == ""<"" :","if dirs [ n , m ] == ""^"" :",28.295596283263514,28.295596283263514,0.0
"def clean(self, *args, **kwargs):     data = super().clean(*args, **kwargs)     if isinstance(data, File):         filename = data.name         ext = os.path.splitext(filename)[1]         ext = ext.lower()         <MASK>             raise forms.ValidationError(_(""Filetype not allowed!""))     return data",if ext not in self . ext_whitelist :,"if not ext . startswith ( "".txt"" ) :",5.063996506781411,5.063996506781411,0.0
"def get_doc_object(obj, what=None):     if what is None:         if inspect.isclass(obj):             what = ""class""         <MASK>             what = ""module""         elif callable(obj):             what = ""function""         else:             what = ""object""     if what == ""class"":         return SphinxClassDoc(obj, """", func_doc=SphinxFunctionDoc)     elif what in (""function"", ""method""):         return SphinxFunctionDoc(obj, """")     else:         return SphinxDocString(pydoc.getdoc(obj))",elif inspect . ismodule ( obj ) :,if obj . __module__ is not None :,4.456882760699063,4.456882760699063,0.0
"def apply_pssm(val):     if val is not None:         val_c = PSSM_VALUES.get(val, None)         <MASK>             assert isinstance(                 val, tuple(PSSM_VALUES.values())             ), ""'store_as' should be one of: %r or an instance of %r not %r"" % (                 tuple(PSSM_VALUES.keys()),                 tuple(PSSM_VALUES.values()),                 val,             )             return val         return val_c()",if val_c is None :,if val_c is not None :,59.4603557501361,59.4603557501361,0.0
"def read_postmaster_opts(self):     """"""returns the list of option names/values from postgres.opts, Empty dict if read failed or no file""""""     result = {}     try:         with open(os.path.join(self._postgresql.data_dir, ""postmaster.opts"")) as f:             data = f.read()             for opt in data.split('"" ""'):                 <MASK>                     name, val = opt.split(""="", 1)                     result[name.strip(""-"")] = val.rstrip('""\n')     except IOError:         logger.exception(""Error when reading postmaster.opts"")     return result","if ""="" in opt and opt . startswith ( ""--"" ) :","if opt . startswith ( ""-"" ) :",29.695224534096084,29.695224534096084,0.0
"def detect(get_page):     retval = False     for vector in WAF_ATTACK_VECTORS:         page, headers, code = get_page(get=vector)         retval = (             re.search(r""F5-TrafficShield"", headers.get(HTTP_HEADER.SERVER, """"), re.I)             is not None         )         retval |= (             re.search(r""\AASINFO="", headers.get(HTTP_HEADER.SET_COOKIE, """"), re.I)             is not None         )         <MASK>             break     return retval",if retval :,if code == 0 :,9.652434877402245,9.652434877402245,0.0
"def on_task_start(self, task, config):     for item in config:         for plugin_name, plugin_config in item.items():             try:                 thelist = plugin.get(plugin_name, self).get_list(plugin_config)             except AttributeError:                 raise PluginError(                     ""Plugin %s does not support list interface"" % plugin_name                 )             <MASK>                 raise plugin.PluginError(thelist.immutable)",if thelist . immutable :,if thelist . immutable :,100.00000000000004,100.00000000000004,1.0
"def nq(t):     p = t[0] if (t and t[0] in ""-+"") else """"     t = t[len(p) :]     if t.startswith(""tag:"") or t.startswith(""in:""):         try:             raw_tag = session.config.get_tag(t.split("":"")[1])             <MASK>                 t = ""in:%s"" % raw_tag.slug         except (IndexError, KeyError, TypeError):             pass     return p + t",if raw_tag and raw_tag . hasattr ( slug ) :,if raw_tag :,11.688396478408103,11.688396478408103,0.0
"def _recur_strip(s):     if is_str(s):         <MASK>             return "" "".join(s.strip().split())         else:             return "" "".join(s.strip().split()).replace(bos_token + "" "", """")     else:         s_ = [_recur_strip(si) for si in s]         return _maybe_list_to_array(s_, s)","if bos_token == """" :","if bos_token in [ ""bos"" , ""bos"" , ""bos"" ] :",14.576846149722611,14.576846149722611,0.0
"def __delitem__(self, key):     ""Deleting tag[key] deletes all 'key' attributes for the tag.""     for item in self.attrs:         <MASK>             self.attrs.remove(item)             # We don't break because bad HTML can define the same             # attribute multiple times.         self._getAttrMap()         if self.attrMap.has_key(key):             del self.attrMap[key]",if item [ 0 ] == key :,if item . startswith ( key ) :,11.59119922599073,11.59119922599073,0.0
"def comment_import_help(init_file, out_file):     f_out = open(out_file, ""w"")     output = """"     updated = False     with open(init_file, ""r"") as f_in:         for line in f_in:             <MASK>                 updated = True                 line = ""# "" + line             output += line     f_out.write(output)     f_out.close()     return updated","if ""import"" in line and ""_help"" in line and not updated :","if line != """" :",2.3534609554455574,2.3534609554455574,0.0
"def prepare_text(lines):     out = []     for s in lines.split(""|""):         s = s.strip()         <MASK>             # line beginning with '/' is in italics             s = r""{\i1}%s{\i0}"" % s[1:].strip()         out.append(s)     return ""\\N"".join(out)","if s . startswith ( ""/"" ) :","if s [ 0 ] == ""/"" :",18.36028134946796,18.36028134946796,0.0
"def sqlctx(sc):     pytest.importorskip(""pyspark"")     from odo.backends.sparksql import HiveContext     try:         yield HiveContext(sc)     finally:         dbpath = ""metastore_db""         logpath = ""derby.log""         <MASK>             assert os.path.isdir(dbpath)             shutil.rmtree(dbpath)         if os.path.exists(logpath):             assert os.path.isfile(logpath)             os.remove(logpath)",if os . path . exists ( dbpath ) :,if os . path . exists ( dbpath ) :,100.00000000000004,100.00000000000004,1.0
"def _user2dict(self, uid):     usdict = None     if uid in self.users:         usdict = self.users[uid]         <MASK>             infos = self.users_info[uid]             for attr in infos:                 usdict[attr[""attr_type""]] = attr[""attr_data""]         usdict[""uid""] = uid     return usdict",if uid in self . users_info :,if uid in self . users_info :,100.00000000000004,100.00000000000004,1.0
"def _validate_options(self):     for option in self.options:         # if value type is bool or int, then we know the options is set         <MASK>             if self.options.required[option] is True and not self.options[option]:                 if option == Constants.PASSWORD_CLEAR:                     option = ""password"".upper()                 raise FrameworkException(                     ""Value required for the '%s' option."" % (option.upper())                 )     return","if not type ( self . options [ option ] ) in [ bool , int ] :",if option in self . options :,6.27685261481145,6.27685261481145,0.0
"def _copy_package_apps(     local_bin_dir: Path, app_paths: List[Path], suffix: str = """" ) -> None:     for src_unresolved in app_paths:         src = src_unresolved.resolve()         app = src.name         dest = Path(local_bin_dir / add_suffix(app, suffix))         <MASK>             mkdir(dest.parent)         if dest.exists():             logger.warning(f""{hazard}  Overwriting file {str(dest)} with {str(src)}"")             dest.unlink()         if src.exists():             shutil.copy(src, dest)",if not dest . parent . is_dir ( ) :,if dest . exists ( ) :,15.749996500436227,15.749996500436227,0.0
"def truncate_seq_pair(tokens_a, tokens_b, max_length):     """"""Truncates a sequence pair in place to the maximum length.""""""     # This is a simple heuristic which will always truncate the longer sequence     # one token at a time. This makes more sense than truncating an equal percent     # of tokens from each, since if one sequence is very short then each token     # that's truncated likely contains more information than a longer sequence.     while True:         total_length = len(tokens_a) + len(tokens_b)         <MASK>             break         if len(tokens_a) > len(tokens_b):             tokens_a.pop()         else:             tokens_b.pop()",if total_length <= max_length :,if total_length > max_length :,53.417359568998464,53.417359568998464,0.0
"def add_channels(cls, voucher, add_channels):     for add_channel in add_channels:         channel = add_channel[""channel""]         defaults = {""currency"": channel.currency_code}         if ""discount_value"" in add_channel.keys():             defaults[""discount_value""] = add_channel.get(""discount_value"")         <MASK>             defaults[""min_spent_amount""] = add_channel.get(""min_amount_spent"", None)         models.VoucherChannelListing.objects.update_or_create(             voucher=voucher,             channel=channel,             defaults=defaults,         )","if ""min_amount_spent"" in add_channel . keys ( ) :","if ""min_amount_spent"" in add_channel . keys ( ) :",100.00000000000004,100.00000000000004,1.0
"def services(self, id=None, name=None):     for service_dict in self.service_ls(id=id, name=name):         service_id = service_dict[""ID""]         service_name = service_dict[""NAME""]         <MASK>             continue         task_list = self.service_ps(service_id)         yield DockerService.from_cli(self, service_dict, task_list)",if not service_name . startswith ( self . _name_prefix ) :,"if service_id == ""default"" or service_name == ""default"" :",8.961672320242714,8.961672320242714,0.0
"def lll(dirname):     for name in os.listdir(dirname):         <MASK>             full = os.path.join(dirname, name)             if os.path.islink(full):                 print(name, ""->"", os.readlink(full))","if name not in ( os . curdir , os . pardir ) :",if os . path . isdir ( name ) :,9.460891720793645,9.460891720793645,0.0
"def convertstore(self, mydict):     targetheader = self.mypofile.header()     targetheader.addnote(""extracted from web2py"", ""developer"")     for source_str in mydict.keys():         target_str = mydict[source_str]         <MASK>             # a convention with new (untranslated) web2py files             target_str = u""""         elif target_str.startswith(u""*** ""):             # an older convention             target_str = u""""         pounit = self.convertunit(source_str, target_str)         self.mypofile.addunit(pounit)     return self.mypofile",if target_str == source_str :,"if target_str . startswith ( u""*** "" ) :",17.395797375642243,17.395797375642243,0.0
"def __init__(self, **kwargs):     for k, v in kwargs.items():         setattr(self, k, v)     self.attempted_charsets = set()     request = cherrypy.serving.request     if request.handler is not None:         # Replace request.handler with self         <MASK>             cherrypy.log(""Replacing request.handler"", ""TOOLS.ENCODE"")         self.oldhandler = request.handler         request.handler = self",if self . debug :,if request . handler == self :,7.809849842300637,7.809849842300637,0.0
"def _fastqc_data_section(self, section_name):     out = []     in_section = False     data_file = os.path.join(self._dir, ""fastqc_data.txt"")     if os.path.exists(data_file):         with open(data_file) as in_handle:             for line in in_handle:                 if line.startswith("">>%s"" % section_name):                     in_section = True                 <MASK>                     if line.startswith("">>END""):                         break                     out.append(line.rstrip(""\r\n""))     return out",elif in_section :,if not in_section :,50.81327481546149,50.81327481546149,0.0
"def bit_length(n):     try:         return n.bit_length()     except AttributeError:         norm = deflate_long(n, False)         hbyte = byte_ord(norm[0])         <MASK>             return 1         bitlen = len(norm) * 8         while not (hbyte & 0x80):             hbyte <<= 1             bitlen -= 1         return bitlen",if hbyte == 0 :,if hbyte == 0 :,100.00000000000004,100.00000000000004,1.0
"def step(self, action):     """"""Repeat action, sum reward, and max over last observations.""""""     total_reward = 0.0     done = None     for i in range(self._skip):         obs, reward, done, info = self.env.step(action)         if i == self._skip - 2:             self._obs_buffer[0] = obs         if i == self._skip - 1:             self._obs_buffer[1] = obs         total_reward += reward         <MASK>             break     # Note that the observation on the done=True frame     # doesn't matter     max_frame = self._obs_buffer.max(axis=0)     return max_frame, total_reward, done, info",if done :,if done :,100.00000000000004,0.0,1.0
"def _sample_translation(reference, max_len):     translation = reference[:]     while np.random.uniform() < 0.8 and 1 < len(translation) < max_len:         trans_len = len(translation)         ind = np.random.randint(trans_len)         action = np.random.choice(actions)         <MASK>             del translation[ind]         elif action == ""replacement"":             ind_rep = np.random.randint(trans_len)             translation[ind] = translation[ind_rep]         else:             ind_insert = np.random.randint(trans_len)             translation.insert(ind, translation[ind_insert])     return translation","if action == ""deletion"" :","if action == ""delete"" :",59.4603557501361,59.4603557501361,0.0
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x):     sign = None     subseq = []     for i in seq:         ki = key(i)         if sign is None:             subseq.append(i)             <MASK>                 sign = ki / abs(ki)         else:             subseq.append(i)             if sign * ki < -slop:                 sign = ki / abs(ki)                 yield subseq                 subseq = [i]     if subseq:         yield subseq",if ki != 0 :,if sign * ki < -slop :,8.643019616048525,8.643019616048525,0.0
def get_dirlist(_rootdir):     dirlist = []     with os.scandir(_rootdir) as rit:         for entry in rit:             <MASK>                 dirlist.append(entry.path)                 dirlist += get_dirlist(entry.path)     return dirlist,"if not entry . name . startswith ( ""."" ) and entry . is_dir ( ) :",if entry . path in dirlist :,2.1138964808863445,2.1138964808863445,0.0
"def __init__(     self,     fixed: MQTTFixedHeader = None,     variable_header: PublishVariableHeader = None,     payload=None, ):     if fixed is None:         header = MQTTFixedHeader(PUBLISH, 0x00)     else:         <MASK>             raise HBMQTTException(                 ""Invalid fixed packet type %s for PublishPacket init""                 % fixed.packet_type             )         header = fixed     super().__init__(header)     self.variable_header = variable_header     self.payload = payload",if fixed . packet_type is not PUBLISH :,if fixed . packet_type != MQTTFixedHeader . PUB :,43.36189090348677,43.36189090348677,0.0
"def get_files(d):     res = []     for p in glob.glob(os.path.join(d, ""*"")):         if not p:             continue         (pth, fname) = os.path.split(p)         <MASK>             continue         if fname == ""PureMVC_Python_1_0"":             continue         if fname[-4:] == "".pyc"":  # ehmm.. no.             continue         if os.path.isdir(p):             get_dir(p)         else:             res.append(p)     return res","if fname == ""output"" :","if fname == ""PureMVC_Python_1_1"" :",30.576902884505124,30.576902884505124,0.0
"def reward(self):     """"""Returns a tuple of sum of raw and processed rewards.""""""     raw_rewards, processed_rewards = 0, 0     for ts in self.time_steps:         # NOTE: raw_reward and processed_reward are None for the first time-step.         <MASK>             raw_rewards += ts.raw_reward         if ts.processed_reward is not None:             processed_rewards += ts.processed_reward     return raw_rewards, processed_rewards",if ts . raw_reward is not None :,if ts . raw_reward is not None :,100.00000000000004,100.00000000000004,1.0
"def _process_file(self, content):     args = []     for line in content.splitlines():         line = line.strip()         <MASK>             args.extend(self._split_option(line))         elif line and not line.startswith(""#""):             args.append(line)     return args","if line . startswith ( ""-"" ) :","if line and line . startswith ( ""--"" ) :",45.384078730076105,45.384078730076105,0.0
"def __on_change_button_clicked(self, widget=None):     """"""compute all primary objects and toggle the 'Change' attribute""""""     self.change_status = not self.change_status     for prim_obj, tmp in self.xobjects:         obj_change = self.top.get_object(""%s_change"" % prim_obj)         <MASK>             continue         self.change_entries[prim_obj].set_val(self.change_status)         obj_change.set_active(self.change_status)",if not obj_change . get_sensitive ( ) :,if obj_change is None :,13.597602315271134,13.597602315271134,0.0
"def aiter_cogs(cls) -> AsyncIterator[Tuple[str, str]]:     yield ""Core"", ""0""     for _dir in data_manager.cog_data_path().iterdir():         fpath = _dir / ""settings.json""         if not fpath.exists():             continue         with fpath.open() as f:             try:                 data = json.load(f)             except json.JSONDecodeError:                 continue         <MASK>             continue         cog_name = _dir.stem         for cog_id, inner in data.items():             if not isinstance(inner, dict):                 continue             yield cog_name, cog_id","if not isinstance ( data , dict ) :",if not data :,10.88482843823664,10.88482843823664,0.0
"def _verifySubs(self):     for inst in self.subs:         if not isinstance(inst, (_Block, _Instantiator, Cosimulation)):             raise BlockError(_error.ArgType % (self.name,))         if isinstance(inst, (_Block, _Instantiator)):             <MASK>                 raise BlockError(_error.InstanceError % (self.name, inst.callername))",if not inst . modctxt :,if not inst . is_instance :,36.55552228545123,36.55552228545123,0.0
"def _is_xml(accepts):     if accepts.startswith(b""application/""):         has_xml = accepts.find(b""xml"")         if has_xml > 0:             semicolon = accepts.find(b"";"")             <MASK>                 return True     return False",if semicolon < 0 or has_xml < semicolon :,if semicolon > 0 :,7.652332131360532,7.652332131360532,0.0
"def _accept_with(cls, orm, target):     if target is orm.mapper:         return mapperlib.Mapper     elif isinstance(target, type):         <MASK>             return target         else:             mapper = _mapper_or_none(target)             if mapper is not None:                 return mapper             else:                 return _MapperEventsHold(target)     else:         return target","if issubclass ( target , mapperlib . Mapper ) :",if target . __class__ is not None :,4.789232204309912,4.789232204309912,0.0
"def _get_font_afm(self, prop):     key = hash(prop)     font = self.afmfontd.get(key)     <MASK>         fname = findfont(prop, fontext=""afm"")         font = self.afmfontd.get(fname)         if font is None:             font = AFM(file(findfont(prop, fontext=""afm"")))             self.afmfontd[fname] = font         self.afmfontd[key] = font     return font",if font is None :,if font is None :,100.00000000000004,100.00000000000004,1.0
"def __call__(self, groupby):     normalize_reduction_funcs(self, ndim=groupby.ndim)     df = groupby     while df.op.output_types[0] not in (OutputType.dataframe, OutputType.series):         df = df.inputs[0]     if self.raw_func == ""size"":         self.output_types = [OutputType.series]     else:         self.output_types = (             [OutputType.dataframe]             <MASK>             else [OutputType.series]         )     if self.output_types[0] == OutputType.dataframe:         return self._call_dataframe(groupby, df)     else:         return self._call_series(groupby, df)",if groupby . op . output_types [ 0 ] == OutputType . dataframe_groupby,"if self . raw_func == ""size"" :",5.165574782208687,5.165574782208687,0.0
"def save(self):     if self.preferences.get(ENCRYPT_ON_DISK, False):         <MASK>             return self.storage.write(                 self.to_dict(encrypt_password=self.encryption_password)             )         elif not self.is_locked:             log.warning(                 ""Disk encryption requested but no password available for encryption. ""                 ""Resetting encryption preferences and saving wallet in an unencrypted state.""             )             self.preferences[ENCRYPT_ON_DISK] = False     return self.storage.write(self.to_dict())",if self . encryption_password is not None :,if self . encryption_password :,54.77927682341229,54.77927682341229,0.0
"def isValidDateString(config_param_name, value, valid_value):     try:         if value == ""DD-MM-YYYY"":             return value         day, month, year = value.split(""-"")         if int(day) < 1 or int(day) > 31:             raise DateStringValueError(config_param_name, value)         <MASK>             raise DateStringValueError(config_param_name, value)         if int(year) < 1900 or int(year) > 2013:             raise DateStringValueError(config_param_name, value)         return value     except Exception:         raise DateStringValueError(config_param_name, value)",if int ( month ) < 1 or int ( month ) > 12 :,if int ( month ) < 1 or month > 31 :,51.716040196048276,51.716040196048276,0.0
"def _capture(self, call_name, data=None, **kwargs):     if data is None:         data = self.get_default_context()     else:         default_context = self.get_default_context()         <MASK>             default_context.update(data)         else:             default_context[""extra""][""extra_data""] = data         data = default_context     client = self.get_sentry_client()     return getattr(client, call_name)(data=data, **kwargs)","if isinstance ( data , dict ) :",if data is not None :,7.654112967106117,7.654112967106117,0.0
"def check(input, expected_output=None, expected_ffi_error=False):     import _cffi_backend     ffi = _cffi_backend.FFI()     if not expected_ffi_error:         ct = ffi.typeof(input)         assert isinstance(ct, ffi.CType)         assert ct.cname == (expected_output or input)     else:         e = py.test.raises(ffi.error, ffi.typeof, input)         <MASK>             assert str(e.value) == expected_ffi_error","if isinstance ( expected_ffi_error , str ) :",if e . value is not None :,3.983253478176822,3.983253478176822,0.0
"def run(self):     """"""Process queries from task queue, stop if processor is None.""""""     while True:         try:             processor, iprot, oprot, otrans, callback = self.queue.get()             <MASK>                 break             processor.process(iprot, oprot)             callback(True, otrans.getvalue())         except Exception:             logging.exception(""Exception while processing request"")             callback(False, """")",if processor is None :,if processor is None :,100.00000000000004,100.00000000000004,1.0
"def search(self, query):     query = query.strip().lower()     results = []     for provider in SidebarItemProvider.all(self.context):         for item in provider.provide():             if ""url"" in item:                 search_source = ""$"".join(                     [item.get(""id"", """"), item.get(""name"", """")]                 ).lower()                 <MASK>                     results.append(                         {                             ""title"": item[""name""],                             ""icon"": item[""icon""],                             ""url"": item[""url""],                         }                     )     return results",if query in search_source :,if search_source . startswith ( query ) :,18.575057999133595,18.575057999133595,0.0
"def handle(self) -> None:     """"""Handles a request ignoring dropped connections.""""""     try:         BaseHTTPRequestHandler.handle(self)     except (ConnectionError, socket.timeout) as e:         self.connection_dropped(e)     except Exception as e:         <MASK>             self.log_error(""SSL error occurred: %s"", e)         else:             raise     if self.server.shutdown_signal:         self.initiate_shutdown()",if self . server . ssl_context is not None and is_ssl_error ( e ) :,if self . server . ssl_error :,22.165788851265287,22.165788851265287,0.0
"def cdn_url_handler(error, endpoint, kwargs):     if endpoint == ""cdn"":         path = kwargs.pop(""path"")         # cdn = app.config.get('cdn', 'http://cdn.staticfile.org/')         # cdn = app.config.get('cdn', '//cdnjs.cloudflare.com/ajax/libs/')         cdn = app.config.get(""cdn"", ""//cdnjscn.b0.upaiyun.com/libs/"")         return urljoin(cdn, path)     else:         exc_type, exc_value, tb = sys.exc_info()         <MASK>             reraise(exc_type, exc_value, tb)         else:             raise error",if exc_value is error :,"if exc_type != ""EINVAL"" :",16.784459625186194,16.784459625186194,0.0
"def pairs(self):     for path in os.listdir(""src""):         if path == "".svn"":             continue         dep = join(""src"", path)         <MASK>             continue         yield dep, join(build_dir, path)",if isdir ( dep ) :,if dep == build_dir :,7.267884212102741,7.267884212102741,0.0
"def get_condition(self):     """"""Return the condition element's name.""""""     for child in self.xml:         <MASK>             cond = child.tag.split(""}"", 1)[-1]             if cond in self.conditions:                 return cond     return ""not-authorized""","if ""{%s}"" % self . namespace in child . tag :","if child . tag == ""condition"" :",10.59473715471634,10.59473715471634,0.0
"def end(self, tag):     # call the appropriate end tag handler     try:         f = self.dispatch[tag]     except KeyError:         <MASK>             return  # unknown tag ?         try:             f = self.dispatch[tag.split("":"")[-1]]         except KeyError:             return  # unknown tag ?     return f(self, """".join(self._data))","if "":"" not in tag :",if not f :,7.733712583165139,7.733712583165139,0.0
"def checkIfSessionCodeExists(self, sessionCode):     if self.emrtFile:         sessionsForExperiment = (             self.emrtFile.root.data_collection.session_meta_data.where(                 ""experiment_id == %d"" % (self.active_experiment_id,)             )         )         sessionCodeMatch = [             sess for sess in sessionsForExperiment if sess[""code""] == sessionCode         ]         <MASK>             return True         return False",if len ( sessionCodeMatch ) > 0 :,if sessionCodeMatch :,7.49553326588684,0.0,0.0
"def save_bytearray(self, obj):     if self.proto < 5:         <MASK>  # bytearray is empty             self.save_reduce(bytearray, (), obj=obj)         else:             self.save_reduce(bytearray, (bytes(obj),), obj=obj)         return     n = len(obj)     if n >= self.framer._FRAME_SIZE_TARGET:         self._write_large_bytes(BYTEARRAY8 + pack(""<Q"", n), obj)     else:         self.write(BYTEARRAY8 + pack(""<Q"", n) + obj)",if not obj :,"if not isinstance ( obj , bytes ) :",11.339582221952005,11.339582221952005,0.0
"def _restore_freeze(self, new):     size_change = []     for k, v in six.iteritems(self._freeze_backup):         newv = new.get(k, [])         <MASK>             size_change.append((self._key_name(k), len(v), len(newv)))     if size_change:         logger.info(             ""These collections were modified but restored in {}: {}"".format(                 self._name,                 "", "".join(map(lambda t: ""({}: {}->{})"".format(*t), size_change)),             )         )     restore_collection(self._freeze_backup)",if len ( v ) != len ( newv ) :,if newv :,1.9758011175389976,0.0,0.0
"def check_options(self, expr, evaluation, options):     for key in options:         if key != ""System`SameTest"":             <MASK>                 evaluation.message(""ContainsOnly"", ""optx"", Symbol(key))             else:                 return evaluation.message(""ContainsOnly"", ""optx"", Symbol(key), expr)     return None",if expr is None :,if expr . is_keyword ( ) :,11.339582221952005,11.339582221952005,0.0
"def bundle_directory(self, dirpath):     """"""Bundle all modules/packages in the given directory.""""""     dirpath = os.path.abspath(dirpath)     for nm in os.listdir(dirpath):         nm = _u(nm)         if nm.startswith("".""):             continue         itempath = os.path.join(dirpath, nm)         if os.path.isdir(itempath):             <MASK>                 self.bundle_package(itempath)         elif nm.endswith("".py""):             self.bundle_module(itempath)","if os . path . exists ( os . path . join ( itempath , ""__init__.py"" ) ) :","if nm . endswith ( "".py"" ) :",7.21025732010192,7.21025732010192,0.0
"def _read_block(self, size):     if self._file_end is not None:         max_size = self._file_end - self._file.tell()         <MASK>             size = max_size         size = max(min(size, max_size), 0)     return self._file.read(size)",if size == - 1 :,if size > max_size :,14.535768424205482,14.535768424205482,0.0
"def question_mark(self):     """"""Shows help for this command and it's sub-commands.""""""     ret = []     if self.param_help_msg or len(self.subcommands) == 0:         ret.append(self._quick_help())     if len(self.subcommands) > 0:         for k, _ in sorted(self.subcommands.items()):             command_path, param_help, cmd_help = self._instantiate_subcommand(                 k             )._quick_help(nested=True)             <MASK>                 ret.append((command_path, param_help, cmd_help))     return (CommandsResponse(STATUS_OK, self.help_formatter(ret)), self.__class__)",if command_path or param_help or cmd_help :,if self . param_help_msg :,13.532330504290597,13.532330504290597,0.0
"def list_domains(self, r53, **kwargs):     marker = None     domains = []     while True:         if marker:             response = self.wrap_aws_rate_limited_call(r53.list_domains(Marker=marker))         else:             response = self.wrap_aws_rate_limited_call(r53.list_domains)         for domain in response.get(""Domains""):             domains.append(domain)         <MASK>             marker = response.get(""NextPageMarker"")         else:             break     return domains","if response . get ( ""NextPageMarker"" ) :","if response . get ( ""NextPageMarker"" ) :",100.00000000000004,100.00000000000004,1.0
"def writer(stream, items):     sep = """"     for item in items:         stream.write(sep)         sep = "" ""         if not isinstance(item, str):             item = str(item)         if not PY3K:             <MASK>                 item = str(item)         stream.write(item)     stream.write(""\n"")","if not isinstance ( item , unicode ) :","if isinstance ( item , ( int , float ) ) :",26.20251007173262,26.20251007173262,0.0
"def f(view, s):     if mode == modes.INTERNAL_NORMAL:         view.run_command(""toggle_comment"")         <MASK>             pt = utils.next_non_white_space_char(view, s.a, white_space="" \t"")         else:             pt = utils.next_non_white_space_char(                 view, self.view.line(s.a).a, white_space="" \t""             )         return R(pt, pt)     return s","if utils . row_at ( self . view , s . a ) != utils . row_at ( self . view , self . view . size ( ) ) :",if self . view . line ( s . a ) == s . a :,10.573314427225927,10.573314427225927,0.0
"def _parse_timestamp(value):     if value:         match = _TIMESTAMP_PATTERN.match(value)         <MASK>             if match.group(2):                 format = ""%Y-%m-%d %H:%M:%S.%f""                 # use the pattern to truncate the value                 value = match.group()             else:                 format = ""%Y-%m-%d %H:%M:%S""             value = datetime.datetime.strptime(value, format)         else:             raise Exception('Cannot convert ""{}"" into a datetime'.format(value))     else:         value = None     return value",if match :,if match :,100.00000000000004,0.0,1.0
"def _compute_log_r(model_trace, guide_trace):     log_r = MultiFrameTensor()     stacks = get_plate_stacks(model_trace)     for name, model_site in model_trace.nodes.items():         if model_site[""type""] == ""sample"":             log_r_term = model_site[""log_prob""]             <MASK>                 log_r_term = log_r_term - guide_trace.nodes[name][""log_prob""]             log_r.add((stacks[name], log_r_term.detach()))     return log_r","if not model_site [ ""is_observed"" ] :",if name in stacks :,2.564755813286796,2.564755813286796,0.0
"def get_translationproject(self):     """"""returns the translation project belonging to this directory.""""""     if self.is_language() or self.is_project():         return None     else:         <MASK>             return self.translationproject         else:             aux_dir = self             while not aux_dir.is_translationproject() and aux_dir.parent is not None:                 aux_dir = aux_dir.parent             return aux_dir.translationproject",if self . is_translationproject ( ) :,if self . is_language ( ) :,59.694917920196445,59.694917920196445,0.0
"def get_hosted_content():     try:         scheme, rest = target.split(""://"", 1)         prefix, host_and_port = rest.split("".interactivetool."")         faked_host = rest         <MASK>             faked_host = rest.split(""/"", 1)[0]         url = ""%s://%s"" % (scheme, host_and_port)         response = requests.get(url, timeout=1, headers={""Host"": faked_host})         return response.text     except Exception as e:         print(e)         return None","if ""/"" in rest :","if prefix == ""interactivetool"" :",7.809849842300637,7.809849842300637,0.0
"def install(self):     log.info(self.openssl_cli)     if not self.has_openssl or self.args.force:         <MASK>             self._download_src()         else:             log.debug(""Already has src {}"".format(self.src_file))         self._unpack_src()         self._build_src()         self._make_install()     else:         log.info(""Already has installation {}"".format(self.install_dir))     # validate installation     version = self.openssl_version     if self.version not in version:         raise ValueError(version)",if not self . has_src :,if self . args . download :,13.540372457315735,13.540372457315735,0.0
"def format(self, formatstr):     pieces = []     for i, piece in enumerate(re_formatchars.split(force_text(formatstr))):         if i % 2:             pieces.append(force_text(getattr(self, piece)()))         <MASK>             pieces.append(re_escaped.sub(r""\1"", piece))     return """".join(pieces)",elif piece :,if i % 2 :,10.682175159905848,10.682175159905848,0.0
"def get_current_events_users(calendar):     now = timezone.make_aware(datetime.now(), timezone.get_current_timezone())     result = []     day = Day(calendar.events.all(), now)     for o in day.get_occurrences():         <MASK>             usernames = o.event.title.split("","")             for username in usernames:                 result.append(User.objects.get(username=username.strip()))     return result",if o . start <= now <= o . end :,if o . event . title :,11.787460936700446,11.787460936700446,0.0
"def from_cfn_params(self, cfn_params):     """"""Initialize param value by parsing CFN input only if the scheduler is awsbatch.""""""     cfn_converter = self.definition.get(""cfn_param_mapping"", None)     if cfn_converter and cfn_params:         <MASK>             # we have the same CFN input parameters for both spot_price and spot_bid_percentage             # so the CFN input could be a float             self.value = int(float(get_cfn_param(cfn_params, cfn_converter)))     return self","if get_cfn_param ( cfn_params , ""Scheduler"" ) == ""awsbatch"" :",if cfn_params . count ( ) == 1 :,11.08282954010985,11.08282954010985,0.0
"def onCompletion(self, text):     res = []     for l in text.split(""\n""):         <MASK>             continue         l = l.split("":"")         if len(l) != 2:             continue         res.append([l[0].strip(), l[1].strip()])     self.panel.setChapters(res)",if not l :,if not l :,100.00000000000004,100.00000000000004,1.0
"def update_ranges(l, i):     for _range in l:         # most common case: extend a range         if i == _range[0] - 1:             _range[0] = i             merge_ranges(l)             return         <MASK>             _range[1] = i             merge_ranges(l)             return     # somewhere outside of range proximity     l.append([i, i])     l.sort(key=lambda x: x[0])",elif i == _range [ 1 ] + 1 :,if i == _range [ 1 ] :,64.069143843707,64.069143843707,0.0
"def process_dollar(token, state, command_line):     if not state.is_range_start_line_parsed:         <MASK>             raise ValueError(""bad range: {0}"".format(state.scanner.state.source))         command_line.line_range.start.append(token)     else:         if command_line.line_range.end:             raise ValueError(""bad range: {0}"".format(state.scanner.state.source))         command_line.line_range.end.append(token)     return parse_line_ref, command_line",if command_line . line_range . start :,if command_line . line_range . start :,100.00000000000004,100.00000000000004,1.0
"def _parse_description(self, text: str):     result = dict(links=[], versions=[])     for line in text.splitlines():         clean = REX_TAG.sub("""", line.strip())         <MASK>             result[""severity""] = clean.split()[1]             continue         if clean.startswith(""Affects:""):             result[""name""] = clean.split()[1]             continue         if "" or higher"" in clean:             result[""versions""] = self._get_versions(clean)         result[""links""].extend(REX_LINK.findall(line))     return result","if clean . startswith ( ""Severity:"" ) :","if ""severity"" in clean :",5.545738798841781,5.545738798841781,0.0
"def apply(self, chart, grammar):     for prod in grammar.productions(empty=True):         for index in compat.xrange(chart.num_leaves() + 1):             new_edge = TreeEdge.from_production(prod, index)             <MASK>                 yield new_edge","if chart . insert ( new_edge , ( ) ) :",if new_edge . is_child ( ) :,17.676084425360003,17.676084425360003,0.0
"def calc(self, arg):     op = arg[""op""]     if op == ""C"":         self.clear()         return str(self.current)     num = decimal.Decimal(arg[""num""])     if self.op:         <MASK>             self.current += num         elif self.op == ""-"":             self.current -= num         elif self.op == ""*"":             self.current *= num         elif self.op == ""/"":             self.current /= num         self.op = op     else:         self.op = op         self.current = num     res = str(self.current)     if op == ""="":         self.clear()     return res","if self . op == ""+"" :","if self . op == ""-"" :",70.71067811865478,70.71067811865478,0.0
"def cascade(self, event=None):     """"""Cascade all Leo windows.""""""     x, y, delta = 50, 50, 50     for frame in g.app.windowList:         w = frame and frame.top         if w:             r = w.geometry()  # a Qt.Rect             # 2011/10/26: Fix bug 823601: cascade-windows fails.             w.setGeometry(QtCore.QRect(x, y, r.width(), r.height()))             # Compute the new offsets.             x += 30             y += 30             <MASK>                 x = 10 + delta                 y = 40 + delta                 delta += 10",if x > 200 :,if delta < 10 :,12.703318703865365,12.703318703865365,0.0
"def redirect(self):     c = self.c     if c.config.getBool(""eval-redirect""):         self.old_stderr = g.stdErrIsRedirected()         self.old_stdout = g.stdOutIsRedirected()         <MASK>             g.redirectStderr()         if not self.old_stdout:             g.redirectStdout()",if not self . old_stderr :,if not self . old_stderr :,100.00000000000004,100.00000000000004,1.0
"def on_event(self, c, button, data):     if self.rvGestureGrab.get_reveal_child():         <MASK>             self.use()         elif button == ""Y"" and data[0] == 0:             self.start_over()","if button == ""A"" and data [ 0 ] == 0 :","if button == ""X"" and data [ 0 ] == 0 :",81.53551038173119,81.53551038173119,0.0
"def __init__(self, in_feats, out_feats, norm=""both"", bias=True, activation=None):     super(DenseGraphConv, self).__init__()     self._in_feats = in_feats     self._out_feats = out_feats     self._norm = norm     with self.name_scope():         self.weight = self.params.get(             ""weight"",             shape=(in_feats, out_feats),             init=mx.init.Xavier(magnitude=math.sqrt(2.0)),         )         <MASK>             self.bias = self.params.get(""bias"", shape=(out_feats,), init=mx.init.Zero())         else:             self.bias = None         self._activation = activation",if bias :,if bias :,100.00000000000004,0.0,1.0
"def _import_top_module(self, name):     # scan sys.path looking for a location in the filesystem that contains     # the module, or an Importer object that can import the module.     for item in sys.path:         if isinstance(item, _StringType):             module = self.fs_imp.import_from_dir(item, name)         else:             module = item.import_top(name)         <MASK>             return module     return None",if module :,if module :,100.00000000000004,0.0,1.0
"def resolver(schemas, f):     if not callable(f):         return     if not hasattr(f, ""accepts""):         return     new_params = []     for p in f.accepts:         <MASK>             new_params.append(p.resolve(schemas))         else:             raise ResolverError(""Invalid parameter definition {0}"".format(p))     # FIXME: for some reason assigning params (f.accepts = new_params) does not work     f.accepts.clear()     f.accepts.extend(new_params)","if isinstance ( p , ( Patch , Ref , Attribute ) ) :",if p . is_param :,3.1795892263857453,3.1795892263857453,0.0
"def get_files(d):     res = []     for p in glob.glob(os.path.join(d, ""*"")):         if not p:             continue         (pth, fname) = os.path.split(p)         if fname == ""output"":             continue         <MASK>             continue         if fname[-4:] == "".pyc"":  # ehmm.. no.             continue         if os.path.isdir(p):             get_dir(p)         else:             res.append(p)     return res","if fname == ""PureMVC_Python_1_0"" :","if fname [ 0 ] == "".pyc"" :",16.21599014882373,16.21599014882373,0.0
"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True):     if leftname in kerning:         for rightname in kerning[leftname]:             if rightname[0] == ""@"":                 for rightname2 in groups[rightname]:                     rightnames.add(rightname2)                     <MASK>                         # TODO: in this case, pick the one rightname that has the highest                         # ranking in glyphorder                         break             else:                 rightnames.add(rightname)",if not includeAll :,if includeAll :,45.13864405503391,0.0,0.0
"def migrate_Stats(self):     for old_obj in self.session_old.query(self.model_from[""Stats""]):         <MASK>             self.entries_count[""Stats""] -= 1             continue         new_obj = self.model_to[""Stats""]()         for key in new_obj.__table__.columns._data.keys():             if key not in old_obj.__table__.columns:                 continue             setattr(new_obj, key, getattr(old_obj, key))         self.session_new.add(new_obj)",if not old_obj . summary :,"if old_obj . __table__ . __name__ == ""Stats"" :",11.96655750514248,11.96655750514248,0.0
"def _readenv(var, msg):     match = _ENV_VAR_PAT.match(var)     if match and match.groups():         envvar = match.groups()[0]         <MASK>             value = os.environ[envvar]             if six.PY2:                 value = value.decode(""utf8"")             return value         else:             raise InvalidConfigException(                 ""{} - environment variable '{}' not set"".format(msg, var)             )     else:         raise InvalidConfigException(             ""{} - environment variable name '{}' does not match pattern '{}'"".format(                 msg, var, _ENV_VAR_PAT_STR             )         )",if envvar in os . environ :,if envvar :,16.605579150202516,0.0,0.0
"def __next__(self):     self._parse_reset()     while True:         try:             line = next(self.input_iter)         except StopIteration:             # End of input OR exception             <MASK>                 raise Error(""newline inside string"")             raise         self.line_num += 1         if ""\0"" in line:             raise Error(""line contains NULL byte"")         pos = 0         while pos < len(line):             pos = self._parse_process_char(line, pos)         self._parse_eol()         if self.state == self.START_RECORD:             break     fields = self.fields     self.fields = []     return fields",if len ( self . field ) > 0 :,"if ""\n"" in line :",5.11459870708889,5.11459870708889,0.0
"def createFields(self):     while self.current_size < self.size:         pos = self.stream.searchBytes(             ""\0\0\1"", self.current_size, self.current_size + 1024 * 1024 * 8         )  # seek forward by at most 1MB         <MASK>             padsize = pos - self.current_size             if padsize:                 yield PaddingBytes(self, ""pad[]"", padsize // 8)         chunk = Chunk(self, ""chunk[]"")         try:             # force chunk to be processed, so that CustomFragments are complete             chunk[""content/data""]         except:             pass         yield chunk",if pos is not None :,if pos > self . current_size :,10.552670315936318,10.552670315936318,0.0
"def spew():     seenUID = False     start()     for part in query:         if part.type == ""uid"":             seenUID = True         <MASK>             yield self.spew_body(part, id, msg, write, flush)         else:             f = getattr(self, ""spew_"" + part.type)             yield f(id, msg, write, flush)         if part is not query[-1]:             space()     if uid and not seenUID:         space()         yield self.spew_uid(id, msg, write, flush)     finish()     flush()","if part . type == ""body"" :",if part . body :,16.62083000646927,16.62083000646927,0.0
"def _limit_value(key, value, config):     if config[key].get(""upper_limit""):         limit = config[key][""upper_limit""]         # auto handle datetime         if isinstance(value, datetime) and isinstance(limit, timedelta):             if config[key][""inverse""] is True:                 if (datetime.now() - limit) > value:                     value = datetime.now() - limit             else:                 <MASK>                     value = datetime.now() + limit         elif value > limit:             value = limit     return value",if ( datetime . now ( ) + limit ) < value :,if value < limit :,3.225009698600463,3.225009698600463,0.0
"def _fix_var_naming(operators, names, mod=""input""):     new_names = []     map = {}     for op in operators:         if mod == ""input"":             iter = op.inputs         else:             iter = op.outputs         for i in iter:             for name in names:                 if i.raw_name == name and name not in map:                     map[i.raw_name] = i.full_name         <MASK>             break     for name in names:         new_names.append(map[name])     return new_names",if len ( map ) == len ( names ) :,if not op . is_naming :,3.983253478176822,3.983253478176822,0.0
"def traverse(tree):     """"""Generator dropping comment nodes""""""     for entry in tree:         # key, values = entry         spaceless = [e for e in entry if not nginxparser.spacey(e)]         if spaceless:             key = spaceless[0]             values = spaceless[1] if len(spaceless) > 1 else None         else:             key = values = """"         if isinstance(key, list):             new = copy.deepcopy(entry)             new[1] = filter_comments(values)             yield new         else:             <MASK>                 yield spaceless","if key != ""#"" and spaceless :","if isinstance ( values , list ) :",5.11459870708889,5.11459870708889,0.0
"def mergeCombiners(self, x, y):     for item in y:         <MASK>             self.heap.push(x, item)         else:             self.heap.push_pop(x, item)     return x",if len ( x ) < self . heap_limit :,"if isinstance ( item , ( list , tuple ) ) :",4.789232204309912,4.789232204309912,0.0
"def test_scatter(self, harness: primitive_harness.Harness):     f_name = harness.params[""f_lax""].__name__     dtype = harness.params[""dtype""]     if jtu.device_under_test() == ""tpu"":         <MASK>             raise unittest.SkipTest(f""TODO: complex {f_name} on TPU fails in JAX"")     self.ConvertAndCompare(harness.dyn_fun, *harness.dyn_args_maker(self.rng()))","if dtype is np . complex64 and f_name in [ ""scatter_min"" , ""scatter_max"" ] :","if dtype != ""complex"" :",1.6586964297308333,1.6586964297308333,0.0
"def TryMerge(self, decoder):     while decoder.avail() > 0:         tag = decoder.getVarInt32()         if tag == TAG_BEGIN_ITEM_GROUP:             (type_id, message) = Item.Decode(decoder)             <MASK>                 self.items[type_id].MergeFrom(Item(message))             else:                 self.items[type_id] = Item(message)             continue         if tag == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         decoder.skipData(tag)",if type_id in self . items :,if tag == TAG_END_ITEM_GROUP :,4.456882760699063,4.456882760699063,0.0
"def process_continuations(lines):     global continuation_pattern     olines = []     while len(lines) != 0:         line = no_comments(lines[0])         line = line.strip()         lines.pop(0)         if line == """":             continue         <MASK>             # combine this line with the next line if the next line exists             line = continuation_pattern.sub("""", line)             if len(lines) >= 1:                 combined_lines = [line + lines[0]]                 lines.pop(0)                 lines = combined_lines + lines                 continue         olines.append(line)     del lines     return olines",if continuation_pattern . search ( line ) :,if line . startswith ( continuation_pattern ) :,22.749707020240194,22.749707020240194,0.0
"def _getListNextPackagesReadyToBuild():     for pkg in Scheduler.listOfPackagesToBuild:         if pkg in Scheduler.listOfPackagesCurrentlyBuilding:             continue         <MASK>             Scheduler.listOfPackagesNextToBuild.put((-Scheduler._getPriority(pkg), pkg))             Scheduler.logger.debug(""Adding "" + pkg + "" to the schedule list"")",if constants . rpmCheck or Scheduler . _checkNextPackageIsReadyToBuild ( pkg ) :,if pkg not in Scheduler . listOfPackagesNextToBuild :,7.433761660133445,7.433761660133445,0.0
"def process_signature(app, what, name, obj, options, signature, return_annotation):     if signature:         # replace Mock function names         signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature)         signature = re.sub(""tensorflow"", ""tf"", signature)         # add scope name to layer signatures:         if hasattr(obj, ""use_scope""):             if obj.use_scope:                 signature = signature[0] + ""variable_scope_name, "" + signature[1:]             <MASK>                 signature = signature[0] + ""[variable_scope_name,] "" + signature[1:]     # signature: arg list     return signature, return_annotation",elif obj . use_scope is None :,"if hasattr ( obj , ""use_scope_name"" ) :",12.011055432195764,12.011055432195764,0.0
"def find_distribution_modules(name=__name__, file=__file__):     current_dist_depth = len(name.split(""."")) - 1     current_dist = os.path.join(         os.path.dirname(file), *([os.pardir] * current_dist_depth)     )     abs = os.path.abspath(current_dist)     dist_name = os.path.basename(abs)     for dirpath, dirnames, filenames in os.walk(abs):         package = (dist_name + dirpath[len(abs) :]).replace(""/"", ""."")         <MASK>             yield package             for filename in filenames:                 if filename.endswith("".py"") and filename != ""__init__.py"":                     yield ""."".join([package, filename])[:-3]","if ""__init__.py"" in filenames :",if os . path . isdir ( package ) :,4.091092899898373,4.091092899898373,0.0
"def transform_value(i, v, *args):     if i not in converter_functions:         # no converter defined on this field, return value as-is         return v     else:         try:             return converter_functions[i](v, *args)         except Exception as e:             if failonerror == ""inline"":                 return e             <MASK>                 raise e             else:                 return errorvalue",elif failonerror :,"if failonerror == ""error"" :",6.567274736060395,6.567274736060395,0.0
"def _get_file(self):     if self._file is None:         self._file = SpooledTemporaryFile(             max_size=self._storage.max_memory_size,             suffix="".S3Boto3StorageFile"",             dir=setting(""FILE_UPLOAD_TEMP_DIR""),         )         <MASK>             self._is_dirty = False             self.obj.download_fileobj(self._file)             self._file.seek(0)         if self._storage.gzip and self.obj.content_encoding == ""gzip"":             self._file = GzipFile(mode=self._mode, fileobj=self._file, mtime=0.0)     return self._file","if ""r"" in self . _mode :",if self . _is_dirty :,18.190371142855746,18.190371142855746,0.0
"def connect(self, host, port, timeout):     fp = Telnet()     for i in range(50):         try:             fp.sock = socket.create_connection(                 (host, int(port)), timeout=int(timeout), source_address=("""", 1023 - i)             )             break         except socket.error as e:             <MASK>                 raise e     self.need_handshake = True     return TCP_Connection(fp)","if ( e . errno , e . strerror ) != ( 98 , ""Address already in use"" ) :",if self . need_handshake :,0.8790071286648791,0.8790071286648791,0.0
"def filtercomments(source):     """"""NOT USED: strips trailing comments and put them at the top.""""""     trailing_comments = []     comment = True     while comment:         if re.search(r""^\s*\/\*"", source):             comment = source[0, source.index(""*/"") + 2]         <MASK>             comment = re.search(r""^\s*\/\/"", source).group(0)         else:             comment = None         if comment:             source = re.sub(r""^\s+"", """", source[len(comment) :])             trailing_comments.append(comment)     return ""\n"".join(trailing_comments) + source","elif re . search ( r""^\s*\/\/"" , source ) :",if comment :,0.09518843576234585,0.0,0.0
"def yview(self, mode=None, value=None, units=None):     if type(value) == str:         value = float(value)     if mode is None:         return self.vsb.get()     elif mode == ""moveto"":         frameHeight = self.innerframe.winfo_reqheight()         self._startY = value * float(frameHeight)     else:  # mode == 'scroll'         clipperHeight = self._clipper.winfo_height()         <MASK>             jump = int(clipperHeight * self._jfraction)         else:             jump = clipperHeight         self._startY = self._startY + value * jump     self.reposition()","if units == ""units"" :","if mode == ""jfraction"" :",27.054113452696992,27.054113452696992,0.0
"def visit(stmt):     """"""Collect information about VTCM buffers and their alignments.""""""     if isinstance(stmt, tvm.tir.AttrStmt):         if stmt.attr_key == ""storage_scope"" and stmt.value == ""local.vtcm"":             vtcm_buffers.append(stmt.node)         <MASK>             if not stmt.node in alignments:                 alignments[stmt.node] = []             alignments[stmt.node].append(stmt.value)","elif stmt . attr_key == ""storage_alignment"" :","if stmt . attr_key == ""alignments"" :",56.75005498026444,56.75005498026444,0.0
"def cost(P):     # wda loss     loss_b = 0     loss_w = 0     for i, xi in enumerate(xc):         xi = np.dot(xi, P)         for j, xj in enumerate(xc[i:]):             xj = np.dot(xj, P)             M = dist(xi, xj)             G = sinkhorn(wc[i], wc[j + i], M, reg, k)             <MASK>                 loss_w += np.sum(G * M)             else:                 loss_b += np.sum(G * M)     # loss inversed because minimization     return loss_w / loss_b",if j == 0 :,if G > 0 :,19.3576934939088,19.3576934939088,0.0
"def __init__(self, comm, in_channels, out_channels, ksize, pad=1):     super(Block, self).__init__()     with self.init_scope():         <MASK>             self.conv = ParallelConvolution2D(                 comm, in_channels, out_channels, ksize, pad=pad, nobias=True             )         else:             self.conv = chainer.links.Convolution2D(                 in_channels, out_channels, ksize, pad=pad, nobias=True             )         self.bn = L.BatchNormalization(out_channels)",if comm . size <= in_channels :,if comm :,6.108851178104657,0.0,0.0
"def halfMultipartScore(nzb_name):     try:         wrong_found = 0         for nr in [1, 2, 3, 4, 5, ""i"", ""ii"", ""iii"", ""iv"", ""v"", ""a"", ""b"", ""c"", ""d"", ""e""]:             for wrong in [""cd"", ""part"", ""dis"", ""disc"", ""dvd""]:                 <MASK>                     wrong_found += 1         if wrong_found == 1:             return -30         return 0     except:         log.error(""Failed doing halfMultipartScore: %s"", traceback.format_exc())     return 0","if ""%s%s"" % ( wrong , nr ) in nzb_name . lower ( ) :",if nr == nr :,0.7422343966767406,0.7422343966767406,0.0
"def should_include(service):     for f in filt:         <MASK>             state = filt[f]             containers = project.containers([service.name], stopped=True)             if not has_container_with_state(containers, state):                 return False         elif f == ""source"":             source = filt[f]             if source == ""image"" or source == ""build"":                 if source not in service.options:                     return False             else:                 raise UserError(""Invalid value for source filter: %s"" % source)         else:             raise UserError(""Invalid filter: %s"" % f)     return True","if f == ""status"" :","if f == ""state"" :",59.4603557501361,59.4603557501361,0.0
"def get_blob_type_declaration_sql(self, column):     length = column.get(""length"")     if length:         if length <= self.LENGTH_LIMIT_TINYBLOB:             return ""TINYBLOB""         <MASK>             return ""BLOB""         if length <= self.LENGTH_LIMIT_MEDIUMBLOB:             return ""MEDIUMBLOB""     return ""LONGBLOB""",if length <= self . LENGTH_LIMIT_BLOB :,if length <= self . LENGTH_LIMIT_BLOB :,100.00000000000004,100.00000000000004,1.0
"def click_outside(event):     if event not in d:         x, y, z = self.blockFaceUnderCursor[0]         <MASK>             y = 64         y += 3         gotoPanel.X, gotoPanel.Y, gotoPanel.Z = x, y, z         if event.num_clicks == 2:             d.dismiss(""Goto"")",if y == 0 :,if event . num_clicks == 1 :,9.980099403873663,9.980099403873663,0.0
"def check_related_active_jobs(self, obj):     active_jobs = obj.get_active_jobs()     if len(active_jobs) > 0:         raise ActiveJobConflict(active_jobs)     time_cutoff = now() - dateutil.relativedelta.relativedelta(minutes=1)     recent_jobs = obj._get_related_jobs().filter(finished__gte=time_cutoff)     for unified_job in recent_jobs.get_real_instances():         <MASK>             raise PermissionDenied(                 _(""Related job {} is still processing events."").format(                     unified_job.log_format                 )             )",if not unified_job . event_processing_finished :,"if unified_job . log_format == ""processing"" :",21.142141714303076,21.142141714303076,0.0
"def run(self):     self.alive = True     if _log.isEnabledFor(_DEBUG):         _log.debug(""started"")     while self.alive:         task = self.queue.get()         <MASK>             function, args, kwargs = task             assert function             try:                 function(*args, **kwargs)             except:                 _log.exception(""calling %s"", function)     if _log.isEnabledFor(_DEBUG):         _log.debug(""stopped"")",if task :,if task :,100.00000000000004,0.0,1.0
"def update_sysconfig_file(fn, adjustments, allow_empty=False):     if not adjustments:         return     (exists, contents) = read_sysconfig_file(fn)     updated_am = 0     for (k, v) in adjustments.items():         if v is None:             continue         v = str(v)         <MASK>             continue         contents[k] = v         updated_am += 1     if updated_am:         lines = [             str(contents),         ]         if not exists:             lines.insert(0, util.make_header())         util.write_file(fn, ""\n"".join(lines) + ""\n"", 0o644)",if len ( v ) == 0 and not allow_empty :,if not allow_empty :,20.96310881302029,20.96310881302029,0.0
"def wrapper(  # type: ignore     self: RequestHandler, *args, **kwargs ) -> Optional[Awaitable[None]]:     if self.request.path.endswith(""/""):         if self.request.method in (""GET"", ""HEAD""):             uri = self.request.path.rstrip(""/"")             <MASK>  # don't try to redirect '/' to ''                 if self.request.query:                     uri += ""?"" + self.request.query                 self.redirect(uri, permanent=True)                 return None         else:             raise HTTPError(404)     return method(self, *args, **kwargs)",if uri :,if not uri :,35.35533905932737,35.35533905932737,0.0
def output_handles_from_execution_plan(execution_plan):     output_handles_for_current_run = set()     for step_level in execution_plan.execution_step_levels():         for step in step_level:             for step_input in step.step_inputs:                 <MASK>                     output_handles_for_current_run.update(step_input.source_handles)     return output_handles_for_current_run,if step_input . source_handles :,if step_input . source_handles :,100.00000000000004,100.00000000000004,1.0
"def _read_value(self, item):     item = _normalize_path(item)     if item in self._store:         <MASK>             del self._store[item]             raise KeyError(item)         return PathResult(item, value=self._store[item])     elif item in self._children:         return PathResult(item, dir=True)     else:         raise KeyError(item)",if item in self . _expire_time and self . _expire_time [ item ] < datetime . now ( ) :,if item in self . _store :,7.452850641949802,7.452850641949802,0.0
"def _line_ranges(statements, lines):     """"""Produce a list of ranges for `format_lines`.""""""     statements = sorted(statements)     lines = sorted(lines)     pairs = []     start = None     lidx = 0     for stmt in statements:         if lidx >= len(lines):             break         if stmt == lines[lidx]:             lidx += 1             <MASK>                 start = stmt             end = stmt         elif start:             pairs.append((start, end))             start = None     if start:         pairs.append((start, end))     return pairs",if not start :,if start :,45.13864405503391,0.0,0.0
"def _update_help_obj_params(help_obj, data_params, params_equal, attr_key_tups):     loaded_params = []     for param_obj in help_obj.parameters:         loaded_param = next(             (n for n in data_params if params_equal(param_obj, n)), None         )         <MASK>             BaseHelpLoader._update_obj_from_data_dict(                 param_obj, loaded_param, attr_key_tups             )         loaded_params.append(param_obj)     help_obj.parameters = loaded_params",if loaded_param :,if loaded_param :,100.00000000000004,100.00000000000004,1.0
"def __get_ratio(self):     """"""Return splitter ratio of the main splitter.""""""     c = self.c     free_layout = c.free_layout     if free_layout:         w = free_layout.get_main_splitter()         <MASK>             aList = w.sizes()             if len(aList) == 2:                 n1, n2 = aList                 # 2017/06/07: guard against division by zero.                 ratio = 0.5 if n1 + n2 == 0 else float(n1) / float(n1 + n2)                 return ratio     return 0.5",if w :,if w :,100.00000000000004,0.0,1.0
"def _check_required_env_variables(vars):     for var in vars:         <MASK>             self.tc.logger.error(                 ""%s is not set. Did you forget to source your build environment setup script?""                 % var             )             raise OEQAPreRun",if not os . environ . get ( var ) :,if var not in self . tc . env_vars :,5.300156689756295,5.300156689756295,0.0
"def clean_indexes():     for coll_name in mongo.collection_types.keys():         coll = mongo.get_collection(coll_name)         indexes = coll_indexes[coll_name]         try:             for index in coll.list_indexes():                 name = index[""name""]                 <MASK>                     continue                 coll.drop_index(name)         except pymongo.errors.OperationFailure:             pass","if name == ""_id"" or name == ""_id_"" or name in indexes :",if name not in indexes :,2.63980029006582,2.63980029006582,0.0
"def _compare_dirs(self, dir1, dir2):     # check that dir1 and dir2 are equivalent,     # return the diff     diff = []     for root, dirs, files in os.walk(dir1):         for file_ in files:             path = os.path.join(root, file_)             target_path = os.path.join(dir2, os.path.split(path)[-1])             <MASK>                 diff.append(file_)     return diff",if not os . path . exists ( target_path ) :,if target_path != dir2 :,12.502047063713437,12.502047063713437,0.0
"def load_state_dict(self, state_dict, strict=True):     """"""Customized load.""""""     self.language_model.load_state_dict(         state_dict[self._language_model_key], strict=strict     )     if mpu.is_pipeline_last_stage():         <MASK>             self.multichoice_head.load_state_dict(                 state_dict[self._multichoice_head_key], strict=strict             )         else:             print_rank_last(                 ""***WARNING*** could not find {} in the checkpoint, ""                 ""initializing to random"".format(self._multichoice_head_key)             )",if self . _multichoice_head_key in state_dict :,if self . _multichoice_head_key in state_dict :,100.00000000000004,100.00000000000004,1.0
"def _parse_timedelta(self, value):     try:         sum = datetime.timedelta()         start = 0         while start < len(value):             m = self._TIMEDELTA_PATTERN.match(value, start)             <MASK>                 raise Exception()             num = float(m.group(1))             units = m.group(2) or ""seconds""             units = self._TIMEDELTA_ABBREV_DICT.get(units, units)             sum += datetime.timedelta(**{units: num})             start = m.end()         return sum     except:         raise",if not m :,if not m :,100.00000000000004,100.00000000000004,1.0
"def SetChildMenuBar(self, pChild):     if not pChild:         # No Child, set Our menu bar back.         if self._pMyMenuBar:             self.SetMenuBar(self._pMyMenuBar)         else:             self.SetMenuBar(self.GetMenuBar())         # Make sure we know our menu bar is in use         self._pMyMenuBar = None     else:         if pChild.GetMenuBar() is None:             return         # Do we need to save the current bar?         <MASK>             self._pMyMenuBar = self.GetMenuBar()         self.SetMenuBar(pChild.GetMenuBar())",if self . _pMyMenuBar is None :,if pChild . GetMenuBar ( ) is not None :,10.552670315936318,10.552670315936318,0.0
"def init_weights(self):     """"""Initialize weights of the head.""""""     # retinanet_bias_init     bias_cls = bias_init_with_prob(0.01)     normal_init(self.conv_reg, std=0.01)     normal_init(self.conv_centerness, std=0.01)     normal_init(self.conv_cls, std=0.01, bias=bias_cls)     for branch in [self.cls_convs, self.reg_convs]:         for module in branch.modules():             <MASK>                 caffe2_xavier_init(module.conv)","if isinstance ( module , ConvModule ) and isinstance ( module . conv , nn . Conv2d ) :",if module . conv :,2.7474047213893553,2.7474047213893553,0.0
"def handle_exception(self, e, result):     for k in sorted(result.thrift_spec):         if result.thrift_spec[k][1] == ""success"":             continue         _, exc_name, exc_cls, _ = result.thrift_spec[k]         <MASK>             setattr(result, exc_name, e)             break     else:         raise","if isinstance ( e , exc_cls ) :",if exc_cls is not None :,18.190371142855746,18.190371142855746,0.0
"def scripts(self):     application_root = current_app.config.get(""APPLICATION_ROOT"")     subdir = application_root != ""/""     scripts = []     for script in get_registered_scripts():         <MASK>             scripts.append(f'<script defer src=""{script}""></script>')         elif subdir:             scripts.append(f'<script defer src=""{application_root}/{script}""></script>')         else:             scripts.append(f'<script defer src=""{script}""></script>')     return markup(""\n"".join(scripts))","if script . startswith ( ""http"" ) :",if script . startswith ( application_root ) :,45.180100180492246,45.180100180492246,0.0
"def test_related_objects_local(self):     result_key = ""get_all_related_objects_with_model_local""     for model, expected in TEST_RESULTS[result_key].items():         objects = [             (field, self._model(model, field))             for field in model._meta.get_fields(include_parents=False)             <MASK>         ]         self.assertEqual(             sorted(self._map_related_query_names(objects), key=self.key_name),             sorted(expected, key=self.key_name),         )",if field . auto_created and not field . concrete,if field . name in expected :,13.805184551789745,13.805184551789745,0.0
"def setTestOutcome(self, event):     """"""Update outcome, exc_info and reason based on configured mappings""""""     if event.exc_info:         ec, ev, tb = event.exc_info         classname = ec.__name__         if classname in self.treatAsFail:             short, long_ = self.labels(classname)             self._setOutcome(event, ""failed"", short, long_)         <MASK>             short, long_ = self.labels(classname, upper=False)             self._setOutcome(event, ""skipped"", short, ""%s: '%s'"" % (long_, ev), str(ev))",elif classname in self . treatAsSkip :,if ev in self . treatAsSkipped :,26.269098944241588,26.269098944241588,0.0
"def small_count(v):     if not v:         return 0     z = [         (1000000000, _(""b"")),         (1000000, _(""m"")),         (1000, _(""k"")),     ]     v = int(v)     for x, y in z:         o, p = divmod(v, x)         if o:             <MASK>                 return ""%d%s"" % (o, y)             return ""%.1f%s"" % (v / float(x), y)     return v",if len ( str ( o ) ) > 2 or not p :,if p :,1.6102756877232731,0.0,0.0
"def __read(self, n):     if self._read_watcher is None:         raise UnsupportedOperation(""read"")     while 1:         try:             return _read(self._fileno, n)         except (IOError, OSError) as ex:             <MASK>                 raise         wait_on_watcher(self._read_watcher, None, None, self.hub)",if ex . args [ 0 ] not in ignored_errors :,if ex . errno == errno . EINPROGRESS :,12.43423351463457,12.43423351463457,0.0
"def locked(self):     inputfiles = set(self.all_inputfiles())     outputfiles = set(self.all_outputfiles())     if os.path.exists(self._lockdir):         for lockfile in self._locks(""input""):             with open(lockfile) as lock:                 for f in lock:                     f = f.strip()                     if f in outputfiles:                         return True         for lockfile in self._locks(""output""):             with open(lockfile) as lock:                 for f in lock:                     f = f.strip()                     <MASK>                         return True     return False",if f in outputfiles or f in inputfiles :,if f in inputfiles :,37.783911519583654,37.783911519583654,0.0
"def _flags_to_int(flags):     # Note, that order does not matter, libev has its own predefined order     if not flags:         return 0     if isinstance(flags, integer_types):         return flags     result = 0     try:         <MASK>             flags = flags.split("","")         for value in flags:             value = value.strip().lower()             if value:                 result |= _flags_str2int[value]     except KeyError as ex:         raise ValueError(             ""Invalid backend or flag: %s\nPossible values: %s""             % (ex, "", "".join(sorted(_flags_str2int.keys())))         )     return result","if isinstance ( flags , basestring ) :",if len ( flags ) > 0 :,13.888095170058955,13.888095170058955,0.0
"def setFg(self, colour, override=False):     if not self.ttkFlag:         self.containerStack[-1][""fg""] = colour         gui.SET_WIDGET_FG(self._getContainerProperty(""container""), colour, override)         for child in self._getContainerProperty(""container"").winfo_children():             <MASK>                 gui.SET_WIDGET_FG(child, colour, override)     else:         gui.trace(""In ttk mode - trying to set FG to %s"", colour)         self.ttkStyle.configure(""TLabel"", foreground=colour)         self.ttkStyle.configure(""TFrame"", foreground=colour)",if not self . _isWidgetContainer ( child ) :,"if child . name == ""fg"" :",5.934202609760488,5.934202609760488,0.0
"def find_scintilla_constants(f):     lexers = []     states = []     for name in f.order:         v = f.features[name]         if v[""Category""] != ""Deprecated"":             if v[""FeatureType""] == ""val"":                 if name.startswith(""SCE_""):                     states.append((name, v[""Value""]))                 <MASK>                     lexers.append((name, v[""Value""]))     return (lexers, states)","elif name . startswith ( ""SCLEX_"" ) :","if v [ ""FeatureType"" ] == ""state"" :",4.065425428798724,4.065425428798724,0.0
"def extract_error_message(response: requests.Response):     if response.content:         try:             content = json.loads(response.content)             <MASK>                 return content[""message""]         except:             logging.debug(f""Failed to parse the response content: {response.content}"")     return response.reason","if ""message"" in content :",if content :,16.605579150202516,0.0,0.0
"def canvas_size(self):     """"""Return the width and height for this sprite canvas""""""     width = height = 0     for image in self.images:         x = image.x + image.absolute_width         y = image.y + image.absolute_height         <MASK>             width = x         if height < y:             height = y     return round_up(width), round_up(height)",if width < x :,if width < x :,100.00000000000004,100.00000000000004,1.0
"def _load_widgets(self):     logger.info(""Loading plugins preferences widgets"")     # Collect the preferences widget for each active plugin     for plugin in self.plugin_manager.get_active_plugins():         plugin_name = plugin.metadata.get(""name"")         try:             preferences_widget = plugin.get_preferences_widget()             <MASK>                 self._tabs.addTab(preferences_widget, plugin_name)         except Exception as reason:             logger.error(                 ""Unable to add the preferences widget (%s): %s"", plugin_name, reason             )             continue",if preferences_widget :,if preferences_widget :,100.00000000000004,100.00000000000004,1.0
"def clean_objects(string, common_attributes):     """"""Return object and attribute lists""""""     string = clean_string(string)     words = string.split()     if len(words) > 1:         prefix_words_are_adj = True         for att in words[:-1]:             if att not in common_attributes:                 prefix_words_are_adj = False         <MASK>             return words[-1:], words[:-1]         else:             return [string], []     else:         return [string], []",if prefix_words_are_adj :,if prefix_words_are_adj :,100.00000000000004,100.00000000000004,1.0
"def _reader():     if shuffle:         random.shuffle(file_list)     while True:         for fn in file_list:             for line in open(fn, ""r""):                 yield self._process_line(line)         <MASK>             break",if not cycle :,if not line :,35.35533905932737,35.35533905932737,0.0
"def load(weights, model, K, fsz, dil):     index = 0     layers = model.layers     for layer in layers._layers:         <MASK>             if layer.W.shape == weights[index].shape:                 layer.W[:] = weights[index]             else:                 layer.W[:] = dilate(weights[index], K, fsz, dil)             index += 1","if hasattr ( layer , ""W"" ) :",if layer . W . shape == weights [ index ] . shape :,3.716499092256817,3.716499092256817,0.0
"def upgrade(migrate_engine):     print(__doc__)     metadata.bind = migrate_engine     liftoverjobs = dict()     jobs = context.query(DeferredJob).filter_by(plugin=""LiftOverTransferPlugin"").all()     for job in jobs:         <MASK>             liftoverjobs[job.params[""parentjob""]] = []         liftoverjobs[job.params[""parentjob""]].append(job.id)     for parent in liftoverjobs:         lifts = liftoverjobs[parent]         deferred = context.query(DeferredJob).filter_by(id=parent).first()         deferred.params[""liftover""] = lifts     context.flush()","if job . params [ ""parentjob"" ] not in liftoverjobs :","if job . params [ ""parentjob"" ] not in liftoverjobs :",100.00000000000004,100.00000000000004,1.0
"def get_refs(self, recursive=False):     """""":see: AbstractExpression.get_refs()""""""     if recursive:         conds_refs = self.refs + sum((c.get_refs(True) for c in self.conds), [])         <MASK>             conds_refs.extend(self.consequent.get_refs(True))         return conds_refs     else:         return self.refs",if self . consequent :,if self . consequent :,100.00000000000004,100.00000000000004,1.0
"def _parse(self, engine):     """"""Parse the layer.""""""     if isinstance(self.args, dict):         <MASK>             self.axis = engine.evaluate(self.args[""axis""], recursive=True)             if not isinstance(self.axis, int):                 raise ParsingError('""axis"" must be an integer.')         if ""momentum"" in self.args:             self.momentum = engine.evaluate(self.args[""momentum""], recursive=True)             if not isinstance(self.momentum, (int, float)):                 raise ParsingError('""momentum"" must be numeric.')","if ""axis"" in self . args :","if ""axis"" in self . args :",100.00000000000004,100.00000000000004,1.0
"def CountMatches(pat, predicate):     num_matches = 0     for i in xrange(256):         b = chr(i)         m = pat.match(b)         left = bool(m)         right = predicate(i)         if left != right:             self.fail(""i = %d, b = %r, match: %s, predicate: %s"" % (i, b, left, right))         <MASK>             num_matches += 1     return num_matches",if m :,if m :,100.00000000000004,0.0,1.0
"def __new__(cls, *args, **kwargs):     if len(args) == 1:         if len(kwargs):             raise ValueError(                 ""You can either use {} with one positional argument or with keyword arguments, not both."".format(                     cls.__name__                 )             )         if not args[0]:             return super().__new__(cls)         <MASK>             return cls     return super().__new__(cls, *args, **kwargs)","if isinstance ( args [ 0 ] , cls ) :","if args [ 0 ] == ""args"" :",25.965358893403383,25.965358893403383,0.0
"def concatenateCharacterTokens(tokens):     pendingCharacters = []     for token in tokens:         type = token[""type""]         if type in (""Characters"", ""SpaceCharacters""):             pendingCharacters.append(token[""data""])         else:             <MASK>                 yield {""type"": ""Characters"", ""data"": """".join(pendingCharacters)}                 pendingCharacters = []             yield token     if pendingCharacters:         yield {""type"": ""Characters"", ""data"": """".join(pendingCharacters)}",if pendingCharacters :,if pendingCharacters :,100.00000000000004,0.0,1.0
"def get_ranges_from_func_set(support_set):     pos_start = 0     pos_end = 0     ranges = []     for pos, func in enumerate(network.function):         if func.type in support_set:             pos_end = pos         else:             <MASK>                 ranges.append((pos_start, pos_end))             pos_start = pos + 1     if pos_end >= pos_start:         ranges.append((pos_start, pos_end))     return ranges",if pos_end >= pos_start :,if pos_end >= pos_start :,100.00000000000004,100.00000000000004,1.0
"def _visit(self, func):     fname = func[0]     if fname in self._flags:         <MASK>             logger.critical(""Fatal error! network ins not Dag."")             import sys             sys.exit(-1)         else:             return     else:         if fname not in self._flags:             self._flags[fname] = 1         for output in func[3]:             for f in self._orig:                 for input in f[2]:                     if output == input:                         self._visit(f)     self._flags[fname] = 2     self._sorted.insert(0, func)",if self . _flags [ fname ] == 1 :,if self . _is_network :,22.17204504793461,22.17204504793461,0.0
"def graph_merge_softmax_with_crossentropy_softmax(node):     if node.op == softmax_with_bias:         x, b = node.inputs         for x_client in x.clients:             <MASK>                 big_client = x_client[0]                 if big_client in [b_client[0] for b_client in b.clients]:                     xx, bb, ll = big_client.inputs                     mergeable_client = big_client.op(x, b, ll)                     copy_stack_trace(node.outputs[0], mergeable_client[1])                     return [mergeable_client[1]]",if x_client [ 0 ] . op == crossentropy_softmax_argmax_1hot_with_bias :,if x_client in [ b_client [ 0 ] ] :,22.530488865470915,22.530488865470915,0.0
"def confidence(self):     if self.bbox:         # Units are measured in Kilometers         distance = Distance(self.northeast, self.southwest, units=""km"")         for score, maximum in [             (10, 0.25),             (9, 0.5),             (8, 1),             (7, 5),             (6, 7.5),             (5, 10),             (4, 15),             (3, 20),             (2, 25),         ]:             if distance < maximum:                 return score             <MASK>                 return 1     # Cannot determine score     return 0",if distance >= 25 :,if score > maximum :,11.51015341649912,11.51015341649912,0.0
"def OnListEndLabelEdit(self, std, extra):     item = extra[0]     text = item[4]     if text is None:         return     item_id = self.GetItem(item[0])[6]     from bdb import Breakpoint     for bplist in Breakpoint.bplist.itervalues():         for bp in bplist:             if id(bp) == item_id:                 <MASK>                     text = None                 bp.cond = text                 break     self.RespondDebuggerData()","if text . strip ( ) . lower ( ) == ""none"" :",if bp . cond is None :,2.389389104935703,2.389389104935703,0.0
"def _handle_autocomplete_request_for_text(text):     if not hasattr(text, ""autocompleter""):         <MASK>             if isinstance(text, CodeViewText):                 text.autocompleter = Completer(text)             elif isinstance(text, ShellText):                 text.autocompleter = ShellCompleter(text)             text.bind(""<1>"", text.autocompleter.on_text_click)         else:             return     text.autocompleter.handle_autocomplete_request()","if isinstance ( text , ( CodeViewText , ShellText ) ) and text . is_python_text ( ) :",if text . is_text :,5.961371027867936,5.961371027867936,0.0
"def visit_Macro(self, node, frame):     macro_frame, macro_ref = self.macro_body(node, frame)     self.newline()     if frame.toplevel:         <MASK>             self.write(""context.exported_vars.add(%r)"" % node.name)         ref = frame.symbols.ref(node.name)         self.writeline(""context.vars[%r] = "" % node.name)     self.write(""%s = "" % frame.symbols.ref(node.name))     self.macro_def(macro_ref, macro_frame)","if not node . name . startswith ( ""_"" ) :",if node . name in frame . symbols :,13.532330504290597,13.532330504290597,0.0
"def execute(cls, ctx, op):     try:         pd.set_option(""mode.use_inf_as_na"", op.use_inf_as_na)         <MASK>             return cls._execute_map(ctx, op)         else:             return cls._execute_combine(ctx, op)     finally:         pd.reset_option(""mode.use_inf_as_na"")",if op . stage == OperandStage . map :,if op . use_inf_as_na :,14.991106946711685,14.991106946711685,0.0
"def ranges(self, start, end):     try:         iterators = [i.ranges(start, end) for i in self.range_iterators]         starts, ends, values = zip(*[next(i) for i in iterators])         starts = list(starts)         ends = list(ends)         values = list(values)         while start < end:             min_end = min(ends)             yield start, min_end, values             start = min_end             for i, iterator in enumerate(iterators):                 <MASK>                     starts[i], ends[i], values[i] = next(iterator)     except StopIteration:         return",if ends [ i ] == min_end :,if i in starts :,4.234348806659263,4.234348806659263,0.0
"def get_explanation(self, spec):     """"""Expand an explanation.""""""     if spec:         try:             a = self.dns_txt(spec)             <MASK>                 return str(self.expand(to_ascii(a[0]), stripdot=False))         except PermError:             # RFC4408 6.2/4 syntax errors cause exp= to be ignored             if self.strict > 1:                 raise  # but report in harsh mode for record checking tools             pass     elif self.strict > 1:         raise PermError(""Empty domain-spec on exp="")     # RFC4408 6.2/4 empty domain spec is ignored     # (unless you give precedence to the grammar).     return None",if len ( a ) == 1 :,if len ( a ) == 1 :,100.00000000000004,100.00000000000004,1.0
"def iter_fields(node, *, include_meta=True, exclude_unset=False):     exclude_meta = not include_meta     for field_name, field in node._fields.items():         if exclude_meta and field.meta:             continue         field_val = getattr(node, field_name, _marker)         if field_val is _marker:             continue         <MASK>             if callable(field.default):                 default = field.default()             else:                 default = field.default             if field_val == default:                 continue         yield field_name, field_val",if exclude_unset :,if exclude_unset :,100.00000000000004,100.00000000000004,1.0
"def __setattr__(self, name, value):     try:         field = self._meta.get_field(name)         <MASK>             value = value[: field.max_length]     except models.fields.FieldDoesNotExist:         pass  # This happens with foreign keys.     super.__setattr__(self, name, value)","if type ( field ) in [ models . CharField , models . TextField ] and type ( value ) == str :",if field . max_length :,0.8188135341326943,0.8188135341326943,0.0
"def create_child(self, value=None, _id=None):     with atomic(savepoint=False):         child_key = self.get_next_child_key()         <MASK>             value = child_key         child = self.__class__.objects.create(id=_id, key=child_key, value=value)         return child",if value is None :,if value is None :,100.00000000000004,100.00000000000004,1.0
"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None):     stream = self.describe_stream(stream_name)     tags = []     result = {""HasMoreTags"": False, ""Tags"": tags}     for key, val in sorted(stream.tags.items(), key=lambda x: x[0]):         <MASK>             result[""HasMoreTags""] = True             break         if exclusive_start_tag_key and key < exclusive_start_tag_key:             continue         tags.append({""Key"": key, ""Value"": val})     return result",if limit and len ( tags ) >= limit :,if limit is not None :,7.807646168419154,7.807646168419154,0.0
"def emit(self, record):     try:         app = get_app()         <MASK>             msg = self.format(record)             debug_buffer = app.layout.get_buffer_by_name(""debug_buffer"")             current_document = debug_buffer.document.text             if current_document:                 msg = ""\n"".join([current_document, msg])             debug_buffer.set_document(Document(text=msg), bypass_readonly=True)         else:             super().emit(record)     except:         self.handleError(record)","if app . is_running and getattr ( app , ""debug"" , False ) :",if app . layout :,3.173613488953928,3.173613488953928,0.0
"def worker():     global error     while True:         (num, q) = pq.get()         <MASK>             pq.task_done()             break         try:             process_one(q)         except Exception as e:             error = e         finally:             pq.task_done()",if q is None or error is not None :,if num == 0 :,4.955725306405571,4.955725306405571,0.0
"def transceiver(self, data):     out = []     for t in range(8):         if data[t] == 0:             continue         value = data[t]         for b in range(8):             <MASK>                 if len(TRANSCEIVER[t]) < b + 1:                     out.append(""(unknown)"")                 else:                     out.append(TRANSCEIVER[t][b])             value <<= 1     self.annotate(""Transceiver compliance"", "", "".join(out))",if value & 0x80 :,if value == 0 :,17.965205598154213,17.965205598154213,0.0
"def skip_to_close_match(self):     nestedCount = 1     while 1:         tok = self.tokenizer.get_next_token()         ttype = tok[""style""]         <MASK>             return         elif self.classifier.is_index_op(tok):             tval = tok[""text""]             if self.opHash.has_key(tval):                 if self.opHash[tval][1] == 1:                     nestedCount += 1                 else:                     nestedCount -= 1                     if nestedCount <= 0:                         break",if ttype == SCE_PL_UNUSED :,"if ttype == ""close"" :",28.46946938149361,28.46946938149361,0.0
"def GenerateVector(self, hits, vector, level):     """"""Generate possible hit vectors which match the rules.""""""     for item in hits.get(level, []):         if vector:             <MASK>                 continue             if item > self.max_separation + vector[-1]:                 break         new_vector = vector + [item]         if level + 1 == len(hits):             yield new_vector         elif level + 1 < len(hits):             for result in self.GenerateVector(hits, new_vector, level + 1):                 yield result",if item < vector [ - 1 ] :,if item < self . max_separation :,19.070828081828378,19.070828081828378,0.0
"def __setattr__(self, name, value):     if name == ""path"":         <MASK>             if value[0] != ""/"":                 raise ValueError(                     'The page path should always start with a slash (""/"").'                 )     elif name == ""load_time"":         if value and not isinstance(value, int):             raise ValueError(                 ""Page load time must be specified in integer milliseconds.""             )     object.__setattr__(self, name, value)","if value and value != """" :",if value :,8.525588607164655,0.0,0.0
"def awaitTermination(self, timeout=None):     if self.scheduler is None:         raise RuntimeError(""StreamimgContext not started"")     try:         deadline = time.time() + timeout if timeout is not None else None         while True:             is_terminated = self._runOnce()             <MASK>                 break             if self.batchCallback:                 self.batchCallback()     except KeyboardInterrupt:         pass     finally:         self.sc.stop()         logger.info(""StreamingContext stopped successfully"")",if is_terminated or ( deadline is not None and time . time ( ) > deadline ) :,if is_terminated :,3.520477365831487,3.520477365831487,0.0
"def stopbutton(self):     if GPIOcontrol:         while mediastopbutton:             time.sleep(0.25)             <MASK>                 print(""Stopped"")                 stop()",if not GPIO . input ( stoppushbutton ) :,if self . stop :,6.316906128202129,6.316906128202129,0.0
"def test_create_connection_timeout(self):     # Issue #9792: create_connection() should not recast timeout errors     # as generic socket errors.     with self.mocked_socket_module():         try:             socket.create_connection((HOST, 1234))         except socket.timeout:             pass         except OSError as exc:             <MASK>                 raise         else:             self.fail(""socket.timeout not raised"")",if support . IPV6_ENABLED or exc . errno != errno . EAFNOSUPPORT :,if exc . errno == errno . ECONNREFUSED :,15.491846006709249,15.491846006709249,0.0
"def handle_exception_and_die(e):     if hasattr(e, ""kind""):         <MASK>             sys.stderr.write(""ABORT: "" + e.msg + ""\n"")             sys.exit(e.value)         elif e.kind == ""exit"":             sys.stderr.write(""EXITING\n"")             sys.exit(e.value)     else:         print(str(e))         sys.exit(1)","if e . kind == ""die"" :","if e . kind == ""abort"" :",70.71067811865478,70.71067811865478,0.0
"def gets(self, key):     with self.client_pool.get_and_release(destroy_on_fail=True) as client:         try:             return client.gets(key)         except Exception:             <MASK>                 return (None, None)             else:                 raise",if self . ignore_exc :,if client . status_code == 404 :,5.934202609760488,5.934202609760488,0.0
"def _execute(self, options, args):     if len(args) < 3:         raise CommandError(_(""Not enough arguments""))     tag = fsn2text(args[0])     value = fsn2text(args[1])     paths = args[2:]     songs = []     for path in paths:         song = self.load_song(path)         <MASK>             raise CommandError(_(""Can not set %r"") % tag)         self.log(""Add %r to %r"" % (value, tag))         song.add(tag, value)         songs.append(song)     self.save_songs(songs)",if not song . can_change ( tag ) :,if not song :,11.10316628653437,11.10316628653437,0.0
"def get_place_name(self, place_handle):     """"""Obtain a place name""""""     text = """"     if place_handle:         place = self.dbstate.db.get_place_from_handle(place_handle)         if place:             place_title = place_displayer.display(self.dbstate.db, place)             <MASK>                 if len(place_title) > 25:                     text = place_title[:24] + ""...""                 else:                     text = place_title     return text","if place_title != """" :",if len ( place_title ) > 24 :,17.747405280050266,17.747405280050266,0.0
"def _Determine_Do(self):     self.applicable = 1     self.value = os.environ.get(self.name, None)     if self.value is None and black.configure.items.has_key(""buildType""):         buildType = black.configure.items[""buildType""].Get()         <MASK>             self.value = ""warn""         else:             self.value = None     self.determined = 1","if buildType == ""debug"" :","if buildType == ""warn"" :",59.4603557501361,59.4603557501361,0.0
"def bundle_directory(self, dirpath):     """"""Bundle all modules/packages in the given directory.""""""     dirpath = os.path.abspath(dirpath)     for nm in os.listdir(dirpath):         nm = _u(nm)         <MASK>             continue         itempath = os.path.join(dirpath, nm)         if os.path.isdir(itempath):             if os.path.exists(os.path.join(itempath, ""__init__.py"")):                 self.bundle_package(itempath)         elif nm.endswith("".py""):             self.bundle_module(itempath)","if nm . startswith ( ""."" ) :",if not os . path . isdir ( nm ) :,10.252286118120933,10.252286118120933,0.0
"def header_fields(self, fields):     headers = dict(self.conn.response.getheaders())     ret = {}     for field in fields:         <MASK>             raise ValueError(""%s was not found in response header"" % (field[1]))         try:             ret[field[0]] = int(headers[field[1]])         except ValueError:             ret[field[0]] = headers[field[1]]     return ret",if not headers . has_key ( field [ 1 ] ) :,if not headers [ field [ 0 ] ] :,14.320952289897711,14.320952289897711,0.0
"def caesar_cipher(s, k):     result = """"     for char in s:         n = ord(char)         <MASK>             n = ((n - 65 + k) % 26) + 65         if 96 < n < 123:             n = ((n - 97 + k) % 26) + 97         result = result + chr(n)     return result",if 64 < n < 91 :,if 65 < n < 97 :,27.77619034011791,27.77619034011791,0.0
"def qtTypeIdent(conn, *args):     # We're not using the conn object at the moment, but - we will     # modify the     # logic to use the server version specific keywords later.     res = None     value = None     for val in args:         # DataType doesn't have len function then convert it to string         if not hasattr(val, ""__len__""):             val = str(val)         <MASK>             continue         value = val         if Driver.needsQuoting(val, True):             value = value.replace('""', '""""')             value = '""' + value + '""'         res = ((res and res + ""."") or """") + value     return res",if len ( val ) == 0 :,"if not hasattr ( val , ""__len__"" ) :",6.917184228205472,6.917184228205472,0.0
"def _parse_timezone(     value: Optional[str], error: Type[Exception] ) -> Union[None, int, timezone]:     if value == ""Z"":         return timezone.utc     elif value is not None:         offset_mins = int(value[-2:]) if len(value) > 3 else 0         offset = 60 * int(value[1:3]) + offset_mins         <MASK>             offset = -offset         try:             return timezone(timedelta(minutes=offset))         except ValueError:             raise error()     else:         return None","if value [ 0 ] == ""-"" :",if offset < 0 :,4.234348806659263,4.234348806659263,0.0
"def indent(elem, level=0):     i = ""\n"" + level * ""  ""     if len(elem):         if not elem.text or not elem.text.strip():             elem.text = i + ""  ""         <MASK>             elem.tail = i         for elem in elem:             indent(elem, level + 1)         if not elem.tail or not elem.tail.strip():             elem.tail = i     else:         if level and (not elem.tail or not elem.tail.strip()):             elem.tail = i",if not elem . tail or not elem . tail . strip ( ) :,if level and len ( elem . text ) > 0 :,6.942047568179753,6.942047568179753,0.0
"def _make_slices(     shape: tp.Tuple[int, ...],     axes: tp.Tuple[int, ...],     size: int,     rng: np.random.RandomState, ) -> tp.List[slice]:     slices = []     for a, s in enumerate(shape):         if a in axes:             <MASK>                 raise ValueError(""Cannot crossover on axis with size 1"")             start = rng.randint(s - size)             slices.append(slice(start, start + size))         else:             slices.append(slice(None))     return slices",if s <= 1 :,if s > 1 :,24.736929544091932,24.736929544091932,0.0
"def _loadTestsFromTestCase(self, event, testCaseClass):     evt = events.LoadFromTestCaseEvent(event.loader, testCaseClass)     result = self.session.hooks.loadTestsFromTestCase(evt)     if evt.handled:         loaded_suite = result or event.loader.suiteClass()     else:         names = self._getTestCaseNames(event, testCaseClass)         <MASK>             names = [""runTest""]         # FIXME return failure test case if name not in testcase class         loaded_suite = event.loader.suiteClass(map(testCaseClass, names))     if evt.extraTests:         loaded_suite.addTests(evt.extraTests)     return loaded_suite","if not names and hasattr ( testCaseClass , ""runTest"" ) :",if not names :,6.734410772670761,6.734410772670761,0.0
"def check_settings(self):     if self.settings_dict[""TIME_ZONE""] is not None:         if not settings.USE_TZ:             raise ImproperlyConfigured(                 ""Connection '%s' cannot set TIME_ZONE because USE_TZ is ""                 ""False."" % self.alias             )         <MASK>             raise ImproperlyConfigured(                 ""Connection '%s' cannot set TIME_ZONE because its engine ""                 ""handles time zones conversions natively."" % self.alias             )",elif self . features . supports_timezones :,if settings . TIME_ZONE :,6.495032985064742,6.495032985064742,0.0
"def collect_conflicting_diffs(path, decisions):     local_conflict_diffs = []     remote_conflict_diffs = []     for d in decisions:         <MASK>             ld = adjust_patch_level(path, d.common_path, d.local_diff)             rd = adjust_patch_level(path, d.common_path, d.remote_diff)             local_conflict_diffs.extend(ld)             remote_conflict_diffs.extend(rd)     return local_conflict_diffs, remote_conflict_diffs",if d . conflict :,if d . common_path :,26.269098944241588,26.269098944241588,0.0
"def short_repr(obj):     if isinstance(         obj,         (type, types.ModuleType, types.BuiltinMethodType, types.BuiltinFunctionType),     ):         return obj.__name__     if isinstance(obj, types.MethodType):         <MASK>             return obj.im_func.__name__ + "" (bound)""         else:             return obj.im_func.__name__     if isinstance(obj, (tuple, list, dict, set)):         return ""%d items"" % len(obj)     if isinstance(obj, weakref.ref):         return ""all_weakrefs_are_one""     return repr(obj)[:40]",if obj . im_self is not None :,"if isinstance ( obj , types . FunctionType ) :",5.934202609760488,5.934202609760488,0.0
"def _massage_uri(uri):     if uri:         <MASK>             uri = uri.replace(""hdfs://"", get_defaultfs())         elif uri.startswith(""/""):             uri = get_defaultfs() + uri     return uri","if uri . startswith ( ""hdfs:///"" ) :","if uri . startswith ( ""hdfs://"" ) :",90.18895596416246,90.18895596416246,0.0
"def chsub(self, msg, chatid):     (cmd, evt, params) = self.tokenize(msg, 3)     if cmd == ""/sub"":         sql = ""replace into telegram_subscriptions(uid, event_type, parameters) values (?, ?, ?)""     else:         <MASK>             sql = ""delete from telegram_subscriptions where uid = ? and (event_type = ? or parameters = ? or 1 = 1)""  # does not look very elegant, but makes unsub'ing everythign possible         else:             sql = ""delete from telegram_subscriptions where uid = ? and event_type = ? and parameters = ?""     with self.bot.database as conn:         conn.execute(sql, [chatid, evt, params])         conn.commit()     return","if evt == ""everything"" :","if cmd == ""/sub"" :",23.356898886410015,23.356898886410015,0.0
"def undefined_symbols(self):     result = []     for p in self.Productions:         <MASK>             continue         for s in p.prod:             if not s in self.Prodnames and not s in self.Terminals and s != ""error"":                 result.append((s, p))     return result",if not p :,if not p . name :,32.46679154750991,32.46679154750991,0.0
"def renumber(self, x1, y1, x2, y2, dx, dy):     out = []     for part in re.split(""(\w+)"", self.formula):         m = re.match(""^([A-Z]+)([1-9][0-9]*)$"", part)         if m is not None:             sx, sy = m.groups()             x = colname2num(sx)             y = int(sy)             <MASK>                 part = cellname(x + dx, y + dy)         out.append(part)     return FormulaCell("""".join(out), self.fmt, self.alignment)",if x1 <= x <= x2 and y1 <= y <= y2 :,if sx != 0 :,1.707863452144561,1.707863452144561,0.0
"def modify_column(self, column: List[Optional[""Cell""]]):     for i in range(len(column)):         gate = column[i]         if gate is self:             continue         <MASK>             # The first parity control to modify the column must merge all             # of the other parity controls into itself.             column[i] = None             self._basis_change += gate._basis_change             self.qubits += gate.qubits         elif gate is not None:             column[i] = gate.controlled_by(self.qubits[0])","elif isinstance ( gate , ParityControlCell ) :",if gate . controlled_by is None :,5.669791110976001,5.669791110976001,0.0
"def update_neighbor(neigh_ip_address, changes):     rets = []     for k, v in changes.items():         <MASK>             rets.append(_update_med(neigh_ip_address, v))         if k == neighbors.ENABLED:             rets.append(update_neighbor_enabled(neigh_ip_address, v))         if k == neighbors.CONNECT_MODE:             rets.append(_update_connect_mode(neigh_ip_address, v))     return all(rets)",if k == neighbors . MULTI_EXIT_DISC :,if k == neighbors . MED :,42.88819424803536,42.88819424803536,0.0
"def writexml(     self,     stream,     indent="""",     addindent="""",     newl="""",     strip=0,     nsprefixes={},     namespace="""", ):     w = _streamWriteWrapper(stream)     if self.raw:         val = self.nodeValue         if not isinstance(val, str):             val = str(self.nodeValue)     else:         v = self.nodeValue         <MASK>             v = str(v)         if strip:             v = "" "".join(v.split())         val = escape(v)     w(val)","if not isinstance ( v , str ) :","if not isinstance ( v , ( str , unicode ) ) :",44.08231875586728,44.08231875586728,0.0
"def _condition(ct):     for qobj in args:         <MASK>             # normal kwargs are an AND anyway, so just use those for now             for child in qobj.children:                 kwargs.update(dict([child]))         else:             raise NotImplementedError(""Unsupported Q object"")     for attr, val in kwargs.items():         if getattr(ct, attr) != val:             return False     return True","if qobj . connector == ""AND"" and not qobj . negated :","if isinstance ( qobj , Q ) :",3.0297048914466935,3.0297048914466935,0.0
"def results_iter(self):     <MASK>         from django.db.models.fields import DateTimeField         fields = [DateTimeField()]     else:         needs_string_cast = self.connection.features.needs_datetime_string_cast     offset = len(self.query.extra_select)     for rows in self.execute_sql(MULTI):         for row in rows:             date = row[offset]             if self.connection.ops.oracle:                 date = self.resolve_columns(row, fields)[offset]             elif needs_string_cast:                 date = typecast_timestamp(str(date))             yield date",if self . connection . ops . oracle :,if not self . connection . ops . oracle :,78.25422900366438,78.25422900366438,0.0
"def get_job_type(self):     if int(self.job_runtime_conf.get(""dsl_version"", 1)) == 2:         job_type = (             self.job_runtime_conf[""job_parameters""].get(""common"", {}).get(""job_type"")         )         <MASK>             job_type = self.job_runtime_conf[""job_parameters""].get(""job_type"", ""train"")     else:         job_type = self.job_runtime_conf[""job_parameters""].get(""job_type"", ""train"")     return job_type",if not job_type :,"if job_type == ""train"" :",17.747405280050266,17.747405280050266,0.0
"def validate_assessment_criteria(self):     if self.assessment_criteria:         total_weightage = 0         for criteria in self.assessment_criteria:             total_weightage += criteria.weightage or 0         <MASK>             frappe.throw(_(""Total Weightage of all Assessment Criteria must be 100%""))",if total_weightage != 100 :,if total_weightage > 100 :,42.38365628278778,42.38365628278778,0.0
"def get_list_of_strings_to_mongo_objects(self, notifications_list=None):     result = []     if len(notifications_list) > 0:         for x in notifications_list:             split_provider_id = x.split("":"")  # email:id             <MASK>                 _id = split_provider_id[1]                 cursor = self.get_by_id(_id)                 if cursor:  # Append if exists                     result.append(cursor)     return result",if len ( split_provider_id ) == 2 :,if split_provider_id :,28.379522624171575,28.379522624171575,0.0
"def dump_predictions_to_database(relation, predictions):     judge = ""iepy-run on {}"".format(datetime.now().strftime(""%Y-%m-%d %H:%M""))     for evidence, relation_is_present in predictions.items():         label = (             EvidenceLabel.YESRELATION             <MASK>             else EvidenceLabel.NORELATION         )         evidence.set_label(relation, label, judge, labeled_by_machine=True)",if relation_is_present,if relation_is_present :,80.91067115702207,80.91067115702207,0.0
"def __init__(self, **kwargs):     # We hard-code the `to` argument for ForeignKey.__init__     dfl = get_model_label(self.default_model_class)     if ""to"" in kwargs.keys():  # pragma: no cover         old_to = get_model_label(kwargs.pop(""to""))         <MASK>             msg = ""%s can only be a ForeignKey to %s; %s passed"" % (                 self.__class__.__name__,                 dfl,                 old_to,             )             warnings.warn(msg, SyntaxWarning)     kwargs[""to""] = dfl     super().__init__(**kwargs)",if old_to . lower ( ) != dfl . lower ( ) :,if old_to != dfl :,19.0183794978402,19.0183794978402,0.0
"def reverse(self):     """"""Reverse *IN PLACE*.""""""     li = self.leftindex     lb = self.leftblock     ri = self.rightindex     rb = self.rightblock     for i in range(self.len >> 1):         lb.data[li], rb.data[ri] = rb.data[ri], lb.data[li]         li += 1         if li >= BLOCKLEN:             lb = lb.rightlink             li = 0         ri -= 1         <MASK>             rb = rb.leftlink             ri = BLOCKLEN - 1",if ri < 0 :,if ri >= BLOCKLEN :,17.965205598154213,17.965205598154213,0.0
"def get_api(user, url):     global API_CACHE     if API_CACHE is None or API_CACHE.get(url) is None:         API_CACHE_LOCK.acquire()         try:             if API_CACHE is None:                 API_CACHE = {}             <MASK>                 API_CACHE[url] = ImpalaDaemonApi(url)         finally:             API_CACHE_LOCK.release()     api = API_CACHE[url]     api.set_user(user)     return api",if API_CACHE . get ( url ) is None :,if api is None :,10.536767850900915,10.536767850900915,0.0
"def invert_index(cls, index, length):     if np.isscalar(index):         return length - index     elif isinstance(index, slice):         start, stop = index.start, index.stop         new_start, new_stop = None, None         if start is not None:             new_stop = length - start         <MASK>             new_start = length - stop         return slice(new_start - 1, new_stop - 1)     elif isinstance(index, Iterable):         new_index = []         for ind in index:             new_index.append(length - ind)     return new_index",if stop is not None :,if stop is not None :,100.00000000000004,100.00000000000004,1.0
"def infer_returned_object(pyfunction, args):     """"""Infer the `PyObject` this `PyFunction` returns after calling""""""     object_info = pyfunction.pycore.object_info     result = object_info.get_exact_returned(pyfunction, args)     if result is not None:         return result     result = _infer_returned(pyfunction, args)     if result is not None:         <MASK>             params = args.get_arguments(pyfunction.get_param_names(special_args=False))             object_info.function_called(pyfunction, params, result)         return result     return object_info.get_returned(pyfunction, args)",if args and pyfunction . get_module ( ) . get_resource ( ) is not None :,if result . __class__ is pyfunction :,2.6897775984187877,2.6897775984187877,0.0
"def _check_imports(lib):     # Make sure no conflicting libraries have been imported.     libs = [""PyQt4"", ""PyQt5"", ""PySide""]     libs.remove(lib)     for lib2 in libs:         lib2 += "".QtCore""         <MASK>             raise RuntimeError(                 ""Refusing to import %s because %s is already "" ""imported."" % (lib, lib2)             )",if lib2 in sys . modules :,if lib2 in libs :,28.641904579795423,28.641904579795423,0.0
"def _poll(fds, timeout):     if timeout is not None:         timeout = int(timeout * 1000)  # timeout is in milliseconds     fd_map = {}     pollster = select.poll()     for fd in fds:         pollster.register(fd, select.POLLIN)         <MASK>             fd_map[fd.fileno()] = fd         else:             fd_map[fd] = fd     ls = []     for fd, event in pollster.poll(timeout):         if event & select.POLLNVAL:             raise ValueError(""invalid file descriptor %i"" % fd)         ls.append(fd_map[fd])     return ls","if hasattr ( fd , ""fileno"" ) :",if event & select . POLLERR :,5.08764122072739,5.08764122072739,0.0
"def default(cls, connection=None):     """"""show the default connection, or make CONNECTION the default""""""     if connection is not None:         target = cls._get_config_filename(connection)         <MASK>             if os.path.exists(cls._default_symlink):                 os.remove(cls._default_symlink)             os.symlink(target, cls._default_symlink)         else:             cls._no_config_file_error(target)     if os.path.exists(cls._default_symlink):         print(""Default connection is "" + cls._default_connection())     else:         print(""There is no default connection set"")",if os . path . exists ( target ) :,if os . path . isdir ( target ) :,65.80370064762461,65.80370064762461,0.0
"def process(self, fuzzresult):     base_url = urljoin(fuzzresult.url, "".."")     for line in fuzzresult.history.content.splitlines():         record = line.split(""/"")         <MASK>             self.queue_url(urljoin(base_url, record[1]))             # Directory             if record[0] == ""D"":                 self.queue_url(urljoin(base_url, record[1]))                 self.queue_url(urljoin(base_url, ""%s/CVS/Entries"" % (record[1])))",if len ( record ) == 6 and record [ 1 ] :,"if record [ 0 ] == ""CVS"" :",9.281844047221343,9.281844047221343,0.0
"def _GetCSVRow(self, value):     row = []     for type_info in value.__class__.type_infos:         <MASK>             row.extend(self._GetCSVRow(value.Get(type_info.name)))         elif isinstance(type_info, rdf_structs.ProtoBinary):             row.append(text.Asciify(value.Get(type_info.name)))         else:             row.append(str(value.Get(type_info.name)))     return row","if isinstance ( type_info , rdf_structs . ProtoEmbedded ) :","if isinstance ( type_info , rdf_structs . RDFType ) :",80.91067115702207,80.91067115702207,0.0
"def get_history(self, state, dict_, passive=PASSIVE_OFF):     if self.key in dict_:         return History.from_scalar_attribute(self, state, dict_[self.key])     else:         <MASK>             passive ^= INIT_OK         current = self.get(state, dict_, passive=passive)         if current is PASSIVE_NO_RESULT:             return HISTORY_BLANK         else:             return History.from_scalar_attribute(self, state, current)",if passive & INIT_OK :,if self . state == STATE_OK :,16.784459625186194,16.784459625186194,0.0
"def _iterate_self_and_parents(self, upto=None):     current = self     result = ()     while current:         result += (current,)         <MASK>             break         elif current._parent is None:             raise sa_exc.InvalidRequestError(                 ""Transaction %s is not on the active transaction list"" % (upto)             )         else:             current = current._parent     return result",if current . _parent is upto :,if current . _parent is self :,70.71067811865478,70.71067811865478,0.0
"def get_by_uri(self, uri: str) -> bytes:     userId, bucket, key = self._parse_uri(uri)     try:         with db.session_scope() as dbsession:             result = db_archivedocument.get(userId, bucket, key, session=dbsession)         <MASK>             return utils.ensure_bytes(self._decode(result))         else:             raise ObjectKeyNotFoundError(userId, bucket, key, caused_by=None)     except Exception as err:         logger.debug(""cannot get data: exception - "" + str(err))         raise err",if result :,if result :,100.00000000000004,0.0,1.0
"def app(scope, receive, send):     while True:         message = await receive()         if message[""type""] == ""websocket.connect"":             await send({""type"": ""websocket.accept""})         elif message[""type""] == ""websocket.receive"":             pass         <MASK>             break","elif message [ ""type"" ] == ""websocket.disconnect"" :",if not message :,1.2143667563059621,1.2143667563059621,0.0
"def recv_some(p, t=0.1, e=1, tr=5, stderr=0):     if tr < 1:         tr = 1     x = time.time() + t     y = []     r = """"     if stderr:         pr = p.recv_err     else:         pr = p.recv     while time.time() < x or r:         r = pr()         if r is None:             break         <MASK>             y.append(r)         else:             time.sleep(max((x - time.time()) / tr, 0))     return """".join(y)",elif r :,if e :,27.516060407455225,0.0,0.0
"def mouse_down(self, event):     if event.button == 1:         <MASK>             p = event.local             if self.scroll_up_rect().collidepoint(p):                 self.scroll_up()                 return             elif self.scroll_down_rect().collidepoint(p):                 self.scroll_down()                 return     if event.button == 4:         self.scroll_up()     if event.button == 5:         self.scroll_down()     GridView.mouse_down(self, event)",if self . scrolling :,if event . button == 2 :,7.267884212102741,7.267884212102741,0.0
"def copy_from(self, other):     if self is other:         return  # Myself!     self.strictness = other.strictness  # sets behaviors in bulk     for name in self.all_behaviors:         self.set_behavior(name, other.get_behavior(name))     for name in self._plain_attrs:         val = getattr(other, name)         if isinstance(val, set):             val = val.copy()         <MASK>             val = val.copy()         setattr(self, name, val)","elif decimal and isinstance ( val , decimal . Decimal ) :","if isinstance ( val , tuple ) :",24.936514388871338,24.936514388871338,0.0
"def __array_wrap__(self, out_arr, context=None):     if self.dim is None:         return out_arr     else:         this = self[:]         <MASK>             return Quantity.__array_wrap__(self[:], out_arr, context=context)         else:             return out_arr","if isinstance ( this , Quantity ) :",if this is not None :,7.654112967106117,7.654112967106117,0.0
"def _ArgumentListHasDictionaryEntry(self, token):     """"""Check if the function argument list has a dictionary as an arg.""""""     if _IsArgumentToFunction(token):         while token:             <MASK>                 length = token.matching_bracket.total_length - token.total_length                 return length + self.stack[-2].indent > self.column_limit             if token.ClosesScope():                 break             if token.OpensScope():                 token = token.matching_bracket             token = token.next_token     return False","if token . value == ""{"" :",if token . is_keyword :,17.112717058426785,17.112717058426785,0.0
"def save_all_changed_extensions(self):     """"""Save configuration changes to the user config file.""""""     has_changes = False     for ext_name in self.extensions:         options = self.extensions[ext_name]         for opt in options:             <MASK>                 has_changes = True     if has_changes:         self.ext_userCfg.Save()","if self . set_extension_value ( ext_name , opt ) :",if opt . value == self . ext_userCfg . value :,8.280453072947422,8.280453072947422,0.0
"def to_dict(self):     out = {}     for key in ACTIVITY_KEYS:         attr = getattr(self, key)         <MASK>             out[key] = str(attr)         else:             out[key] = attr     if self.streak:         out[""streak""] = self.streak     return out","if isinstance ( attr , ( datetime . timedelta , datetime . datetime ) ) :","if isinstance ( attr , str ) :",21.874242445215227,21.874242445215227,0.0
"def clean_publication_date(cls, cleaned_input):     for add_channel in cleaned_input.get(""add_channels"", []):         is_published = add_channel.get(""is_published"")         publication_date = add_channel.get(""publication_date"")         <MASK>             add_channel[""publication_date""] = datetime.date.today()",if is_published and not publication_date :,if is_published :,26.013004751144457,26.013004751144457,0.0
"def _random_blur(self, batch, sigma_max):     for i in range(len(batch)):         <MASK>             # Random sigma             sigma = random.uniform(0.0, sigma_max)             batch[i] = scipy.ndimage.filters.gaussian_filter(batch[i], sigma)     return batch",if bool ( random . getrandbits ( 1 ) ) :,if self . _is_blur :,4.995138898472386,4.995138898472386,0.0
"def conninfo_parse(dsn):     ret = {}     length = len(dsn)     i = 0     while i < length:         if dsn[i].isspace():             i += 1             continue         param_match = PARAMETER_RE.match(dsn[i:])         if not param_match:             return         param = param_match.group(1)         i += param_match.end()         <MASK>             return         value, end = read_param_value(dsn[i:])         if value is None:             return         i += end         ret[param] = value     return ret",if i >= length :,if param not in ret :,9.652434877402245,9.652434877402245,0.0
"def set_environment_vars(env, source_env):     """"""Copy allowed environment variables from |source_env|.""""""     if not source_env:         return     for name, value in six.iteritems(source_env):         if is_forwarded_environment_variable(name):             # Avoid creating circular dependencies from importing environment by             # using os.getenv.             <MASK>                 value = file_host.rebase_to_worker_root(value)             env[name] = value","if os . getenv ( ""TRUSTED_HOST"" ) and should_rebase_environment_value ( name ) :",if file_host . is_worker_root ( ) :,4.04853595631649,4.04853595631649,0.0
"def toterminal(self, tw):     # the entries might have different styles     last_style = None     for i, entry in enumerate(self.reprentries):         if entry.style == ""long"":             tw.line("""")         entry.toterminal(tw)         <MASK>             next_entry = self.reprentries[i + 1]             if (                 entry.style == ""long""                 or entry.style == ""short""                 and next_entry.style == ""long""             ):                 tw.sep(self.entrysep)     if self.extraline:         tw.line(self.extraline)",if i < len ( self . reprentries ) - 1 :,if i == 0 :,6.60902979597904,6.60902979597904,0.0
"def __init__(self, loc, tabs=None):     if os.path.isdir(loc):         for item in os.listdir(loc):             <MASK>                 continue             path = os.path.join(loc, item)             self.append(CronTab(user=False, tabfile=path))     elif os.path.isfile(loc):         self.append(CronTab(user=False, tabfile=loc))","if item [ 0 ] == ""."" :",if tabs and tabs [ item ] :,5.67557355463946,5.67557355463946,0.0
"def import_data(self, fname):     """"""Import data in current namespace""""""     if self.count():         nsb = self.currentWidget()         nsb.refresh_table()         nsb.import_data(fname)         <MASK>             self.dockwidget.setVisible(True)             self.dockwidget.raise_()",if self . dockwidget and not self . ismaximized :,if self . count ( ) > 0 :,17.0653267718276,17.0653267718276,0.0
"def get_menu_items(node):     aList = []     for child in node.children:         for tag in (""@menu"", ""@item""):             <MASK>                 name = child.h[len(tag) + 1 :].strip()                 if tag == ""@menu"":                     aList.append((""%s %s"" % (tag, name), get_menu_items(child), None))                 else:                     b = g.splitLines("""".join(child.b))                     aList.append((tag, name, b[0] if b else """"))                 break     return aList",if child . h . startswith ( tag ) :,if child . h :,26.013004751144457,26.013004751144457,0.0
"def __init__(self, *args, **kw):     if len(args) > 1:         raise TypeError(""MultiDict can only be called with one positional "" ""argument"")     if args:         if hasattr(args[0], ""iteritems""):             items = list(args[0].iteritems())         <MASK>             items = list(args[0].items())         else:             items = list(args[0])         self._items = items     else:         self._items = []     if kw:         self._items.extend(kw.items())","elif hasattr ( args [ 0 ] , ""items"" ) :","if hasattr ( args [ 0 ] , ""items"" ) :",91.21679090703874,91.21679090703874,0.0
"def open(self) -> ""KeyValueDb"":     """"""Create a new data base or open existing one""""""     if os.path.exists(self._name):         if not os.path.isfile(self._name):             raise IOError(""%s exists and is not a file"" % self._name)         <MASK>             # ignore empty files             return self         with open(self._name, ""rb"") as _in:  # binary mode             self.set_records(pickle.load(_in))     else:         # make sure path exists         mkpath(os.path.dirname(self._name))         self.commit()     return self",if os . path . getsize ( self . _name ) == 0 :,if self . _in :,6.483996016841,6.483996016841,0.0
"def sortModules(self):     super(NeuronDecomposableNetwork, self).sortModules()     self._constructParameterInfo()     # contains a list of lists of indices     self.decompositionIndices = {}     for neuron in self._neuronIterator():         self.decompositionIndices[neuron] = []     for w in range(self.paramdim):         inneuron, outneuron = self.paramInfo[w]         <MASK>             self.decompositionIndices[inneuron].append(w)         else:             self.decompositionIndices[outneuron].append(w)",if self . espStyleDecomposition and outneuron [ 0 ] in self . outmodules :,if inneuron :,0.8861688619210014,0.0,0.0
"def visit_Options(self, node: qlast.Options) -> None:     for i, opt in enumerate(node.options.values()):         <MASK>             self.write("" "")         self.write(opt.name)         if not isinstance(opt, qlast.Flag):             self.write(f"" {opt.val}"")",if i > 0 :,if i == 0 :,22.957488466614336,22.957488466614336,0.0
"def is_child_of(self, item_hash, possible_child_hash):     if self.get_last(item_hash) != self.get_last(possible_child_hash):         return None     while True:         if possible_child_hash == item_hash:             return True         <MASK>             return False         possible_child_hash = self.items[possible_child_hash].previous_hash",if possible_child_hash not in self . items :,if possible_child_hash in self . items :,74.26141117870938,74.26141117870938,0.0
"def __call__(self, text, **kargs):     words = jieba.tokenize(text, mode=""search"")     token = Token()     for (w, start_pos, stop_pos) in words:         <MASK>             continue         token.original = token.text = w         token.pos = start_pos         token.startchar = start_pos         token.endchar = stop_pos         yield token",if not accepted_chars . match ( w ) and len ( w ) <= 1 :,"if w == """" :",1.672612571673144,1.672612571673144,0.0
"def test_analysis_jobs_cypher_syntax(neo4j_session):     parameters = {         ""AWS_ID"": None,         ""UPDATE_TAG"": None,         ""OKTA_ORG_ID"": None,     }     for job_name in contents(""cartography.data.jobs.analysis""):         <MASK>             continue         try:             cartography.util.run_analysis_job(job_name, neo4j_session, parameters)         except Exception as e:             pytest.fail(                 f""run_analysis_job failed for analysis job '{job_name}' with exception: {e}""             )","if not job_name . endswith ( "".json"" ) :",if not job_name :,20.96310881302029,20.96310881302029,0.0
"def _interleave_dataset_results_and_tensors(dataset_results, flat_run_tensors):     flattened_results = []     for idx in range(len(dataset_results) + len(flat_run_tensors)):         <MASK>             flattened_results.append(dataset_results[idx])         else:             flattened_results.append(flat_run_tensors.pop(0))     return flattened_results",if dataset_results . get ( idx ) :,if idx in dataset_results :,18.938334565508196,18.938334565508196,0.0
"def test_k_is_stochastic_parameter(self):     # k as stochastic parameter     aug = iaa.MedianBlur(k=iap.Choice([3, 5]))     seen = [False, False]     for i in sm.xrange(100):         observed = aug.augment_image(self.base_img)         <MASK>             seen[0] += True         elif np.array_equal(observed, self.blur5x5):             seen[1] += True         else:             raise Exception(""Unexpected result in MedianBlur@2"")         if all(seen):             break     assert np.all(seen)","if np . array_equal ( observed , self . blur3x3 ) :",if i == 0 :,2.544354209531657,2.544354209531657,0.0
"def pickPath(self, color):     self.path[color] = ()     currentPos = self.starts[color]     while True:         minDist = None         minGuide = None         for guide in self.guides[color]:             guideDist = dist(currentPos, guide)             if minDist == None or guideDist < minDist:                 minDist = guideDist                 minGuide = guide         if dist(currentPos, self.ends[color]) == 1:             return         <MASK>             return         self.path[color] = self.path[color] + (minGuide,)         currentPos = minGuide         self.guides[color].remove(minGuide)",if minGuide == None :,if guideDist == minDist :,19.304869754804482,19.304869754804482,0.0
"def UpdateRepository(self):     if hasattr(self, ""commit_update""):         <MASK>             if not path.isdir("".git/""):                 self.gitZipRepo()             call([""git"", ""reset"", ""--hard"", ""origin/{}"".format(self.getBranch)])             self.ProcessCall_([""git"", ""pull"", ""origin"", self.getBranch])             self.ProcessCall_([""pip"", ""install"", ""-r"", ""requirements.txt""])","if self . commit_update [ ""Updates"" ] != [ ] :",if self . commit_update ( ) :,28.046732918876714,28.046732918876714,0.0
"def callback(result=Cr.NS_OK, message=None, success=None):     if success is None:         <MASK>             success = Ci.koIAsyncCallback.RESULT_SUCCESSFUL         else:             success = Ci.koIAsyncCallback.RESULT_ERROR     data = Namespace(result=result, message=message, _com_interfaces_=[Ci.koIErrorInfo])     self._invoke_activate_callbacks(success, data)",if Cr . NS_SUCCEEDED ( result ) :,if result == Cr . NS_OK :,30.213753973567687,30.213753973567687,0.0
"def get_location(device):     location = []     node = device     while node:         position = node.get_position() or """"         <MASK>             position = "" [%s]"" % position         location.append(node.name + position)         node = node.parent     return "" / "".join(reversed(location))",if position :,if position :,100.00000000000004,0.0,1.0
"def load_checkpoint(path, model, optimizer, reset_optimizer):     global global_step     global global_epoch     print(""Load checkpoint from: {}"".format(path))     checkpoint = _load(path)     model.load_state_dict(checkpoint[""state_dict""])     if not reset_optimizer:         optimizer_state = checkpoint[""optimizer""]         <MASK>             print(""Load optimizer state from {}"".format(path))             optimizer.load_state_dict(checkpoint[""optimizer""])     global_step = checkpoint[""global_step""]     global_epoch = checkpoint[""global_epoch""]     return model",if optimizer_state is not None :,if optimizer_state :,38.80684294761701,38.80684294761701,0.0
"def run_command(self, command: str, data: Dict[str, object]) -> Dict[str, object]:     """"""Run a specific command from the registry.""""""     key = ""cmd_"" + command     method = getattr(self.__class__, key, None)     if method is None:         return {""error"": ""Unrecognized command '%s'"" % command}     else:         <MASK>             # Only the above commands use some error formatting.             del data[""is_tty""]             del data[""terminal_width""]         return method(self, **data)","if command not in { ""check"" , ""recheck"" , ""run"" } :","if data [ ""is_tty"" ] :",2.666409111505393,2.666409111505393,0.0
"def call_init(self, node, instance):     # Call __init__ on each binding.     for b in instance.bindings:         <MASK>             continue         self._initialized_instances.add(b.data)         node = self._call_init_on_binding(node, b)     return node",if b . data in self . _initialized_instances :,if b . data is None :,21.28139770959968,21.28139770959968,0.0
"def get_request_headers() -> Dict:     url = urlparse(uri)     candidates = [         ""%s://%s"" % (url.scheme, url.netloc),         ""%s://%s/"" % (url.scheme, url.netloc),         uri,         ""*"",     ]     for u in candidates:         <MASK>             headers = dict(DEFAULT_REQUEST_HEADERS)             headers.update(self.config.linkcheck_request_headers[u])             return headers     return {}",if u in self . config . linkcheck_request_headers :,if u in self . config . linkcheck_request_headers :,100.00000000000004,100.00000000000004,1.0
"def get_next_video_frame(self, skip_empty_frame=True):     if not self.video_format:         return     while True:         # We skip video packets which are not video frames         # This happens in mkv files for the first few frames.         video_packet = self._get_video_packet()         if video_packet.image == 0:             self._decode_video_packet(video_packet)         <MASK>             break     if _debug:         print(""Returning"", video_packet)     return video_packet.image",if video_packet . image is not None or not skip_empty_frame :,if skip_empty_frame :,20.15216974557266,20.15216974557266,0.0
"def convert_path(ctx, tpath):     for points, code in tpath.iter_segments():         if code == Path.MOVETO:             ctx.move_to(*points)         elif code == Path.LINETO:             ctx.line_to(*points)         <MASK>             ctx.curve_to(                 points[0], points[1], points[0], points[1], points[2], points[3]             )         elif code == Path.CURVE4:             ctx.curve_to(*points)         elif code == Path.CLOSEPOLY:             ctx.close_path()",elif code == Path . CURVE3 :,if code == Path . CURVE2 :,54.10822690539397,54.10822690539397,0.0
"def __init__(     self, layout, value=None, string=None, *, dtype: np.dtype = np.float64 ) -> None:     """"""Constructor.""""""     self.layout = layout     if value is None:         if string is None:             self.value = np.zeros((self.layout.gaDims,), dtype=dtype)         else:             self.value = layout.parse_multivector(string).value     else:         self.value = np.array(value)         <MASK>             raise ValueError(                 ""value must be a sequence of length %s"" % self.layout.gaDims             )","if self . value . shape != ( self . layout . gaDims , ) :",if len ( value ) != self . layout . gaDims :,27.978036534915965,27.978036534915965,0.0
"def to_dict(self):     contexts_ = {}     for k, data in self.contexts.items():         data_ = data.copy()         if ""context"" in data_:             del data_[""context""]         <MASK>             del data_[""loaded""]         contexts_[k] = data_     return dict(contexts=contexts_)","if ""loaded"" in data_ :","if ""loaded"" in data_ :",100.00000000000004,100.00000000000004,1.0
"def include_module(module):     if not include_these:         return True     result = False     for check in include_these:         if ""/*"" in check:             <MASK>                 result = True         else:             if (os.getcwd() + ""/"" + check + "".py"") == module:                 result = True     if result:         print_status(""Including module: "" + module)     return result",if check [ : - 1 ] in module :,if module == check :,5.893383794376546,5.893383794376546,0.0
"def extract_from(msg_body, content_type=""text/plain""):     try:         if content_type == ""text/plain"":             return extract_from_plain(msg_body)         <MASK>             return extract_from_html(msg_body)     except Exception:         log.exception(""ERROR extracting message"")     return msg_body","elif content_type == ""text/html"" :","if content_type == ""text/html"" :",90.36020036098445,90.36020036098445,0.0
"def test_list(self):     self._create_locations()     response = self.client.get(self.geojson_boxedlocation_list_url)     self.assertEqual(response.status_code, 200)     self.assertEqual(len(response.data[""features""]), 2)     for feature in response.data[""features""]:         self.assertIn(""bbox"", feature)         fid = feature[""id""]         <MASK>             self.assertEqual(feature[""bbox""], self.bl1.bbox_geometry.extent)         elif fid == 2:             self.assertEqual(feature[""bbox""], self.bl2.bbox_geometry.extent)         else:             self.fail(""Unexpected id: {0}"".format(fid))     BoxedLocation.objects.all().delete()",if fid == 1 :,if fid == 1 :,100.00000000000004,100.00000000000004,1.0
"def overrideCommand(self, commandName, func):     # Override entries in c.k.masterBindingsDict     k = self     d = k.masterBindingsDict     for key in d:         d2 = d.get(key)         for key2 in d2:             bi = d2.get(key2)             <MASK>                 bi.func = func                 d2[key2] = bi",if bi . commandName == commandName :,if bi is not None :,12.872632311973014,12.872632311973014,0.0
"def _lookup(components, specs, provided, name, i, l):     if i < l:         for spec in specs[i].__sro__:             comps = components.get(spec)             <MASK>                 r = _lookup(comps, specs, provided, name, i + 1, l)                 if r is not None:                     return r     else:         for iface in provided:             comps = components.get(iface)             if comps:                 r = comps.get(name)                 if r is not None:                     return r     return None",if comps :,if comps :,100.00000000000004,0.0,1.0
"def to_representation(self, value):     old_social_string_fields = [""twitter"", ""github"", ""linkedIn""]     request = self.context.get(""request"")     show_old_format = (         request         and is_deprecated(request.version, self.min_version)         and request.method == ""GET""     )     if show_old_format:         social = value.copy()         for key in old_social_string_fields:             if social.get(key):                 social[key] = value[key][0]             <MASK>                 social[key] = """"         value = social     return super(SocialField, self).to_representation(value)",elif social . get ( key ) == [ ] :,if social . get ( key ) :,42.88819424803536,42.88819424803536,0.0
"def process_ref_attribute(self, node, array_type=None):     ref = qname_attr(node, ""ref"")     if ref:         ref = self._create_qname(ref)         # Some wsdl's reference to xs:schema, we ignore that for now. It         # might be better in the future to process the actual schema file         # so that it is handled correctly         <MASK>             return         return xsd_elements.RefAttribute(             node.tag, ref, self.schema, array_type=array_type         )","if ref . namespace == ""http://www.w3.org/2001/XMLSchema"" :",if not self . schema :,0.7422343966767406,0.7422343966767406,0.0
"def unescape(text):     """"""Removes '\\' escaping from 'text'.""""""     rv = """"     i = 0     while i < len(text):         <MASK>             rv += text[i + 1]             i += 1         else:             rv += text[i]         i += 1     return rv","if i + 1 < len ( text ) and text [ i ] == ""\\"" :",if text [ i ] == '\\' :,23.437330946644725,23.437330946644725,0.0
"def wait_child_process(signum, frame):     try:         while True:             child_pid, status = os.waitpid(-1, os.WNOHANG)             if child_pid == 0:                 stat_logger.info(""no child process was immediately available"")                 break             exitcode = status >> 8             stat_logger.info(                 ""child process %s exit with exitcode %s"", child_pid, exitcode             )     except OSError as e:         <MASK>             stat_logger.warning(                 ""current process has no existing unwaited-for child processes.""             )         else:             raise",if e . errno == errno . ECHILD :,if exitcode == 0 :,9.911450612811139,9.911450612811139,0.0
"def translate_from_sortname(name, sortname):     """"""'Translate' the artist name by reversing the sortname.""""""     for c in name:         ctg = unicodedata.category(c)         if ctg[0] == ""L"" and unicodedata.name(c).find(""LATIN"") == -1:             for separator in ("" & "", ""; "", "" and "", "" vs. "", "" with "", "" y ""):                 <MASK>                     parts = sortname.split(separator)                     break             else:                 parts = [sortname]                 separator = """"             return separator.join(map(_reverse_sortname, parts))     return name",if separator in sortname :,"if ctg [ 0 ] == ""L"" :",4.456882760699063,4.456882760699063,0.0
"def python_value(self, value):     if value:         <MASK>             pp = lambda x: x.time()             return format_date_time(value, self.formats, pp)         elif isinstance(value, datetime.datetime):             return value.time()     if value is not None and isinstance(value, datetime.timedelta):         return (datetime.datetime.min + value).time()     return value","if isinstance ( value , basestring ) :","if isinstance ( value , datetime . datetime ) :",45.180100180492246,45.180100180492246,0.0
"def __init__(self, fileobj, info):     pages = []     complete = False     while not complete:         page = OggPage(fileobj)         <MASK>             pages.append(page)             complete = page.complete or (len(page.packets) > 1)     data = OggPage.to_packets(pages)[0][7:]     super(OggTheoraCommentDict, self).__init__(data, framing=False)     self._padding = len(data) - self._size",if page . serial == info . serial :,if page . complete :,15.719010513286515,15.719010513286515,0.0
"def configure(self):     # hack to configure 'from_' and 'to' and avoid exception     if ""from_"" in self.wmeta.properties:         from_ = float(self.wmeta.properties[""from_""])         to = float(self.wmeta.properties.get(""to"", 0))         <MASK>             to = from_ + 1             self.wmeta.properties[""to""] = str(to)     super(TKSpinbox, self).configure()",if from_ > to :,if from_ < to :,37.99178428257963,37.99178428257963,0.0
"def get_error_diagnostics(self):     diagnostics = []     if self.stdout is not None:         with open(self.stdout.name) as fds:             contents = fds.read().strip()             <MASK>                 diagnostics.append(""ab STDOUT:\n"" + contents)     if self.stderr is not None:         with open(self.stderr.name) as fds:             contents = fds.read().strip()             if contents.strip():                 diagnostics.append(""ab STDERR:\n"" + contents)     return diagnostics",if contents . strip ( ) :,if contents . strip ( ) :,100.00000000000004,100.00000000000004,1.0
"def set_environment_vars(env, source_env):     """"""Copy allowed environment variables from |source_env|.""""""     if not source_env:         return     for name, value in six.iteritems(source_env):         <MASK>             # Avoid creating circular dependencies from importing environment by             # using os.getenv.             if os.getenv(""TRUSTED_HOST"") and should_rebase_environment_value(name):                 value = file_host.rebase_to_worker_root(value)             env[name] = value",if is_forwarded_environment_variable ( name ) :,if os . path . isfile ( value ) :,8.171014300726602,8.171014300726602,0.0
"def update_content(self, more_content: StringList) -> None:     if isinstance(self.object, TypeVar):         attrs = [repr(self.object.__name__)]         for constraint in self.object.__constraints__:             attrs.append(stringify_typehint(constraint))         if self.object.__covariant__:             attrs.append(""covariant=True"")         <MASK>             attrs.append(""contravariant=True"")         more_content.append(_(""alias of TypeVar(%s)"") % "", "".join(attrs), """")         more_content.append("""", """")     super().update_content(more_content)",if self . object . __contravariant__ :,if self . object . __contravariant__ :,100.00000000000004,100.00000000000004,1.0
"def after(self, event, state):     group = event.group     for plugin in self.get_plugins():         <MASK>             continue         metrics.incr(""notifications.sent"", instance=plugin.slug)         yield self.future(plugin.rule_notify)","if not safe_execute ( plugin . should_notify , group = group , event = event ) :",if plugin . rule_notify is None :,3.933852380954249,3.933852380954249,0.0
"def distinct(expr, *on):     fields = frozenset(expr.fields)     _on = []     append = _on.append     for n in on:         if isinstance(n, Field):             <MASK>                 n = n._name             else:                 raise ValueError(""{0} is not a field of {1}"".format(n, expr))         if not isinstance(n, _strtypes):             raise TypeError(""on must be a name or field, not: {0}"".format(n))         elif n not in fields:             raise ValueError(""{0} is not a field of {1}"".format(n, expr))         append(n)     return Distinct(expr, tuple(_on))",if n . _child . isidentical ( expr ) :,if n . _name :,23.350308364304226,23.350308364304226,0.0
"def build_filter(arg):     filt = {}     if arg is not None:         <MASK>             raise UserError(""Arguments to --filter should be in form KEY=VAL"")         key, val = arg.split(""="", 1)         filt[key] = val     return filt","if ""="" not in arg :","if ""KEY"" not in arg :",59.4603557501361,59.4603557501361,0.0
"def pickline(file, key, casefold=1):     try:         f = open(file, ""r"")     except IOError:         return None     pat = re.escape(key) + "":""     prog = re.compile(pat, casefold and re.IGNORECASE)     while 1:         line = f.readline()         if not line:             break         <MASK>             text = line[len(key) + 1 :]             while 1:                 line = f.readline()                 if not line or not line[0].isspace():                     break                 text = text + line             return text.strip()     return None",if prog . match ( line ) :,if line [ 0 ] == prog :,6.742555929751843,6.742555929751843,0.0
"def delete_doc(elastic_document_id, node, index=None, category=None):     index = index or INDEX     if not category:         <MASK>             category = ""preprint""         elif node.is_registration:             category = ""registration""         else:             category = node.project_or_component     client().delete(         index=index,         doc_type=category,         id=elastic_document_id,         refresh=True,         ignore=[404],     )","if isinstance ( node , Preprint ) :",if node . is_preprint :,7.492442692259767,7.492442692259767,0.0
"def update(self, preds, labels):     if not _is_numpy_(labels):         raise ValueError(""The 'labels' must be a numpy ndarray."")     if not _is_numpy_(preds):         raise ValueError(""The 'predictions' must be a numpy ndarray."")     for i, lbl in enumerate(labels):         value = preds[i, 1]         bin_idx = int(value * self._num_thresholds)         assert bin_idx <= self._num_thresholds         <MASK>             self._stat_pos[bin_idx] += 1.0         else:             self._stat_neg[bin_idx] += 1.0",if lbl :,if lbl == self . _thresholds :,10.552670315936318,10.552670315936318,0.0
"def checkStatusClient(self):     if str(self.comboxBoxIPAddress.currentText()) != """":         <MASK>             self.btnEnable.setEnabled(False)             self.btncancel.setEnabled(True)             return None         self.btnEnable.setEnabled(True)         self.btncancel.setEnabled(False)","if self . ClientsLogged [ str ( self . comboxBoxIPAddress . currentText ( ) ) ] [ ""Status"" ] :","if self . comboxBoxIPAddress . currentText ( ) != """" :",28.66690190299954,28.66690190299954,0.0
"def colorizeDiffs(sheet, col, row, cellval):     if not row or not col:         return None     vcolidx = sheet.visibleCols.index(col)     rowidx = sheet.rows.index(row)     if vcolidx < len(othersheet.visibleCols) and rowidx < len(othersheet.rows):         otherval = othersheet.visibleCols[vcolidx].getDisplayValue(             othersheet.rows[rowidx]         )         <MASK>             return ""color_diff""     else:         return ""color_diff_add""",if cellval . display != otherval :,if otherval == cellval :,8.69675138635599,8.69675138635599,0.0
"def identwaf(self, findall=False):     detected = list()     try:         self.attackres = self.performCheck(self.centralAttack)     except RequestBlocked:         return detected     for wafvendor in self.checklist:         self.log.info(""Checking for %s"" % wafvendor)         <MASK>             detected.append(wafvendor)             if not findall:                 break     self.knowledge[""wafname""] = detected     return detected",if self . wafdetections [ wafvendor ] ( self ) :,if detected . count ( ) > 0 :,5.708765135015525,5.708765135015525,0.0
"def get_repository_metadata_by_repository_id_changeset_revision(     app, id, changeset_revision, metadata_only=False ):     """"""Get a specified metadata record for a specified repository in the tool shed.""""""     if metadata_only:         repository_metadata = get_repository_metadata_by_changeset_revision(             app, id, changeset_revision         )         <MASK>             return repository_metadata.metadata         return None     return get_repository_metadata_by_changeset_revision(app, id, changeset_revision)",if repository_metadata and repository_metadata . metadata :,if repository_metadata :,22.88581105225991,22.88581105225991,0.0
"def getmultiline(self):     line = self.getline()     if line[3:4] == ""-"":         code = line[:3]         while 1:             nextline = self.getline()             line = line + (""\n"" + nextline)             <MASK>                 break     return line","if nextline [ : 3 ] == code and nextline [ 3 : 4 ] != ""-"" :","if code != "" "" :",3.9228027456724393,3.9228027456724393,0.0
"def _validate_reports(value, *args, **kwargs):     from osf.models import OSFUser     for key, val in value.items():         if not OSFUser.load(key):             raise ValidationValueError(""Keys must be user IDs"")         <MASK>             raise ValidationTypeError(""Values must be dictionaries"")         if (             ""category"" not in val             or ""text"" not in val             or ""date"" not in val             or ""retracted"" not in val         ):             raise ValidationValueError(                 (""Values must include `date`, `category`, "", ""`text`, `retracted` keys"")             )","if not isinstance ( val , dict ) :",if not val . keys ( ) :,15.25487608028144,15.25487608028144,0.0
"def deselectItem(self, item):     if self.isSelected(item):         <MASK>             listItem = self._getListItem(item)             selections = self.getSelectedItems()             selections.remove(self.loadHandler.getSelection(listItem))             self.setSelections(selections)         else:             self.deselectAll()",if self . multiSelect :,if self . loadHandler . isSelected ( listItem ) :,16.784459625186194,16.784459625186194,0.0
"def __init__(self, **kwargs):     if self.name is None:         raise RuntimeError(""RenderPrimitive cannot be used directly"")     self.option_values = {}     for key, val in kwargs.items():         if not key in self.options:             raise ValueError(                 ""primitive `{0}' has no option `{1}'"".format(self.name, key)             )         self.option_values[key] = val     # set up defaults     for name, (description, default) in self.options.items():         <MASK>             self.option_values[name] = default",if not name in self . option_values :,if description :,3.361830360737634,0.0,0.0
"def setup_smart_indent(self, view, lang):     # Configure a ""per-view"" instance     if type(view) == gedit.View:         <MASK>             setattr(view, ""smart_indent_instance"", SmartIndent())             handler_id = view.connect(                 ""key-press-event"", view.smart_indent_instance.key_press_handler             )             self.handler_ids.append((handler_id, view))         view.smart_indent_instance.set_language(lang, view)","if getattr ( view , ""smart_indent_instance"" , False ) == False :","if not isinstance ( view , gedit . View ) :",8.016891111916946,8.016891111916946,0.0
"def get_strings_of_set(word, char_set, threshold=20):     count = 0     letters = """"     strings = []     for char in word:         <MASK>             letters += char             count += 1         else:             if count > threshold:                 strings.append(letters)             letters = """"             count = 0     if count > threshold:         strings.append(letters)     return strings",if char in char_set :,if char_set [ count ] == char :,18.36028134946796,18.36028134946796,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             self.set_logout_url(d.getPrefixedString())             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt == 0 :,100.00000000000004,100.00000000000004,1.0
def __create_table(self):     for i in range(256):         crcreg = i         for j in range(8):             <MASK>                 crcreg = self.__CRCPOLYNOMIAL ^ (crcreg >> 1)             else:                 crcreg >>= 1         self.__crctable[i] = crcreg,if ( crcreg & 1 ) != 0 :,if crcreg & 1 :,16.62083000646927,16.62083000646927,0.0
"def destroy(self):     """"""Flush all entries and empty cache""""""     # Note: this method is currently also used for dropping the cache     for i in range(len(self.cached_rows)):         id_ = self.cached_rows[i]         self.cached_rows[i] = None         <MASK>             try:                 inode = self.attrs[id_]             except KeyError:                 # We may have deleted that inode                 pass             else:                 del self.attrs[id_]                 self.setattr(inode)     assert len(self.attrs) == 0",if id_ is not None :,if id_ in self . attrs :,22.089591134157878,22.089591134157878,0.0
"def set_config(self):     """"""Set configuration options for QTextEdit.""""""     c = self.c     w = self.widget     w.setWordWrapMode(QtGui.QTextOption.NoWrap)     if 0:  # This only works when there is no style sheet.         n = c.config.getInt(""qt-rich-text-zoom-in"")         <MASK>             w.zoomIn(n)             w.updateMicroFocus()     # tab stop in pixels - no config for this (yet)     w.setTabStopWidth(24)","if n not in ( None , 0 ) :",if n > 0 :,9.346579571601447,9.346579571601447,0.0
"def mouseDragEvent(self, ev):     if self.movable and ev.button() == QtCore.Qt.LeftButton:         if ev.isStart():             self.moving = True             self.cursorOffset = self.pos() - self.mapToParent(ev.buttonDownPos())             self.startPosition = self.pos()         ev.accept()         if not self.moving:             return         self.setPos(self.cursorOffset + self.mapToParent(ev.pos()))         self.sigDragged.emit(self)         <MASK>             self.moving = False             self.sigPositionChangeFinished.emit(self)",if ev . isFinish ( ) :,if ev . isEnd ( ) :,41.11336169005196,41.11336169005196,0.0
"def reparentChildren(self, newParent):     if newParent.childNodes:         newParent.childNodes[-1]._element.tail += self._element.text     else:         if not newParent._element.text:             newParent._element.text = """"         <MASK>             newParent._element.text += self._element.text     self._element.text = """"     base.Node.reparentChildren(self, newParent)",if self . _element . text is not None :,if self . _element . text :,59.755798910891144,59.755798910891144,0.0
"def _no_sp_or_bp(self, bl):     for s in bl.vex.statements:         for e in chain([s], s.expressions):             <MASK>                 reg = self.get_reg_name(self.project.arch, e.offset)                 if reg == ""ebp"" or reg == ""esp"":                     return False             elif e.tag == ""Ist_Put"":                 reg = self.get_reg_name(self.project.arch, e.offset)                 if reg == ""ebp"" or reg == ""esp"":                     return False     return True","if e . tag == ""Iex_Get"" :","if e . tag == ""Ist_Sp"" :",58.59059370151705,58.59059370151705,0.0
"def _get_import_chain(self, *, until=None):     stack = inspect.stack()[2:]     try:         for frameinfo in stack:             try:                 <MASK>                     continue                 data = dedent("""".join(frameinfo.code_context))                 if data.strip() == until:                     raise StopIteration                 yield frameinfo.filename, frameinfo.lineno, data.strip()                 del data             finally:                 del frameinfo     finally:         del stack",if not frameinfo . code_context :,if frameinfo . code_context is None :,48.54917717073236,48.54917717073236,0.0
"def stream_docker_log(log_stream):     async for line in log_stream:         if ""stream"" in line and line[""stream""].strip():             logger.debug(line[""stream""].strip())         elif ""status"" in line:             logger.debug(line[""status""].strip())         <MASK>             logger.error(line[""error""].strip())             raise DockerBuildError","elif ""error"" in line :","if ""error"" in line :",80.91067115702207,80.91067115702207,0.0
"def get_cycle_path(self, curr_node, goal_node_index):     for dep in curr_node[""deps""]:         if dep == goal_node_index:             return [curr_node[""address""]]     for dep in curr_node[""deps""]:         path = self.get_cycle_path(             self.get_by_address(dep), goal_node_index         )  # self.nodelist[dep], goal_node_index)         <MASK>             path.insert(0, curr_node[""address""])             return path     return []",if len ( path ) > 0 :,if path :,7.49553326588684,0.0,0.0
"def prompt(default=None):     editor = ""nano""     with tempfile.NamedTemporaryFile(mode=""r+"") as tmpfile:         <MASK>             tmpfile.write(default)             tmpfile.flush()         child_pid = os.fork()         is_child = child_pid == 0         if is_child:             os.execvp(editor, [editor, tmpfile.name])         else:             os.waitpid(child_pid, 0)             tmpfile.seek(0)             return tmpfile.read().strip()",if default :,if default :,100.00000000000004,0.0,1.0
"def _get_annotated_template(self, template):     changed = False     if template.get(""version"", ""0.12.0"") >= ""0.13.0"":         using_js = self.spider._filter_js_urls(template[""url""])         body = ""rendered_body"" if using_js else ""original_body""         <MASK>             template[""body""] = body             changed = True     if changed or not template.get(""annotated""):         _build_sample(template)     return template","if template . get ( ""body"" ) != body :",if body :,2.247320757600649,0.0,0.0
"def collect(self, paths):     for path in paths or ():         relpath = os.path.relpath(path, self._artifact_root)         dst = os.path.join(self._directory, relpath)         safe_mkdir(os.path.dirname(dst))         <MASK>             shutil.copytree(path, dst)         else:             shutil.copy(path, dst)         self._relpaths.add(relpath)",if os . path . isdir ( path ) :,if self . _tree :,5.484411595600381,5.484411595600381,0.0
"def dependencies(context=None):     """"""Return all dependencies detected by knowit.""""""     deps = OrderedDict([])     try:         initialize(context)         for name, provider_cls in _provider_map.items():             <MASK>                 deps[name] = available_providers[name].version             else:                 deps[name] = {}     except Exception:         pass     return deps",if name in available_providers :,if provider_cls . version_id in available_providers :,30.26643726685862,30.26643726685862,0.0
"def _getaddrinfo(self, host_bytes, port, family, socktype, proto, flags):     while True:         ares = self.cares         try:             return self.__getaddrinfo(host_bytes, port, family, socktype, proto, flags)         except gaierror:             <MASK>                 raise",if ares is self . cares :,if ares . status_code != gaierror . EADDRINFO :,8.054496384843702,8.054496384843702,0.0
"def write_entries(cmd, basename, filename):     ep = cmd.distribution.entry_points     if isinstance(ep, basestring) or ep is None:         data = ep     elif ep is not None:         data = []         for section, contents in ep.items():             <MASK>                 contents = EntryPoint.parse_group(section, contents)                 contents = ""\n"".join(map(str, contents.values()))             data.append(""[%s]\n%s\n\n"" % (section, contents))         data = """".join(data)     cmd.write_or_delete_file(""entry points"", filename, data, True)","if not isinstance ( contents , basestring ) :",if section in contents :,6.316906128202129,6.316906128202129,0.0
"def _highlight_do(self):     new_hl_text = self.highlight_text.text()     if new_hl_text != self.hl_text:         self.hl_text = new_hl_text         if self.hl is not None:             self.hl.setDocument(None)             self.hl = None         <MASK>             self.hl = Highlighter(self.hl_text, parent=self.doc)         self.clear_highlight_button.setEnabled(bool(self.hl))",if self . hl_text :,if self . hl_text is not None :,53.7284965911771,53.7284965911771,0.0
"def traverse(node, functions=[]):     if hasattr(node, ""grad_fn""):         node = node.grad_fn     if hasattr(node, ""variable""):         node = graph.nodes_by_id.get(id(node.variable))         <MASK>             node.functions = list(functions)             del functions[:]     if hasattr(node, ""next_functions""):         functions.append(type(node).__name__)         for f in node.next_functions:             if f[0]:                 functions.append(type(f[0]).__name__)                 traverse(f[0], functions)     if hasattr(node, ""saved_tensors""):         for t in node.saved_tensors:             traverse(t)",if node :,"if hasattr ( node , ""functions"" ) :",5.522397783539471,5.522397783539471,0.0
"def compress(self, data_list):     if data_list:         page_id = data_list[1]         if page_id in EMPTY_VALUES:             <MASK>                 return None             raise forms.ValidationError(self.error_messages[""invalid_page""])         return Page.objects.get(pk=page_id)     return None",if not self . required :,if page_id not in self . page_ids :,8.516593018819643,8.516593018819643,0.0
"def test_field_attr_existence(self):     for name, item in ast.__dict__.items():         if self._is_ast_node(name, item):             if name == ""Index"":                 # Index(value) just returns value now.                 # The argument is required.                 continue             x = item()             <MASK>                 self.assertEqual(type(x._fields), tuple)","if isinstance ( x , ast . AST ) :","if isinstance ( x , _fields ) :",45.93613320783059,45.93613320783059,0.0
"def handle_starttag(self, tag, attrs):     if tag == ""base"":         self.base_url = dict(attrs).get(""href"")     if self.scan_tag(tag):         for attr, value in attrs:             if self.scan_attr(attr):                 <MASK>                     value = strip_html5_whitespace(value)                 url = self.process_attr(value)                 link = Link(url=url)                 self.links.append(link)                 self.current_link = link",if self . strip :,if self . scan_tag ( attr ) :,16.784459625186194,16.784459625186194,0.0
"def _initialize_asset_map(cls):     # Generating a list of acceptable asset files reduces the possibility of     # path attacks.     cls._asset_name_to_path = {}     assets = os.listdir(ASSETS_PATH)     for asset in assets:         path = os.path.join(ASSETS_PATH, asset)         <MASK>             cls._asset_name_to_path[os.path.basename(path)] = path",if os . path . isfile ( path ) :,if os . path . isfile ( path ) :,100.00000000000004,100.00000000000004,1.0
"def dataReceived(self, data):     self.buf += data     if self._paused:         log.startLogging(sys.stderr)         log.msg(""dataReceived while transport paused!"")         self.transport.loseConnection()     else:         self.transport.write(data)         <MASK>             self.transport.loseConnection()         else:             self.pause()","if self . buf . endswith ( b""\n0\n"" ) :",if self . _paused :,6.132184825737391,6.132184825737391,0.0
"def test_case_sensitive(self):     with support.EnvironmentVarGuard() as env:         env.unset(""PYTHONCASEOK"")         <MASK>             self.skipTest(""os.environ changes not reflected in "" ""_os.environ"")         loader = self.find_module()         self.assertIsNone(loader)","if b""PYTHONCASEOK"" in _bootstrap_external . _os . environ :","if env . get ( ""PYTHONCASEOK"" ) != ""true"" :",10.883006131731655,10.883006131731655,0.0
"def manifest(self):     """"""The current manifest dictionary.""""""     if self.reload:         if not self.exists(self.manifest_path):             return {}         mtime = self.getmtime(self.manifest_path)         <MASK>             self._manifest = self.get_manifest()             self._mtime = mtime     return self._manifest",if self . _mtime is None or mtime > self . _mtime :,if mtime > self . mtime :,16.204686537755634,16.204686537755634,0.0
"def test_named_parameters_and_constraints(self):     likelihood = gpytorch.likelihoods.GaussianLikelihood()     model = ExactGPModel(None, None, likelihood)     for name, _param, constraint in model.named_parameters_and_constraints():         <MASK>             self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan)         elif name == ""mean_module.constant"":             self.assertIsNone(constraint)         elif name == ""covar_module.raw_outputscale"":             self.assertIsInstance(constraint, gpytorch.constraints.Positive)         elif name == ""covar_module.base_kernel.raw_lengthscale"":             self.assertIsInstance(constraint, gpytorch.constraints.Positive)","if name == ""likelihood.noise_covar.raw_noise"" :","if name == ""mean_module.constant"" :",27.82095306497951,27.82095306497951,0.0
"def process_plugin_result(name, result):     if result:         try:             jsonify(test=result)         except Exception:             logger.exception(                 ""Error while jsonifying settings from plugin {}, please contact the plugin author about this"".format(                     name                 )             )             raise         else:             <MASK>                 del result[""__enabled""]             data[name] = result","if ""__enabled"" in result :",if name in data :,6.316906128202129,6.316906128202129,0.0
"def benchmarking(net, ctx, num_iteration, datashape=300, batch_size=64):     input_shape = (batch_size, 3) + (datashape, datashape)     data = mx.random.uniform(-1.0, 1.0, shape=input_shape, ctx=ctx, dtype=""float32"")     dryrun = 5     for i in range(dryrun + num_iteration):         <MASK>             tic = time.time()         ids, scores, bboxes = net(data)         ids.asnumpy()         scores.asnumpy()         bboxes.asnumpy()     toc = time.time() - tic     return toc",if i == dryrun :,if i % 2 :,19.3576934939088,19.3576934939088,0.0
"def merge_weekdays(base_wd, icu_wd):     result = []     for left, right in zip(base_wd, icu_wd):         <MASK>             result.append(left)             continue         left = set(left.split(""|""))         right = set(right.split(""|""))         result.append(""|"".join(left | right))     return result",if left == right :,if left == right :,100.00000000000004,100.00000000000004,1.0
"def create_key(self, request):     if self._ignored_parameters:         url, body = self._remove_ignored_parameters(request)     else:         url, body = request.url, request.body     key = hashlib.sha256()     key.update(_to_bytes(request.method.upper()))     key.update(_to_bytes(url))     if request.body:         key.update(_to_bytes(body))     else:         <MASK>             for name, value in sorted(request.headers.items()):                 key.update(_to_bytes(name))                 key.update(_to_bytes(value))     return key.hexdigest()",if self . _include_get_headers and request . headers != _DEFAULT_HEADERS :,if request . headers :,2.2493847365531097,2.2493847365531097,0.0
"def test_invalid_mountinfo(self):     line = (         ""20 1 252:1 / / rw,relatime - ext4 /dev/mapper/vg0-root""         ""rw,errors=remount-ro,data=ordered""     )     elements = line.split()     for i in range(len(elements) + 1):         lines = ["" "".join(elements[0:i])]         <MASK>             expected = None         else:             expected = (""/dev/mapper/vg0-root"", ""ext4"", ""/"")         self.assertEqual(expected, util.parse_mount_info(""/"", lines))",if i < 10 :,if not lines :,14.794015674776452,14.794015674776452,0.0
"def nested_filter(self, items, mask):     keep_current = self.current_mask(mask)     keep_nested_lookup = self.nested_masks(mask)     for k, v in items:         keep_nested = keep_nested_lookup.get(k)         if k in keep_current:             if keep_nested is not None:                 <MASK>                     yield k, dict(self.nested_filter(v.items(), keep_nested))             else:                 yield k, v","if isinstance ( v , dict ) :",if keep_nested is not None :,6.567274736060395,6.567274736060395,0.0
"def traverse_trees(node_pos, sample, trees: List[HeteroDecisionTreeGuest]):     if node_pos[""reach_leaf_node""].all():         return node_pos     for t_idx, tree in enumerate(trees):         cur_node_idx = node_pos[""node_pos""][t_idx]         # reach leaf         if cur_node_idx == -1:             continue         rs, reach_leaf = HeteroSecureBoostingTreeGuest.traverse_a_tree(             tree, sample, cur_node_idx         )         <MASK>             node_pos[""reach_leaf_node""][t_idx] = True         node_pos[""node_pos""][t_idx] = rs     return node_pos",if reach_leaf :,if rs and reach_leaf :,43.47208719449914,43.47208719449914,0.0
"def _pop_waiting_trial_id(self) -> Optional[int]:     # TODO(c-bata): Reduce database query counts for extracting waiting trials.     for trial in self._storage.get_all_trials(self._study_id, deepcopy=False):         <MASK>             continue         if not self._storage.set_trial_state(trial._trial_id, TrialState.RUNNING):             continue         _logger.debug(""Trial {} popped from the trial queue."".format(trial.number))         return trial._trial_id     return None",if trial . state != TrialState . WAITING :,if not trial . number :,9.911450612811139,9.911450612811139,0.0
"def get_step_best(self, step_models):     best_score = None     best_model = """"     for model in step_models:         model_info = self.models_trained[model]         score = model_info.get_score()         <MASK>             continue         if best_score is None or score < best_score:             best_score = score             best_model = model     LOGGER.info(f""step {self.n_step}, best model {best_model}"")     return best_model",if score is None :,if score is None :,100.00000000000004,100.00000000000004,1.0
"def iter_filters(filters, block_end=False):     queue = deque(filters)     while queue:         f = queue.popleft()         if f is not None and f.type in (""or"", ""and"", ""not""):             <MASK>                 queue.appendleft(None)             for gf in f.filters:                 queue.appendleft(gf)         yield f",if block_end :,if block_end :,100.00000000000004,100.00000000000004,1.0
"def _buffer_decode(self, input, errors, final):     if self.decoder is None:         (output, consumed, byteorder) = codecs.utf_16_ex_decode(input, errors, 0, final)         if byteorder == -1:             self.decoder = codecs.utf_16_le_decode         <MASK>             self.decoder = codecs.utf_16_be_decode         elif consumed >= 2:             raise UnicodeError(""UTF-16 stream does not start with BOM"")         return (output, consumed)     return self.decoder(input, self.errors, final)",elif byteorder == 1 :,if consumed == 0 :,17.965205598154213,17.965205598154213,0.0
"def _load_db(self):     try:         with open(self.db) as db:             content = db.read(8)             db.seek(0)             <MASK>                 data = StringIO()                 if self.encryptor:                     self.encryptor.decrypt(db, data)                 else:                     raise EncryptionError(                         ""Encrpyted credential storage: {}"".format(self.db)                     )                 return json.loads(data.getvalue())             else:                 return json.load(db)     except:         return {""creds"": []}","if content == ( ""Salted__"" ) :",if content :,3.136388772461349,0.0,0.0
"def _getbytes(self, start, l=1):     out = []     for ad in range(l):         offset = ad + start + self.base_address         <MASK>             raise IOError(""not enough bytes"")         out.append(int_to_byte(Byte(offset)))     return b"""".join(out)",if not is_mapped ( offset ) :,if offset > self . size :,6.495032985064742,6.495032985064742,0.0
"def cache_sqs_queues_across_accounts() -> bool:     function: str = f""{__name__}.{sys._getframe().f_code.co_name}""     # First, get list of accounts     accounts_d: list = async_to_sync(get_account_id_to_name_mapping)()     # Second, call tasks to enumerate all the roles across all accounts     for account_id in accounts_d.keys():         if config.get(""environment"") == ""prod"":             cache_sqs_queues_for_account.delay(account_id)         else:             <MASK>                 cache_sqs_queues_for_account.delay(account_id)     stats.count(f""{function}.success"")     return True","if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :","if config . get ( ""environment"" ) == ""prod"" :",16.716822873683913,16.716822873683913,0.0
"def insertLine(self, refnum, linenum, line):     i = -1     for i, row in enumerate(self.rows):         if row[0] == linenum:             <MASK>                 row[refnum + 1] = line                 return             # else keep looking         elif row[0] > linenum:             break     self.rows.insert(i, self.newRow(linenum, refnum, line))",if row [ refnum + 1 ] is None :,if i == refnum + 1 :,18.190371142855746,18.190371142855746,0.0
"def __setattr__(self, name, val):     if self.__dict__.get(name, ""hamster_graphics_no_value_really"") == val:         return     Sprite.__setattr__(self, name, val)     if name == ""image_data"":         self._surface = None         <MASK>             self.__dict__[""width""] = self.image_data.get_width()             self.__dict__[""height""] = self.image_data.get_height()",if self . image_data :,if self . image_data :,100.00000000000004,100.00000000000004,1.0
"def process_signature(app, what, name, obj, options, signature, return_annotation):     if signature:         # replace Mock function names         signature = re.sub(""<Mock name='([^']+)'.*>"", ""\g<1>"", signature)         signature = re.sub(""tensorflow"", ""tf"", signature)         # add scope name to layer signatures:         if hasattr(obj, ""use_scope""):             <MASK>                 signature = signature[0] + ""variable_scope_name, "" + signature[1:]             elif obj.use_scope is None:                 signature = signature[0] + ""[variable_scope_name,] "" + signature[1:]     # signature: arg list     return signature, return_annotation",if obj . use_scope :,if obj . use_scope is not None :,53.7284965911771,53.7284965911771,0.0
"def L_op(self, inputs, outputs, gout):     (x,) = inputs     (gz,) = gout     if x.type in complex_types:         raise NotImplementedError()     if outputs[0].type in discrete_types:         <MASK>             return [x.zeros_like(dtype=theano.config.floatX)]         else:             return [x.zeros_like()]     return (gz * (1 - sqr(tanh(x))),)",if x . type in discrete_types :,if inputs [ 0 ] . type in float_types :,22.03359678996931,22.03359678996931,0.0
"def confirm_on_console(topic, msg):     done = False     print(topic)     while not done:         output = raw_input(msg + "":[y/n]"")         <MASK>             return True         if output.lower() == ""n"":             return False","if output . lower ( ) == ""y"" :","if output . lower ( ) == ""y"" :",100.00000000000004,100.00000000000004,1.0
"def replace_documentation_for_matching_shape(self, event_name, section, **kwargs):     if self._shape_name == section.context.get(""shape""):         self._replace_documentation(event_name, section)     for section_name in section.available_sections:         sub_section = section.get_section(section_name)         <MASK>             self._replace_documentation(event_name, sub_section)         else:             self.replace_documentation_for_matching_shape(event_name, sub_section)","if self . _shape_name == sub_section . context . get ( ""shape"" ) :","if sub_section . context . get ( ""shape"" ) == self . _shape_name :",77.34617260188894,77.34617260188894,0.0
"def confirm_on_console(topic, msg):     done = False     print(topic)     while not done:         output = raw_input(msg + "":[y/n]"")         if output.lower() == ""y"":             return True         <MASK>             return False","if output . lower ( ) == ""n"" :","if output . lower ( ) == ""n"" :",100.00000000000004,100.00000000000004,1.0
"def __getitem__(self, index):     if self._check():         if isinstance(index, int):             <MASK>                 raise IndexError(index)             if self.features[index] is None:                 feature = self.device.feature_request(FEATURE.FEATURE_SET, 0x10, index)                 if feature:                     (feature,) = _unpack(""!H"", feature[:2])                     self.features[index] = FEATURE[feature]             return self.features[index]         elif isinstance(index, slice):             indices = index.indices(len(self.features))             return [self.__getitem__(i) for i in range(*indices)]",if index < 0 or index >= len ( self . features ) :,if index < 0 :,9.569649651041097,9.569649651041097,0.0
"def _parse_locator(self, locator):     prefix = None     criteria = locator     if not locator.startswith(""//""):         locator_parts = locator.partition(""="")         <MASK>             prefix = locator_parts[0]             criteria = locator_parts[2].strip()     return (prefix, criteria)",if len ( locator_parts [ 1 ] ) > 0 :,if len ( locator_parts ) == 3 :,41.36817680097793,41.36817680097793,0.0
"def trakt_episode_data_generate(self, data):     # Find how many unique season we have     uniqueSeasons = []     for season, episode in data:         <MASK>             uniqueSeasons.append(season)     # build the query     seasonsList = []     for searchedSeason in uniqueSeasons:         episodesList = []         for season, episode in data:             if season == searchedSeason:                 episodesList.append({""number"": episode})         seasonsList.append({""number"": searchedSeason, ""episodes"": episodesList})     post_data = {""seasons"": seasonsList}     return post_data",if season not in uniqueSeasons :,if season in uniqueSeasons :,40.93653765389909,40.93653765389909,0.0
"def __init__(self, data, n_bins):     bin_width = span / n_bins     bins = [0] * n_bins     for x in data:         b = int(mpfloor((x - minimum) / bin_width))         <MASK>             b = 0         elif b >= n_bins:             b = n_bins - 1         bins[b] += 1     self.bins = bins     self.bin_width = bin_width",if b < 0 :,if b < 0 :,100.00000000000004,100.00000000000004,1.0
"def infer_context(typ, context=""http://schema.org""):     parsed_context = urlparse(typ)     if parsed_context.netloc:         base = """".join([parsed_context.scheme, ""://"", parsed_context.netloc])         <MASK>             context = urljoin(base, parsed_context.path)             typ = parsed_context.fragment.strip(""/"")         elif parsed_context.path:             context = base             typ = parsed_context.path.strip(""/"")     return context, typ",if parsed_context . path and parsed_context . fragment :,if parsed_context . fragment :,42.43728456769501,42.43728456769501,0.0
"def parse(self, items):     for index, item in enumerate(items):         keys = self.build_key(item)         if keys is None:             continue         # Update `items`         self.items[tuple(keys)] = (index, item)         # Update `table`         <MASK>             log.info(""Unable to update table (keys: %r)"", keys)","if not self . path_set ( self . table , keys , ( index , item ) ) :",if not self . table . exists ( keys ) :,14.94611618633665,14.94611618633665,0.0
"def dict_to_XML(tag, dictionary, **kwargs):     """"""Return XML element converting dicts recursively.""""""     elem = Element(tag, **kwargs)     for key, val in dictionary.items():         if tag == ""layers"":             child = dict_to_XML(""layer"", val, name=key)         elif isinstance(val, MutableMapping):             child = dict_to_XML(key, val)         else:             <MASK>                 child = Element(""variable"", name=key)             else:                 child = Element(key)             child.text = str(val)         elem.append(child)     return elem","if tag == ""config"" :","if tag == ""variable"" :",59.4603557501361,59.4603557501361,0.0
"def _get_config_value(self, section, key):     if section:         <MASK>             self.log.error(""Error: Config section '%s' not found"", section)             return None         return self.config[section].get(key, self.config[key])     else:         return self.config[key]",if section not in self . config :,if section not in self . config :,100.00000000000004,100.00000000000004,1.0
"def h_line_down(self, input):     end_this_line = self.value.find(""\n"", self.cursor_position)     if end_this_line == -1:         if self.scroll_exit:             self.h_exit_down(None)         else:             self.cursor_position = len(self.value)     else:         self.cursor_position = end_this_line + 1         for x in range(self.cursorx):             <MASK>                 break             elif self.value[self.cursor_position] == ""\n"":                 break             else:                 self.cursor_position += 1",if self . cursor_position > len ( self . value ) - 1 :,"if self . value [ self . cursor_position ] == ""\r"" :",31.24577801157053,31.24577801157053,0.0
"def printsumfp(fp, filename, out=sys.stdout):     m = md5()     try:         while 1:             data = fp.read(bufsize)             <MASK>                 break             if isinstance(data, str):                 data = data.encode(fp.encoding)             m.update(data)     except IOError as msg:         sys.stderr.write(""%s: I/O error: %s\n"" % (filename, msg))         return 1     out.write(""%s %s\n"" % (m.hexdigest(), filename))     return 0",if not data :,if not data :,100.00000000000004,100.00000000000004,1.0
"def main(input):     logging.info(""Running Azure Cloud Custodian Policy %s"", input)     context = {         ""config_file"": join(function_directory, ""config.json""),         ""auth_file"": join(function_directory, ""auth.json""),     }     event = None     subscription_id = None     if isinstance(input, QueueMessage):         <MASK>             return         event = input.get_json()         subscription_id = ResourceIdParser.get_subscription_id(event[""subject""])     handler.run(event, context, subscription_id)",if input . dequeue_count > max_dequeue_count :,"if not input . get ( ""subject"" ) :",7.431878014503621,7.431878014503621,0.0
"def maybeExtractTarball(self):     if self.tarball:         tar = self.computeTarballOptions() + [""-xvf"", self.tarball]         res = yield self._Cmd(tar, abandonOnFailure=False)         <MASK>  # error with tarball.. erase repo dir and tarball             yield self._Cmd([""rm"", ""-f"", self.tarball], abandonOnFailure=False)             yield self.runRmdir(self.repoDir(), abandonOnFailure=False)",if res :,if res :,100.00000000000004,0.0,1.0
"def execute(self, arbiter, props):     watcher = self._get_watcher(arbiter, props.pop(""name""))     action = 0     for key, val in props.get(""options"", {}).items():         <MASK>             new_action = 0             for name, _val in val.items():                 action = watcher.set_opt(""hooks.%s"" % name, _val)                 if action == 1:                     new_action = 1         else:             new_action = watcher.set_opt(key, val)         if new_action == 1:             action = 1     # trigger needed action     return watcher.do_action(action)","if key == ""hooks"" :",if action == 0 :,13.83254362586636,13.83254362586636,0.0
"def _import_playlists(self, fns, library):     added = 0     for filename in fns:         name = _name_for(filename)         with open(filename, ""rb"") as f:             <MASK>                 playlist = parse_m3u(f, name, library=library)             elif filename.endswith("".pls""):                 playlist = parse_pls(f, name, library=library)             else:                 print_w(""Unsupported playlist type for '%s'"" % filename)                 continue         self.changed(playlist)         library.add(playlist)         added += 1     return added","if filename . endswith ( "".m3u"" ) or filename . endswith ( "".m3u8"" ) :","if filename . endswith ( "".m3u"" ) :",38.966271115357685,38.966271115357685,0.0
"def unwrap_term_buckets(self, timestamp, term_buckets):     for term_data in term_buckets:         <MASK>             self.unwrap_interval_buckets(                 timestamp, term_data[""key""], term_data[""interval_aggs""][""buckets""]             )         else:             self.check_matches(timestamp, term_data[""key""], term_data)","if ""interval_aggs"" in term_data :","if term_data [ ""interval_aggs"" ] :",44.833867003844574,44.833867003844574,0.0
"def _get_exception(flags, timeout_ms, payload_size):     if flags & FLAG_ERROR:         if flags & FLAG_TIMEOUT:             return SpicommTimeoutError(timeout_ms / 1000.0)         <MASK>             return SpicommOverflowError(payload_size)         return SpicommError()     return None",if flags & FLAG_OVERFLOW :,if flags & FLAG_OVERFLOW :,100.00000000000004,100.00000000000004,1.0
"def _get_pattern(self, pattern_id):     """"""Get pattern item by id.""""""     for key in (Tag.PATTERNS1, Tag.PATTERNS2, Tag.PATTERNS3):         if key in self.tagged_blocks:             data = self.tagged_blocks.get_data(key)             for pattern in data:                 <MASK>                     return pattern     return None",if pattern . pattern_id == pattern_id :,if pattern_id == pattern . id :,54.088044192555245,54.088044192555245,0.0
"def print_quiet(self, context, *args, **kwargs):     for index, (key, value) in enumerate(         itertools.chain(enumerate(args), kwargs.items())     ):         <MASK>             print(                 self.format_quiet(index, key, value, fields=context.get_input_fields())             )","if self . filter ( index , key , value ) :",if context . get_input_fields ( ) :,8.606119900909883,8.606119900909883,0.0
"def complete(self, block):     with self._condition:         if not self._final:             return False         if self._complete():             self._calculate_state_root_if_not_already_done()             return True         <MASK>             self._condition.wait_for(self._complete)             self._calculate_state_root_if_not_already_done()             return True         return False",if block :,if block :,100.00000000000004,0.0,1.0
"def compression_rotator(source, dest):     with open(source, ""rb"") as sf:         with gzip.open(dest, ""wb"") as wf:             while True:                 data = sf.read(CHUNK_SIZE)                 <MASK>                     break                 wf.write(data)     os.remove(source)",if not data :,if not data :,100.00000000000004,100.00000000000004,1.0
"def mockup(self, records):     provider = TransipProvider("""", """", """")     _dns_entries = []     for record in records:         <MASK>             entries_for = getattr(provider, ""_entries_for_{}"".format(record._type))             # Root records have '@' as name             name = record.name             if name == """":                 name = provider.ROOT_RECORD             _dns_entries.extend(entries_for(name, record))             # NS is not supported as a DNS Entry,             # so it should cover the if statement             _dns_entries.append(DnsEntry(""@"", ""3600"", ""NS"", ""ns01.transip.nl.""))     self.mockupEntries = _dns_entries",if record . _type in provider . SUPPORTS :,"if record . type == ""dns"" :",17.747405280050266,17.747405280050266,0.0
"def parse_known_args(self, args=None, namespace=None):     entrypoint = self.prog.split("" "")[0]     try:         defs = get_defaults_for_argparse(entrypoint)         ignore = defs.pop(""Ignore"", None)         self.set_defaults(**defs)         <MASK>             set_notebook_diff_ignores(ignore)     except ValueError:         pass     return super(ConfigBackedParser, self).parse_known_args(         args=args, namespace=namespace     )",if ignore :,if ignore :,100.00000000000004,0.0,1.0
"def _maybeRebuildAtlas(self, threshold=4, minlen=1000):     n = len(self.fragmentAtlas)     if (n > minlen) and (n > threshold * len(self.data)):         self.fragmentAtlas.rebuild(             list(zip(*self._style([""symbol"", ""size"", ""pen"", ""brush""])))         )         self.data[""sourceRect""] = 0         <MASK>             self._sourceQRect.clear()         self.updateSpots()",if _USE_QRECT :,"if self . data [ ""sourceQRect"" ] :",4.990049701936832,4.990049701936832,0.0
"def dispatch_return(self, frame, arg):     if self.stop_here(frame) or frame == self.returnframe:         # Ignore return events in generator except when stepping.         <MASK>             return self.trace_dispatch         try:             self.frame_returning = frame             self.user_return(frame, arg)         finally:             self.frame_returning = None         if self.quitting:             raise BdbQuit         # The user issued a 'next' or 'until' command.         if self.stopframe is frame and self.stoplineno != -1:             self._set_stopinfo(None, None)     return self.trace_dispatch",if self . stopframe and frame . f_code . co_flags & CO_GENERATOR :,if self . stopframe is frame :,8.194094675927117,8.194094675927117,0.0
"def tearDown(self):     if not self.is_playback():         try:             if self.hosted_service_name is not None:                 self.sms.delete_hosted_service(self.hosted_service_name)         except:             pass         try:             <MASK>                 self.sms.delete_storage_account(self.storage_account_name)         except:             pass         try:             self.sms.delete_affinity_group(self.affinity_group_name)         except:             pass     return super(LegacyMgmtAffinityGroupTest, self).tearDown()",if self . storage_account_name is not None :,if self . storage_account_name is not None :,100.00000000000004,100.00000000000004,1.0
"def make_log_msg(self, msg, *other_messages):     MAX_MESSAGE_LENGTH = 1000     if not other_messages:         # assume that msg is a single string         return msg[-MAX_MESSAGE_LENGTH:]     else:         if len(msg):             msg += ""\n...\n""             NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH - len(msg)         else:             NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH         <MASK>             msg += other_messages[0][-NEXT_MESSAGE_OFFSET:]             return self.make_log_msg(msg, *other_messages[1:])         else:             return self.make_log_msg(msg)",if NEXT_MESSAGE_OFFSET > 0 :,if next_message_offset < MAX_MESSAGE_LENGTH :,13.065113298388567,13.065113298388567,0.0
"def wrapper(  # type: ignore     self: RequestHandler, *args, **kwargs ) -> Optional[Awaitable[None]]:     if self.request.path.endswith(""/""):         if self.request.method in (""GET"", ""HEAD""):             uri = self.request.path.rstrip(""/"")             if uri:  # don't try to redirect '/' to ''                 <MASK>                     uri += ""?"" + self.request.query                 self.redirect(uri, permanent=True)                 return None         else:             raise HTTPError(404)     return method(self, *args, **kwargs)",if self . request . query :,if self . request . query :,100.00000000000004,100.00000000000004,1.0
"def process_lib(vars_, coreval):     for d in vars_:         var = d.upper()         if var == ""QTCORE"":             continue         value = env[""LIBPATH_"" + var]         <MASK>             core = env[coreval]             accu = []             for lib in value:                 if lib in core:                     continue                 accu.append(lib)             env[""LIBPATH_"" + var] = accu",if value :,if not value :,35.35533905932737,35.35533905932737,0.0
"def _attach_children(self, other, exclude_worldbody, dry_run=False):     for other_child in other.all_children():         <MASK>             self_child = self.get_children(other_child.spec.name)             self_child._attach(                 other_child, exclude_worldbody, dry_run             )  # pylint: disable=protected-access",if not other_child . spec . repeated :,if self_child . spec . name in self . spec . children :,26.760322756637922,26.760322756637922,0.0
"def getDictFromTree(tree):     ret_dict = {}     for child in tree.getchildren():         if child.getchildren():             ## Complex-type child. Recurse             content = getDictFromTree(child)         else:             content = child.text         <MASK>             if not type(ret_dict[child.tag]) == list:                 ret_dict[child.tag] = [ret_dict[child.tag]]             ret_dict[child.tag].append(content or """")         else:             ret_dict[child.tag] = content or """"     return ret_dict",if ret_dict . has_key ( child . tag ) :,if content :,0.8861688619210014,0.0,0.0
"def nsUriMatch(self, value, wanted, strict=0, tt=type(())):     """"""Return a true value if two namespace uri values match.""""""     if value == wanted or (type(wanted) is tt) and value in wanted:         return 1     if not strict and value is not None:         wanted = type(wanted) is tt and wanted or (wanted,)         value = value[-1:] != ""/"" and value or value[:-1]         for item in wanted:             <MASK>                 return 1     return 0",if item == value or item [ : - 1 ] == value :,if item not in value :,4.336109473768157,4.336109473768157,0.0
"def update_repository(self, ignore_issues=False, force=False):     """"""Update.""""""     if not await self.common_update(ignore_issues, force):         return     # Get appdaemon objects.     if self.repository_manifest:         <MASK>             self.content.path.remote = """"     if self.content.path.remote == ""apps"":         self.data.domain = get_first_directory_in_directory(             self.tree, self.content.path.remote         )         self.content.path.remote = f""apps/{self.data.name}""     # Set local path     self.content.path.local = self.localpath",if self . data . content_in_root :,"if self . data . name == ""appdaemon"" :",33.18077402843942,33.18077402843942,0.0
"def addOutput(self, data, isAsync=None, **kwargs):     isAsync = _get_async_param(isAsync, **kwargs)     if isAsync:         self.terminal.eraseLine()         self.terminal.cursorBackward(len(self.lineBuffer) + len(self.ps[self.pn]))     self.terminal.write(data)     if isAsync:         <MASK>             self.terminal.nextLine()         self.terminal.write(self.ps[self.pn])         if self.lineBuffer:             oldBuffer = self.lineBuffer             self.lineBuffer = []             self.lineBufferIndex = 0             self._deliverBuffer(oldBuffer)",if self . _needsNewline ( ) :,if self . lineBufferIndex == 0 :,22.089591134157878,22.089591134157878,0.0
"def is_installed(self, dlc_title="""") -> bool:     installed = False     if dlc_title:         dlc_version = self.get_dlc_info(""version"", dlc_title)         installed = True if dlc_version else False         # Start: Code for compatibility with minigalaxy 1.0         <MASK>             status = self.legacy_get_dlc_status(dlc_title)             installed = True if status in [""installed"", ""updatable""] else False         # End: Code for compatibility with minigalaxy 1.0     else:         if self.install_dir and os.path.exists(self.install_dir):             installed = True     return installed",if not installed :,if self . legacy_get_dlc_status ( dlc_title ) :,2.908317710573757,2.908317710573757,0.0
"def close(self):     self.selector.close()     if self.sock:         sockname = None         try:             sockname = self.sock.getsockname()         except (socket.error, OSError):             pass         self.sock.close()         <MASK>             # it was a Unix domain socket, remove it from the filesystem             if os.path.exists(sockname):                 os.remove(sockname)     self.sock = None",if type ( sockname ) is str :,if sockname :,7.49553326588684,0.0,0.0
"def post_file(self, file_path, graph_type=""edges"", file_type=""csv""):     dataset_id = self.dataset_id     tok = self.token     base_path = self.server_base_path     with open(file_path, ""rb"") as file:         out = requests.post(             f""{base_path}/api/v2/upload/datasets/{dataset_id}/{graph_type}/{file_type}"",             verify=self.certificate_validation,             headers={""Authorization"": f""Bearer {tok}""},             data=file.read(),         ).json()         <MASK>             raise Exception(out)         return out","if not out [ ""success"" ] :",if not out . status_code :,19.493995755254467,19.493995755254467,0.0
"def _get_vqa_v2_image_raw_dataset(directory, image_root_url, image_urls):     """"""Extract the VQA V2 image data set to directory unless it's there.""""""     for url in image_urls:         filename = os.path.basename(url)         download_url = os.path.join(image_root_url, url)         path = generator_utils.maybe_download(directory, filename, download_url)         unzip_dir = os.path.join(directory, filename.strip("".zip""))         <MASK>             zipfile.ZipFile(path, ""r"").extractall(directory)",if not tf . gfile . Exists ( unzip_dir ) :,if unzip_dir :,9.121704909091086,9.121704909091086,0.0
"def __call__(self, environ, start_response):     for key in ""REQUEST_URL"", ""REQUEST_URI"", ""UNENCODED_URL"":         <MASK>             continue         request_uri = unquote(environ[key])         script_name = unquote(environ.get(""SCRIPT_NAME"", """"))         if request_uri.startswith(script_name):             environ[""PATH_INFO""] = request_uri[len(script_name) :].split(""?"", 1)[0]             break     return self.app(environ, start_response)",if key not in environ :,if key not in environ :,100.00000000000004,100.00000000000004,1.0
"def _instrument_model(self, model):     for key, value in list(         model.__dict__.items()     ):  # avoid ""dictionary keys changed during iteration""         if isinstance(value, tf.keras.layers.Layer):             new_layer = self._instrument(value)             if new_layer is not value:                 setattr(model, key, new_layer)         <MASK>             for i, item in enumerate(value):                 if isinstance(item, tf.keras.layers.Layer):                     value[i] = self._instrument(item)     return model","elif isinstance ( value , list ) :","if isinstance ( value , list ) :",84.08964152537145,84.08964152537145,0.0
"def __init__(self, parent, dir, mask, with_dirs=True):     filelist = []     dirlist = [""..""]     self.dir = dir     self.file = """"     mask = mask.upper()     pattern = self.MakeRegex(mask)     for i in os.listdir(dir):         if i == ""."" or i == "".."":             continue         path = os.path.join(dir, i)         if os.path.isdir(path):             dirlist.append(i)             continue         path = path.upper()         value = i.upper()         <MASK>             filelist.append(i)     self.files = filelist     if with_dirs:         self.dirs = dirlist",if pattern . match ( value ) is not None :,if pattern . match ( value ) :,59.755798910891144,59.755798910891144,0.0
"def get_text(self, nodelist):     """"""Return a string representation of the motif's properties listed on nodelist .""""""     retlist = []     for node in nodelist:         if node.nodeType == Node.TEXT_NODE:             retlist.append(node.wholeText)         <MASK>             retlist.append(self.get_text(node.childNodes))     return re.sub(r""\s+"", "" "", """".join(retlist))",elif node . hasChildNodes :,if node . nodeType == Node . ELEMENT_NODE :,7.495553473355845,7.495553473355845,0.0
"def _persist_metadata(self, dirname, filename):     metadata_path = ""{0}/{1}.json"".format(dirname, filename)     if self.media_metadata or self.comments or self.include_location:         if self.posts:             if self.latest:                 self.merge_json({""GraphImages"": self.posts}, metadata_path)             else:                 self.save_json({""GraphImages"": self.posts}, metadata_path)         <MASK>             if self.latest:                 self.merge_json({""GraphStories"": self.stories}, metadata_path)             else:                 self.save_json({""GraphStories"": self.stories}, metadata_path)",if self . stories :,if self . stories :,100.00000000000004,100.00000000000004,1.0
"def _get_python_wrapper_content(self, job_class, args):     job = job_class([""-r"", ""hadoop""] + list(args))     job.sandbox()     with job.make_runner() as runner:         runner._create_setup_wrapper_scripts()         <MASK>             with open(runner._spark_python_wrapper_path) as f:                 return f.read()         else:             return None",if runner . _spark_python_wrapper_path :,if runner . _spark_python_wrapper_path :,100.00000000000004,100.00000000000004,1.0
"def computeLeadingWhitespaceWidth(s, tab_width):     w = 0     for ch in s:         <MASK>             w += 1         elif ch == ""\t"":             w += abs(tab_width) - (w % abs(tab_width))         else:             break     return w","if ch == "" "" :","if ch == ""\n"" :",51.33450480401705,51.33450480401705,0.0
def run(self):     # if the i3status process dies we want to restart it.     # We give up restarting if we have died too often     for _ in range(10):         <MASK>             break         self.spawn_i3status()         # check if we never worked properly and if so quit now         if not self.ready:             break         # limit restart rate         self.lock.wait(5),if not self . py3_wrapper . running :,if self . stopped :,9.346579571601447,9.346579571601447,0.0
"def translate_len(     builder: IRBuilder, expr: CallExpr, callee: RefExpr ) -> Optional[Value]:     # Special case builtins.len     if len(expr.args) == 1 and expr.arg_kinds == [ARG_POS]:         expr_rtype = builder.node_type(expr.args[0])         <MASK>             # len() of fixed-length tuple can be trivially determined statically,             # though we still need to evaluate it.             builder.accept(expr.args[0])             return Integer(len(expr_rtype.types))         else:             obj = builder.accept(expr.args[0])             return builder.builtin_len(obj, -1)     return None","if isinstance ( expr_rtype , RTuple ) :",if expr_rtype . types :,18.094495256969623,18.094495256969623,0.0
"def parse_auth(val):     if val is not None:         authtype, params = val.split("" "", 1)         if authtype in known_auth_schemes:             <MASK>                 # this is the ""Authentication: Basic XXXXX=="" case                 pass             else:                 params = parse_auth_params(params)         return authtype, params     return val","if authtype == ""Basic"" and '""' not in params :",if params is None :,1.9026155630072006,1.9026155630072006,0.0
"def toxml(self):     text = self.value     self.parent.setBidi(getBidiType(text))     if not text.startswith(HTML_PLACEHOLDER_PREFIX):         if self.parent.nodeName == ""p"":             text = text.replace(""\n"", ""\n   "")         <MASK>             text = ""\n     "" + text.replace(""\n"", ""\n     "")     text = self.doc.normalizeEntities(text)     return text","elif self . parent . nodeName == ""li"" and self . parent . childNodes [ 0 ] == self :","if self . parent . nodeName == ""p"" :",23.853726289570925,23.853726289570925,0.0
"def get_all_related_many_to_many_objects(self):     try:  # Try the cache first.         return self._all_related_many_to_many_objects     except AttributeError:         rel_objs = []         for klass in get_models():             for f in klass._meta.many_to_many:                 <MASK>                     rel_objs.append(RelatedObject(f.rel.to, klass, f))         self._all_related_many_to_many_objects = rel_objs         return rel_objs",if f . rel and self == f . rel . to . _meta :,if f . rel . to :,20.15216974557266,20.15216974557266,0.0
"def state_highstate(self, state, dirpath):     opts = copy.copy(self.config)     opts[""file_roots""] = dict(base=[dirpath])     HIGHSTATE = HighState(opts)     HIGHSTATE.push_active()     try:         high, errors = HIGHSTATE.render_highstate(state)         <MASK>             import pprint             pprint.pprint(""\n"".join(errors))             pprint.pprint(high)         out = HIGHSTATE.state.call_high(high)         # pprint.pprint(out)     finally:         HIGHSTATE.pop_active()",if errors :,if errors :,100.00000000000004,0.0,1.0
"def _update_target_host(self, target, target_host):     """"""Update target host.""""""     target_host = None if target_host == """" else target_host     if not target_host:         for device_type, tgt in target.items():             <MASK>                 target_host = tgt                 break     if not target_host:         target_host = ""llvm"" if tvm.runtime.enabled(""llvm"") else ""stackvm""     if isinstance(target_host, str):         target_host = tvm.target.Target(target_host)     return target_host",if device_type . value == tvm . nd . cpu ( 0 ) . device_type :,"if device_type == ""stackvm"" :",10.807256086619267,10.807256086619267,0.0
"def __console_writer(self):     while True:         self.__writer_event.wait()         self.__writer_event.clear()         if self.__console_view:             <MASK>                 self.log.debug(""Writing console view to STDOUT"")                 sys.stdout.write(self.console_markup.clear)                 sys.stdout.write(self.__console_view)                 sys.stdout.write(self.console_markup.TOTAL_RESET)",if not self . short_only :,if self . __console_view :,11.99014838091355,11.99014838091355,0.0
"def goToPrevMarkedHeadline(self, event=None):     """"""Select the next marked node.""""""     c = self     p = c.p     if not p:         return     p.moveToThreadBack()     wrapped = False     while 1:         if p and p.isMarked():             break         elif p:             p.moveToThreadBack()         <MASK>             break         else:             wrapped = True             p = c.rootPosition()     if not p:         g.blue(""done"")     c.treeSelectHelper(p)  # Sets focus.",elif wrapped :,if wrapped :,55.03212081491043,0.0,0.0
"def delete_map(self, query=None):     query_map = self.interpolated_map(query=query)     for alias, drivers in six.iteritems(query_map.copy()):         for driver, vms in six.iteritems(drivers.copy()):             for vm_name, vm_details in six.iteritems(vms.copy()):                 if vm_details == ""Absent"":                     query_map[alias][driver].pop(vm_name)             if not query_map[alias][driver]:                 query_map[alias].pop(driver)         <MASK>             query_map.pop(alias)     return query_map",if not query_map [ alias ] :,if not query_map [ alias ] :,100.00000000000004,100.00000000000004,1.0
"def get_shadows_zip(filename):     import zipfile     shadow_pkgs = set()     with zipfile.ZipFile(filename) as lib_zip:         already_test = []         for fname in lib_zip.namelist():             pname, fname = os.path.split(fname)             <MASK>                 continue             if pname not in already_test and ""/"" not in pname:                 already_test.append(pname)                 if is_shadowing(pname):                     shadow_pkgs.add(pname)     return shadow_pkgs",if fname or ( pname and fname ) :,if not os . path . isfile ( fname ) :,15.851165692617148,15.851165692617148,0.0
"def make_chains(chains_info):     chains = [[] for _ in chains_info[0][1]]     for i, num_ids in enumerate(chains_info[:-1]):         num, ids = num_ids         for j, ident in enumerate(ids):             <MASK>                 next_chain_info = chains_info[i + 1]                 previous = next_chain_info[1][j]                 block = SimpleBlock(num, ident, previous)                 chains[j].append(block)     chains = {i: make_generator(chain) for i, chain in enumerate(chains)}     return chains","if ident != """" :",if i + 1 in chains_info :,5.669791110976001,5.669791110976001,0.0
"def filter_input(mindate, maxdate, files):     mindate = parse(mindate) if mindate is not None else datetime.datetime.min     maxdate = parse(maxdate) if maxdate is not None else datetime.datetime.max     for line in fileinput.input(files):         tweet = json.loads(line)         created_at = parse(tweet[""created_at""])         created_at = created_at.replace(tzinfo=None)         <MASK>             print(json.dumps(tweet))",if mindate < created_at and maxdate > created_at :,if created_at != mindate :,13.08508177121638,13.08508177121638,0.0
"def get(self):     """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called.""""""     if self._exception is not _NONE:         if self._exception is None:             return self.value         getcurrent().throw(*self._exception)  # pylint:disable=undefined-variable     else:         <MASK>             raise ConcurrentObjectUseError(                 ""This Waiter is already used by %r"" % (self.greenlet,)             )         self.greenlet = getcurrent()  # pylint:disable=undefined-variable         try:             return self.hub.switch()         finally:             self.greenlet = None",if self . greenlet is not None :,if self . greenlet is not _NONE :,61.04735835807847,61.04735835807847,0.0
"def default_loader(href, parse, encoding=None):     with open(href) as file:         <MASK>             data = ElementTree.parse(file).getroot()         else:             data = file.read()             if encoding:                 data = data.decode(encoding)     return data","if parse == ""xml"" :",if parse :,11.898417391331403,0.0,0.0
def is_all_qud(world):     m = True     for obj in world:         <MASK>             if obj.nice:                 m = m and True             else:                 m = m and False         else:             m = m and True     return m,if obj . blond :,if obj . qud :,42.72870063962342,42.72870063962342,0.0
"def run(self, edit):     if not self.has_selection():         region = sublime.Region(0, self.view.size())         originalBuffer = self.view.substr(region)         prefixed = self.prefix(originalBuffer)         if prefixed:             self.view.replace(edit, region, prefixed)         return     for region in self.view.sel():         <MASK>             continue         originalBuffer = self.view.substr(region)         prefixed = self.prefix(originalBuffer)         if prefixed:             self.view.replace(edit, region, prefixed)",if region . empty ( ) :,if region not in self . view . sel ( ) :,16.59038701421971,16.59038701421971,0.0
"def add_fields(self, params):     for (key, val) in params.iteritems():         <MASK>             new_params = {}             for k in val:                 new_params[""%s__%s"" % (key, k)] = val[k]             self.add_fields(new_params)         else:             self.add_field(key, val)","if isinstance ( val , dict ) :",if val :,7.49553326588684,0.0,0.0
"def find_magic(self, f, pos, magic):     f.seek(pos)     block = f.read(32 * 1024)     if len(block) < len(magic):         return -1     p = block.find(magic)     while p < 0:         pos += len(block) - len(magic) + 1         block = block[1 - len(magic) :] + f.read(32 << 10)         <MASK>             return -1         p = block.find(magic)     return pos + p",if len ( block ) == len ( magic ) - 1 :,if not block :,1.7256245272235644,1.7256245272235644,0.0
"def check_strings(self):     """"""Check that all strings have been consumed.""""""     for i, aList in enumerate(self.string_tokens):         <MASK>             g.trace(""warning: line %s. unused strings"" % i)             for z in aList:                 print(self.dump_token(z))",if aList :,if i not in self . unused_strings :,4.990049701936832,4.990049701936832,0.0
"def get_tokens_unprocessed(self, text):     from pygments.lexers._cocoa_builtins import (         COCOA_INTERFACES,         COCOA_PROTOCOLS,         COCOA_PRIMITIVES,     )     for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):         <MASK>             if (                 value in COCOA_INTERFACES                 or value in COCOA_PROTOCOLS                 or value in COCOA_PRIMITIVES             ):                 token = Name.Builtin.Pseudo         yield index, token, value",if token is Name or token is Name . Class :,if token in COCOA_INTERFACES :,8.208611846457007,8.208611846457007,0.0
"def key_from_key_value_dict(key_info):     res = []     if not ""key_value"" in key_info:         return res     for value in key_info[""key_value""]:         <MASK>             e = base64_to_long(value[""rsa_key_value""][""exponent""])             m = base64_to_long(value[""rsa_key_value""][""modulus""])             key = RSA.construct((m, e))             res.append(key)     return res","if ""rsa_key_value"" in value :","if ""rsa_key_value"" in value :",100.00000000000004,100.00000000000004,1.0
"def run(self, edit):     if not self.has_selection():         region = sublime.Region(0, self.view.size())         originalBuffer = self.view.substr(region)         prefixed = self.prefix(originalBuffer)         <MASK>             self.view.replace(edit, region, prefixed)         return     for region in self.view.sel():         if region.empty():             continue         originalBuffer = self.view.substr(region)         prefixed = self.prefix(originalBuffer)         if prefixed:             self.view.replace(edit, region, prefixed)",if prefixed :,if prefixed :,100.00000000000004,0.0,1.0
def finalize(self):     if self.ct < 1:         return     elif self.ct == 1:         return 0     total = ct = 0     dtp = None     while self.heap:         <MASK>             if dtp is None:                 dtp = heapq.heappop(self.heap)                 continue         dt = heapq.heappop(self.heap)         diff = dt - dtp         ct += 1         total += total_seconds(diff)         dtp = dt     return float(total) / ct,if total == 0 :,if dtp == None :,19.304869754804482,19.304869754804482,0.0
"def _test_configuration(self):     config_path = self._write_config()     try:         self._log.debug(""testing configuration"")         verboseflag = ""-Q""         <MASK>             verboseflag = ""-v""         p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, ""-f"", config_path])         if p.wait() != 0:             raise RuntimeError(""configuration test failed"")         self._log.debug(""configuration seems ok"")     finally:         os.remove(config_path)",if self . _log . isEnabledFor ( logging . DEBUG ) :,if self . PATH_SLAPTEST :,11.787460936700446,11.787460936700446,0.0
"def exe(self, ret):     if not ret:         self.assertEqual(ret, """")     else:         assert os.path.isabs(ret), ret         # Note: os.stat() may return False even if the file is there         # hence we skip the test, see:         # http://stackoverflow.com/questions/3112546/os-path-exists-lies         <MASK>             assert os.path.isfile(ret), ret             if hasattr(os, ""access"") and hasattr(os, ""X_OK""):                 # XXX may fail on OSX                 self.assertTrue(os.access(ret, os.X_OK))",if POSIX :,if os . stat ( ret ) :,6.567274736060395,6.567274736060395,0.0
"def _do_cleanup(sg_name, device_id):     masking_view_list = self.rest.get_masking_views_from_storage_group(array, sg_name)     for masking_view in masking_view_list:         <MASK>             self.rest.delete_masking_view(array, masking_view)             self.rest.remove_vol_from_sg(array, sg_name, device_id, extra_specs)             self.rest.delete_volume(array, device_id)             self.rest.delete_storage_group(array, sg_name)","if ""STG-"" in masking_view :",if masking_view . device_id == device_id :,12.011055432195764,12.011055432195764,0.0
"def hide_tooltip_if_necessary(self, key):     """"""Hide calltip when necessary""""""     try:         calltip_char = self.get_character(self.calltip_position)         before = self.is_cursor_before(self.calltip_position, char_offset=1)         other = key in (Qt.Key_ParenRight, Qt.Key_Period, Qt.Key_Tab)         <MASK>             QToolTip.hideText()     except (IndexError, TypeError):         QToolTip.hideText()","if calltip_char not in ( ""?"" , ""("" ) or before or other :",if before and other :,1.26492199361622,1.26492199361622,0.0
"def list_tags_for_stream(self, stream_name, exclusive_start_tag_key=None, limit=None):     stream = self.describe_stream(stream_name)     tags = []     result = {""HasMoreTags"": False, ""Tags"": tags}     for key, val in sorted(stream.tags.items(), key=lambda x: x[0]):         if limit and len(tags) >= limit:             result[""HasMoreTags""] = True             break         <MASK>             continue         tags.append({""Key"": key, ""Value"": val})     return result",if exclusive_start_tag_key and key < exclusive_start_tag_key :,if exclusive_start_tag_key and val :,38.99895694424209,38.99895694424209,0.0
"def parametrize_function_name(request, function_name):     suffixes = []     if ""parametrize"" in request.keywords:         argnames = request.keywords[""parametrize""].args[::2]         argnames = [x.strip() for names in argnames for x in names.split("","")]         for name in argnames:             value = request.getfuncargvalue(name)             <MASK>                 value = value.__name__             suffixes.append(""{}={}"".format(name, value))     return ""+"".join([function_name] + suffixes)",if inspect . isclass ( value ) :,if value is not None :,7.654112967106117,7.654112967106117,0.0
"def add_entities(self, positions):     e1 = EntityFactory()     for p in positions:         <MASK>             start, length = p         else:             start, length = p, 1         EntityOccurrenceFactory(             document=self.doc,             entity=e1,             offset=start,             offset_end=start + length,             alias=""AB"",         )","if isinstance ( p , tuple ) :",if p < self . max_length :,6.27465531099474,6.27465531099474,0.0
"def transform_value(value):     if isinstance(value, collections.MutableMapping):         <MASK>             return DBRef(value[""_ns""], transform_value(value[""_id""]))         else:             return transform_dict(SON(value))     elif isinstance(value, list):         return [transform_value(v) for v in value]     return value","if ""_id"" in value and ""_ns"" in value :","if isinstance ( value , ( str , unicode ) ) :",3.471023784089875,3.471023784089875,0.0
"def remove(self, items):     """"""Remove messages from lease management.""""""     with self._add_remove_lock:         # Remove the ack ID from lease management, and decrement the         # byte counter.         for item in items:             if self._leased_messages.pop(item.ack_id, None) is not None:                 self._bytes -= item.byte_size             else:                 _LOGGER.debug(""Item %s was not managed."", item.ack_id)         <MASK>             _LOGGER.debug(""Bytes was unexpectedly negative: %d"", self._bytes)             self._bytes = 0",if self . _bytes < 0 :,if self . _bytes < 0 :,100.00000000000004,100.00000000000004,1.0
"def parse_hgsub(lines):     """"""Fills OrderedDict with hgsub file content passed as list of lines""""""     rv = OrderedDict()     for l in lines:         ls = l.strip()         <MASK>             continue         name, value = l.split(""="", 1)         rv[name.strip()] = value.strip()     return rv","if not ls or ls [ 0 ] == ""#"" :","if ls == """" :",12.300790484177304,12.300790484177304,0.0
"def del_(self, key):     initial_hash = hash_ = self.hash(key)     while True:         if self._keys[hash_] is self._empty:             # That key was never assigned             return None         elif self._keys[hash_] == key:             # key found, assign with deleted sentinel             self._keys[hash_] = self._deleted             self._values[hash_] = self._deleted             self._len -= 1             return         hash_ = self._rehash(hash_)         <MASK>             # table is full and wrapped around             return None",if initial_hash == hash_ :,if hash_ == initial_hash :,27.77619034011791,27.77619034011791,0.0
"def atom(token, no_symbol=False):     try:         return int(token)     except ValueError:         try:             return float(token)         except ValueError:             <MASK>                 return token[1:-1]             elif no_symbol:                 return token             else:                 return Symbol(token)","if token . startswith ( ""'"" ) or token . startswith ( '""' ) :",if token [ 0 ] == '-' :,3.4738650706548713,3.4738650706548713,0.0
"def __Suffix_Noun_Step1b(self, token):     for suffix in self.__suffix_noun_step1b:         <MASK>             token = token[:-1]             self.suffixe_noun_step1b_success = True             break     return token",if token . endswith ( suffix ) and len ( token ) > 5 :,if suffix . startswith ( token [ - 1 ] ) :,7.2147997676033215,7.2147997676033215,0.0
"def _guardAgainstUnicode(self, data):     # Only accept byte strings or ascii unicode values, otherwise     # there is no way to correctly decode the data into bytes.     if _pythonMajorVersion < 3:         <MASK>             data = data.encode(""utf8"")     else:         if isinstance(data, str):             # Only accept ascii unicode values.             try:                 return data.encode(""ascii"")             except UnicodeEncodeError:                 pass             raise ValueError(""pyDes can only work with encoded strings, not Unicode."")     return data","if isinstance ( data , unicode ) :","if isinstance ( data , bytes ) :",59.4603557501361,59.4603557501361,0.0
"def populate_resource_parameters(self, tool_source):     root = getattr(tool_source, ""root"", None)     if (         root is not None         and hasattr(self.app, ""job_config"")         and hasattr(self.app.job_config, ""get_tool_resource_xml"")     ):         resource_xml = self.app.job_config.get_tool_resource_xml(             root.get(""id""), self.tool_type         )         if resource_xml is not None:             inputs = root.find(""inputs"")             <MASK>                 inputs = parse_xml_string(""<inputs/>"")                 root.append(inputs)             inputs.append(resource_xml)",if inputs is None :,if inputs is None :,100.00000000000004,100.00000000000004,1.0
"def test_arguments_regex(self):     argument_matches = (         (""pip=1.1"", (""pip"", ""1.1"")),         (""pip==1.1"", None),         (""pip=1.2=1"", (""pip"", ""1.2=1"")),     )     for argument, match in argument_matches:         <MASK>             self.assertIsNone(salt.utils.args.KWARG_REGEX.match(argument))         else:             self.assertEqual(                 salt.utils.args.KWARG_REGEX.match(argument).groups(), match             )",if match is None :,if match is None :,100.00000000000004,100.00000000000004,1.0
"def _get_sidebar_selected(self):     sidebar_selected = None     if self.businessline_id:         sidebar_selected = ""bl_%s"" % self.businessline_id         if self.service_id:             sidebar_selected += ""_s_%s"" % self.service_id             <MASK>                 sidebar_selected += ""_env_%s"" % self.environment_id     return sidebar_selected",if self . environment_id :,if self . environment_id :,100.00000000000004,100.00000000000004,1.0
"def get_ip_info(ipaddress):     """"""Returns device information by IP address""""""     result = {}     try:         ip = IPAddress.objects.select_related().get(address=ipaddress)     except IPAddress.DoesNotExist:         pass     else:         if ip.venture is not None:             result[""venture_id""] = ip.venture.id         <MASK>             result[""device_id""] = ip.device.id             if ip.device.venture is not None:                 result[""venture_id""] = ip.device.venture.id     return result",if ip . device is not None :,if ip . device is not None :,100.00000000000004,100.00000000000004,1.0
"def apply(self, db, person):     for family_handle in person.get_family_handle_list():         family = db.get_family_from_handle(family_handle)         if family:             for event_ref in family.get_event_ref_list():                 if event_ref:                     event = db.get_event_from_handle(event_ref.ref)                     if not event.get_place_handle():                         return True                     <MASK>                         return True     return False",if not event . get_date_object ( ) :,if not event . get_place_handle ( ) :,54.52469119630866,54.52469119630866,0.0
"def killIfDead():     if not self._isalive:         self.log.debug(             ""WampLongPoll: killing inactive WAMP session with transport '{0}'"".format(                 self._transport_id             )         )         self.onClose(False, 5000, ""session inactive"")         self._receive._kill()         <MASK>             del self._parent._transports[self._transport_id]     else:         self.log.debug(             ""WampLongPoll: transport '{0}' is still alive"".format(self._transport_id)         )         self._isalive = False         self.reactor.callLater(killAfter, killIfDead)",if self . _transport_id in self . _parent . _transports :,if self . _parent :,15.020723831494387,15.020723831494387,0.0
"def offsets(self):     offsets = {}     offset_so_far = 0     for name, ty in self.fields.items():         <MASK>             l.warning(                 ""Found a bottom field in struct %s. Ignore and increment the offset using the default ""                 ""element size."",                 self.name,             )             continue         if not self._pack:             align = ty.alignment             if offset_so_far % align != 0:                 offset_so_far += align - offset_so_far % align         offsets[name] = offset_so_far         offset_so_far += ty.size // self._arch.byte_width     return offsets","if isinstance ( ty , SimTypeBottom ) :",if name not in offsets :,6.916271812933183,6.916271812933183,0.0
"def get_override_css(self):     """"""handls allow_css_overrides setting.""""""     if self.settings.get(""allow_css_overrides""):         filename = self.view.file_name()         filetypes = self.settings.get(""markdown_filetypes"")         <MASK>             for filetype in filetypes:                 if filename.endswith(filetype):                     css_filename = filename.rpartition(filetype)[0] + "".css""                     if os.path.isfile(css_filename):                         return u""<style>%s</style>"" % load_utf8(css_filename)     return """"",if filename and filetypes :,if filetypes :,32.34325178227722,0.0,0.0
"def setFullCSSSource(self, fullsrc, inline=False):     self.fullsrc = fullsrc     if type(self.fullsrc) == six.binary_type:         self.fullsrc = six.text_type(self.fullsrc, ""utf-8"")     if inline:         self.inline = inline     if self.fullsrc:         self.srcFullIdx = self.fullsrc.find(self.src)         if self.srcFullIdx < 0:             del self.srcFullIdx         self.ctxsrcFullIdx = self.fullsrc.find(self.ctxsrc)         <MASK>             del self.ctxsrcFullIdx",if self . ctxsrcFullIdx < 0 :,if self . ctxsrcFullIdx < 0 :,100.00000000000004,100.00000000000004,1.0
"def title(self):     ret = theme[""title""]     if isinstance(self.name, six.string_types):         width = self.statwidth()         return (             ret + self.name[0:width].center(width).replace("" "", ""-"") + theme[""default""]         )     for i, name in enumerate(self.name):         width = self.colwidth()         ret = ret + name[0:width].center(width).replace("" "", ""-"")         if i + 1 != len(self.vars):             <MASK>                 ret = ret + theme[""frame""] + char[""dash""] + theme[""title""]             else:                 ret = ret + char[""space""]     return ret",if op . color :,"if name [ i ] == ""frame"" :",4.456882760699063,4.456882760699063,0.0
"def _get_requested_databases(self):     """"""Returns a list of databases requested, not including ignored dbs""""""     requested_databases = []     if (self._requested_namespaces is not None) and (self._requested_namespaces != []):         for requested_namespace in self._requested_namespaces:             <MASK>                 return []             elif requested_namespace[0] not in IGNORE_DBS:                 requested_databases.append(requested_namespace[0])     return requested_databases","if requested_namespace [ 0 ] is ""*"" :",if requested_namespace [ 0 ] in requested_databases :,53.3167536340577,53.3167536340577,0.0
"def add_channels(cls, voucher, add_channels):     for add_channel in add_channels:         channel = add_channel[""channel""]         defaults = {""currency"": channel.currency_code}         <MASK>             defaults[""discount_value""] = add_channel.get(""discount_value"")         if ""min_amount_spent"" in add_channel.keys():             defaults[""min_spent_amount""] = add_channel.get(""min_amount_spent"", None)         models.VoucherChannelListing.objects.update_or_create(             voucher=voucher,             channel=channel,             defaults=defaults,         )","if ""discount_value"" in add_channel . keys ( ) :","if ""discount_value"" in add_channel . keys ( ) :",100.00000000000004,100.00000000000004,1.0
"def read_xml(path):     with tf.gfile.GFile(path) as f:         root = etree.fromstring(f.read())     annotations = {}     for node in root.getchildren():         key, val = node2dict(node)         # If `key` is object, it's actually a list.         <MASK>             annotations.setdefault(key, []).append(val)         else:             annotations[key] = val     return annotations","if key == ""object"" :","if isinstance ( val , list ) :",6.567274736060395,6.567274736060395,0.0
"def get_ip_info(ipaddress):     """"""Returns device information by IP address""""""     result = {}     try:         ip = IPAddress.objects.select_related().get(address=ipaddress)     except IPAddress.DoesNotExist:         pass     else:         <MASK>             result[""venture_id""] = ip.venture.id         if ip.device is not None:             result[""device_id""] = ip.device.id             if ip.device.venture is not None:                 result[""venture_id""] = ip.device.venture.id     return result",if ip . venture is not None :,if ip . eventure is not None :,50.000000000000014,50.000000000000014,0.0
"def test_large_headers(self):     with ExpectLog(gen_log, ""Unsatisfiable read"", required=False):         try:             self.fetch(""/"", headers={""X-Filler"": ""a"" * 1000}, raise_error=True)             self.fail(""did not raise expected exception"")         except HTTPError as e:             # 431 is ""Request Header Fields Too Large"", defined in RFC             # 6585. However, many implementations just close the             # connection in this case, resulting in a missing response.             <MASK>                 self.assertIn(e.response.code, (431, 599))",if e . response is not None :,if e . response . code != 404 :,27.77619034011791,27.77619034011791,0.0
"def validate_reserved_serial_no_consumption(self):     for item in self.items:         if item.s_warehouse and not item.t_warehouse and item.serial_no:             for sr in get_serial_nos(item.serial_no):                 sales_order = frappe.db.get_value(""Serial No"", sr, ""sales_order"")                 <MASK>                     msg = _(                         ""(Serial No: {0}) cannot be consumed as it's reserverd to fullfill Sales Order {1}.""                     ).format(sr, sales_order)                     frappe.throw(_(""Item {0} {1}"").format(item.item_code, msg))",if sales_order :,if sales_order :,100.00000000000004,100.00000000000004,1.0
"def force_decode(string, encoding):     if isinstance(string, str):         <MASK>             string = string.decode(encoding)         else:             try:                 # try decoding with utf-8, should only work for real UTF-8                 string = string.decode(""utf-8"")             except UnicodeError:                 # last resort -- can't fail                 string = string.decode(""latin1"")     return string",if encoding :,if encoding :,100.00000000000004,0.0,1.0
"def _add_cs(master_cs, sub_cs, prefix, delimiter=""."", parent_hp=None):     new_parameters = []     for hp in sub_cs.get_hyperparameters():         new_parameter = copy.deepcopy(hp)         # Allow for an empty top-level parameter         if new_parameter.name == """":             new_parameter.name = prefix         <MASK>             new_parameter.name = ""{}{}{}"".format(prefix, SPLITTER, new_parameter.name)         new_parameters.append(new_parameter)     for hp in new_parameters:         _add_hp(master_cs, hp)","elif not prefix == """" :","if delimiter == """" :",53.29462628216855,53.29462628216855,0.0
"def __call__(self, *args, **kwargs):     if self.log_file is not None:         kwargs[""file""] = self.log_file         print(*args, **kwargs)         <MASK>             # get immediate feedback             self.log_file.flush()     elif self.log_func is not None:         self.log_func(*args, **kwargs)","if hasattr ( self . log_file , ""flush"" ) :",if self . log_file is not None :,28.64190457979541,28.64190457979541,0.0
"def df_index_expr(self, length_expr=None, as_range=False):     """"""Generate expression to get or create index of DF""""""     if isinstance(self.index, types.NoneType):         <MASK>             length_expr = df_length_expr(self)         if as_range:             return f""range({length_expr})""         else:             return f""numpy.arange({length_expr})""     return ""self._index""",if length_expr is None :,if length_expr is None :,100.00000000000004,100.00000000000004,1.0
"def _setWeight(self, value):     if value is None:         self._fontWeight = None     else:         <MASK>             raise TextFormatException(f""Not a supported fontWeight: {value}"")         self._fontWeight = value.lower()","if value . lower ( ) not in ( ""normal"" , ""bold"" ) :",if value not in self . _fontWeight :,5.490133261314248,5.490133261314248,0.0
"def _test_configuration(self):     config_path = self._write_config()     try:         self._log.debug(""testing configuration"")         verboseflag = ""-Q""         if self._log.isEnabledFor(logging.DEBUG):             verboseflag = ""-v""         p = subprocess.Popen([self.PATH_SLAPTEST, verboseflag, ""-f"", config_path])         <MASK>             raise RuntimeError(""configuration test failed"")         self._log.debug(""configuration seems ok"")     finally:         os.remove(config_path)",if p . wait ( ) != 0 :,if not p . wait ( ) :,43.79518644116555,43.79518644116555,0.0
"def filter_queryset(self, request, queryset, view):     kwargs = {}     for field in view.filterset_fields:         value = request.GET.get(field)         if not value:             continue         if field == ""node_id"":             value = get_object_or_none(Node, pk=value)             kwargs[""node""] = value             continue         <MASK>             field = ""asset""         kwargs[field] = value     if kwargs:         queryset = queryset.filter(**kwargs)     logger.debug(""Filter {}"".format(kwargs))     return queryset","elif field == ""asset_id"" :","if field == ""asset_id"" :",88.01117367933934,88.01117367933934,0.0
"def _find_closing_brace(string, start_pos):     """"""Finds the corresponding closing brace after start_pos.""""""     bracks_open = 1     for idx, char in enumerate(string[start_pos:]):         if char == ""("":             <MASK>                 bracks_open += 1         elif char == "")"":             if string[idx + start_pos - 1] != ""\\"":                 bracks_open -= 1             if not bracks_open:                 return start_pos + idx + 1","if string [ idx + start_pos - 1 ] != ""\\"" :",if idx + start_pos > 0 :,17.860244166902365,17.860244166902365,0.0
"def _set_hostport(self, host, port):     if port is None:         i = host.rfind("":"")         j = host.rfind(""]"")  # ipv6 addresses have [...]         if i > j:             try:                 port = int(host[i + 1 :])             except ValueError:                 raise InvalidURL(""nonnumeric port: '%s'"" % host[i + 1 :])             host = host[:i]         else:             port = self.default_port         <MASK>             host = host[1:-1]     self.host = host     self.port = port","if host and host [ 0 ] == ""["" and host [ - 1 ] == ""]"" :",if i > j :,0.2841830445403334,0.2841830445403334,0.0
"def __getstate__(self):     state = {}     for cls in type(self).mro():         cls_slots = getattr(cls, ""__slots__"", ())         for slot in cls_slots:             if slot != ""__weakref__"":                 <MASK>                     state[slot] = getattr(self, slot)     state[""_cookiejar_cookies""] = list(self.cookiejar)     del state[""cookiejar""]     return state","if hasattr ( self , slot ) :","if slot != ""__weakref__"" :",4.456882760699063,4.456882760699063,0.0
"def _evp_pkey_from_der_traditional_key(self, bio_data, password):     key = self._lib.d2i_PrivateKey_bio(bio_data.bio, self._ffi.NULL)     if key != self._ffi.NULL:         key = self._ffi.gc(key, self._lib.EVP_PKEY_free)         <MASK>             raise TypeError(""Password was given but private key is not encrypted."")         return key     else:         self._consume_errors()         return None",if password is not None :,if password != self . _lib . EVP_PKEY_encrypted :,5.816635421147513,5.816635421147513,0.0
"def is_special(s, i, directive):     """"""Return True if the body text contains the @ directive.""""""     # j = skip_line(s,i) ; trace(s[i:j],':',directive)     assert directive and directive[0] == ""@""     # 10/23/02: all directives except @others must start the line.     skip_flag = directive in (""@others"", ""@all"")     while i < len(s):         <MASK>             return True, i         else:             i = skip_line(s, i)             if skip_flag:                 i = skip_ws(s, i)     return False, -1","if match_word ( s , i , directive ) :",if skip_flag :,3.466791587270993,3.466791587270993,0.0
"def _decorator(coro_func):     fut = asyncio.ensure_future(coro_func())     self._tests.append((coro_func.__name__, fut))     if timeout_sec is not None:         timeout_at = self._loop.time() + timeout_sec         handle = self.MASTER_LOOP.call_at(             timeout_at, self._set_exception_if_not_done, fut, asyncio.TimeoutError()         )         fut.add_done_callback(lambda *args: handle.cancel())         <MASK>             self._global_timeout_at = timeout_at     return coro_func",if timeout_at > self . _global_timeout_at :,if handle is not None :,2.544354209531657,2.544354209531657,0.0
"def _load(self, db, owner):     self.__init(owner)     db_result = db(         ""SELECT ship_id, state_id FROM ai_combat_ship WHERE owner_id = ?"",         self.owner.worldid,     )     for (         ship_id,         state_id,     ) in db_result:         ship = WorldObject.get_object_by_id(ship_id)         state = self.shipStates[state_id]         # add move callbacks corresponding to given state         <MASK>             ship.add_move_callback(Callback(BehaviorMoveCallback._arrived, ship))         self.add_new_unit(ship, state)",if state == self . shipStates . moving :,if state . is_move :,10.175282441454787,10.175282441454787,0.0
"def addError(self, test, err):     if err[0] is SkipTest:         <MASK>             self.stream.writeln(str(err[1]))         elif self.dots:             self.stream.write(""s"")             self.stream.flush()         return     _org_AddError(self, test, err)",if self . showAll :,if self . dots :,42.72870063962342,42.72870063962342,0.0
"def _construct(self, node):     self.flatten_mapping(node)     ret = self.construct_pairs(node)     keys = [d[0] for d in ret]     keys_sorted = sorted(keys, key=_natsort_key)     for key in keys:         expected = keys_sorted.pop(0)         <MASK>             raise ConstructorError(                 None,                 None,                 ""keys out of order: ""                 ""expected {} got {} at {}"".format(expected, key, node.start_mark),             )     return dict(ret)",if key != expected :,if expected != node . start_mark :,10.552670315936318,10.552670315936318,0.0
"def sample_pos_items_for_u(u, num):     # sample num pos items for u-th user     pos_items = self.train_items[u]     n_pos_items = len(pos_items)     pos_batch = []     while True:         if len(pos_batch) == num:             break         pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]         pos_i_id = pos_items[pos_id]         <MASK>             pos_batch.append(pos_i_id)     return pos_batch",if pos_i_id not in pos_batch :,if pos_i_id != u :,43.98917247584221,43.98917247584221,0.0
"def _get_id(self, type, id):     fields = id.split("":"")     if len(fields) >= 3:         <MASK>             logger.warning(                 ""Expected id of type %s but found type %s %s"", type, fields[-2], id             )         return fields[-1]     fields = id.split(""/"")     if len(fields) >= 3:         itype = fields[-2]         if type != itype:             logger.warning(                 ""Expected id of type %s but found type %s %s"", type, itype, id             )         return fields[-1].split(""?"")[0]     return id",if type != fields [ - 2 ] :,if type != fields [ 0 ] :,59.11602603314155,59.11602603314155,0.0
"def uninstall_environments(self, environments):     environments = [         env         if not env.startswith(self.conda_context.envs_path)         else os.path.basename(env)         for env in environments     ]     return_codes = [self.conda_context.exec_remove([env]) for env in environments]     final_return_code = 0     for env, return_code in zip(environments, return_codes):         <MASK>             log.debug(""Conda environment '%s' successfully removed."" % env)         else:             log.debug(""Conda environment '%s' could not be removed."" % env)             final_return_code = return_code     return final_return_code",if return_code == 0 :,if return_code == 0 :,100.00000000000004,100.00000000000004,1.0
"def _add_hit_offset(self, context_list, string_name, original_offset, value):     for context in context_list:         hits_by_context_dict = self.hits_by_context.setdefault(context, {})         <MASK>             hits_by_context_dict[string_name] = (                 original_offset,                 value.encode(""base64""),             )",if string_name not in hits_by_context_dict :,if string_name in hits_by_context_dict :,78.81929718099911,78.81929718099911,0.0
"def detab(self, text, length=None):     """"""Remove a tab from the front of each line of the given text.""""""     if length is None:         length = self.tab_length     newtext = []     lines = text.split(""\n"")     for line in lines:         if line.startswith("" "" * length):             newtext.append(line[length:])         <MASK>             newtext.append("""")         else:             break     return ""\n"".join(newtext), ""\n"".join(lines[len(newtext) :])",elif not line . strip ( ) :,if len ( lines ) > 0 :,7.267884212102741,7.267884212102741,0.0
"def dump(self):     print(self.package_name)     for package, value in self.entries:         print(str(package.version))         <MASK>             print(""    [FILTERED]"")         elif isinstance(value, list):             variants = value             for variant in variants:                 print(""    %s"" % str(variant))         else:             print(""    %s"" % str(package))",if value is None :,"if isinstance ( value , tuple ) :",7.267884212102741,7.267884212102741,0.0
"def __lexical_scope(*args, **kwargs):     try:         scope = Scope(quasi)         <MASK>             binding_name_set_stack[-1].add_child(scope)         binding_name_set_stack.append(scope)         return func(*args, **kwargs)     finally:         if binding_name_set_stack[-1] is scope:             binding_name_set_stack.pop()",if quasi :,if binding_name_set_stack [ - 1 ] is scope :,3.1251907639724417,3.1251907639724417,0.0
"def getnotes(self, origin=None):     if origin is None:         result = self.translator_comments         <MASK>             if result:                 result += ""\n"" + self.developer_comments             else:                 result = self.developer_comments         return result     elif origin == ""translator"":         return self.translator_comments     elif origin in (""programmer"", ""developer"", ""source code""):         return self.developer_comments     else:         raise ValueError(""Comment type not valid"")",if self . developer_comments :,"if origin == ""developer"" :",7.267884212102741,7.267884212102741,0.0
"def fix_datetime_fields(data: TableData, table: TableName) -> None:     for item in data[table]:         for field_name in DATE_FIELDS[table]:             <MASK>                 item[field_name] = datetime.datetime.fromtimestamp(                     item[field_name], tz=datetime.timezone.utc                 )",if item [ field_name ] is not None :,if field_name in item :,16.417223692914014,16.417223692914014,0.0
"def _check_for_cart_error(cart):     if cart._safe_get_element(""Cart.Request.Errors"") is not None:         error = cart._safe_get_element(""Cart.Request.Errors.Error.Code"").text         <MASK>             raise CartInfoMismatchException(                 ""CartGet failed: AWS.ECommerceService.CartInfoMismatch ""                 ""make sure AssociateTag, CartId and HMAC are correct ""                 ""(dont use URLEncodedHMAC!!!)""             )         raise CartException(""CartGet failed: "" + error)","if error == ""AWS.ECommerceService.CartInfoMismatch"" :","if error != ""CartInfoMismatch"" :",18.325568129983218,18.325568129983218,0.0
"def check_bounds(geometry):     if isinstance(geometry[0], (list, tuple)):         return list(map(check_bounds, geometry))     else:         <MASK>             raise ValueError(                 ""Longitude is out of bounds, check your JSON format or data""             )         if geometry[1] > 90 or geometry[1] < -90:             raise ValueError(                 ""Latitude is out of bounds, check your JSON format or data""             )",if geometry [ 0 ] > 180 or geometry [ 0 ] < - 180 :,if geometry [ 0 ] > 180 or geometry [ 0 ] < -90 :,81.07492451395736,81.07492451395736,0.0
"def _mapper_output_protocol(self, step_num, step_map):     map_key = self._step_key(step_num, ""mapper"")     if map_key in step_map:         <MASK>             return self.output_protocol()         else:             return self.internal_protocol()     else:         # mapper is not a script substep, so protocols don't apply at all         return RawValueProtocol()",if step_map [ map_key ] >= ( len ( step_map ) - 1 ) :,"if step_map [ map_key ] == ""script"" :",36.343459007702876,36.343459007702876,0.0
"def asset(*paths):     for path in paths:         fspath = www_root + ""/assets/"" + path         etag = """"         try:             if env.cache_static:                 etag = asset_etag(fspath)             else:                 os.stat(fspath)         except FileNotFoundError as e:             <MASK>                 if not os.path.exists(fspath + "".spt""):                     tell_sentry(e, {})             else:                 continue         except Exception as e:             tell_sentry(e, {})         return asset_url + path + (etag and ""?etag="" + etag)",if path == paths [ - 1 ] :,if e . errno == 404 :,10.229197414177778,10.229197414177778,0.0
"def ping(self, payload: Union[str, bytes] = """") -> None:     if self.trace_enabled and self.ping_pong_trace_enabled:         <MASK>             payload = payload.decode(""utf-8"")         self.logger.debug(             ""Sending a ping data frame ""             f""(session id: {self.session_id}, payload: {payload})""         )     data = _build_data_frame_for_sending(payload, FrameHeader.OPCODE_PING)     with self.sock_send_lock:         self.sock.send(data)","if isinstance ( payload , bytes ) :",if payload :,7.49553326588684,0.0,0.0
"def is_ac_power_connected():     for power_source_path in Path(""/sys/class/power_supply/"").iterdir():         try:             with open(power_source_path / ""type"", ""r"") as f:                 if f.read().strip() != ""Mains"":                     continue             with open(power_source_path / ""online"", ""r"") as f:                 <MASK>                     return True         except IOError:             continue     return False","if f . read ( 1 ) == ""1"" :","if f . read ( ) != ""Mains"" :",38.386658421682654,38.386658421682654,0.0
"def handle_noargs(self, **options):     self.style = color_style()     print(""Running Django's own validation:"")     self.validate(display_num_errors=True)     for model in loading.get_models():         <MASK>             self.validate_base_model(model)         if hasattr(model, ""_feincms_content_models""):             self.validate_content_type(model)","if hasattr ( model , ""_create_content_base"" ) :","if hasattr ( model , ""_feincms_base_models"" ) :",54.10822690539399,54.10822690539399,0.0
"def _init_weights(self, module):     if isinstance(module, nn.Linear):         module.weight.data.normal_(mean=0.0, std=self.config.init_std)         <MASK>             module.bias.data.zero_()     elif isinstance(module, nn.Embedding):         module.weight.data.normal_(mean=0.0, std=self.config.init_std)         if module.padding_idx is not None:             module.weight.data[module.padding_idx].zero_()",if module . bias is not None :,if module . bias_idx is not None :,52.53819788848316,52.53819788848316,0.0
"def walk(msg, callback, data):     partnum = 0     for part in msg.walk():         # multipart/* are just containers         if part.get_content_maintype() == ""multipart"":             continue         ctype = part.get_content_type()         <MASK>             ctype = OCTET_TYPE         filename = part.get_filename()         if not filename:             filename = PART_FN_TPL % (partnum)         headers = dict(part)         LOG.debug(headers)         headers[""Content-Type""] = ctype         payload = util.fully_decoded_payload(part)         callback(data, filename, payload, headers)         partnum = partnum + 1",if ctype is None :,if not ctype :,16.37226966703825,16.37226966703825,0.0
"def _mark_lcs(mask, dirs, m, n):     while m != 0 and n != 0:         if dirs[m, n] == ""|"":             m -= 1             n -= 1             mask[m] = 1         <MASK>             m -= 1         elif dirs[m, n] == ""<"":             n -= 1         else:             raise UnboundLocalError(""Illegal move"")     return mask","elif dirs [ m , n ] == ""^"" :","if dirs [ m , n ] == "">"" :",70.16035864257111,70.16035864257111,0.0
"def valid_localparts(strip_delimiters=False):     for line in ABRIDGED_LOCALPART_VALID_TESTS.split(""\n""):         # strip line, skip over empty lines         line = line.strip()         <MASK>             continue         # skip over comments or empty lines         match = COMMENT.match(line)         if match:             continue         # skip over localparts with delimiters         if strip_delimiters:             if "","" in line or "";"" in line:                 continue         yield line","if line == """" :",if not line :,9.930283522141846,9.930283522141846,0.0
"def fetch(self, *tileables, **kw):     ret_list = False     if len(tileables) == 1 and isinstance(tileables[0], (tuple, list)):         ret_list = True         tileables = tileables[0]     elif len(tileables) > 1:         ret_list = True     result = self._sess.fetch(*tileables, **kw)     ret = []     for r, t in zip(result, tileables):         <MASK>             ret.append(r.item())         else:             ret.append(r)     if ret_list:         return ret     return ret[0]","if hasattr ( t , ""isscalar"" ) and t . isscalar ( ) and getattr ( r , ""size"" , None ) == 1 :",if t . item ( ) :,0.7691046017395218,0.7691046017395218,0.0
"def _convert(container):     if _value_marker in container:         force_list = False         values = container.pop(_value_marker)         if container.pop(_list_marker, False):             force_list = True             values.extend(_convert(x[1]) for x in sorted(container.items()))         <MASK>             values = values[0]         if not container:             return values         return _convert(container)     elif container.pop(_list_marker, False):         return [_convert(x[1]) for x in sorted(container.items())]     return dict_cls((k, _convert(v)) for k, v in iteritems(container))",if not force_list and len ( values ) == 1 :,if force_list :,7.468220329575271,7.468220329575271,0.0
"def _transform_init_kwargs(cls, kwargs):     transformed = []     for field in list(kwargs.keys()):         prop = getattr(cls, field, None)         <MASK>             value = kwargs.pop(field)             _transform_single_init_kwarg(prop, field, value, kwargs)             transformed.append((field, value))     return transformed","if isinstance ( prop , MoneyProperty ) :",if prop is not None :,7.654112967106117,7.654112967106117,0.0
"def haslayer(self, cls):     """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax.""""""     if self.__class__ == cls or self.__class__.__name__ == cls:         return 1     for f in self.packetfields:         fvalue_gen = self.getfieldval(f.name)         if fvalue_gen is None:             continue         if not f.islist:             fvalue_gen = SetGen(fvalue_gen, _iterpacket=0)         for fvalue in fvalue_gen:             if isinstance(fvalue, Packet):                 ret = fvalue.haslayer(cls)                 <MASK>                     return ret     return self.payload.haslayer(cls)",if ret :,if ret :,100.00000000000004,0.0,1.0
def insert_broken_add_sometimes(node):     if node.op == theano.tensor.add:         last_time_replaced[0] = not last_time_replaced[0]         <MASK>             return [off_by_half(*node.inputs)]     return False,if last_time_replaced [ 0 ] :,if node . inputs :,4.673289785800722,4.673289785800722,0.0
"def testReadChunk10(self):     # ""Test BZ2File.read() in chunks of 10 bytes""     self.createTempFile()     with BZ2File(self.filename) as bz2f:         text = """"         while 1:             str = bz2f.read(10)             <MASK>                 break             text += str         self.assertEqual(text, self.TEXT)",if not str :,if str is None :,14.058533129758727,14.058533129758727,0.0
"def generate_sv_faces(dcel_mesh, point_index, only_select=False, del_flag=None):     # This part of function creates faces in SV format     # It ignores  boundless super face     sv_faces = []     for i, face in enumerate(dcel_mesh.faces):         if face.inners and face.outer:             ""Face ({}) has inner components! Sverchok cant show polygons with holes."".format(                 i             )         if not face.outer or del_flag in face.flags:             continue         <MASK>             continue         sv_faces.append([point_index[hedge.origin] for hedge in face.outer.loop_hedges])     return sv_faces",if only_select and not face . select :,if not only_select :,20.415280320726456,20.415280320726456,0.0
"def __check_dict_contains(dct, dict_name, keys, comment="""", result=True):     for key in keys:         <MASK>             result = False             comment = __append_comment(                 ""Missing {0} in {1}"".format(key, dict_name), comment             )     return result, comment",if key not in six . iterkeys ( dct ) :,if key not in dct :,24.439253249722206,24.439253249722206,0.0
"def _dump_arg_defaults(kwargs):     """"""Inject default arguments for dump functions.""""""     if current_app:         kwargs.setdefault(""cls"", current_app.json_encoder)         <MASK>             kwargs.setdefault(""ensure_ascii"", False)         kwargs.setdefault(""sort_keys"", current_app.config[""JSON_SORT_KEYS""])     else:         kwargs.setdefault(""sort_keys"", True)         kwargs.setdefault(""cls"", JSONEncoder)","if not current_app . config [ ""JSON_AS_ASCII"" ] :","if current_app . config [ ""JSON_SORT_KEYS"" ] :",58.83398368397459,58.83398368397459,0.0
"def _on_change(self):     changed = False     self.save()     for key, value in self.data.items():         if isinstance(value, bool):             if value:                 changed = True                 break         if isinstance(value, int):             if value != 1:                 changed = True                 break         elif value is None:             continue         <MASK>             changed = True             break     self._reset_button.disabled = not changed",elif len ( value ) != 0 :,"if key != ""disabled"" :",10.786826322527466,10.786826322527466,0.0
"def parse_win_proxy(val):     proxies = []     for p in val.split("";""):         <MASK>             tab = p.split(""="", 1)             if tab[0] == ""socks"":                 tab[0] = ""SOCKS4""             proxies.append(                 (tab[0].upper(), tab[1], None, None)             )  # type, addr:port, username, password         else:             proxies.append((""HTTP"", p, None, None))     return proxies","if ""="" in p :","if p . startswith ( ""HTTP"" ) :",6.27465531099474,6.27465531099474,0.0
"def predict(collect_dir, keys):     run_all = len(keys) == 0     validate_keys(keys)     for exp_cfg in cfg:         <MASK>             key = exp_cfg[""key""]             _predict(key, exp_cfg[""sample_img""], collect_dir)","if run_all or exp_cfg [ ""key"" ] in keys :",if run_all :,7.834966465489322,7.834966465489322,0.0
"def convert_port_bindings(port_bindings):     result = {}     for k, v in six.iteritems(port_bindings):         key = str(k)         <MASK>             key += ""/tcp""         if isinstance(v, list):             result[key] = [_convert_port_binding(binding) for binding in v]         else:             result[key] = [_convert_port_binding(v)]     return result","if ""/"" not in key :","if k == ""tcp"" :",7.809849842300637,7.809849842300637,0.0
"def assert_conll_writer_output(     dataset: InternalBioNerDataset,     expected_output: List[str],     sentence_splitter: SentenceSplitter = None, ):     outfile_path = tempfile.mkstemp()[1]     try:         sentence_splitter = (             sentence_splitter             <MASK>             else NoSentenceSplitter(tokenizer=SpaceTokenizer())         )         writer = CoNLLWriter(sentence_splitter=sentence_splitter)         writer.write_to_conll(dataset, Path(outfile_path))         contents = [l.strip() for l in open(outfile_path).readlines() if l.strip()]     finally:         os.remove(outfile_path)     assert contents == expected_output",if sentence_splitter,if sentence_splitter is not None :,34.57207846419409,34.57207846419409,0.0
"def post(self, request, *args, **kwargs):     self.comment_obj = get_object_or_404(Comment, id=request.POST.get(""commentid""))     if request.user == self.comment_obj.commented_by:         form = LeadCommentForm(request.POST, instance=self.comment_obj)         <MASK>             return self.form_valid(form)         return self.form_invalid(form)     data = {""error"": ""You don't have permission to edit this comment.""}     return JsonResponse(data)",if form . is_valid ( ) :,if form . is_valid ( ) :,100.00000000000004,100.00000000000004,1.0
"def trivia_list(self, ctx: commands.Context):     """"""List available trivia categories.""""""     lists = set(p.stem for p in self._all_lists())     if await ctx.embed_requested():         await ctx.send(             embed=discord.Embed(                 title=_(""Available trivia lists""),                 colour=await ctx.embed_colour(),                 description="", "".join(sorted(lists)),             )         )     else:         msg = box(bold(_(""Available trivia lists"")) + ""\n\n"" + "", "".join(sorted(lists)))         <MASK>             await ctx.author.send(msg)         else:             await ctx.send(msg)",if len ( msg ) > 1000 :,if ctx . author_requested ( ) :,6.742555929751843,6.742555929751843,0.0
"def validate(self):     result = validators.SUCCESS     msgs = []     for validator in self._validators:         res, err = validator.validate()         if res == validators.ERROR:             result = res         elif res == validators.WARNING and result != validators.ERROR:             result = res         <MASK>             msgs.append(err)     return result, ""\n"".join(msgs)",if len ( err ) > 0 :,if err :,7.49553326588684,0.0,0.0
"def get_code(self, fullname=None):     fullname = self._fix_name(fullname)     if self.code is None:         mod_type = self.etc[2]         if mod_type == imp.PY_SOURCE:             source = self.get_source(fullname)             self.code = compile(source, self.filename, ""exec"")         <MASK>             self._reopen()             try:                 self.code = read_code(self.file)             finally:                 self.file.close()         elif mod_type == imp.PKG_DIRECTORY:             self.code = self._get_delegate().get_code()     return self.code",elif mod_type == imp . PY_COMPILED :,if mod_type == imp . PY_FILE :,72.92571723872932,72.92571723872932,0.0
"def flush_file(self, key, f):     f.flush()     if self.compress:         f.compress = zlib.compressobj(             9, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0         )     if len(self.files) > self.MAX_OPEN_FILES:         if self.compress:             open_files = sum(1 for f in self.files.values() if f.fileobj is not None)             <MASK>                 f.fileobj.close()                 f.fileobj = None         else:             f.close()             self.files.pop(key)",if open_files > self . MAX_OPEN_FILES :,if open_files > 0 :,27.30664777474173,27.30664777474173,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             self.add_version(d.getPrefixedString())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 10 :,if tt == 0 :,53.7284965911771,53.7284965911771,0.0
"def init_author_file(self):     self.author_map = {}     if self.ui.config(""git"", ""authors""):         f = open(self.repo.wjoin(self.ui.config(""git"", ""authors"")))         try:             for line in f:                 line = line.strip()                 <MASK>                     continue                 from_, to = RE_AUTHOR_FILE.split(line, 2)                 self.author_map[from_] = to         finally:             f.close()","if not line or line . startswith ( ""#"" ) :",if not line :,6.734410772670761,6.734410772670761,0.0
"def decode_imsi(self, imsi):     new_imsi = """"     for a in imsi:         c = hex(a)         <MASK>             new_imsi += str(c[3]) + str(c[2])         else:             new_imsi += str(c[2]) + ""0""     mcc = new_imsi[1:4]     mnc = new_imsi[4:6]     return new_imsi, mcc, mnc",if len ( c ) == 4 :,if c [ 0 ] == 0 :,11.99014838091355,11.99014838091355,0.0
"def _get_infoset(self, prefname):     """"""Return methods with the name starting with prefname.""""""     infoset = []     excludes = (""%sinfoset"" % prefname,)     preflen = len(prefname)     for name in dir(self.__class__):         if name.startswith(prefname) and name not in excludes:             member = getattr(self.__class__, name)             <MASK>                 infoset.append(name[preflen:].replace(""_"", "" ""))     return infoset","if isinstance ( member , MethodType ) :",if member . __name__ == prefname :,4.456882760699063,4.456882760699063,0.0
"def skip_to_close_match(self):     nestedCount = 1     while 1:         tok = self.tokenizer.get_next_token()         ttype = tok[""style""]         if ttype == SCE_PL_UNUSED:             return         elif self.classifier.is_index_op(tok):             tval = tok[""text""]             if self.opHash.has_key(tval):                 if self.opHash[tval][1] == 1:                     nestedCount += 1                 else:                     nestedCount -= 1                     <MASK>                         break",if nestedCount <= 0 :,if nestedCount == 0 :,37.99178428257963,37.99178428257963,0.0
"def findMarkForUnitTestNodes(self):     """"""return the position of *all* non-ignored @mark-for-unit-test nodes.""""""     c = self.c     p, result, seen = c.rootPosition(), [], []     while p:         <MASK>             p.moveToNodeAfterTree()         else:             seen.append(p.v)             if g.match_word(p.h, 0, ""@ignore""):                 p.moveToNodeAfterTree()             elif p.h.startswith(""@mark-for-unit-tests""):                 result.append(p.copy())                 p.moveToNodeAfterTree()             else:                 p.moveToThreadNext()     return result",if p . v in seen :,if p . v in seen :,100.00000000000004,100.00000000000004,1.0
"def assert_parts_cleaned(self, earlier_parts, current_parts, expected_parts, hint):     cleaned_parts = []     for earlier in earlier_parts:         earlier_part = earlier[""part""]         earlier_step = earlier[""step""]         found = False         for current in current_parts:             if earlier_part == current[""part""] and earlier_step == current[""step""]:                 found = True                 break         <MASK>             cleaned_parts.append(dict(part=earlier_part, step=earlier_step))     self.assertThat(cleaned_parts, HasLength(len(expected_parts)), hint)     for expected in expected_parts:         self.assertThat(cleaned_parts, Contains(expected), hint)",if not found :,if found :,45.13864405503391,0.0,0.0
"def unmark_first_parents(event=None):     """"""Mark the node and all its parents.""""""     c = event.get(""c"")     if not c:         return     changed = []     for parent in c.p.self_and_parents():         <MASK>             parent.v.clearMarked()             parent.setAllAncestorAtFileNodesDirty()             changed.append(parent.copy())     if changed:         # g.es(""unmarked: "" + ', '.join([z.h for z in changed]))         c.setChanged()         c.redraw()     return changed",if parent . isMarked ( ) :,if parent . v :,28.641904579795423,28.641904579795423,0.0
"def stop(self):     self._log(""Monitor stop"")     self._stop_requested = True     try:         <MASK>             fd = os.open(self.fifo_path, os.O_WRONLY)             os.write(fd, b""X"")             os.close(fd)     except Exception as e:         self._log(""err while closing: {0}"".format(str(e)))     if self._thread:         self._thread.join()         self._thread = None",if os . path . exists ( self . fifo_path ) :,if self . fifo_path :,24.601580968354618,24.601580968354618,0.0
"def DeleteEmptyCols(self):     cols2delete = []     for c in range(0, self.GetCols()):         f = True         for r in range(0, self.GetRows()):             <MASK>                 f = False         if f:             cols2delete.append(c)     for i in range(0, len(cols2delete)):         self.ShiftColsLeft(cols2delete[i] + 1)         cols2delete = [x - 1 for x in cols2delete]","if self . FindItemAtPosition ( ( r , c ) ) is not None :",if c == r :,2.5612540390806937,2.5612540390806937,0.0
"def _load_objects(self, obj_id_zset, limit, chunk_size=1000):     ct = i = 0     while True:         id_chunk = obj_id_zset[i : i + chunk_size]         <MASK>             return         i += chunk_size         for raw_data in self._data[id_chunk]:             if not raw_data:                 continue             if self._use_json:                 yield json.loads(decode(raw_data))             else:                 yield raw_data             ct += 1             if limit and ct == limit:                 return",if not id_chunk :,if not id_chunk :,100.00000000000004,100.00000000000004,1.0
"def _convert_example(example, use_bfloat16):     """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16.""""""     for key in list(example.keys()):         val = example[key]         if tf.keras.backend.is_sparse(val):             val = tf.sparse.to_dense(val)         <MASK>             val = tf.cast(val, tf.int32)         if use_bfloat16 and val.dtype == tf.float32:             val = tf.cast(val, tf.bfloat16)         example[key] = val",if val . dtype == tf . int64 :,if val . dtype == tf . int64 :,100.00000000000004,100.00000000000004,1.0
"def print_callees(self, *amount):     width, list = self.get_print_list(amount)     if list:         self.calc_callees()         self.print_call_heading(width, ""called..."")         for func in list:             <MASK>                 self.print_call_line(width, func, self.all_callees[func])             else:                 self.print_call_line(width, func, {})         print >>self.stream         print >>self.stream     return self",if func in self . all_callees :,if func in self . all_callees :,100.00000000000004,100.00000000000004,1.0
"def on_task_input(self, task, config):     if config is False:         return     for entry in task.entries:         <MASK>             log_once(                 ""Corrected `%s` url (replaced &amp; with &)"" % entry[""title""],                 logger=log,             )             entry[""url""] = entry[""url""].replace(""&amp;"", ""&"")","if ""&amp;"" in entry [ ""url"" ] :","if entry [ ""title"" ] == ""url"" :",23.2334219683501,23.2334219683501,0.0
"def function(self, inputs, outputs, ignore_empty=False):     f = function(inputs, outputs, mode=self.mode)     if self.mode is not None or theano.config.mode != ""FAST_COMPILE"":         topo = f.maker.fgraph.toposort()         topo_ = [node for node in topo if not isinstance(node.op, self.ignore_topo)]         if ignore_empty:             assert len(topo_) <= 1, topo_         else:             assert len(topo_) == 1, topo_         <MASK>             assert type(topo_[0].op) is self.op     return f",if len ( topo_ ) > 0 :,if len ( topo_ ) == 1 :,53.7284965911771,53.7284965911771,0.0
"def _get_env_command(self) -> Sequence[str]:     """"""Get command sequence for `env` with configured flags.""""""     env_list = [""env""]     # Pass through configurable environment variables.     for key in [""http_proxy"", ""https_proxy""]:         value = self.build_provider_flags.get(key)         <MASK>             continue         # Ensure item is treated as string and append it.         value = str(value)         env_list.append(f""{key}={value}"")     return env_list",if not value :,if not value :,100.00000000000004,100.00000000000004,1.0
"def _compare_single_run(self, compares_done):     try:         compare_id, redo = self.in_queue.get(             timeout=float(self.config[""ExpertSettings""][""block_delay""])         )     except Empty:         pass     else:         if self._decide_whether_to_process(compare_id, redo, compares_done):             if redo:                 self.db_interface.delete_old_compare_result(compare_id)             compares_done.add(compare_id)             self._process_compare(compare_id)             <MASK>                 self.callback()",if self . callback :,if self . callback :,100.00000000000004,100.00000000000004,1.0
"def clean(self):     # TODO: check for clashes if the random code is already taken     if not self.code:         self.code = u""static-%s"" % uuid.uuid4()     if not self.site:         placeholders = StaticPlaceholder.objects.filter(             code=self.code, site__isnull=True         )         if self.pk:             placeholders = placeholders.exclude(pk=self.pk)         <MASK>             raise ValidationError(                 _(""A static placeholder with the same site and code already exists"")             )",if placeholders . exists ( ) :,if not placeholders :,9.930283522141846,9.930283522141846,0.0
"def load_parser(self):     result = OrderedDict()     for name, flags in self.filenames:         filename = self.get_filename(name)         for match in sorted(glob(filename), key=self.file_key):             # Needed to allow overlapping globs, more specific first             <MASK>                 continue             result[match] = TextParser(match, os.path.relpath(match, self.base), flags)     return result",if match in result :,if match in result :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, selectable, name=None):     baseselectable = selectable     while isinstance(baseselectable, Alias):         baseselectable = baseselectable.element     self.original = baseselectable     self.supports_execution = baseselectable.supports_execution     if self.supports_execution:         self._execution_options = baseselectable._execution_options     self.element = selectable     if name is None:         <MASK>             name = getattr(self.original, ""name"", None)         name = _anonymous_label(""%%(%d %s)s"" % (id(self), name or ""anon""))     self.name = name",if self . original . named_with_column :,if name is None :,3.8261660656802645,3.8261660656802645,0.0
"def load_tour(self, tour_id):     for tour_dir in self.tour_directories:         tour_path = os.path.join(tour_dir, tour_id + "".yaml"")         if not os.path.exists(tour_path):             tour_path = os.path.join(tour_dir, tour_id + "".yml"")         <MASK>             return self._load_tour_from_path(tour_path)",if os . path . exists ( tour_path ) :,if os . path . exists ( tour_path ) :,100.00000000000004,100.00000000000004,1.0
"def _get_md_bg_color_down(self):     t = self.theme_cls     c = self.md_bg_color  # Default to no change on touch     # Material design specifies using darker hue when on Dark theme     if t.theme_style == ""Dark"":         <MASK>             c = t.primary_dark         elif self.md_bg_color == t.accent_color:             c = t.accent_dark     return c",if self . md_bg_color == t . primary_color :,if self . md_bg_color == t . primary_color :,100.00000000000004,100.00000000000004,1.0
"def get_data(self, state=None, request=None):     if self.load_in_memory:         data, shapes = self._in_memory_get_data(state, request)     else:         data, shapes = self._out_of_memory_get_data(state, request)     for i in range(len(data)):         if shapes[i] is not None:             <MASK>                 data[i] = data[i].reshape(shapes[i])             else:                 for j in range(len(data[i])):                     data[i][j] = data[i][j].reshape(shapes[i][j])     return tuple(data)","if isinstance ( request , numbers . Integral ) :",if shapes [ i ] == self . load_out_of_memory :,3.0098043843528286,3.0098043843528286,0.0
"def onClicked(event):     if not self.path:         <MASK>             os.makedirs(mh.getPath(""render""))         self.path = mh.getPath(""render"")     filename, ftype = mh.getSaveFileName(         os.path.splitext(self.path)[0],         ""PNG Image (*.png);;JPEG Image (*.jpg);;Thumbnail (*.thumb);;All files (*.*)"",     )     if filename:         if ""Thumbnail"" in ftype:             self.image.save(filename, iformat=""PNG"")         else:             self.image.save(filename)         self.path = os.path.dirname(filename)","if not os . path . exists ( mh . getPath ( ""render"" ) ) :",if not os . path . isfile ( self . path ) :,30.808280803942317,30.808280803942317,0.0
"def _build_dom(cls, content, mode):     assert mode in (""html"", ""xml"")     if mode == ""html"":         <MASK>             THREAD_STORAGE.html_parser = HTMLParser()         dom = defusedxml.lxml.parse(             StringIO(content), parser=THREAD_STORAGE.html_parser         )         return dom.getroot()     else:         if not hasattr(THREAD_STORAGE, ""xml_parser""):             THREAD_STORAGE.xml_parser = XMLParser()         dom = defusedxml.lxml.parse(BytesIO(content), parser=THREAD_STORAGE.xml_parser)         return dom.getroot()","if not hasattr ( THREAD_STORAGE , ""html_parser"" ) :","if not hasattr ( THREAD_STORAGE , ""html_parser"" ) :",100.00000000000004,100.00000000000004,1.0
"def convert_path(ctx, tpath):     for points, code in tpath.iter_segments():         <MASK>             ctx.move_to(*points)         elif code == Path.LINETO:             ctx.line_to(*points)         elif code == Path.CURVE3:             ctx.curve_to(                 points[0], points[1], points[0], points[1], points[2], points[3]             )         elif code == Path.CURVE4:             ctx.curve_to(*points)         elif code == Path.CLOSEPOLY:             ctx.close_path()",if code == Path . MOVETO :,if code == Path . MOVE :,70.71067811865478,70.71067811865478,0.0
"def _targets(self, sigmaparser):     # build list of matching target mappings     targets = set()     for condfield in self.conditions:         if condfield in sigmaparser.values:             rulefieldvalues = sigmaparser.values[condfield]             for condvalue in self.conditions[condfield]:                 <MASK>                     targets.update(self.conditions[condfield][condvalue])     return targets",if condvalue in rulefieldvalues :,if condvalue in rulefieldvalues :,100.00000000000004,100.00000000000004,1.0
"def create_image_upload():     if request.method == ""POST"":         image = request.form[""image""]         <MASK>             image_file = uploaded_file(file_content=image)             image_url = upload_local(                 image_file, UPLOAD_PATHS[""temp""][""image""].format(uuid=uuid4())             )             return jsonify({""status"": ""ok"", ""image_url"": image_url})         else:             return jsonify({""status"": ""no_image""})",if image :,if image :,100.00000000000004,0.0,1.0
"def lookup_actions(self, resp):     actions = {}     for action, conditions in self.actions.items():         for condition, opts in conditions:             for key, val in condition:                 if key[-1] == ""!"":                     <MASK>                         break                 else:                     if not resp.match(key, val):                         break             else:                 actions[action] = opts     return actions","if resp . match ( key [ : - 1 ] , val ) :",if val == resp :,2.5612540390806937,2.5612540390806937,0.0
"def accept_quality(accept, default=1):     """"""Separates out the quality score from the accepted content_type""""""     quality = default     if accept and "";"" in accept:         accept, rest = accept.split("";"", 1)         accept_quality = RE_ACCEPT_QUALITY.search(rest)         <MASK>             quality = float(accept_quality.groupdict().get(""quality"", quality).strip())     return (quality, accept.strip())",if accept_quality :,if accept_quality :,100.00000000000004,100.00000000000004,1.0
"def save(self, session=None, to=None, pickler=None):     if to and pickler:         self._save_to = (pickler, to)     if self._save_to and len(self) > 0:         with self._lock:             pickler, fn = self._save_to             <MASK>                 session.ui.mark(_(""Saving %s state to %s"") % (self, fn))             pickler(self, fn)",if session :,if fn :,34.66806371753173,0.0,0.0
"def get_safe_settings():     ""Returns a dictionary of the settings module, with sensitive settings blurred out.""     settings_dict = {}     for k in dir(settings):         if k.isupper():             <MASK>                 settings_dict[k] = ""********************""             else:                 settings_dict[k] = getattr(settings, k)     return settings_dict",if HIDDEN_SETTINGS . search ( k ) :,"if k == ""blurred"" :",5.660233915657916,5.660233915657916,0.0
def _init_table_h():     _table_h = []     for i in range(256):         part_l = i         part_h = 0         for j in range(8):             rflag = part_l & 1             part_l >>= 1             if part_h & 1:                 part_l |= 1 << 31             part_h >>= 1             <MASK>                 part_h ^= 0xD8000000         _table_h.append(part_h)     return _table_h,if rflag :,if rflag :,100.00000000000004,0.0,1.0
"def dns_query(server, timeout, protocol, qname, qtype, qclass):     request = dns.message.make_query(qname, qtype, qclass)     if protocol == ""tcp"":         response = dns.query.tcp(             request, server, timeout=timeout, one_rr_per_rrset=True         )     else:         response = dns.query.udp(             request, server, timeout=timeout, one_rr_per_rrset=True         )         <MASK>             response = dns.query.tcp(                 request, server, timeout=timeout, one_rr_per_rrset=True             )     return response",if response . flags & dns . flags . TC :,"if protocol == ""tcp"" :",4.513617516969122,4.513617516969122,0.0
"def sum_and_divide(self, losses):     if self.total_divisor != 0:         output = torch.sum(losses) / self.total_divisor         <MASK>             # remove from autograd graph if necessary             self.total_divisor = self.total_divisor.item()         return output     return torch.sum(losses * 0)",if torch . is_tensor ( self . total_divisor ) :,if output == self . total_divisor :,28.64190457979541,28.64190457979541,0.0
"def __iter__(self):     for chunk in self.source:         if chunk is not None:             self.wait_counter = 0             yield chunk         <MASK>             self.wait_counter += 1         else:             logger.warning(                 ""Data poller has been receiving no data for {} seconds.\n""                 ""Closing data poller"".format(self.wait_cntr_max * self.poll_period)             )             break         time.sleep(self.poll_period)",elif self . wait_counter < self . wait_cntr_max :,if self . wait_cntr_max > self . wait_cntr_max :,56.35190098079901,56.35190098079901,0.0
"def test_find_directive_from_block(self):     blocks = self.config.parser_root.find_blocks(""virtualhost"")     found = False     for vh in blocks:         <MASK>             servername = vh.find_directives(""servername"")             self.assertEqual(servername[0].parameters[0], ""certbot.demo"")             found = True     self.assertTrue(found)","if vh . filepath . endswith ( ""sites-enabled/certbot.conf"" ) :","if vh . name == ""certbot"" :",10.59473715471634,10.59473715471634,0.0
"def assign_products(request, discount_id):     """"""Assign products to given property group with given property_group_id.""""""     discount = lfs_get_object_or_404(Discount, pk=discount_id)     for temp_id in request.POST.keys():         <MASK>             temp_id = temp_id.split(""-"")[1]             product = Product.objects.get(pk=temp_id)             discount.products.add(product)     html = [[""#products-inline"", products_inline(request, discount_id, as_string=True)]]     result = json.dumps(         {""html"": html, ""message"": _(u""Products have been assigned."")}, cls=LazyEncoder     )     return HttpResponse(result, content_type=""application/json"")","if temp_id . startswith ( ""product"" ) :","if temp_id . startswith ( ""-"" ) :",73.48889200874659,73.48889200874659,0.0
"def ChangeStyle(self, combos):     style = 0     for combo in combos:         if combo.GetValue() == 1:             <MASK>                 style = style | HTL.TR_VIRTUAL             else:                 try:                     style = style | eval(""wx."" + combo.GetLabel())                 except:                     style = style | eval(""HTL."" + combo.GetLabel())     if self.GetAGWWindowStyleFlag() != style:         self.SetAGWWindowStyleFlag(style)","if combo . GetLabel ( ) == ""TR_VIRTUAL"" :",if combo . GetValue ( ) == 2 :,26.331153487510537,26.331153487510537,0.0
"def _set_autocomplete(self, notebook):     if notebook:         try:             <MASK>                 notebook = NotebookInfo(notebook)             obj, x = build_notebook(notebook)             self.form.widgets[""namespace""].notebook = obj             self.form.widgets[""page""].notebook = obj             logger.debug(""Notebook for autocomplete: %s (%s)"", obj, notebook)         except:             logger.exception(""Could not set notebook: %s"", notebook)     else:         self.form.widgets[""namespace""].notebook = None         self.form.widgets[""page""].notebook = None         logger.debug(""Notebook for autocomplete unset"")","if isinstance ( notebook , str ) :",if x is None :,6.9717291216921975,6.9717291216921975,0.0
"def emitSubDomainData(self, subDomainData, event):     self.emitRawRirData(subDomainData, event)     for subDomainElem in subDomainData:         <MASK>             return None         subDomain = subDomainElem.get(""subdomain"", """").strip()         if subDomain:             self.emitHostname(subDomain, event)",if self . checkForStop ( ) :,if not subDomainElem :,8.9730240870212,8.9730240870212,0.0
"def get_all_subnets(self, subnet_ids=None, filters=None):     # Extract a list of all subnets     matches = itertools.chain(*[x.values() for x in self.subnets.values()])     if subnet_ids:         matches = [sn for sn in matches if sn.id in subnet_ids]         <MASK>             unknown_ids = set(subnet_ids) - set(matches)             raise InvalidSubnetIdError(unknown_ids)     if filters:         matches = generic_filter(filters, matches)     return matches",if len ( subnet_ids ) > len ( matches ) :,if not matches :,2.215745752614824,2.215745752614824,0.0
"def _compat_map(self, avs):     apps = {}     for av in avs:         av.version = self         app_id = av.application         <MASK>             apps[amo.APP_IDS[app_id]] = av     return apps",if app_id in amo . APP_IDS :,if app_id in amo . APP_IDS :,100.00000000000004,100.00000000000004,1.0
"def generator(self, data):     if self._config.SILENT:         silent_vars = self._get_silent_vars()     for task in data:         for var, val in task.environment_variables():             if self._config.SILENT:                 <MASK>                     continue             yield (                 0,                 [                     int(task.UniqueProcessId),                     str(task.ImageFileName),                     Address(task.Peb.ProcessParameters.Environment),                     str(var),                     str(val),                 ],             )",if var in silent_vars :,if var not in silent_vars :,59.4603557501361,59.4603557501361,0.0
"def warn_if_repeatable_read(self):     if ""mysql"" in self.current_engine().lower():         cursor = self.connection_for_read().cursor()         if cursor.execute(""SELECT @@tx_isolation""):             isolation = cursor.fetchone()[0]             <MASK>                 warnings.warn(                     TxIsolationWarning(                         ""Polling results with transaction isolation level ""                         ""repeatable-read within the same transaction ""                         ""may give outdated results. Be sure to commit the ""                         ""transaction for each poll iteration.""                     )                 )","if isolation == ""REPEATABLE-READ"" :","if isolation == ""repeatable-read"" :",59.4603557501361,59.4603557501361,0.0
"def filter_by_level(record, level_per_module):     name = record[""name""]     level = 0     if name in level_per_module:         level = level_per_module[name]     elif name is not None:         lookup = """"         <MASK>             level = level_per_module[""""]         for n in name.split("".""):             lookup += n             if lookup in level_per_module:                 level = level_per_module[lookup]             lookup += "".""     if level is False:         return False     return record[""level""].no >= level","if """" in level_per_module :","if name . startswith ( ""."" ) :",5.934202609760488,5.934202609760488,0.0
"def _readStream(self, handle: str, path: str) -> None:     eof = False     file = Path(path)     with file.open(""w"") as f:         while not eof:             response = await self._client.send(""IO.read"", {""handle"": handle})             eof = response.get(""eof"", False)             <MASK>                 f.write(response.get(""data"", """"))     await self._client.send(""IO.close"", {""handle"": handle})",if path :,if not eof :,18.99589214128981,18.99589214128981,0.0
"def sendall(self, data, flags=0):     if self._sslobj:         <MASK>             raise ValueError(                 ""non-zero flags not allowed in calls to sendall() on %s""                 % self.__class__             )         amount = len(data)         count = 0         while count < amount:             v = self.send(data[count:])             count += v         return amount     else:         return socket.sendall(self, data, flags)",if flags != 0 :,if flags == 0 :,37.99178428257963,37.99178428257963,0.0
"def run(self):     utils.assert_main_thread()     # As a convenience, we'll set up the connection     # if there isn't one. So F5 (etc) can be hit     # to get started.     if not channel:         <MASK>             SwiDebugStartChromeCommand.run(self)         else:             self.window.run_command(""swi_debug_start"")     elif paused:         logger.info(""Resuming..."")         channel.send(webkit.Debugger.resume())     else:         logger.info(""Pausing..."")         channel.send(webkit.Debugger.setSkipAllPauses(False))         channel.send(webkit.Debugger.pause())",if not chrome_launched ( ) :,if self . window . is_running :,6.27465531099474,6.27465531099474,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             length = d.getVarInt32()             tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)             d.skip(length)             self.add_presence_response().TryMerge(tmp)             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt != 0 :,37.99178428257963,37.99178428257963,0.0
"def _replace_home(x):     if xp.ON_WINDOWS:         home = (             builtins.__xonsh__.env[""HOMEDRIVE""] + builtins.__xonsh__.env[""HOMEPATH""][0]         )         if x.startswith(home):             x = x.replace(home, ""~"", 1)         <MASK>             x = x.replace(os.sep, os.altsep)         return x     else:         home = builtins.__xonsh__.env[""HOME""]         if x.startswith(home):             x = x.replace(home, ""~"", 1)         return x","if builtins . __xonsh__ . env . get ( ""FORCE_POSIX_PATHS"" ) :",if x . startswith ( os . sep ) :,3.3266284965577233,3.3266284965577233,0.0
"def semanticTags(self, semanticTags):     if semanticTags is None:         self.__semanticTags = OrderedDict()     # check     for key, value in list(semanticTags.items()):         <MASK>             raise TypeError(""At least one key is not a valid int position"")         if not isinstance(value, list):             raise TypeError(                 ""At least one value of the provided dict is not a list of string""             )         for x in value:             if not isinstance(x, str):                 raise TypeError(                     ""At least one value of the provided dict is not a list of string""                 )     self.__semanticTags = semanticTags","if not isinstance ( key , int ) :","if not isinstance ( key , int ) :",100.00000000000004,100.00000000000004,1.0
"def _recv():     try:         return sock.recv(bufsize)     except SSLWantReadError:         pass     except socket.error as exc:         error_code = extract_error_code(exc)         if error_code is None:             raise         <MASK>             raise     r, w, e = select.select((sock,), (), (), sock.gettimeout())     if r:         return sock.recv(bufsize)",if error_code != errno . EAGAIN or error_code != errno . EWOULDBLOCK :,if error_code == SSLWantReadError :,10.053796480840363,10.053796480840363,0.0
"def _authenticate(self):     oauth_token = self.options.get(""oauth_token"")     if oauth_token and not self.api.oauth_token:         self.logger.info(""Attempting to authenticate using OAuth token"")         self.api.oauth_token = oauth_token         user = self.api.user(schema=_user_schema)         <MASK>             self.logger.info(""Successfully logged in as {0}"", user)         else:             self.logger.error(                 ""Failed to authenticate, the access token "" ""is not valid""             )     else:         return JustinTVPluginBase._authenticate(self)",if user :,if user :,100.00000000000004,0.0,1.0
"def reverse(self, *args):     assert self._path is not None, ""Cannot reverse url regex "" + self.regex.pattern     assert len(args) == self._group_count, ""required number of arguments "" ""not found""     if not len(args):         return self._path     converted_args = []     for a in args:         <MASK>             a = str(a)         converted_args.append(escape.url_escape(utf8(a), plus=False))     return self._path % tuple(converted_args)","if not isinstance ( a , ( unicode_type , bytes ) ) :","if isinstance ( a , str ) :",17.81197244687178,17.81197244687178,0.0
"def determine_block_hints(self, text):     hints = """"     if text:         <MASK>             hints += str(self.best_indent)         if text[-1] not in ""\n\x85\u2028\u2029"":             hints += ""-""         elif len(text) == 1 or text[-2] in ""\n\x85\u2028\u2029"":             hints += ""+""     return hints","if text [ 0 ] in "" \n\x85\u2028\u2029"" :",if len ( text ) == 2 :,2.5795879170461364,2.5795879170461364,0.0
"def find_package_modules(package, mask):     import fnmatch     if hasattr(package, ""__loader__"") and hasattr(package.__loader__, ""_files""):         path = package.__name__.replace(""."", os.path.sep)         mask = os.path.join(path, mask)         for fnm in package.__loader__._files.iterkeys():             <MASK>                 yield os.path.splitext(fnm)[0].replace(os.path.sep, ""."")     else:         path = package.__path__[0]         for fnm in os.listdir(path):             if fnmatch.fnmatchcase(fnm, mask):                 yield ""%s.%s"" % (package.__name__, os.path.splitext(fnm)[0])","if fnmatch . fnmatchcase ( fnm , mask ) :","if fnmatch . fnmatch ( fnm , mask ) :",65.80370064762461,65.80370064762461,0.0
"def _condition(ct):     for qobj in args:         if qobj.connector == ""AND"" and not qobj.negated:             # normal kwargs are an AND anyway, so just use those for now             for child in qobj.children:                 kwargs.update(dict([child]))         else:             raise NotImplementedError(""Unsupported Q object"")     for attr, val in kwargs.items():         <MASK>             return False     return True","if getattr ( ct , attr ) != val :",if attr not in ct . attributes :,5.367626065580593,5.367626065580593,0.0
"def process(self, resources):     session = local_session(self.manager.session_factory)     client = session.client(""logs"")     state = self.data.get(""state"", True)     key = self.resolve_key(self.data.get(""kms-key""))     for r in resources:         try:             <MASK>                 client.associate_kms_key(logGroupName=r[""logGroupName""], kmsKeyId=key)             else:                 client.disassociate_kms_key(logGroupName=r[""logGroupName""])         except client.exceptions.ResourceNotFoundException:             continue",if state :,if state :,100.00000000000004,0.0,1.0
"def get_xmm(env, ii):     if is_gather(ii):         <MASK>             return gen_reg_simd_unified(env, ""xmm_evex"", True)         return gen_reg_simd_unified(env, ""xmm"", False)     if ii.space == ""evex"":         return gen_reg(env, ""xmm_evex"")     return gen_reg(env, ""xmm"")","if ii . space == ""evex"" :","if ii . space == ""evex"" :",100.00000000000004,100.00000000000004,1.0
"def parent(self):     """"""Return the parent device.""""""     if self._has_parent is None:         _parent = self._ctx.backend.get_parent(self._ctx.dev)         self._has_parent = _parent is not None         <MASK>             self._parent = Device(_parent, self._ctx.backend)         else:             self._parent = None     return self._parent",if self . _has_parent :,if _parent is not None :,13.540372457315735,13.540372457315735,0.0
"def cascade(self, event=None):     """"""Cascade all Leo windows.""""""     x, y, delta = 50, 50, 50     for frame in g.app.windowList:         w = frame and frame.top         <MASK>             r = w.geometry()  # a Qt.Rect             # 2011/10/26: Fix bug 823601: cascade-windows fails.             w.setGeometry(QtCore.QRect(x, y, r.width(), r.height()))             # Compute the new offsets.             x += 30             y += 30             if x > 200:                 x = 10 + delta                 y = 40 + delta                 delta += 10",if w :,if w :,100.00000000000004,0.0,1.0
"def _GetGoodDispatchAndUserName(IDispatch, userName, clsctx):     # Get a dispatch object, and a 'user name' (ie, the name as     # displayed to the user in repr() etc.     if userName is None:         if isinstance(IDispatch, str):             userName = IDispatch         <MASK>             # We always want the displayed name to be a real string             userName = IDispatch.encode(""ascii"", ""replace"")     elif type(userName) == unicode:         # As above - always a string...         userName = userName.encode(""ascii"", ""replace"")     else:         userName = str(userName)     return (_GetGoodDispatch(IDispatch, clsctx), userName)","elif isinstance ( IDispatch , unicode ) :",if type ( userName ) == str :,6.27465531099474,6.27465531099474,0.0
"def _infer_return_type(*args):     """"""Look at the type of all args and divine their implied return type.""""""     return_type = None     for arg in args:         if arg is None:             continue         if isinstance(arg, bytes):             if return_type is str:                 raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")             return_type = bytes         else:             <MASK>                 raise TypeError(""Can't mix bytes and non-bytes in "" ""path components."")             return_type = str     if return_type is None:         return str  # tempfile APIs return a str by default.     return return_type",if return_type is bytes :,if return_type is bytes :,100.00000000000004,100.00000000000004,1.0
"def test_ESPnetDataset_h5file_1(h5file_1):     dataset = IterableESPnetDataset(         path_name_type_list=[(h5file_1, ""data4"", ""hdf5"")],         preprocess=preprocess,     )     for key, data in dataset:         if key == ""a"":             assert data[""data4""].shape == (                 100,                 80,             )         <MASK>             assert data[""data4""].shape == (                 150,                 80,             )","if key == ""b"" :","if key == ""h"" :",59.4603557501361,59.4603557501361,0.0
"def iter_fields(node, *, include_meta=True, exclude_unset=False):     exclude_meta = not include_meta     for field_name, field in node._fields.items():         if exclude_meta and field.meta:             continue         field_val = getattr(node, field_name, _marker)         <MASK>             continue         if exclude_unset:             if callable(field.default):                 default = field.default()             else:                 default = field.default             if field_val == default:                 continue         yield field_name, field_val",if field_val is _marker :,if not field_val :,24.598127518343304,24.598127518343304,0.0
"def then(self, matches, when_response, context):     if is_iterable(when_response):         ret = []         when_response = list(when_response)         for match in when_response:             <MASK>                 if self.match_name:                     match.name = self.match_name                 matches.append(match)                 ret.append(match)         return ret     if self.match_name:         when_response.name = self.match_name     if when_response not in matches:         matches.append(when_response)         return when_response",if match not in matches :,if match . name not in matches :,41.11336169005198,41.11336169005198,0.0
"def _set_chat_ids(self, chat_id: SLT[int]) -> None:     with self.__lock:         <MASK>             raise RuntimeError(                 f""Can't set {self.chat_id_name} in conjunction with (already set) ""                 f""{self.username_name}s.""             )         self._chat_ids = self._parse_chat_id(chat_id)",if chat_id and self . _usernames :,if self . chat_id_name in self . _chat_ids :,16.451929399933114,16.451929399933114,0.0
"def discover(self, *objlist):     ret = []     for l in self.splitlines():         <MASK>             continue         if l[0] == ""Filename"":             continue         try:             int(l[2])             int(l[3])         except:             continue         #           ret.append(improve(l[0]))         ret.append(l[0])     ret.sort()     for item in objlist:         ret.append(item)     return ret",if len ( l ) < 5 :,"if l [ 0 ] == ""ID"" :",4.9323515694897075,4.9323515694897075,0.0
"def get_changed_module(self):     source = self.resource.read()     change_collector = codeanalyze.ChangeCollector(source)     if self.replacement is not None:         change_collector.add_change(self.skip_start, self.skip_end, self.replacement)     for occurrence in self.occurrence_finder.find_occurrences(self.resource):         start, end = occurrence.get_primary_range()         <MASK>             self.handle.occurred_inside_skip(change_collector, occurrence)         else:             self.handle.occurred_outside_skip(change_collector, occurrence)     result = change_collector.get_changed()     if result is not None and result != source:         return result",if self . skip_start <= start < self . skip_end :,if start > end :,2.815135668143185,2.815135668143185,0.0
"def hpat_pandas_series_var_impl(     self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None ):     if skipna is None:         skipna = True     if skipna:         valuable_length = len(self._data) - numpy.sum(numpy.isnan(self._data))         <MASK>             return numpy.nan         return (             numpy_like.nanvar(self._data) * valuable_length / (valuable_length - ddof)         )     if len(self._data) <= ddof:         return numpy.nan     return self._data.var() * len(self._data) / (len(self._data) - ddof)",if valuable_length <= ddof :,if valuable_length <= ddof :,100.00000000000004,100.00000000000004,1.0
"def to_dict(self, validate=True, ignore=(), context=None):     context = context or {}     condition = getattr(self, ""condition"", Undefined)     copy = self  # don't copy unless we need to     if condition is not Undefined:         <MASK>             pass         elif ""field"" in condition and ""type"" not in condition:             kwds = parse_shorthand(condition[""field""], context.get(""data"", None))             copy = self.copy(deep=[""condition""])             copy.condition.update(kwds)     return super(ValueChannelMixin, copy).to_dict(         validate=validate, ignore=ignore, context=context     )","if isinstance ( condition , core . SchemaBase ) :","if ""data"" in condition :",5.630400552901077,5.630400552901077,0.0
"def get_field_result(self, result, field_name):     if isinstance(result.field, models.ImageField):         <MASK>             img = getattr(result.obj, field_name)             result.text = mark_safe(                 '<a href=""%s"" target=""_blank"" title=""%s"" data-gallery=""gallery""><img src=""%s"" class=""field_img""/></a>'                 % (img.url, result.label, img.url)             )             self.include_image = True     return result",if result . value :,if not self . include_image :,7.267884212102741,7.267884212102741,0.0
"def run(self):     try:         while True:             dp = self.queue_get_stoppable(self.inq)             <MASK>                 return             # cannot ignore None here. will lead to unsynced send/recv             obj = self.func(dp)             self.queue_put_stoppable(self.outq, obj)     except Exception:         if self.stopped():             pass  # skip duplicated error messages         else:             raise     finally:         self.stop()",if self . stopped ( ) :,if dp is None :,8.51528917838043,8.51528917838043,0.0
"def _evaluate_local_single(self, iterator):     for batch in iterator:         in_arrays = convert._call_converter(self.converter, batch, self.device)         with function.no_backprop_mode():             if isinstance(in_arrays, tuple):                 results = self.calc_local(*in_arrays)             elif isinstance(in_arrays, dict):                 results = self.calc_local(**in_arrays)             else:                 results = self.calc_local(in_arrays)         <MASK>             self._progress_hook(batch)         yield results",if self . _progress_hook :,if self . progress_hook :,51.54486831107658,51.54486831107658,0.0
"def merge(self, other):     d = self._name2ft     for name, (f, t) in other._name2ft.items():         <MASK>             # Don't print here by default, since doing             #     so breaks some of the buildbots             # print ""*** DocTestRunner.merge: '"" + name + ""' in both"" \             #    "" testers; summing outcomes.""             f2, t2 = d[name]             f = f + f2             t = t + t2         d[name] = f, t",if name in d :,if name in d :,100.00000000000004,100.00000000000004,1.0
"def _addSettingsToPanels(self, category, left, right):     count = len(profile.getSubCategoriesFor(category)) + len(         profile.getSettingsForCategory(category)     )     p = left     n = 0     for title in profile.getSubCategoriesFor(category):         n += 1 + len(profile.getSettingsForCategory(category, title))         <MASK>             p = right         configBase.TitleRow(p, _(title))         for s in profile.getSettingsForCategory(category, title):             configBase.SettingRow(p, s.getName())",if n > count / 2 :,if n > count :,47.39878501170795,47.39878501170795,0.0
"def __init__(self, parent, dir, mask, with_dirs=True):     filelist = []     dirlist = [""..""]     self.dir = dir     self.file = """"     mask = mask.upper()     pattern = self.MakeRegex(mask)     for i in os.listdir(dir):         <MASK>             continue         path = os.path.join(dir, i)         if os.path.isdir(path):             dirlist.append(i)             continue         path = path.upper()         value = i.upper()         if pattern.match(value) is not None:             filelist.append(i)     self.files = filelist     if with_dirs:         self.dirs = dirlist","if i == ""."" or i == "".."" :",if i == parent :,10.148002183214462,10.148002183214462,0.0
def check_network_private(test_network):     test_net = ipaddress.IPNetwork(test_network)     test_start = test_net.network     test_end = test_net.broadcast     for network in settings.vpn.safe_priv_subnets:         network = ipaddress.IPNetwork(network)         net_start = network.network         net_end = network.broadcast         <MASK>             return True     return False,if test_start >= net_start and test_end <= net_end :,if network . network_id == test_start :,8.989384128441142,8.989384128441142,0.0
"def _end_description(self):     if self._summaryKey == ""content"":         self._end_content()     else:         value = self.popContent(""description"")         context = self._getContext()         <MASK>             context[""textinput""][""description""] = value         elif self.inimage:             context[""image""][""description""] = value     self._summaryKey = None",if self . intextinput :,if self . intextinput :,100.00000000000004,100.00000000000004,1.0
def compute_nullable_nonterminals(self):     nullable = {}     num_nullable = 0     while 1:         for p in self.grammar.Productions[1:]:             <MASK>                 nullable[p.name] = 1                 continue             for t in p.prod:                 if not t in nullable:                     break             else:                 nullable[p.name] = 1         if len(nullable) == num_nullable:             break         num_nullable = len(nullable)     return nullable,if p . len == 0 :,if p . name in nullable :,22.772101321113862,22.772101321113862,0.0
"def process_bind_param(self, value, dialect):     if value is not None:         if MAX_METADATA_VALUE_SIZE is not None:             for k, v in list(value.items()):                 sz = total_size(v)                 <MASK>                     del value[k]                     log.warning(                         ""Refusing to bind metadata key {} due to size ({})"".format(                             k, sz                         )                     )         value = json_encoder.encode(value).encode()     return value",if sz > MAX_METADATA_VALUE_SIZE :,if sz > MAX_METADATA_VALUE_SIZE :,100.00000000000004,100.00000000000004,1.0
"def process_input_line(self, line, store_history=True):     """"""process the input, capturing stdout""""""     stdout = sys.stdout     splitter = self.IP.input_splitter     try:         sys.stdout = self.cout         splitter.push(line)         more = splitter.push_accepts_more()         <MASK>             try:                 source_raw = splitter.source_raw_reset()[1]             except:                 # recent ipython #4504                 source_raw = splitter.raw_reset()             self.IP.run_cell(source_raw, store_history=store_history)     finally:         sys.stdout = stdout",if not more :,if more :,45.13864405503391,0.0,0.0
"def _dump_section(self, name, values, f):     doc = ""__doc__""     <MASK>         print(""# %s"" % values[doc], file=f)     print(""%s("" % name, file=f)     for k, v in values.items():         if k.endswith(""__doc__""):             continue         doc = k + ""__doc__""         if doc in values:             print(""    # %s"" % values[doc], file=f)         print(""    %s = %s,"" % (k, pprint.pformat(v, indent=8)), file=f)     print("")\n"", file=f)",if doc in values :,if doc in values :,100.00000000000004,100.00000000000004,1.0
"def open_session(self, app, request):     sid = request.cookies.get(app.session_cookie_name)     if sid:         stored_session = self.cls.objects(sid=sid).first()         if stored_session:             expiration = stored_session.expiration             <MASK>                 expiration = expiration.replace(tzinfo=utc)             if expiration > datetime.datetime.utcnow().replace(tzinfo=utc):                 return MongoEngineSession(                     initial=stored_session.data, sid=stored_session.sid                 )     return MongoEngineSession(sid=str(uuid.uuid4()))",if not expiration . tzinfo :,if expiration :,14.599305297825525,0.0,0.0
"def table_entry(mode1, bind_type1, mode2, bind_type2):     with sock(mode1) as sock1:         bind(sock1, bind_type1)         try:             with sock(mode2) as sock2:                 bind(sock2, bind_type2)         except OSError as exc:             <MASK>                 return ""INUSE""             elif exc.winerror == errno.WSAEACCES:                 return ""ACCESS""             raise         else:             return ""Success""",if exc . winerror == errno . WSAEADDRINUSE :,if exc . errno == errno . EINPROGRESS :,39.281465090051285,39.281465090051285,0.0
"def __init__(self, ruleset):     # Organize rules by path     self.ruleset = ruleset     self.rules = {}     for filename in self.ruleset.rules:         for rule in self.ruleset.rules[filename]:             <MASK>                 continue             manage_dictionary(self.rules, rule.path, [])             self.rules[rule.path].append(rule)",if not rule . enabled :,if not rule . path :,53.7284965911771,53.7284965911771,0.0
"def talk(self, words):     if self.writeSentence(words) == 0:         return     r = []     while 1:         i = self.readSentence()         if len(i) == 0:             continue         reply = i[0]         attrs = {}         for w in i[1:]:             j = w.find(""="", 1)             <MASK>                 attrs[w] = """"             else:                 attrs[w[:j]] = w[j + 1 :]         r.append((reply, attrs))         if reply == ""!done"":             return r",if j == - 1 :,if j == - 1 :,100.00000000000004,100.00000000000004,1.0
"def _check_decorator_overload(name: str, old: str, new: str) -> int:     """"""Conditions for a decorator to overload an existing one.""""""     properties = _property_decorators(name)     if old == new:         return _MERGE     elif old in properties and new in properties:         p_old, p_new = properties[old].precedence, properties[new].precedence         <MASK>             return _DISCARD         elif p_old == p_new:             return _MERGE         else:             return _REPLACE     raise OverloadedDecoratorError(name, """")",if p_old > p_new :,if p_old == p_new :,52.53819788848316,52.53819788848316,0.0
"def validate_pk(self):     try:         self._key = serialization.load_pem_private_key(             self.key, password=None, backend=default_backend()         )         if self._key.key_size > 2048:             AWSValidationException(                 ""The private key length is not supported. Only 1024-bit and 2048-bit are allowed.""             )     except Exception as err:         <MASK>             raise         raise AWSValidationException(             ""The private key is not PEM-encoded or is not valid.""         )","if isinstance ( err , AWSValidationException ) :","if err . errno == ""ENOENT"" :",5.522397783539471,5.522397783539471,0.0
"def _add_custom_statement(self, custom_statements):     if custom_statements is None:         return     self.resource_policy[""Version""] = ""2012-10-17""     if self.resource_policy.get(""Statement"") is None:         self.resource_policy[""Statement""] = custom_statements     else:         <MASK>             custom_statements = [custom_statements]         statement = self.resource_policy[""Statement""]         if not isinstance(statement, list):             statement = [statement]         for s in custom_statements:             if s not in statement:                 statement.append(s)         self.resource_policy[""Statement""] = statement","if not isinstance ( custom_statements , list ) :","if not isinstance ( custom_statements , list ) :",100.00000000000004,100.00000000000004,1.0
"def load(self, repn):     for key in repn:         tmp = self._convert(key)         <MASK>             self.declare(tmp)         item = dict.__getitem__(self, tmp)         item._active = True         item.load(repn[key])",if tmp not in self :,if tmp is not None :,19.304869754804482,19.304869754804482,0.0
"def on_press_release(x):     """"""Keyboard callback function.""""""     global is_recording, enable_trigger_record     press = keyboard.KeyboardEvent(""down"", 28, ""space"")     release = keyboard.KeyboardEvent(""up"", 28, ""space"")     if x.event_type == ""down"" and x.name == press.name:         if (not is_recording) and enable_trigger_record:             sys.stdout.write(""Start Recording ... "")             sys.stdout.flush()             is_recording = True     if x.event_type == ""up"" and x.name == release.name:         <MASK>             is_recording = False",if is_recording == True :,if enable_trigger_record :,7.492442692259767,7.492442692259767,0.0
"def apply_mask(self, mask, data_t, data_f):     ind_t, ind_f = 0, 0     out = []     for m in cycle(mask):         if m:             if ind_t == len(data_t):                 return out             out.append(data_t[ind_t])             ind_t += 1         else:             <MASK>                 return out             out.append(data_f[ind_f])             ind_f += 1     return out",if ind_f == len ( data_f ) :,if ind_f == len ( data_f ) :,100.00000000000004,100.00000000000004,1.0
"def oo_contains_rule(source, apiGroups, resources, verbs):     """"""Return true if the specified rule is contained within the provided source""""""     rules = source[""rules""]     if rules:         for rule in rules:             if set(rule[""apiGroups""]) == set(apiGroups):                 if set(rule[""resources""]) == set(resources):                     <MASK>                         return True     return False","if set ( rule [ ""verbs"" ] ) == set ( verbs ) :",if verbs :,0.37318062716230643,0.0,0.0
"def _maybe_commit_artifact(self, artifact_id):     artifact_status = self._artifacts[artifact_id]     if artifact_status[""pending_count""] == 0 and artifact_status[""commit_requested""]:         for callback in artifact_status[""pre_commit_callbacks""]:             callback()         <MASK>             self._api.commit_artifact(artifact_id)         for callback in artifact_status[""post_commit_callbacks""]:             callback()","if artifact_status [ ""finalize"" ] :","if artifact_status [ ""commit_requested"" ] :",53.107253497886994,53.107253497886994,0.0
"def shuffler(iterator, pool_size=10 ** 5, refill_threshold=0.9):     yields_between_refills = round(pool_size * (1 - refill_threshold))     # initialize pool; this step may or may not exhaust the iterator.     pool = take_n(pool_size, iterator)     while True:         random.shuffle(pool)         for i in range(yields_between_refills):             yield pool.pop()         next_batch = take_n(yields_between_refills, iterator)         <MASK>             break         pool.extend(next_batch)     # finish consuming whatever's left - no need for further randomization.     yield from pool",if not next_batch :,if next_batch is None :,27.77619034011791,27.77619034011791,0.0
"def __getitem__(self, key, _get_mode=False):     if not _get_mode:         if isinstance(key, (int, long)):             return self._list[key]         <MASK>             return self.__class__(self._list[key])     ikey = key.lower()     for k, v in self._list:         if k.lower() == ikey:             return v     # micro optimization: if we are in get mode we will catch that     # exception one stack level down so we can raise a standard     # key error instead of our special one.     if _get_mode:         raise KeyError()     raise BadRequestKeyError(key)","elif isinstance ( key , slice ) :","if isinstance ( key , ( str , unicode ) ) :",25.211936184349828,25.211936184349828,0.0
"def find(self, path):     if os.path.isfile(path) or os.path.islink(path):         self.num_files = self.num_files + 1         if self.match_function(path):             self.files.append(path)     elif os.path.isdir(path):         for content in os.listdir(path):             file = os.path.join(path, content)             if os.path.isfile(file) or os.path.islink(file):                 self.num_files = self.num_files + 1                 <MASK>                     self.files.append(file)             else:                 self.find(file)",if self . match_function ( file ) :,if self . match_function ( file ) :,100.00000000000004,100.00000000000004,1.0
"def validate_nb(self, nb):     super(MetadataValidatorV3, self).validate_nb(nb)     ids = set([])     for cell in nb.cells:         <MASK>             continue         grade = cell.metadata[""nbgrader""][""grade""]         solution = cell.metadata[""nbgrader""][""solution""]         locked = cell.metadata[""nbgrader""][""locked""]         if not grade and not solution and not locked:             continue         grade_id = cell.metadata[""nbgrader""][""grade_id""]         if grade_id in ids:             raise ValidationError(""Duplicate grade id: {}"".format(grade_id))         ids.add(grade_id)","if ""nbgrader"" not in cell . metadata :","if not cell . metadata [ ""nbgrader"" ] :",25.965358893403383,25.965358893403383,0.0
"def _skip_start(self):     start, stop = self.start, self.stop     for chunk in self.app_iter:         self._pos += len(chunk)         if self._pos < start:             continue         elif self._pos == start:             return b""""         else:             chunk = chunk[start - self._pos :]             <MASK>                 chunk = chunk[: stop - self._pos]                 assert len(chunk) == stop - start             return chunk     else:         raise StopIteration()",if stop is not None and self . _pos > stop :,if stop > start :,5.129511626573594,5.129511626573594,0.0
"def _SetUser(self, users):     for user in users.items():         username = user[0]         settings = user[1]         room = settings[""room""][""name""] if ""room"" in settings else None         file_ = settings[""file""] if ""file"" in settings else None         if ""event"" in settings:             if ""joined"" in settings[""event""]:                 self._client.userlist.addUser(username, room, file_)             <MASK>                 self._client.removeUser(username)         else:             self._client.userlist.modUser(username, room, file_)","elif ""left"" in settings [ ""event"" ] :","if ""joined"" in settings [ ""event"" ] :",72.92571723872932,72.92571723872932,0.0
"def run_tests():     # type: () -> None     x = 5     with switch(x) as case:         if case(0):             print(""zero"")             print(""zero"")         <MASK>             print(""one or two"")         elif case(3, 4):             print(""three or four"")         else:             print(""default"")             print(""another"")","elif case ( 1 , 2 ) :","if case ( 1 , 2 ) :",84.08964152537145,84.08964152537145,0.0
"def _populate():     for fname in glob.glob(os.path.join(os.path.dirname(__file__), ""data"", ""*.json"")):         with open(fname) as inf:             data = json.load(inf)             data = data[list(data.keys())[0]]             data = data[list(data.keys())[0]]             for item in data:                 <MASK>                     LOGGER.warning(""Repeated emoji {}"".format(item[""key""]))                 else:                     TABLE[item[""key""]] = item[""value""]","if item [ ""key"" ] in TABLE :","if item [ ""key"" ] in TABLE :",100.00000000000004,100.00000000000004,1.0
"def slot_to_material(bobject: bpy.types.Object, slot: bpy.types.MaterialSlot):     mat = slot.material     # Pick up backed material if present     if mat is not None:         baked_mat = mat.name + ""_"" + bobject.name + ""_baked""         <MASK>             mat = bpy.data.materials[baked_mat]     return mat",if baked_mat in bpy . data . materials :,if baked_mat in bpy . data . materials :,100.00000000000004,100.00000000000004,1.0
"def __keyPress(self, widget, event):     if event.key == ""G"" and event.modifiers & event.Modifiers.Control:         if not all(hasattr(p, ""isGanged"") for p in self.getPlugs()):             return False         <MASK>             self.__ungang()         else:             self.__gang()         return True     return False",if all ( p . isGanged ( ) for p in self . getPlugs ( ) ) :,if event . modifiers & event . Modifiers . Ganged :,2.8049136322597743,2.8049136322597743,0.0
"def check_expected(result, expected, contains=False):     if sys.version_info[0] >= 3:         if isinstance(result, str):             result = result.encode(""ascii"")         <MASK>             expected = expected.encode(""ascii"")     resultlines = result.splitlines()     expectedlines = expected.splitlines()     if len(resultlines) != len(expectedlines):         return False     for rline, eline in zip(resultlines, expectedlines):         if contains:             if eline not in rline:                 return False         else:             if not rline.endswith(eline):                 return False     return True","if isinstance ( expected , str ) :","if isinstance ( expected , str ) :",100.00000000000004,100.00000000000004,1.0
"def hosts_to_domains(self, hosts, exclusions=[]):     domains = []     for host in hosts:         elements = host.split(""."")         # recursively walk through the elements         # extracting all possible (sub)domains         while len(elements) >= 2:             # account for domains stored as hosts             if len(elements) == 2:                 domain = ""."".join(elements)             else:                 # drop the host element                 domain = ""."".join(elements[1:])             <MASK>                 domains.append(domain)             del elements[0]     return domains",if domain not in domains + exclusions :,if elements [ 0 ] in exclusions :,13.134549472120788,13.134549472120788,0.0
"def hsconn_sender(self):     while not self.stop_event.is_set():         try:             # Block, but timeout, so that we can exit the loop gracefully             request = self.send_queue.get(True, 6.0)             <MASK>                 # Socket got closed and set to None in another thread...                 self.socket.sendall(request)             if self.send_queue is not None:                 self.send_queue.task_done()         except queue.Empty:             pass         except OSError:             self.stop_event.set()",if self . socket is not None :,if request is None :,13.943458243384402,13.943458243384402,0.0
"def get_url_args(self, item):     if self.url_args:         <MASK>             url_args = self.url_args(item)         else:             url_args = dict(self.url_args)         url_args[""id""] = item.id         return url_args     else:         return dict(operation=self.label, id=item.id)","if hasattr ( self . url_args , ""__call__"" ) :",if self . url_args :,13.892958602871047,13.892958602871047,0.0
"def list_projects(self):     projects = []     page = 1     while True:         repos = self._client.get(             ""/user/repos"", {""sort"": ""full_name"", ""page"": page, ""per_page"": 100}         )         page += 1         for repo in repos:             projects.append(                 {                     ""id"": repo[""full_name""],                     ""name"": repo[""full_name""],                     ""description"": repo[""description""],                     ""is_private"": repo[""private""],                 }             )         <MASK>             break     return projects",if len ( repos ) < 100 :,if not repos :,7.733712583165139,7.733712583165139,0.0
"def scripts(self):     application_root = current_app.config.get(""APPLICATION_ROOT"")     subdir = application_root != ""/""     scripts = []     for script in get_registered_scripts():         if script.startswith(""http""):             scripts.append(f'<script defer src=""{script}""></script>')         <MASK>             scripts.append(f'<script defer src=""{application_root}/{script}""></script>')         else:             scripts.append(f'<script defer src=""{script}""></script>')     return markup(""\n"".join(scripts))",elif subdir :,if subdir :,55.03212081491043,0.0,0.0
"def print_map(node, l):     if node.title not in l:         l[node.title] = []     for n in node.children:         <MASK>             w = {n.title: []}             l[node.title].append(w)             print_map(n, w)         else:             l[node.title].append(n.title)",if len ( n . children ) > 0 :,if n . type == Node . ELEMENT :,9.980099403873663,9.980099403873663,0.0
"def _validate_distinct_on_different_types_and_field_orders(     self, collection, query, expected_results, get_mock_result ):     self.count = 0     self.get_mock_result = get_mock_result     query_iterable = collection.query_items(query, enable_cross_partition_query=True)     results = list(query_iterable)     for i in range(len(expected_results)):         if isinstance(results[i], dict):             self.assertDictEqual(results[i], expected_results[i])         <MASK>             self.assertListEqual(results[i], expected_results[i])         else:             self.assertEqual(results[i], expected_results[i])     self.count = 0","elif isinstance ( results [ i ] , list ) :","if isinstance ( results [ i ] , list ) :",89.31539818068698,89.31539818068698,0.0
"def run(self):     for k, v in iteritems(self.objs):         <MASK>             continue         if (             v[""_class""] == ""Question""             or v[""_class""] == ""Message""             or v[""_class""] == ""Announcement""         ):             v[""admin""] = None     return self.objs","if k . startswith ( ""_"" ) :","if k != ""admin"" :",10.816059393812111,10.816059393812111,0.0
"def qvec(self):     #        if self.polrep != 'stokes':     #            raise Exception(""qvec is not defined unless self.polrep=='stokes'"")     qvec = np.array([])     if self.polrep == ""stokes"":         qvec = self._imdict[""Q""]     elif self.polrep == ""circ"":         <MASK>             qvec = np.real(0.5 * (self.lrvec + self.rlvec))     return qvec",if len ( self . rlvec ) != 0 and len ( self . lrvec ) != 0 :,"if self . polrep == ""stokes"" :",3.6764930696373117,3.6764930696373117,0.0
"def display_value(self, key, w):     if key == ""vdevices"":         # Very special case         nids = [n[""deviceID""] for n in self.get_value(""devices"")]         for device in self.app.devices.values():             <MASK>                 b = Gtk.CheckButton(device.get_title(), False)                 b.set_tooltip_text(device[""id""])                 self[""vdevices""].pack_start(b, False, False, 0)                 b.set_active(device[""id""] in nids)         self[""vdevices""].show_all()     else:         EditorDialog.display_value(self, key, w)","if device [ ""id"" ] != self . app . daemon . get_my_id ( ) :",if device . get_active ( ) :,7.158142784142574,7.158142784142574,0.0
"def _set_xflux_setting(self, **kwargs):     for key, value in kwargs.items():         if key in self._settings_map:             <MASK>                 self._set_xflux_screen_color(value)                 self._current_color = str(value)                 # hackish - changing the current color unpauses xflux,                 # must reflect that with state change                 if self.state == self.states[""PAUSED""]:                     self.state = self.states[""RUNNING""]             else:                 self._xflux.sendline(self._settings_map[key] + str(value))             self._c()","if key == ""color"" :",if self . state == self . states [ RUNNING ] :,7.347053125977879,7.347053125977879,0.0
"def apply_acceleration(self, veh_ids, acc):     """"""See parent class.""""""     # to hand the case of a single vehicle     if type(veh_ids) == str:         veh_ids = [veh_ids]         acc = [acc]     for i, vid in enumerate(veh_ids):         <MASK>             this_vel = self.get_speed(vid)             next_vel = max([this_vel + acc[i] * self.sim_step, 0])             self.kernel_api.vehicle.slowDown(vid, next_vel, 1e-3)",if acc [ i ] is not None and vid in self . get_ids ( ) :,if i % 2 == 0 :,1.8376089065437553,1.8376089065437553,0.0
"def largest_factor_relatively_prime(a, b):     """"""Return the largest factor of a relatively prime to b.""""""     while 1:         d = gcd(a, b)         <MASK>             break         b = d         while 1:             q, r = divmod(a, d)             if r > 0:                 break             a = q     return a",if d <= 1 :,if d > 0 :,19.3576934939088,19.3576934939088,0.0
"def check_status(self):     try:         du = psutil.disk_usage(""/"")         <MASK>             raise ServiceWarning(                 ""{host} {percent}% disk usage exceeds {disk_usage}%"".format(                     host=host, percent=du.percent, disk_usage=DISK_USAGE_MAX                 )             )     except ValueError as e:         self.add_error(ServiceReturnedUnexpectedResult(""ValueError""), e)",if DISK_USAGE_MAX and du . percent >= DISK_USAGE_MAX :,if du . percent > DISK_USAGE_MAX :,38.02970594133841,38.02970594133841,0.0
"def build_reply(self, msg, text=None, private=False, threaded=False):     response = self.build_message(text)     if msg.is_group:         <MASK>             response.frm = self.bot_identifier             response.to = IRCPerson(str(msg.frm))         else:             response.frm = IRCRoomOccupant(str(self.bot_identifier), msg.frm.room)             response.to = msg.frm.room     else:         response.frm = self.bot_identifier         response.to = msg.frm     return response",if private :,if private :,100.00000000000004,0.0,1.0
"def _dict_refs(obj, named):     """"""Return key and value objects of a dict/proxy.""""""     try:         <MASK>             for k, v in _items(obj):                 s = str(k)                 yield _NamedRef(""[K] "" + s, k)                 yield _NamedRef(""[V] "" + s + "": "" + _repr(v), v)         else:             for k, v in _items(obj):                 yield k                 yield v     except (KeyError, ReferenceError, TypeError) as x:         warnings.warn(""Iterating '%s': %r"" % (_classof(obj), x))",if named :,if named :,100.00000000000004,0.0,1.0
"def fetch_images():     images = []     marker = None     while True:         batch = image_service.detail(             context,             filters=filters,             marker=marker,             sort_key=""created_at"",             sort_dir=""desc"",         )         <MASK>             break         images += batch         marker = batch[-1][""id""]     return images",if not batch :,"if batch [ 0 ] [ ""id"" ] == ""created_at"" :",2.8265205879007453,2.8265205879007453,0.0
"def compress(self, data_list):     warn_untested()     if data_list:         if data_list[1] in forms.fields.EMPTY_VALUES:             error = self.error_messages[""invalid_year""]             raise forms.ValidationError(error)         <MASK>             error = self.error_messages[""invalid_month""]             raise forms.ValidationError(error)         year = int(data_list[1])         month = int(data_list[0])         # find last day of the month         day = monthrange(year, month)[1]         return date(year, month, day)     return None",if data_list [ 0 ] in forms . fields . EMPTY_VALUES :,if data_list [ 0 ] in forms . fields . DATE_VALUES :,81.53551038173119,81.53551038173119,0.0
"def _diff_dict(self, old, new):     diff = {}     removed = []     added = []     for key, value in old.items():         if key not in new:             removed.append(key)         <MASK>             # modified is indicated by a remove and add             removed.append(key)             added.append(key)     for key, value in new.items():         if key not in old:             added.append(key)     if removed:         diff[""removed""] = sorted(removed)     if added:         diff[""added""] = sorted(added)     return diff",elif old [ key ] != new [ key ] :,if value is not None :,2.9859662827819125,2.9859662827819125,0.0
"def add_filters(self, function):     try:         subscription = self.exists(function)         <MASK>             response = self._sns.call(                 ""set_subscription_attributes"",                 SubscriptionArn=subscription[""SubscriptionArn""],                 AttributeName=""FilterPolicy"",                 AttributeValue=json.dumps(self.filters),             )             kappa.event_source.sns.LOG.debug(response)     except Exception:         kappa.event_source.sns.LOG.exception(             ""Unable to add filters for SNS topic %s"", self.arn         )",if subscription :,if subscription :,100.00000000000004,0.0,1.0
"def init_weights(self, pretrained=None):     if isinstance(pretrained, str):         logger = logging.getLogger()         load_checkpoint(self, pretrained, strict=False, logger=logger)     elif pretrained is None:         for m in self.modules():             if isinstance(m, nn.Conv2d):                 kaiming_init(m)             <MASK>                 constant_init(m, 1)     else:         raise TypeError(""pretrained must be a str or None"")","elif isinstance ( m , ( _BatchNorm , nn . GroupNorm ) ) :","if isinstance ( m , nn . Conv2d ) :",24.936514388871323,24.936514388871323,0.0
def test_is_native_login(self):     for campaign in self.campaign_lists:         native = campaigns.is_native_login(campaign)         <MASK>             assert_true(native)         else:             assert_false(native)     native = campaigns.is_proxy_login(self.invalid_campaign)     assert_true(native is None),"if campaign == ""prereg"" or campaign == ""erpc"" :",if native is not None :,2.153749340017052,2.153749340017052,0.0
"def _process_filter(self, query, host_state):     """"""Recursively parse the query structure.""""""     if not query:         return True     cmd = query[0]     method = self.commands[cmd]     cooked_args = []     for arg in query[1:]:         if isinstance(arg, list):             arg = self._process_filter(arg, host_state)         <MASK>             arg = self._parse_string(arg, host_state)         if arg is not None:             cooked_args.append(arg)     result = method(self, cooked_args)     return result","elif isinstance ( arg , basestring ) :","if isinstance ( arg , str ) :",41.11336169005198,41.11336169005198,0.0
"def find_go_files_mtime(app_files):     files, mtime = [], 0     for f, mt in app_files.items():         <MASK>             continue         if APP_CONFIG.nobuild_files.match(f):             continue         files.append(f)         mtime = max(mtime, mt)     return files, mtime","if not f . endswith ( "".go"" ) :",if not f or not mt :,12.859818292229834,12.859818292229834,0.0
"def ExcludePath(self, path):     """"""Check to see if this is a service url and matches inbound_services.""""""     skip = False     for reserved_path in self.reserved_paths.keys():         <MASK>             if (                 not self.inbound_services                 or self.reserved_paths[reserved_path] not in self.inbound_services             ):                 return (True, self.reserved_paths[reserved_path])     return (False, None)",if path . startswith ( reserved_path ) :,if reserved_path in path :,18.938334565508196,18.938334565508196,0.0
"def param_cov(self) -> DataFrame:     """"""Parameter covariance""""""     if self._param_cov is not None:         param_cov = self._param_cov     else:         params = np.asarray(self.params)         <MASK>             param_cov = self.model.compute_param_cov(params)         else:             param_cov = self.model.compute_param_cov(params, robust=False)     return DataFrame(param_cov, columns=self._names, index=self._names)","if self . cov_type == ""robust"" :",if self . model . robust :,13.597602315271134,13.597602315271134,0.0
"def test_calculate_all_attentions(module, atype):     m = importlib.import_module(module)     args = make_arg(atype=atype)     <MASK>         batch = prepare_inputs(""pytorch"")     else:         raise NotImplementedError     model = m.E2E(6, 5, args)     with chainer.no_backprop_mode():         if ""pytorch"" in module:             att_ws = model.calculate_all_attentions(*batch)[0]         else:             raise NotImplementedError         print(att_ws.shape)","if ""pytorch"" in module :","if ""pytorch"" in module :",100.00000000000004,100.00000000000004,1.0
"def __eq__(self, other):     try:         if self.type != other.type:             return False         <MASK>             return self.askAnswer == other.askAnswer         elif self.type == ""SELECT"":             return self.vars == other.vars and self.bindings == other.bindings         else:             return self.graph == other.graph     except:         return False","if self . type == ""ASK"" :","if self . type == ""ASK"" :",100.00000000000004,100.00000000000004,1.0
"def validate_memory(self, value):     for k, v in value.viewitems():         if v is None:  # use NoneType to unset a value             continue         if not re.match(PROCTYPE_MATCH, k):             raise serializers.ValidationError(""Process types can only contain [a-z]"")         <MASK>             raise serializers.ValidationError(                 ""Limit format: <number><unit>, where unit = B, K, M or G""             )     return value","if not re . match ( MEMLIMIT_MATCH , str ( v ) ) :",if not v . isdigit ( ) :,6.60902979597904,6.60902979597904,0.0
"def get_connections(data_about):     data = data_about.find(""h3"", text=""Connections"").findNext()     connections = {}     for row in data.find_all(""tr""):         key = row.find_all(""td"")[0].text         value = row.find_all(""td"")[1]         <MASK>             connections[key] = get_all_links(value)         else:             connections[key] = value.text     return connections","if ""Teams"" in key :",if key in connections :,10.126442477235686,10.126442477235686,0.0
"def _compute_map(self, first_byte, second_byte=None):     if first_byte != 0x0F:         return ""XED_ILD_MAP0""     else:         if second_byte == None:             return ""XED_ILD_MAP1""         if second_byte == 0x38:             return ""XED_ILD_MAP2""         if second_byte == 0x3A:             return ""XED_ILD_MAP3""         <MASK>             return ""XED_ILD_MAPAMD""     die(""Unhandled escape {} / map {} bytes"".format(first_byte, second_byte))",if second_byte == 0x0F and self . amd_enabled :,if first_byte == 0x3A :,18.07288326053166,18.07288326053166,0.0
"def compress(self, data_list):     if data_list:         page_id = data_list[1]         <MASK>             if not self.required:                 return None             raise forms.ValidationError(self.error_messages[""invalid_page""])         return Page.objects.get(pk=page_id)     return None",if page_id in EMPTY_VALUES :,if page_id is None :,32.66828640925501,32.66828640925501,0.0
"def find_module(self, fullname, path=None):     path = path or self.path_entry     # print('looking for ""%s"" in %s ...' % (fullname, path))     for _ext in [""js"", ""pyj"", ""py""]:         _filepath = os.path.join(self.path_entry, ""%s.%s"" % (fullname, _ext))         <MASK>             print(""module found at %s:%s"" % (_filepath, fullname))             return VFSModuleLoader(_filepath, fullname)     print(""module %s not found"" % fullname)     raise ImportError()     return None",if _filepath in VFS :,if _filepath in path :,53.7284965911771,53.7284965911771,0.0
"def __decToBin(self, myDec):     n = 0     binOfDec = """"     while myDec > 2 ** n:         n = n + 1     if (myDec < 2 ** n) & (myDec != 0):         n = n - 1     while n >= 0:         <MASK>             myDec = myDec - 2 ** n             binOfDec = binOfDec + ""1""         else:             binOfDec = binOfDec + ""0""         n = n - 1     return binOfDec",if myDec >= 2 ** n :,if myDec > 2 ** n :,61.01950432112583,61.01950432112583,0.0
"def __str__(self):     try:         <MASK>             NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value))         return NVMLError._errcode_to_string[self.value]     except NVMLError_Uninitialized:         return ""NVML Error with code %d"" % self.value",if self . value not in NVMLError . _errcode_to_string :,if self . value in NVMLError . _errcode_to_string :,80.52253761904356,80.52253761904356,0.0
"def abspath(pathdir: str) -> str:     if Path is not None and isinstance(pathdir, Path):         return pathdir.abspath()     else:         pathdir = path.abspath(pathdir)         <MASK>             try:                 pathdir = pathdir.decode(fs_encoding)             except UnicodeDecodeError as exc:                 raise UnicodeDecodeError(                     ""multibyte filename not supported on ""                     ""this filesystem encoding ""                     ""(%r)"" % fs_encoding                 ) from exc         return pathdir","if isinstance ( pathdir , bytes ) :",if fs_encoding is not None :,6.567274736060395,6.567274736060395,0.0
"def _get_vtkjs(self):     if self._vtkjs is None and self.object is not None:         <MASK>             if isfile(self.object):                 with open(self.object, ""rb"") as f:                     vtkjs = f.read()             else:                 data_url = urlopen(self.object)                 vtkjs = data_url.read()         elif hasattr(self.object, ""read""):             vtkjs = self.object.read()         self._vtkjs = vtkjs     return self._vtkjs","if isinstance ( self . object , string_types ) and self . object . endswith ( "".vtkjs"" ) :","if hasattr ( self . object , ""read"" ) :",16.493457938929108,16.493457938929108,0.0
"def _set_uid(self, val):     if val is not None:         if pwd is None:             self.bus.log(""pwd module not available; ignoring uid."", level=30)             val = None         <MASK>             val = pwd.getpwnam(val)[2]     self._uid = val","elif isinstance ( val , text_or_bytes ) :",if val :,1.7260212584862158,0.0,0.0
"def get_attached_nodes(self, external_account):     for node in self.get_nodes_with_oauth_grants(external_account):         <MASK>             continue         node_settings = node.get_addon(self.oauth_provider.short_name)         if node_settings is None:             continue         if node_settings.external_account == external_account:             yield node",if node is None :,if node is None :,100.00000000000004,100.00000000000004,1.0
"def from_obj(cls, py_obj):     if not isinstance(py_obj, Image):         raise TypeError(""py_obj must be a wandb.Image"")     else:         if hasattr(py_obj, ""_boxes"") and py_obj._boxes:             box_keys = list(py_obj._boxes.keys())         else:             box_keys = []         <MASK>             mask_keys = list(py_obj.masks.keys())         else:             mask_keys = []         return cls(box_keys, mask_keys)","if hasattr ( py_obj , ""masks"" ) and py_obj . masks :","if hasattr ( py_obj , ""_masks"" ) and py_obj . masks :",84.92326635760686,84.92326635760686,0.0
"def write(self, *bits):     for bit in bits:         if not self.bytestream:             self.bytestream.append(0)         byte = self.bytestream[self.bytenum]         if self.bitnum == 8:             <MASK>                 byte = 0                 self.bytestream += bytes([byte])             self.bytenum += 1             self.bitnum = 0         mask = 2 ** self.bitnum         if bit:             byte |= mask         else:             byte &= ~mask         self.bytestream[self.bytenum] = byte         self.bitnum += 1",if self . bytenum == len ( self . bytestream ) - 1 :,if byte :,0.6349677360219386,0.0,0.0
"def destroy(self, wipe=False):     if self.state == self.UP:         image = self.image()         <MASK>             return self.confirm_destroy(image, self.full_name, abort=False)         else:             self.warn(""tried to destroy {0} which didn't exist"".format(self.full_name))     return True",if image :,if image :,100.00000000000004,0.0,1.0
"def get_host_metadata(self):     meta = {}     if self.agent_url:         try:             resp = requests.get(                 self.agent_url + ECS_AGENT_METADATA_PATH, timeout=1             ).json()             <MASK>                 match = AGENT_VERSION_EXP.search(resp.get(""Version""))                 if match is not None and len(match.groups()) == 1:                     meta[""ecs_version""] = match.group(1)         except Exception as e:             self.log.debug(""Error getting ECS version: %s"" % str(e))     return meta","if ""Version"" in resp :",if resp . status_code == 200 :,5.522397783539471,5.522397783539471,0.0
"def _path_type(st, lst):     parts = []     if st:         <MASK>             parts.append(""file"")         elif stat.S_ISDIR(st.st_mode):             parts.append(""dir"")         else:             parts.append(""other"")     if lst:         if stat.S_ISLNK(lst.st_mode):             parts.append(""link"")     return "" "".join(parts)",if stat . S_ISREG ( st . st_mode ) :,if stat . S_ISLNK ( st . st_mode ) :,78.25422900366432,78.25422900366432,0.0
"def changed(self, action):     # Something was changed in the 'files' list     if len(action.key) >= 1 and action.key[0].lower() == ""files"":         # Refresh project files model         <MASK>             # Don't clear the existing items if only inserting new things             self.update_model(clear=False)         else:             # Clear existing items             self.update_model(clear=True)","if action . type == ""insert"" :","if action . key [ 0 ] == ""files"" :",21.97281387499715,21.97281387499715,0.0
"def process(self, resources, event=None):     client = local_session(self.manager.session_factory).client(""es"")     for r in resources:         <MASK>             result = self.manager.retry(                 client.describe_elasticsearch_domain_config,                 DomainName=r[""DomainName""],                 ignore_err_codes=(""ResourceNotFoundException"",),             )             if result:                 r[self.policy_attribute] = json.loads(                     result.get(""DomainConfig"").get(""AccessPolicies"").get(""Options"")                 )     return super().process(resources)",if self . policy_attribute not in r :,if self . policy_attribute in r :,66.90484408935988,66.90484408935988,0.0
"def line_items(self):     line_items = []     for line in self.lines_str:         line = line.split(""|"")         line = line[1:-1]  # del first and last empty item (consequence of split)         items = []         for item in line:             i = re.search(r""(\S+([ \t]+\S+)*)+"", item)             <MASK>                 items.append(i.group())             else:                 items.append("" "")         line_items.append(items)     return line_items",if i :,if i :,100.00000000000004,0.0,1.0
"def on_data(res):     if terminate.is_set():         return     if args.strings and not args.no_content:         if type(res) == tuple:             f, v = res             if type(f) == unicode:                 f = f.encode(""utf-8"")             if type(v) == unicode:                 v = v.encode(""utf-8"")             self.success(""{}: {}"".format(f, v))         <MASK>             self.success(res)     else:         self.success(res)",elif not args . content_only :,if args . strings :,12.975849993980741,12.975849993980741,0.0
"def get_servers(self, detail=True, search_opts=None):     rel_url = ""/servers/detail"" if detail else ""/servers""     if search_opts is not None:         qparams = {}         for opt, val in search_opts.iteritems():             qparams[opt] = val         <MASK>             query_string = ""?%s"" % urllib.urlencode(qparams)             rel_url += query_string     return self.api_get(rel_url)[""servers""]",if qparams :,if val :,34.66806371753173,0.0,0.0
"def run(self):     while not self.__exit__:         <MASK>             sleep(10)             continue         o = self.playlist[0]         self.playlist.remove(o)         obj = json.loads(o)         if not ""args"" in obj:             obj[""args""] = {""ua"": """", ""header"": """", ""title"": """", ""referer"": """"}         obj[""play""] = False         self.handle = launch_player(obj[""urls""], obj[""ext""], **obj[""args""])         self.handle.wait()",if len ( self . playlist ) == 0 :,if self . playlist is None :,15.685718045401451,15.685718045401451,0.0
"def get_to_download_runs_ids(session, headers):     last_date = 0     result = []     while 1:         r = session.get(RUN_DATA_API.format(last_date=last_date), headers=headers)         <MASK>             run_logs = r.json()[""data""][""records""]             result.extend([i[""logs""][0][""stats""][""id""] for i in run_logs])             last_date = r.json()[""data""][""lastTimestamp""]             since_time = datetime.utcfromtimestamp(last_date / 1000)             print(f""pares keep ids data since {since_time}"")             time.sleep(1)  # spider rule             if not last_date:                 break     return result",if r . ok :,if r . status_code == 200 :,16.784459625186194,16.784459625186194,0.0
"def __saveWork(self, work, results):     """"""Stores the resulting last log line to the cache with the proxy key""""""     del work     # pylint: disable=broad-except     try:         <MASK>             __cached = self.__cache[results[0]]             __cached[self.__TIME] = time.time()             __cached[self.__LINE] = results[1]             __cached[self.__LLU] = results[2]     except KeyError as e:         # Could happen while switching jobs with work in the queue         pass     except Exception as e:         list(map(logger.warning, cuegui.Utils.exceptionOutput(e)))",if results :,if work . id == work . id :,4.990049701936832,4.990049701936832,0.0
"def read_notes(rec):     found = []     for tag in range(500, 595):         if tag in (505, 520):             continue         fields = rec.get_fields(str(tag))         <MASK>             continue         for f in fields:             x = f.get_lower_subfields()             if x:                 found.append("" "".join(x).strip("" ""))     if found:         return ""\n\n"".join(found)",if not fields :,if not fields :,100.00000000000004,100.00000000000004,1.0
"def serialize_to(self, stream, alternate_script=None):     stream.write(self.txo_ref.tx_ref.hash)     stream.write_uint32(self.txo_ref.position)     if alternate_script is not None:         stream.write_string(alternate_script)     else:         <MASK>             stream.write_string(self.coinbase)         else:             stream.write_string(self.script.source)     stream.write_uint32(self.sequence)",if self . is_coinbase :,if self . coinbase is not None :,24.446151121745064,24.446151121745064,0.0
"def func_named(self, arg):     result = None     target = ""do_"" + arg     if target in dir(self):         result = target     else:         <MASK>  # accept shortened versions of commands             funcs = [fname for fname in self.keywords if fname.startswith(arg)]             if len(funcs) == 1:                 result = ""do_"" + funcs[0]     return result",if self . abbrev :,if arg in self . keywords :,15.619699684601283,15.619699684601283,0.0
"def static_login(self, token, *, bot):     # Necessary to get aiohttp to stop complaining about session creation     self.__session = aiohttp.ClientSession(         connector=self.connector, ws_response_class=DiscordClientWebSocketResponse     )     old_token, old_bot = self.token, self.bot_token     self._token(token, bot=bot)     try:         data = await self.request(Route(""GET"", ""/users/@me""))     except HTTPException as exc:         self._token(old_token, bot=old_bot)         <MASK>             raise LoginFailure(""Improper token has been passed."") from exc         raise     return data",if exc . response . status == 401 :,if not token :,4.238556455648295,4.238556455648295,0.0
"def render_buttons(self):     for x, button in enumerate(self.button_list):         gcolor = Gdk.color_parse(self.color_list[x])         <MASK>             fgcolor = Gdk.color_parse(""#FFFFFF"")         else:             fgcolor = Gdk.color_parse(""#000000"")         button.set_label(self.color_list[x])         button.set_sensitive(True)         button.modify_bg(Gtk.StateType.NORMAL, gcolor)         button.modify_fg(Gtk.StateType.NORMAL, fgcolor)","if util . get_hls_val ( self . color_list [ x ] , ""light"" ) < 99 :",if x == 0 :,0.45018791827775173,0.45018791827775173,0.0
"def _set_text(self, data):     lines = []     for key, value in data.items():         lines.append("""")         txt = yaml.dump({key: value}, default_flow_style=False)         title = self.titles.get(key)         <MASK>             lines.append(""# %s"" % title)         lines.append(txt.rstrip())     txt = ""\n"".join(lines) + ""\n""     txt = txt.lstrip()     self.edit.setPlainText(txt)",if title :,if title :,100.00000000000004,0.0,1.0
"def build_path(self):     for variable in re_path_template.findall(self.path):         name = variable.strip(""{}"")         <MASK>             # No 'user' parameter provided, fetch it from Auth instead.             value = self.api.auth.get_username()         else:             try:                 value = quote(self.session.params[name])             except KeyError:                 raise TweepError(                     ""No parameter value found for path variable: %s"" % name                 )             del self.session.params[name]         self.path = self.path.replace(variable, value)","if name == ""user"" and ""user"" not in self . session . params and self . api . auth :","if name == ""user"" :",10.384000820777368,10.384000820777368,0.0
"def _calculate_writes_for_built_in_indices(self, entity):     writes = 0     for prop_name in entity.keys():         <MASK>             prop_vals = entity[prop_name]             if isinstance(prop_vals, (list)):                 num_prop_vals = len(prop_vals)             else:                 num_prop_vals = 1             writes += 2 * num_prop_vals     return writes",if not prop_name in entity . unindexed_properties ( ) :,if prop_name in self . _built_in_indices :,21.142141714303076,21.142141714303076,0.0
"def create_connection(self, address, protocol_factory=None, **kw):     """"""Helper method for creating a connection to an ``address``.""""""     protocol_factory = protocol_factory or self.create_protocol     if isinstance(address, tuple):         host, port = address         <MASK>             self.logger.debug(""Create connection %s:%s"", host, port)         _, protocol = await self._loop.create_connection(             protocol_factory, host, port, **kw         )         await protocol.event(""connection_made"")     else:         raise NotImplementedError(""Could not connect to %s"" % str(address))     return protocol",if self . debug :,if self . _loop :,32.46679154750991,32.46679154750991,0.0
def _increment_bracket_num(self):     self._current_bracket -= 1     if self._current_bracket < 0:         self._current_bracket = self._get_num_brackets() - 1         self._current_iteration += 1         <MASK>             self._current_bracket = 0,if self . _current_iteration > self . hyperband_iterations :,if self . _current_iteration > self . _num_brackets ( ) :,57.04346472015739,57.04346472015739,0.0
"def get_cycle_path(self, curr_node, goal_node_index):     for dep in curr_node[""deps""]:         <MASK>             return [curr_node[""address""]]     for dep in curr_node[""deps""]:         path = self.get_cycle_path(             self.get_by_address(dep), goal_node_index         )  # self.nodelist[dep], goal_node_index)         if len(path) > 0:             path.insert(0, curr_node[""address""])             return path     return []",if dep == goal_node_index :,if len ( path ) > 0 :,5.11459870708889,5.11459870708889,0.0
"def as_dict(path="""", version=""latest"", section=""meta-data""):     result = {}     dirs = dir(path, version, section)     if not dirs:         return None     for item in dirs:         if item.endswith(""/""):             records = as_dict(path + item, version, section)             <MASK>                 result[item[:-1]] = records         elif is_dict.match(item):             idx, name = is_dict.match(item).groups()             records = as_dict(path + idx + ""/"", version, section)             if records:                 result[name] = records         else:             result[item] = valueconv(get(path + item, version, section))     return result",if records :,if records :,100.00000000000004,0.0,1.0
"def preprocess_raw_enwik9(input_filename, output_filename):     with open(input_filename, ""r"") as f1:         with open(output_filename, ""w"") as f2:             while True:                 line = f1.readline()                 if not line:                     break                 line = list(enwik9_norm_transform([line]))[0]                 if line != "" "" and line != """":                     <MASK>                         line = line[1:]                     f2.writelines(line + ""\n"")","if line [ 0 ] == "" "" :","if line [ 0 ] == ""\n"" :",67.0422683816333,67.0422683816333,0.0
"def _handle_unsubscribe(self, web_sock):     index = None     with await self._subscriber_lock:         for i, (subscriber_web_sock, _) in enumerate(self._subscribers):             <MASK>                 index = i                 break         if index is not None:             del self._subscribers[index]         if not self._subscribers:             asyncio.ensure_future(self._unregister_subscriptions())",if subscriber_web_sock == web_sock :,if subscriber_web_sock == web_sock :,100.00000000000004,100.00000000000004,1.0
"def formatmonthname(self, theyear, themonth, withyear=True):     with TimeEncoding(self.locale) as encoding:         s = month_name[themonth]         if encoding is not None:             s = s.decode(encoding)         <MASK>             s = ""%s %s"" % (s, theyear)         return '<tr><th colspan=""7"" class=""month"">%s</th></tr>' % s",if withyear :,if withyear :,100.00000000000004,0.0,1.0
"def generate_sitemaps(filename):     rows = (line.strip().split(""\t"") for line in open(filename))     for sortkey, chunk in itertools.groupby(rows, lambda row: row[0]):         things = []         _chunk = list(chunk)         for segment in _chunk:             sortkey = segment.pop(0)             last_modified = segment.pop(-1)             path = """".join(segment)             things.append(web.storage(path=path, last_modified=last_modified))         <MASK>             write(""sitemaps/sitemap_%s.xml.gz"" % sortkey, sitemap(things))",if things :,"if sortkey == ""sitemap"" :",6.567274736060395,6.567274736060395,0.0
"def use_index(     self, term: Union[str, Index], *terms: Union[str, Index] ) -> ""QueryBuilder"":     for t in (term, *terms):         <MASK>             self._use_indexes.append(t)         elif isinstance(t, str):             self._use_indexes.append(Index(t))","if isinstance ( t , Index ) :","if isinstance ( t , Index ) :",100.00000000000004,100.00000000000004,1.0
"def get_changed(self):     if self._is_expression():         result = self._get_node_text(self.ast)         <MASK>             return None         return result     else:         collector = codeanalyze.ChangeCollector(self.source)         last_end = -1         for match in self.matches:             start, end = match.get_region()             if start < last_end:                 if not self._is_expression():                     continue             last_end = end             replacement = self._get_matched_text(match)             collector.add_change(start, end, replacement)         return collector.get_changed()",if result == self . source :,if result is None :,12.975849993980741,12.975849993980741,0.0
"def quiet_f(*args):     vars = {arg_name: Real(arg) for arg_name, arg in zip(arg_names, args)}     value = dynamic_scoping(quiet_expr.evaluate, vars, evaluation)     if expect_list:         if value.has_form(""List"", None):             value = [extract_pyreal(item) for item in value.leaves]             <MASK>                 return None             return value         else:             return None     else:         value = extract_pyreal(value)         if value is None or isinf(value) or isnan(value):             return None         return value",if any ( item is None for item in value ) :,"if not value . has_form ( ""List"" ) :",7.768562846380176,7.768562846380176,0.0
"def _reemit_nested_event(self, event: Event):     source_index = self.index(event.source)     for attr in (""index"", ""new_index""):         <MASK>             src_index = ensure_tuple_index(event.index)             setattr(event, attr, (source_index,) + src_index)     if not hasattr(event, ""index""):         setattr(event, ""index"", source_index)     # reemit with this object's EventEmitter of the same type if present     # otherwise just emit with the EmitterGroup itself     getattr(self.events, event.type, self.events)(event)","if hasattr ( event , attr ) :","if hasattr ( event , attr ) :",100.00000000000004,100.00000000000004,1.0
"def check(self):     """"""Perform required checks to conclude if it's safe to operate""""""     if self.interpreter.manual is None:         <MASK>             self.error = self.process.error             self.tip = self.process.tip             return False     start = time.time()     while not self._status():         if time.time() - start >= 2:  # 2s             self.error = ""can't connect to the minserver on {}:{}"".format(                 self.interpreter.host, self.interpreter.port             )             self.tip = ""check your vagrant machine is running""             return False         time.sleep(0.1)     return True",if not self . process . healthy :,if self . process . error :,39.44243648327556,39.44243648327556,0.0
"def apply(self):     new_block = self.block.copy()     new_block.clear()     for inst in self.block.body:         <MASK>             const_assign = self._assign_const(inst)             new_block.append(const_assign)             inst = self._assign_getitem(inst, index=const_assign.target)         new_block.append(inst)     return new_block","if isinstance ( inst , Assign ) and inst . value in self . getattrs :","if isinstance ( inst , ( str , unicode ) ) :",24.70918283591955,24.70918283591955,0.0
"def _get_orientation(self):     if self.state:         rotation = [0] * 9         inclination = [0] * 9         gravity = []         geomagnetic = []         gravity = self.listener_a.values         geomagnetic = self.listener_m.values         <MASK>             ff_state = SensorManager.getRotationMatrix(                 rotation, inclination, gravity, geomagnetic             )             if ff_state:                 values = [0, 0, 0]                 values = SensorManager.getOrientation(rotation, values)             return values",if gravity [ 0 ] is not None and geomagnetic [ 0 ] is not None :,if self . state :,1.1524190727977786,1.1524190727977786,0.0
def getFirstSubGraph(graph):     if len(graph) == 0:         return None     subg = {}     todo = [graph.keys()[0]]     while len(todo) > 0:         <MASK>             subg[todo[0]] = graph[todo[0]]             todo.extend(graph[todo[0]])             del graph[todo[0]]         del todo[0]     return subg,if todo [ 0 ] in graph . keys ( ) :,if len ( subg ) > 0 :,5.008676082360365,5.008676082360365,0.0
"def decorated_function(*args, **kwargs):     rv = f(*args, **kwargs)     if ""Last-Modified"" not in rv.headers:         try:             result = date             if callable(result):                 result = result(rv)             <MASK>                 from werkzeug.http import http_date                 result = http_date(result)             if result:                 rv.headers[""Last-Modified""] = result         except Exception:             logging.getLogger(__name__).exception(                 ""Error while calculating the lastmodified value for response {!r}"".format(                     rv                 )             )     return rv","if not isinstance ( result , basestring ) :","if isinstance ( result , datetime ) :",37.70794596593207,37.70794596593207,0.0
"def set_invoice_details(self, row):     invoice_details = self.invoice_details.get(row.voucher_no, {})     if row.due_date:         invoice_details.pop(""due_date"", None)     row.update(invoice_details)     if row.voucher_type == ""Sales Invoice"":         <MASK>             self.set_delivery_notes(row)         if self.filters.show_sales_person and row.sales_team:             row.sales_person = "", "".join(row.sales_team)             del row[""sales_team""]",if self . filters . show_delivery_notes :,if self . filters . show_delivery_notes :,100.00000000000004,100.00000000000004,1.0
"def process(output):     modules = {}     for line in output:         name, size, instances, depends, state, _ = line.split("" "", 5)         instances = int(instances)         module = {             ""size"": size,             ""instances"": instances,             ""state"": state,         }         <MASK>             module[""depends""] = [value for value in depends.split("","") if value]         modules[name] = module     return modules","if depends != ""-"" :",if depends :,11.898417391331403,0.0,0.0
"def _get_host_from_zc_service_info(service_info: zeroconf.ServiceInfo):     """"""Get hostname or IP + port from zeroconf service_info.""""""     host = None     port = None     if (         service_info         and service_info.port         and (service_info.server or len(service_info.addresses) > 0)     ):         <MASK>             host = socket.inet_ntoa(service_info.addresses[0])         else:             host = service_info.server.lower()         port = service_info.port     return (host, port)",if len ( service_info . addresses ) > 0 :,if service_info . server :,22.273858658245697,22.273858658245697,0.0
"def _init_weights(self, module):     if isinstance(module, nn.Linear):         module.weight.data.normal_(mean=0.0, std=self.config.init_std)         if module.bias is not None:             module.bias.data.zero_()     elif isinstance(module, nn.Embedding):         module.weight.data.normal_(mean=0.0, std=self.config.init_std)         <MASK>             module.weight.data[module.padding_idx].zero_()",if module . padding_idx is not None :,if module . padding_idx is not None :,100.00000000000004,100.00000000000004,1.0
"def visitFromImport(self, import_stmt, import_info):     new_pairs = []     if not import_info.is_star_import():         for name, alias in import_info.names_and_aliases:             try:                 pyname = self.pymodule[alias or name]                 <MASK>                     continue             except exceptions.AttributeNotFoundError:                 pass             new_pairs.append((name, alias))     return importinfo.FromImport(import_info.module_name, import_info.level, new_pairs)","if occurrences . same_pyname ( self . pyname , pyname ) :",if pyname is None :,2.3238598963754593,2.3238598963754593,0.0
"def _apply_patches(self):     try:         s = Subprocess(             log=self.logfile, cwd=self.build_dir, verbose=self.options.verbose         )         for patch in self.patches:             <MASK>                 for ed, source in patch.items():                     s.shell(""ed - %s < %s"" % (source, ed))             else:                 s.shell(""patch -p0 < %s"" % patch)     except:         logger.error(""Failed to patch `%s`.\n%s"" % (self.build_dir, sys.exc_info()[1]))         sys.exit(1)",if type ( patch ) is dict :,if self . options . verbose :,6.770186228657864,6.770186228657864,0.0
"def __init__(self, parent, dir, mask, with_dirs=True):     filelist = []     dirlist = [""..""]     self.dir = dir     self.file = """"     mask = mask.upper()     pattern = self.MakeRegex(mask)     for i in os.listdir(dir):         if i == ""."" or i == "".."":             continue         path = os.path.join(dir, i)         <MASK>             dirlist.append(i)             continue         path = path.upper()         value = i.upper()         if pattern.match(value) is not None:             filelist.append(i)     self.files = filelist     if with_dirs:         self.dirs = dirlist",if os . path . isdir ( path ) :,if pattern . match ( path ) is not None :,16.59038701421971,16.59038701421971,0.0
"def remove_invalid_dirs(paths, bp_dir, module_name):     ret = []     for path in paths:         <MASK>             ret.append(path)         else:             logging.warning('Dir ""%s"" of module ""%s"" does not exist', path, module_name)     return ret","if os . path . isdir ( os . path . join ( bp_dir , path ) ) :",if bp_dir == module_name :,5.907598028399351,5.907598028399351,0.0
"def update_sockets(self):     inputs = self.inputs     inputs_n = ""ABabcd""     penta_sockets = pentagon_dict[self.grid_type].input_sockets     for socket in inputs_n:         if socket in penta_sockets:             <MASK>                 inputs[socket].hide_safe = False         else:             inputs[socket].hide_safe = True",if inputs [ socket ] . hide_safe :,if inputs [ socket ] . hide_safe :,100.00000000000004,100.00000000000004,1.0
"def __cut(sentence):     global emit_P     prob, pos_list = viterbi(sentence, ""BMES"", start_P, trans_P, emit_P)     begin, nexti = 0, 0     # print pos_list, sentence     for i, char in enumerate(sentence):         pos = pos_list[i]         if pos == ""B"":             begin = i         elif pos == ""E"":             yield sentence[begin : i + 1]             nexti = i + 1         <MASK>             yield char             nexti = i + 1     if nexti < len(sentence):         yield sentence[nexti:]","elif pos == ""S"" :",if nexti < len ( sentence ) :,5.522397783539471,5.522397783539471,0.0
"def validate(self):     if self.data.get(""encrypted"", True):         key = self.data.get(""target_key"")         <MASK>             raise PolicyValidationError(                 ""Encrypted snapshot copy requires kms key on %s"" % (self.manager.data,)             )     return self",if not key :,if not key :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, patch_files, patch_directories):     files = []     files_data = {}     for filename_data in patch_files:         if isinstance(filename_data, list):             filename, data = filename_data         else:             filename = filename_data             data = None         <MASK>             filename = ""{0}{1}"".format(FakeState.deploy_dir, filename)         files.append(filename)         if data:             files_data[filename] = data     self.files = files     self.files_data = files_data     self.directories = patch_directories",if not filename . startswith ( os . sep ) :,"if not isinstance ( filename , ( str , unicode ) ) :",9.669265690880861,9.669265690880861,0.0
"def validate_name_and_description(body, check_length=True):     for attribute in [""name"", ""description"", ""display_name"", ""display_description""]:         value = body.get(attribute)         if value is not None:             if isinstance(value, six.string_types):                 body[attribute] = value.strip()             <MASK>                 try:                     utils.check_string_length(                         body[attribute], attribute, min_length=0, max_length=255                     )                 except exception.InvalidInput as error:                     raise webob.exc.HTTPBadRequest(explanation=error.msg)",if check_length :,if check_length :,100.00000000000004,100.00000000000004,1.0
"def pick(items, sel):     for x, s in zip(items, sel):         if match(s):             yield x         <MASK>             yield x.restructure(x.head, pick(x.leaves, s.leaves), evaluation)",elif not x . is_atom ( ) and not s . is_atom ( ) :,"if match ( x . head , s . head ) :",6.803615611413528,6.803615611413528,0.0
"def wait_or_kill(self):     """"""Wait for the program to terminate, or kill it after 5s.""""""     if self.instance.poll() is None:         # We try one more time to kill gracefully using Ctrl-C.         logger.info(""Interrupting %s and waiting..."", self.coord)         self.instance.send_signal(signal.SIGINT)         # FIXME on py3 this becomes self.instance.wait(timeout=5)         t = monotonic_time()         while monotonic_time() - t < 5:             <MASK>                 logger.info(""Terminated %s."", self.coord)                 break             time.sleep(0.1)         else:             self.kill()",if self . instance . poll ( ) is not None :,if self . instance . is_alive ( ) :,39.23542209424606,39.23542209424606,0.0
"def sort_collection(self, models, many):     ordering = self.ordering     if not many or not ordering:         return models     for key in reversed(ordering):         reverse = key[0] == ""-""         <MASK>             key = key[1:]         models = sorted(models, key=partial(deep_getattr, key=key), reverse=reverse)     return models",if reverse :,if reverse :,100.00000000000004,0.0,1.0
"def get_palette_for_custom_classes(self, class_names, palette=None):     if self.label_map is not None:         # return subset of palette         palette = []         for old_id, new_id in sorted(self.label_map.items(), key=lambda x: x[1]):             <MASK>                 palette.append(self.PALETTE[old_id])         palette = type(self.PALETTE)(palette)     elif palette is None:         if self.PALETTE is None:             palette = np.random.randint(0, 255, size=(len(class_names), 3))         else:             palette = self.PALETTE     return palette",if new_id != - 1 :,if new_id in class_names :,31.55984539112946,31.55984539112946,0.0
"def _find_tcl_dir():     lib_dirs = [os.path.dirname(_x) for _x in sys.path if _x.lower().endswith(""lib"")]     for lib_dir in lib_dirs:         base_dir = os.path.join(lib_dir, TclLibrary.FOLDER)         <MASK>             for root, _, files in os.walk(base_dir):                 if TclLibrary.INIT_TCL in files:                     return root",if os . path . exists ( base_dir ) :,if base_dir :,11.141275535087015,11.141275535087015,0.0
"def __next__(self):     """"""Special paging functionality""""""     if self.iter is None:         self.iter = iter(self.objs)     try:         return next(self.iter)     except StopIteration:         self.iter = None         self.objs = []         <MASK>             self.page += 1             self._connection.get_response(self.action, self.params, self.page, self)             return next(self)         else:             raise",if int ( self . page ) < int ( self . total_pages ) :,if self . page < self . limit :,10.318351668184548,10.318351668184548,0.0
"def parse(cls, api, json):     lst = List(api)     setattr(lst, ""_json"", json)     for k, v in json.items():         if k == ""user"":             setattr(lst, k, User.parse(api, v))         <MASK>             setattr(lst, k, parse_datetime(v))         else:             setattr(lst, k, v)     return lst","elif k == ""created_at"" :","if k == ""datetime"" :",32.01911827891038,32.01911827891038,0.0
"def real_type(self):     # Find the real type representation by updating it as required     real_type = self.type     if self.flag_indicator:         real_type = ""#""     if self.is_vector:         <MASK>             real_type = ""Vector<{}>"".format(real_type)         else:             real_type = ""vector<{}>"".format(real_type)     if self.is_generic:         real_type = ""!{}"".format(real_type)     if self.is_flag:         real_type = ""flags.{}?{}"".format(self.flag_index, real_type)     return real_type",if self . use_vector_id :,if self . is_vector_vector :,31.02016197007,31.02016197007,0.0
"def check_fs(path):     with open(path, ""rb"") as f:         code = python_bytes_to_unicode(f.read(), errors=""replace"")         <MASK>             module = _load_module(evaluator, path, code)             module_name = sys_path.dotted_path_in_sys_path(                 evaluator.project.sys_path, path             )             if module_name is not None:                 add_module(evaluator, module_name, module)             return module",if name in code :,if code is not None :,10.682175159905853,10.682175159905853,0.0
"def infoCalendar(users):     calendarId = normalizeCalendarId(sys.argv[5], checkPrimary=True)     i = 0     count = len(users)     for user in users:         i += 1         user, cal = buildCalendarGAPIObject(user)         if not cal:             continue         result = gapi.call(             cal.calendarList(), ""get"", soft_errors=True, calendarId=calendarId         )         <MASK>             print(f""User: {user}, Calendar:{display.current_count(i, count)}"")             _showCalendar(result, 1, 1)",if result :,if result :,100.00000000000004,0.0,1.0
"def set_hidestate_input_sockets_to_cope_with_switchnum(self):     tndict = get_indices_that_should_be_visible(self.node_state)     for key, value in tndict.items():         socket = self.inputs[key]         desired_hide_state = not (value)         <MASK>             socket.hide_safe = desired_hide_state",if not socket . hide == desired_hide_state :,if not desired_hide_state :,40.95851941825763,40.95851941825763,0.0
"def get_class_name(item):     class_name, module_name = None, None     for parent in reversed(item.listchain()):         if isinstance(parent, pytest.Class):             class_name = parent.name         <MASK>             module_name = parent.module.__name__             break     # heuristic:     # - better to group gpu and task tests, since tests from those modules     #   are likely to share caching more     # - split up the rest by class name because slow tests tend to be in     #   the same module     if class_name and "".tasks."" not in module_name:         return ""{}.{}"".format(module_name, class_name)     else:         return module_name","elif isinstance ( parent , pytest . Module ) :",if parent . module :,5.171845311465849,5.171845311465849,0.0
"def run(self):     versions = versioneer.get_versions()     tempdir = tempfile.mkdtemp()     generated = os.path.join(tempdir, ""rundemo"")     with open(generated, ""wb"") as f:         for line in open(""src/rundemo-template"", ""rb""):             <MASK>                 f.write((""versions = %r\n"" % (versions,)).encode(""ascii""))             else:                 f.write(line)     self.scripts = [generated]     rc = build_scripts.run(self)     os.unlink(generated)     os.rmdir(tempdir)     return rc","if line . strip ( ) . decode ( ""ascii"" ) == ""#versions"" :",if len ( versions ) > 0 :,1.8425889581875001,1.8425889581875001,0.0
"def get_user_context(request, escape=False):     if isinstance(request, HttpRequest):         user = getattr(request, ""user"", None)         result = {""ip_address"": request.META[""REMOTE_ADDR""]}         <MASK>             result.update(                 {                     ""email"": user.email,                     ""id"": user.id,                 }             )             if user.name:                 result[""name""] = user.name     else:         result = {}     return mark_safe(json.dumps(result))",if user and user . is_authenticated ( ) :,if escape :,2.4088567143060917,0.0,0.0
"def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]:     """"""Convert tokens to spans.""""""     tokens = iter(line_tokenize())     line_no = 0     _line_start = line_start - 1     # Skip over tokens until line start     while line_no < _line_start:         _token_type, token = next(tokens)         yield (token, None)         <MASK>             line_no += 1     # Generate spans until line end     for token_type, token in tokens:         yield (token, _get_theme_style(token_type))         if token.endswith(""\n""):             line_no += 1             if line_no >= line_end:                 break","if token . endswith ( ""\n"" ) :","if token . startswith ( ""\n"" ) :",70.16879391277372,70.16879391277372,0.0
"def encode(self, encodeFun, value, defMode, maxChunkSize):     substrate, isConstructed = self.encodeValue(encodeFun, value, defMode, maxChunkSize)     tagSet = value.getTagSet()     if tagSet:         <MASK>  # primitive form implies definite mode             defMode = 1         return (             self.encodeTag(tagSet[-1], isConstructed)             + self.encodeLength(len(substrate), defMode)             + substrate             + self._encodeEndOfOctets(encodeFun, defMode)         )     else:         return substrate  # untagged value",if not isConstructed :,if not isConstructed :,100.00000000000004,100.00000000000004,1.0
def _run(self):     while True:         request = self._requests.get()         <MASK>             self.shutdown()             break         self.process(request)         self._requests.task_done(),if request is None :,if request is None :,100.00000000000004,100.00000000000004,1.0
"def _decode_payload(self, payload):     # we need to decrypt it     if payload[""enc""] == ""aes"":         try:             payload[""load""] = self.crypticle.loads(payload[""load""])         except salt.crypt.AuthenticationError:             <MASK>                 raise             payload[""load""] = self.crypticle.loads(payload[""load""])     return payload",if not self . _update_aes ( ) :,if not self . crypticle . is_valid ( ) :,31.702331385234313,31.702331385234313,0.0
"def test_row(self, row):     for idx, test in self.patterns.items():         try:             value = row[idx]         except IndexError:             value = """"         result = test(value)         if self.any_match:             <MASK>                 return not self.inverse  # True         else:             if not result:                 return self.inverse  # False     if self.any_match:         return self.inverse  # False     else:         return not self.inverse  # True",if result :,if not result :,35.35533905932737,35.35533905932737,0.0
"def setup_parameter_node(self, param_node):     if param_node.bl_idname == ""SvNumberNode"":         if self.use_prop or self.get_prop_name():             value = self.sv_get()[0][0]             print(""V"", value)             if isinstance(value, int):                 param_node.selected_mode = ""int""                 param_node.int_ = value             <MASK>                 param_node.selected_mode = ""float""                 param_node.float_ = value","elif isinstance ( value , float ) :","if isinstance ( value , float ) :",84.08964152537145,84.08964152537145,0.0
"def iter_modules(self, by_clients=False, clients_filter=None):     """"""iterate over all modules""""""     clients = None     if by_clients:         clients = self.get_clients(clients_filter)         if not clients:             return     self._refresh_modules()     for module_name in self.modules:         try:             module = self.get_module(module_name)         except PupyModuleDisabled:             continue         <MASK>             for client in clients:                 if module.is_compatible_with(client):                     yield module                     break         else:             yield module",if clients is not None :,if client :,12.753667906901528,0.0,0.0
"def filter_pricing_rule_based_on_condition(pricing_rules, doc=None):     filtered_pricing_rules = []     if doc:         for pricing_rule in pricing_rules:             if pricing_rule.condition:                 try:                     <MASK>                         filtered_pricing_rules.append(pricing_rule)                 except:                     pass             else:                 filtered_pricing_rules.append(pricing_rule)     else:         filtered_pricing_rules = pricing_rules     return filtered_pricing_rules","if frappe . safe_eval ( pricing_rule . condition , None , doc . as_dict ( ) ) :",if doc . condition == pricing_rule . condition :,14.28375357136274,14.28375357136274,0.0
"def build_query_string(kv_data, ignore_none=True):     # {""a"": 1, ""b"": ""test""} -> ""?a=1&b=test""     query_string = """"     for k, v in kv_data.iteritems():         <MASK>             continue         if query_string != """":             query_string += ""&""         else:             query_string = ""?""         query_string += k + ""="" + str(v)     return query_string",if ignore_none is True and kv_data [ k ] is None :,if ignore_none :,7.834966465489322,7.834966465489322,0.0
"def sample(self, **config):     """"""Sample a configuration from this search space.""""""     ret = {}     ret.update(self.data)     kwspaces = self.kwspaces     kwspaces.update(config)     striped_keys = [k.split(SPLITTER)[0] for k in config.keys()]     for k, v in kwspaces.items():         if k in striped_keys:             <MASK>                 sub_config = _strip_config_space(config, prefix=k)                 ret[k] = v.sample(**sub_config)             else:                 ret[k] = v     return ret","if isinstance ( v , NestedSpace ) :",if v . is_subconfig :,7.492442692259767,7.492442692259767,0.0
"def task_failed(self, task_id, hostname, reason):     logger.debug(""task %d failed with message %s"", task_id, str(reason))     if hostname in self.host_dict:         host_status = self.host_dict[hostname]         host_status.task_failed(task_id)         <MASK>             self.task_host_failed_dict[task_id] = set()         self.task_host_failed_dict[task_id].add(hostname)",if task_id not in self . task_host_failed_dict :,if host_status . task_failed ( task_id ) :,19.846438615928754,19.846438615928754,0.0
"def match(path):     for pat, _type, _property, default_title in patterns:         m = web.re_compile(""^"" + pat).match(path)         <MASK>             prefix = m.group()             extra = web.lstrips(path, prefix)             tokens = extra.split(""/"", 2)             # `extra` starts with ""/"". So first token is always empty.             middle = web.listget(tokens, 1, """")             suffix = web.listget(tokens, 2, """")             if suffix:                 suffix = ""/"" + suffix             return _type, _property, default_title, prefix, middle, suffix     return None, None, None, None, None, None",if m :,if m :,100.00000000000004,0.0,1.0
"def _get_cached_resources(self, ids):     key = self.get_cache_key(None)     if self._cache.load():         resources = self._cache.get(key)         <MASK>             self.log.debug(""Using cached results for get_resources"")             m = self.get_model()             id_set = set(ids)             return [r for r in resources if r[m.id] in id_set]     return None",if resources is not None :,if resources :,23.174952587773145,0.0,0.0
"def has_api_behaviour(self, protocol):     config = get_config()     try:         r = self.session.get(             f""{protocol}://{self.event.host}:{self.event.port}"",             timeout=config.network_timeout,         )         <MASK>             return True     except requests.exceptions.SSLError:         logger.debug(             f""{[protocol]} protocol not accepted on {self.event.host}:{self.event.port}""         )     except Exception:         logger.debug(             f""Failed probing {self.event.host}:{self.event.port}"", exc_info=True         )","if ( ""k8s"" in r . text ) or ( '""code""' in r . text and r . status_code != 200 ) :",if r . status_code == requests . codes . ok :,8.74002874613511,8.74002874613511,0.0
"def get_file_type(self, context, parent_context=None):     file_type = context.get(self.file_type_name, None)     if file_type == """":         <MASK>             file_type = parent_context.get(self.file_type_name, self.default_file_type)         else:             file_type = self.default_file_type     return file_type",if parent_context :,if parent_context :,100.00000000000004,100.00000000000004,1.0
"def selectionToChunks(self, remove=False, add=False):     box = self.selectionBox()     if box:         if box == self.level.bounds:             self.selectedChunks = set(self.level.allChunks)             return         selectedChunks = self.selectedChunks         boxedChunks = set(box.chunkPositions)         <MASK>             remove = True         if remove and not add:             selectedChunks.difference_update(boxedChunks)         else:             selectedChunks.update(boxedChunks)     self.selectionTool.selectNone()",if boxedChunks . issubset ( selectedChunks ) :,if boxedChunks . intersection ( selectedChunks ) :,50.000000000000014,50.000000000000014,0.0
"def _run_split_on_punc(self, text, never_split=None):     """"""Splits punctuation on a piece of text.""""""     if never_split is not None and text in never_split:         return [text]     chars = list(text)     i = 0     start_new_word = True     output = []     while i < len(chars):         char = chars[i]         <MASK>             output.append([char])             start_new_word = True         else:             if start_new_word:                 output.append([])             start_new_word = False             output[-1].append(char)         i += 1     return ["""".join(x) for x in output]",if _is_punctuation ( char ) :,if start_new_word :,6.979367151952678,6.979367151952678,0.0
"def _save_images(notebook):     if os.getenv(""NB_NO_IMAGES"") == ""1"":         return     logged = False     for filename, img_bytes in _iter_notebook_images(notebook):         <MASK>             log.info(""Saving images"")             logged = True         with open(filename, ""wb"") as f:             f.write(img_bytes)",if not logged :,if not logged :,100.00000000000004,100.00000000000004,1.0
"def pickPath(self, color):     self.path[color] = ()     currentPos = self.starts[color]     while True:         minDist = None         minGuide = None         for guide in self.guides[color]:             guideDist = dist(currentPos, guide)             <MASK>                 minDist = guideDist                 minGuide = guide         if dist(currentPos, self.ends[color]) == 1:             return         if minGuide == None:             return         self.path[color] = self.path[color] + (minGuide,)         currentPos = minGuide         self.guides[color].remove(minGuide)",if minDist == None or guideDist < minDist :,if guideDist == 0 :,10.480083056996865,10.480083056996865,0.0
"def _terminal_messenger(tp=""write"", msg="""", out=sys.stdout):     try:         <MASK>             out.write(msg)         elif tp == ""flush"":             out.flush()         elif tp == ""write_flush"":             out.write(msg)             out.flush()         elif tp == ""print"":             print(msg, file=out)         else:             raise ValueError(""Unsupported type: "" + tp)     except IOError as e:         logger.critical(""{}: {}"".format(type(e).__name__, ucd(e)))         pass","if tp == ""write"" :","if tp == ""write_write"" :",63.894310424627285,63.894310424627285,0.0
"def __new__(mcs, name, bases, attrs):     include_profile = include_trace = include_garbage = True     bases = list(bases)     if name == ""SaltLoggingClass"":         for base in bases:             <MASK>                 include_trace = False             if hasattr(base, ""garbage""):                 include_garbage = False     if include_profile:         bases.append(LoggingProfileMixin)     if include_trace:         bases.append(LoggingTraceMixin)     if include_garbage:         bases.append(LoggingGarbageMixin)     return super(LoggingMixinMeta, mcs).__new__(mcs, name, tuple(bases), attrs)","if hasattr ( base , ""trace"" ) :","if hasattr ( base , ""trace"" ) :",100.00000000000004,100.00000000000004,1.0
"def generatePidEncryptionTable():     table = []     for counter1 in range(0, 0x100):         value = counter1         for counter2 in range(0, 8):             <MASK>                 value = value >> 1             else:                 value = value >> 1                 value = value ^ 0xEDB88320         table.append(value)     return table",if value & 1 == 0 :,if counter2 == counter1 :,13.83254362586636,13.83254362586636,0.0
"def pytest_collection_modifyitems(items):     for item in items:         if item.nodeid.startswith(""tests/params""):             <MASK>                 item.add_marker(pytest.mark.stage(""unit""))             if ""init"" not in item.keywords:                 item.add_marker(pytest.mark.init(rng_seed=123))","if ""stage"" not in item . keywords :","if ""unit"" not in item . keywords :",70.71067811865478,70.71067811865478,0.0
"def python_value(self, value):     if value:         if isinstance(value, basestring):             pp = lambda x: x.time()             return format_date_time(value, self.formats, pp)         <MASK>             return value.time()     if value is not None and isinstance(value, datetime.timedelta):         return (datetime.datetime.min + value).time()     return value","elif isinstance ( value , datetime . datetime ) :","if isinstance ( value , datetime . datetime ) :",88.01117367933934,88.01117367933934,0.0
"def list_interesting_hosts(self):     hosts = []     targets = self.target[""other""]     for target in targets:         <MASK>             hosts.append(                 {""ip"": target.ip, ""description"": target.domain + "" / "" + target.name}             )     return hosts",if self . is_interesting ( target ) and target . status and target . status != 400 :,if target . domain :,1.0356305364183098,1.0356305364183098,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             length = d.getVarInt32()             tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)             d.skip(length)             self.mutable_cost().TryMerge(tmp)             continue         <MASK>             self.add_version(d.getVarInt64())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 24 :,if tt == 1 :,53.7284965911771,53.7284965911771,0.0
"def _wait_for_finish(self) -> PollExitResponse:     while True:         if self._backend:             poll_exit_resp = self._backend.interface.communicate_poll_exit()         logger.info(""got exit ret: %s"", poll_exit_resp)         if poll_exit_resp:             done = poll_exit_resp.done             pusher_stats = poll_exit_resp.pusher_stats             <MASK>                 self._on_finish_progress(pusher_stats, done)             if done:                 return poll_exit_resp         time.sleep(2)",if pusher_stats :,if pusher_stats :,100.00000000000004,100.00000000000004,1.0
"def listing_items(method):     marker = None     once = True     items = []     while once or items:         for i in items:             yield i         if once or marker:             <MASK>                 items = method(parms={""marker"": marker})             else:                 items = method()             if len(items) == 10000:                 marker = items[-1]             else:                 marker = None             once = False         else:             items = []",if marker :,if marker :,100.00000000000004,0.0,1.0
"def call(monad, *args):     for arg, name in izip(args, (""hour"", ""minute"", ""second"", ""microsecond"")):         if not isinstance(arg, NumericMixin) or arg.type is not int:             throw(                 TypeError,                 ""'%s' argument of time(...) function must be of 'int' type. Got: %r""                 % (name, type2str(arg.type)),             )         <MASK>             throw(NotImplementedError)     return ConstMonad.new(time(*tuple(arg.value for arg in args)))","if not isinstance ( arg , ConstMonad ) :","if not isinstance ( arg , float ) :",66.06328636027612,66.06328636027612,0.0
"def group_by_sign(seq, slop=sin(pi / 18), key=lambda x: x):     sign = None     subseq = []     for i in seq:         ki = key(i)         if sign is None:             subseq.append(i)             if ki != 0:                 sign = ki / abs(ki)         else:             subseq.append(i)             <MASK>                 sign = ki / abs(ki)                 yield subseq                 subseq = [i]     if subseq:         yield subseq",if sign * ki < - slop :,if ki != slop :,13.83254362586636,13.83254362586636,0.0
"def walk_links(self):     link_info_list = []     for item in self.content:         <MASK>             link_info = LinkInfo(link=item, name=item.name, sections=())             link_info_list.append(link_info)         else:             link_info_list.extend(item.walk_links())     return link_info_list","if isinstance ( item , Link ) :",if item . name in link_info_list :,4.9323515694897075,4.9323515694897075,0.0
"def get_subkeys(self, key):     # TODO: once we revamp the registry emulation,     # make this better     parent_path = key.get_path()     subkeys = []     for k in self.keys:         test_path = k.get_path()         if test_path.lower().startswith(parent_path.lower()):             sub = test_path[len(parent_path) :]             if sub.startswith(""\\""):                 sub = sub[1:]             end_slash = sub.find(""\\"")             <MASK>                 sub = sub[:end_slash]             if not sub:                 continue             subkeys.append(sub)     return subkeys",if end_slash >= 0 :,if end_slash > - 1 :,54.10822690539397,54.10822690539397,0.0
"def load_dict(dict_path, reverse=False):     word_dict = {}     with open(dict_path, ""rb"") as fdict:         for idx, line in enumerate(fdict):             line = cpt.to_text(line)             <MASK>                 word_dict[idx] = line.strip(""\n"")             else:                 word_dict[line.strip(""\n"")] = idx     return word_dict",if reverse :,if reverse :,100.00000000000004,0.0,1.0
"def test_network(coords, feats, model, batch_sizes, forward_only=True):     for batch_size in batch_sizes:         bcoords = batched_coordinates([coords for i in range(batch_size)])         bfeats = torch.cat([feats for i in range(batch_size)], 0)         <MASK>             with torch.no_grad():                 time, length = forward(bcoords, bfeats, model)         else:             time, length = train(bcoords, bfeats, model)         print(f""{net.__name__}\t{voxel_size}\t{batch_size}\t{length}\t{time}"")         torch.cuda.empty_cache()",if forward_only :,if forward_only :,100.00000000000004,100.00000000000004,1.0
"def markUVs(self, indices=None):     if isinstance(indices, tuple):         indices = indices[0]     ntexco = len(self.texco)     if indices is None:         self.utexc = True     else:         <MASK>             self.utexc = np.zeros(ntexco, dtype=bool)         if self.utexc is not True:             self.utexc[indices] = True",if self . utexc is False :,if indices not in self . utexc :,23.356898886410015,23.356898886410015,0.0
"def has_module(self, module, version):     has_module = False     for directory in self.directories:         module_directory = join(directory, module)         has_module_directory = isdir(module_directory)         <MASK>             has_module = has_module_directory or exists(                 module_directory             )  # could be a bare modulefile         else:             modulefile = join(module_directory, version)             has_modulefile = exists(modulefile)             has_module = has_module_directory and has_modulefile         if has_module:             break     return has_module",if not version :,if version is None :,14.058533129758727,14.058533129758727,0.0
"def get_editops(self):     if not self._editops:         <MASK>             self._editops = editops(self._opcodes, self._str1, self._str2)         else:             self._editops = editops(self._str1, self._str2)     return self._editops",if self . _opcodes :,if self . _opcodes :,100.00000000000004,100.00000000000004,1.0
"def to_representation(self, data):     value = super(CredentialTypeSerializer, self).to_representation(data)     # translate labels and help_text for credential fields ""managed by Tower""     if value.get(""managed_by_tower""):         value[""name""] = _(value[""name""])         for field in value.get(""inputs"", {}).get(""fields"", []):             field[""label""] = _(field[""label""])             <MASK>                 field[""help_text""] = _(field[""help_text""])     return value","if ""help_text"" in field :","if field . get ( ""help_text"" ) :",35.65506208559251,35.65506208559251,0.0
"def sort_nested_dictionary_lists(d):     for k, v in d.items():         if isinstance(v, list):             for i in range(0, len(v)):                 <MASK>                     v[i] = await sort_nested_dictionary_lists(v[i])                 d[k] = sorted(v)         if isinstance(v, dict):             d[k] = await sort_nested_dictionary_lists(v)     return d","if isinstance ( v [ i ] , dict ) :",if v [ i ] :,24.439253249722206,24.439253249722206,0.0
"def messageSourceStamps(self, source_stamps):     text = """"     for ss in source_stamps:         source = """"         if ss[""branch""]:             source += ""[branch %s] "" % ss[""branch""]         if ss[""revision""]:             source += str(ss[""revision""])         else:             source += ""HEAD""         <MASK>             source += "" (plus patch)""         discriminator = """"         if ss[""codebase""]:             discriminator = "" '%s'"" % ss[""codebase""]         text += ""Build Source Stamp%s: %s\n"" % (discriminator, source)     return text","if ss [ ""patch"" ] is not None :","if ss [ ""patch"" ] :",59.755798910891144,59.755798910891144,0.0
"def fit_one(self, x):     for i, xi in x.items():         <MASK>             self.median[i].update(xi)         if self.with_scaling:             self.iqr[i].update(xi)     return self",if self . with_centering :,if self . with_scaling :,64.34588841607616,64.34588841607616,0.0
"def start_response(self, status, headers, exc_info=None):     if exc_info:         try:             if self.started:                 six.reraise(exc_info[0], exc_info[1], exc_info[2])         finally:             exc_info = None     self.request.status = int(status[:3])     for key, val in headers:         <MASK>             self.request.set_content_length(int(val))         elif key.lower() == ""content-type"":             self.request.content_type = val         else:             self.request.headers_out.add(key, val)     return self.write","if key . lower ( ) == ""content-length"" :","if key . lower ( ) == ""content-length"" :",100.00000000000004,100.00000000000004,1.0
"def _osp2ec(self, bytes):     compressed = self._from_bytes(bytes)     y = compressed >> self._bits     x = compressed & (1 << self._bits) - 1     if x == 0:         y = self._curve.b     else:         result = self.sqrtp(             x ** 3 + self._curve.a * x + self._curve.b, self._curve.field.p         )         if len(result) == 1:             y = result[0]         <MASK>             y1, y2 = result             y = y1 if (y1 & 1 == y) else y2         else:             return None     return ec.Point(self._curve, x, y)",elif len ( result ) == 2 :,if y == 0 :,10.89644800332157,10.89644800332157,0.0
"def trace(self, ee, rname):     print(type(self))     self.traceIndent()     guess = """"     if self.inputState.guessing > 0:         guess = "" [guessing]""     print((ee + rname + guess))     for i in xrange(1, self.k + 1):         if i != 1:             print("", "")         <MASK>             v = self.LT(i).getText()         else:             v = ""null""         print(""LA(%s) == %s"" % (i, v))     print(""\n"")",if self . LT ( i ) :,if self . inputState . l == 1 :,16.784459625186194,16.784459625186194,0.0
"def _table_schema(self, table):     rows = self.db.execute_sql(""PRAGMA table_info('%s')"" % table).fetchall()     # Build list of fields from table information     result = {}     for _, name, data_type, not_null, _, primary_key in rows:         parts = [data_type]         if primary_key:             parts.append(""PRIMARY KEY"")         <MASK>             parts.append(""NOT NULL"")         result[name] = "" "".join(parts)     return result",if not_null :,if not_null :,100.00000000000004,100.00000000000004,1.0
"def _parse_csrf(self, response):     for d in response:         if d.startswith(""Set-Cookie:""):             for c in d.split("":"", 1)[1].split("";""):                 if c.strip().startswith(""CSRF-Token-""):                     self._CSRFtoken = c.strip("" \r\n"")                     log.verbose(""Got new cookie: %s"", self._CSRFtoken)                     break             <MASK>                 break",if self . _CSRFtoken != None :,if not self . _CSRFtoken :,34.191776499651844,34.191776499651844,0.0
"def _update_from_item(self, row, download_item):     progress_stats = download_item.progress_stats     for key in self.columns:         column = self.columns[key][0]         <MASK>             # Not the best place but we build the playlist status here             status = ""{0} {1}/{2}"".format(                 progress_stats[""status""],                 progress_stats[""playlist_index""],                 progress_stats[""playlist_size""],             )             self.SetStringItem(row, column, status)         else:             self.SetStringItem(row, column, progress_stats[key])","if key == ""status"" and progress_stats [ ""playlist_index"" ] :","if progress_stats [ key ] == ""playlist"" :",21.53927278526507,21.53927278526507,0.0
"def unmarshal_package_repositories(cls, data: Any) -> List[""PackageRepository""]:     repositories = list()     if data is not None:         <MASK>             raise RuntimeError(f""invalid package-repositories: {data!r}"")         for repository in data:             package_repo = cls.unmarshal(repository)             repositories.append(package_repo)     return repositories","if not isinstance ( data , list ) :","if not isinstance ( data , list ) :",100.00000000000004,100.00000000000004,1.0
"def remove_message(e=None):     itop = scanbox.nearest(0)     sel = scanbox.curselection()     if not sel:         dialog(             root,             ""No Message To Remove"",             ""Please select a message to remove"",             """",             0,             ""OK"",         )         return     todo = []     for i in sel:         line = scanbox.get(i)         <MASK>             todo.append(string.atoi(scanparser.group(1)))     mhf.removemessages(todo)     rescan()     fixfocus(min(todo), itop)",if scanparser . match ( line ) >= 0 :,if line :,2.7574525891364066,0.0,0.0
"def test_patches():     print(         ""Botocore version: {} aiohttp version: {}"".format(             botocore.__version__, aiohttp.__version__         )     )     success = True     for obj, digests in chain(_AIOHTTP_DIGESTS.items(), _API_DIGESTS.items()):         digest = hashlib.sha1(getsource(obj).encode(""utf-8"")).hexdigest()         <MASK>             print(                 ""Digest of {}:{} not found in: {}"".format(                     obj.__qualname__, digest, digests                 )             )             success = False     assert success",if digest not in digests :,if digest not in digests :,100.00000000000004,100.00000000000004,1.0
"def sample_admin_user():     """"""List of iris messages""""""     with iris_ctl.db_from_config(sample_db_config) as (conn, cursor):         cursor.execute(             ""SELECT `name` FROM `target` JOIN `user` on `target`.`id` = `user`.`target_id` WHERE `user`.`admin` = TRUE LIMIT 1""         )         result = cursor.fetchone()         <MASK>             return result[0]",if result :,if result :,100.00000000000004,0.0,1.0
"def _addRightnames(groups, kerning, leftname, rightnames, includeAll=True):     if leftname in kerning:         for rightname in kerning[leftname]:             <MASK>                 for rightname2 in groups[rightname]:                     rightnames.add(rightname2)                     if not includeAll:                         # TODO: in this case, pick the one rightname that has the highest                         # ranking in glyphorder                         break             else:                 rightnames.add(rightname)","if rightname [ 0 ] == ""@"" :",if rightname in groups :,7.121297464907233,7.121297464907233,0.0
"def build(self, input_shape):     if isinstance(input_shape, list) and len(input_shape) == 2:         self.data_mode = ""disjoint""         self.F = input_shape[0][-1]     else:         <MASK>             self.data_mode = ""single""         else:             self.data_mode = ""batch""         self.F = input_shape[-1]",if len ( input_shape ) == 2 :,if input_shape [ 0 ] == 1 :,19.081654556856684,19.081654556856684,0.0
"def update_ranges(l, i):     for _range in l:         # most common case: extend a range         <MASK>             _range[0] = i             merge_ranges(l)             return         elif i == _range[1] + 1:             _range[1] = i             merge_ranges(l)             return     # somewhere outside of range proximity     l.append([i, i])     l.sort(key=lambda x: x[0])",if i == _range [ 0 ] - 1 :,if i == _range [ 0 ] :,73.98067488982613,73.98067488982613,0.0
"def transform(a, cmds):     buf = a.split(""\n"")     for cmd in cmds:         ctype, line, col, char = cmd         <MASK>             if char != ""\n"":                 buf[line] = buf[line][:col] + buf[line][col + len(char) :]             else:                 buf[line] = buf[line] + buf[line + 1]                 del buf[line + 1]         elif ctype == ""I"":             buf[line] = buf[line][:col] + char + buf[line][col:]         buf = ""\n"".join(buf).split(""\n"")     return ""\n"".join(buf)","if ctype == ""D"" :","if ctype == ""I"" :",59.4603557501361,59.4603557501361,0.0
"def _media_files_drag_received(widget, context, x, y, data, info, timestamp):     uris = data.get_uris()     files = []     for uri in uris:         try:             uri_tuple = GLib.filename_from_uri(uri)         except:             continue         uri, unused = uri_tuple         if os.path.exists(uri) == True:             <MASK>                 files.append(uri)     if len(files) == 0:         return     open_dropped_files(files)",if utils . is_media_file ( uri ) == True :,if not uri_tuple :,2.5612540390806937,2.5612540390806937,0.0
"def __walk_proceed_remote_dir_act(self, r, args):     dirjs, filejs = args     j = r.json()     if ""list"" not in j:         self.pd(             ""Key 'list' not found in the response of directory listing request:\n{}"".format(                 j             )         )         return const.ERequestFailed     paths = j[""list""]     for path in paths:         <MASK>             dirjs.append(path)         else:             filejs.append(path)     return const.ENoError","if path [ ""isdir"" ] :",if path in dirjs :,12.975849993980741,12.975849993980741,0.0
"def TaskUpdatesVerbose(task, progress):     if isinstance(task.info.progress, int):         info = task.info         <MASK>             progress = ""%d%% (%s)"" % (info.progress, info.state)         print(             ""Task %s (key:%s, desc:%s) - %s""             % (info.name.info.name, info.key, info.description, progress)         )","if not isinstance ( progress , str ) :",if info . state :,5.70796903405875,5.70796903405875,0.0
"def dump_constants(header):     output = StringIO.StringIO()     output.write(header)     for attribute in dir(FSEvents):         value = getattr(FSEvents, attribute)         <MASK>             output.write(""    %s = %s\n"" % (attribute, hex(value)))     content = output.getvalue()     output.close()     return content","if attribute . startswith ( ""k"" ) and isinstance ( value , int ) :",if value :,0.37318062716230643,0.0,0.0
"def _ensure_data_is_loaded(     self,     sql_object,     input_params,     stdin_file,     stdin_filename=""-"",     stop_after_analysis=False, ):     data_loads = []     # Get each ""table name"" which is actually the file name     for filename in sql_object.qtable_names:         data_load = self._load_data(             filename,             input_params,             stdin_file=stdin_file,             stdin_filename=stdin_filename,             stop_after_analysis=stop_after_analysis,         )         <MASK>             data_loads.append(data_load)     return data_loads",if data_load is not None :,if data_load :,38.80684294761701,38.80684294761701,0.0
"def _get_instantiation(self):     if self._data is None:         f, l, c, o = c_object_p(), c_uint(), c_uint(), c_uint()         SourceLocation_loc(self, byref(f), byref(l), byref(c), byref(o))         <MASK>             f = File(f)         else:             f = None         self._data = (f, int(l.value), int(c.value), int(c.value))     return self._data",if f :,if o . value == 0 :,6.567274736060395,6.567274736060395,0.0
"def _get_all_info_lines(data):     infos = []     for row in data:         splitrow = row.split()         <MASK>             if splitrow[0] == ""INFO:"":                 infos.append("" "".join(splitrow[1:]))     return infos",if len ( splitrow ) > 0 :,if len ( splitrow ) == 2 :,46.713797772819994,46.713797772819994,0.0
"def _brush_modified_cb(self, settings):     """"""Updates the brush's base setting adjustments on brush changes""""""     for cname in settings:         adj = self.brush_adjustment.get(cname, None)         <MASK>             continue         value = self.brush.get_base_value(cname)         adj.set_value(value)",if adj is None :,if adj is None :,100.00000000000004,100.00000000000004,1.0
"def migrate_node_facts(facts):     """"""Migrate facts from various roles into node""""""     params = {         ""common"": (""dns_ip""),     }     if ""node"" not in facts:         facts[""node""] = {}     # pylint: disable=consider-iterating-dictionary     for role in params.keys():         if role in facts:             for param in params[role]:                 <MASK>                     facts[""node""][param] = facts[role].pop(param)     return facts",if param in facts [ role ] :,if params [ role ] [ param ] :,23.356898886410015,23.356898886410015,0.0
"def serialize_content_range(value):     if isinstance(value, (tuple, list)):         <MASK>             raise ValueError(                 ""When setting content_range to a list/tuple, it must ""                 ""be length 2 or 3 (not %r)"" % value             )         if len(value) == 2:             begin, end = value             length = None         else:             begin, end, length = value         value = ContentRange(begin, end, length)     value = str(value).strip()     if not value:         return None     return value","if len ( value ) not in ( 2 , 3 ) :",if len ( value ) != 3 :,31.128780276284857,31.128780276284857,0.0
"def clean(self):     data = super().clean()     if data.get(""expires""):         <MASK>             data[""expires""] = make_aware(                 datetime.combine(data[""expires""], time(hour=23, minute=59, second=59)),                 self.instance.event.timezone,             )         else:             data[""expires""] = data[""expires""].replace(hour=23, minute=59, second=59)         if data[""expires""] < now():             raise ValidationError(_(""The new expiry date needs to be in the future.""))     return data","if isinstance ( data [ ""expires"" ] , date ) :",if self . instance . event . timezone :,3.6353588668522963,3.6353588668522963,0.0
"def _build(self, obj, stream, context):     if self.include_name:         name, obj = obj         for sc in self.subcons:             <MASK>                 sc._build(obj, stream, context)                 return     else:         for sc in self.subcons:             stream2 = BytesIO()             context2 = context.__copy__()             try:                 sc._build(obj, stream2, context2)             except Exception:                 pass             else:                 context.__update__(context2)                 stream.write(stream2.getvalue())                 return     raise SelectError(""no subconstruct matched"", obj)",if sc . name == name :,if name == obj :,24.598127518343304,24.598127518343304,0.0
"def records(account_id):     """"""Fetch locks data""""""     s = boto3.Session()     table = s.resource(""dynamodb"").Table(""Sphere11.Dev.ResourceLocks"")     results = table.scan()     for r in results[""Items""]:         <MASK>             r[""LockDate""] = datetime.fromtimestamp(r[""LockDate""])         if ""RevisionDate"" in r:             r[""RevisionDate""] = datetime.fromtimestamp(r[""RevisionDate""])     print(tabulate.tabulate(results[""Items""], headers=""keys"", tablefmt=""fancy_grid""))","if ""LockDate"" in r :","if ""LockDate"" in r :",100.00000000000004,100.00000000000004,1.0
"def visitIf(self, node, scope):     for test, body in node.tests:         if isinstance(test, ast.Const):             <MASK>                 if not test.value:                     continue         self.visit(test, scope)         self.visit(body, scope)     if node.else_:         self.visit(node.else_, scope)",if type ( test . value ) in self . _const_types :,if not body :,1.2143667563059621,1.2143667563059621,0.0
"def validate_max_discount(self):     if self.rate_or_discount == ""Discount Percentage"" and self.get(""items""):         for d in self.items:             max_discount = frappe.get_cached_value(""Item"", d.item_code, ""max_discount"")             <MASK>                 throw(                     _(""Max discount allowed for item: {0} is {1}%"").format(                         self.item_code, max_discount                     )                 )",if max_discount and flt ( self . discount_percentage ) > flt ( max_discount ) :,if max_discount :,2.8823230849212025,2.8823230849212025,0.0
"def has_invalid_cce(yaml_file, product_yaml=None):     rule = yaml.open_and_macro_expand(yaml_file, product_yaml)     if ""identifiers"" in rule and rule[""identifiers""] is not None:         for i_type, i_value in rule[""identifiers""].items():             if i_type[0:3] == ""cce"":                 <MASK>                     return True     return False","if not checks . is_cce_value_valid ( ""CCE-"" + str ( i_value ) ) :","if i_value == ""cce"" :",4.922124425661365,4.922124425661365,0.0
"def parse_calendar_eras(data, calendar):     eras = data.setdefault(""eras"", {})     for width in calendar.findall(""eras/*""):         width_type = NAME_MAP[width.tag]         widths = eras.setdefault(width_type, {})         for elem in width.getiterator():             if elem.tag == ""era"":                 _import_type_text(widths, elem, type=int(elem.attrib.get(""type"")))             <MASK>                 eras[width_type] = Alias(                     _translate_alias([""eras"", width_type], elem.attrib[""path""])                 )","elif elem . tag == ""alias"" :",if width_type in COLORS :,4.278179264606695,4.278179264606695,0.0
"def validate_grammar() -> None:     for fn in _NONTERMINAL_CONVERSIONS_SEQUENCE:         fn_productions = get_productions(fn)         if all(p.name == fn_productions[0].name for p in fn_productions):             # all the production names are the same, ensure that the `convert_` function             # is named correctly             production_name = fn_productions[0].name             expected_name = f""convert_{production_name}""             <MASK>                 raise Exception(                     f""The conversion function for '{production_name}' ""                     + f""must be called '{expected_name}', not '{fn.__name__}'.""                 )",if fn . __name__ != expected_name :,if expected_name != fn . __name__ :,59.5640359271809,59.5640359271809,0.0
"def split_ratio(row):     if float(row[""Numerator""]) > 0:         <MASK>             n, m = row[""Splitratio""].split("":"")             return float(m) / float(n)         else:             return eval(row[""Splitratio""])     else:         return 1","if "":"" in row [ ""Splitratio"" ] :","if row [ ""Splitratio"" ] :",52.734307450329375,52.734307450329375,0.0
"def _handle_def_errors(testdef):     # If the test generation had an error, raise     if testdef.error:         if testdef.exception:             <MASK>                 raise testdef.exception             else:                 raise Exception(testdef.exception)         else:             raise Exception(""Test parse failure"")","if isinstance ( testdef . exception , Exception ) :","if testdef . error == ""Test parse failure"" :",8.054496384843702,8.054496384843702,0.0
"def _get_quota_availability(self):     quotas_ok = defaultdict(int)     qa = QuotaAvailability()     qa.queue(*[k for k, v in self._quota_diff.items() if v > 0])     qa.compute(now_dt=self.now_dt)     for quota, count in self._quota_diff.items():         if count <= 0:             quotas_ok[quota] = 0             break         avail = qa.results[quota]         <MASK>             quotas_ok[quota] = min(count, avail[1])         else:             quotas_ok[quota] = count     return quotas_ok",if avail [ 1 ] is not None and avail [ 1 ] < count :,if avail :,0.8267431044391091,0.0,0.0
"def reverse(self):     """"""Reverse *IN PLACE*.""""""     li = self.leftindex     lb = self.leftblock     ri = self.rightindex     rb = self.rightblock     for i in range(self.len >> 1):         lb.data[li], rb.data[ri] = rb.data[ri], lb.data[li]         li += 1         <MASK>             lb = lb.rightlink             li = 0         ri -= 1         if ri < 0:             rb = rb.leftlink             ri = BLOCKLEN - 1",if li >= BLOCKLEN :,if li < 0 :,19.3576934939088,19.3576934939088,0.0
"def __manipulate_item(self, item):     if self._Cursor__manipulate:         db = self._Cursor__collection.database         son = db._fix_outgoing(item, self._Cursor__collection)     else:         son = item     if self.__wrap is not None:         <MASK>             return getattr(self._Cursor__collection, son[self.__wrap.type_field])(son)         return self.__wrap(son, collection=self._Cursor__collection)     else:         return son",if self . __wrap . type_field in son :,if self . __wrap . type_field is not None :,69.30977286178778,69.30977286178778,0.0
"def apply_transforms(self):     """"""Apply all of the stored transforms, in priority order.""""""     self.document.reporter.attach_observer(self.document.note_transform_message)     while self.transforms:         <MASK>             # Unsorted initially, and whenever a transform is added.             self.transforms.sort()             self.transforms.reverse()             self.sorted = 1         priority, transform_class, pending, kwargs = self.transforms.pop()         transform = transform_class(self.document, startnode=pending)         transform.apply(**kwargs)         self.applied.append((priority, transform_class, pending, kwargs))",if not self . sorted :,if self . sorted :,57.89300674674101,57.89300674674101,0.0
"def format_sql(sql, params):     rv = []     if isinstance(params, dict):         # convert sql with named parameters to sql with unnamed parameters         conv = _FormatConverter(params)         <MASK>             sql = sql_to_string(sql)             sql = sql % conv             params = conv.params         else:             params = ()     for param in params or ():         if param is None:             rv.append(""NULL"")         param = safe_repr(param)         rv.append(param)     return sql, rv",if params :,if conv . params :,23.643540225079384,23.643540225079384,0.0
"def on_execution_item(self, cpath, execution):     if not isinstance(execution, dict):         return     if ""executor"" in execution and execution.get(""executor"") != ""jmeter"":         return     scenario = execution.get(""scenario"", None)     <MASK>         return     if isinstance(scenario, str):         scenario_name = scenario         scenario = self.get_named_scenario(scenario_name)         if not scenario:             scenario = None         scenario_path = Path(""scenarios"", scenario_name)     else:         scenario_path = cpath.copy()         scenario_path.add_component(""scenario"")     if scenario is not None:         self.check_jmeter_scenario(scenario_path, scenario)",if not scenario :,if not scenario :,100.00000000000004,100.00000000000004,1.0
"def _poll_ipc_requests(self) -> None:     try:         if self._ipc_requests.empty():             return         while not self._ipc_requests.empty():             args = self._ipc_requests.get()             try:                 for filename in args:                     <MASK>                         self.get_editor_notebook().show_file(filename)             except Exception as e:                 logger.exception(""Problem processing ipc request"", exc_info=e)         self.become_active_window()     finally:         self.after(50, self._poll_ipc_requests)",if os . path . isfile ( filename ) :,if self . _ipc_requests . get ( ) :,8.91376552139813,8.91376552139813,0.0
"def get_scroll_distance_to_element(driver, element):     try:         scroll_position = driver.execute_script(""return window.scrollY;"")         element_location = None         element_location = element.location[""y""]         element_location = element_location - 130         <MASK>             element_location = 0         distance = element_location - scroll_position         return distance     except Exception:         return 0",if element_location < 0 :,if scroll_position < 0 :,27.77619034011791,27.77619034011791,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             self.set_access_token(d.getPrefixedString())             continue         <MASK>             self.set_expiration_time(d.getVarInt64())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 16 :,if tt == 1 :,53.7284965911771,53.7284965911771,0.0
"def _validate_and_define(params, key, value):     (key, force_generic) = _validate_key(_unescape(key))     if key in params:         raise SyntaxError(f'duplicate key ""{key}""')     cls = _class_for_key.get(key, GenericParam)     emptiness = cls.emptiness()     if value is None:         if emptiness == Emptiness.NEVER:             raise SyntaxError(""value cannot be empty"")         value = cls.from_value(value)     else:         <MASK>             value = cls.from_wire_parser(dns.wire.Parser(_unescape(value)))         else:             value = cls.from_value(value)     params[key] = value",if force_generic :,if force_generic :,100.00000000000004,100.00000000000004,1.0
"def iter_fields(node, *, include_meta=True, exclude_unset=False):     exclude_meta = not include_meta     for field_name, field in node._fields.items():         <MASK>             continue         field_val = getattr(node, field_name, _marker)         if field_val is _marker:             continue         if exclude_unset:             if callable(field.default):                 default = field.default()             else:                 default = field.default             if field_val == default:                 continue         yield field_name, field_val",if exclude_meta and field . meta :,if not field . meta :,32.58798048281462,32.58798048281462,0.0
"def tearDown(self):     """"""Shutdown the server.""""""     try:         if self.server:             self.server.stop()         <MASK>             self.root_logger.removeHandler(self.sl_hdlr)             self.sl_hdlr.close()     finally:         BaseTest.tearDown(self)",if self . sl_hdlr :,if self . sl_hdlr :,100.00000000000004,100.00000000000004,1.0
"def _wait_for_async_copy(self, share_name, file_path):     count = 0     share_client = self.fsc.get_share_client(share_name)     file_client = share_client.get_file_client(file_path)     properties = file_client.get_file_properties()     while properties.copy.status != ""success"":         count = count + 1         <MASK>             self.fail(""Timed out waiting for async copy to complete."")         self.sleep(6)         properties = file_client.get_file_properties()     self.assertEqual(properties.copy.status, ""success"")",if count > 10 :,if count > self . timeout_count :,19.070828081828378,19.070828081828378,0.0
"def __new__(     cls,     message_type: OrderBookMessageType,     content: Dict[str, any],     timestamp: Optional[float] = None,     *args,     **kwargs, ):     if timestamp is None:         <MASK>             raise ValueError(                 ""timestamp must not be None when initializing snapshot messages.""             )         timestamp = int(time.time())     return super(KucoinOrderBookMessage, cls).__new__(         cls, message_type, content, timestamp=timestamp, *args, **kwargs     )",if message_type is OrderBookMessageType . SNAPSHOT :,if timestamp < 0 :,5.70796903405875,5.70796903405875,0.0
"def _drop_unique_features(     X: DataFrame, feature_metadata: FeatureMetadata, max_unique_ratio ) -> list:     features_to_drop = []     X_len = len(X)     max_unique_value_count = X_len * max_unique_ratio     for column in X:         unique_value_count = len(X[column].unique())         <MASK>             features_to_drop.append(column)         elif feature_metadata.get_feature_type_raw(column) in [             R_CATEGORY,             R_OBJECT,         ] and (unique_value_count > max_unique_value_count):             features_to_drop.append(column)     return features_to_drop",if unique_value_count == 1 :,if unique_value_count < max_unique_value_count :,33.64932442330152,33.64932442330152,0.0
"def get_src_findex_by_pad(s, S, padding_mode, align_corners):     if padding_mode == ""zero"":         return get_src_findex_with_zero_pad(s, S)     elif padding_mode == ""reflect"":         <MASK>             return get_src_findex_with_reflect_pad(s, S, True)         else:             sf = get_src_findex_with_reflect_pad(s, S, False)             return get_src_findex_with_repeat_pad(sf, S)     elif padding_mode == ""repeat"":         return get_src_findex_with_repeat_pad(s, S)",if align_corners :,if align_corners :,100.00000000000004,100.00000000000004,1.0
"def _iterate_self_and_parents(self, upto=None):     current = self     result = ()     while current:         result += (current,)         if current._parent is upto:             break         <MASK>             raise sa_exc.InvalidRequestError(                 ""Transaction %s is not on the active transaction list"" % (upto)             )         else:             current = current._parent     return result",elif current . _parent is None :,if not current . _active :,22.772101321113862,22.772101321113862,0.0
"def __setattr__(self, name: str, val: Any):     if name.startswith(""COMPUTED_""):         <MASK>             old_val = self[name]             if old_val == val:                 return             raise KeyError(                 ""Computed attributed '{}' already exists ""                 ""with a different value! old={}, new={}."".format(name, old_val, val)             )         self[name] = val     else:         super().__setattr__(name, val)",if name in self :,if val is None :,12.703318703865365,12.703318703865365,0.0
"def get_fnlist(bbhandler, pkg_pn, preferred):     """"""Get all recipe file names""""""     <MASK>         (latest_versions, preferred_versions) = bb.providers.findProviders(             bbhandler.config_data, bbhandler.cooker.recipecaches[""""], pkg_pn         )     fn_list = []     for pn in sorted(pkg_pn):         if preferred:             fn_list.append(preferred_versions[pn][1])         else:             fn_list.extend(pkg_pn[pn])     return fn_list",if preferred :,if latest_versions :,12.703318703865365,12.703318703865365,0.0
"def links_extracted(self, _, links):     links_deduped = {}     for link in links:         link_fingerprint = link.meta[FIELD_FINGERPRINT]         <MASK>             continue         links_deduped[link_fingerprint] = link     [         self._redis_pipeline.hmset(fingerprint, self._create_link_extracted(link))         for (fingerprint, link) in links_deduped.items()     ]     self._redis_pipeline.execute()",if link_fingerprint in links_deduped :,if not link_fingerprint :,20.82186541080652,20.82186541080652,0.0
"def __call__(self, name, rawtext, text, lineno, inliner, options=None, content=None):     options = options or {}     content = content or []     issue_nos = [each.strip() for each in utils.unescape(text).split("","")]     config = inliner.document.settings.env.app.config     ret = []     for i, issue_no in enumerate(issue_nos):         node = self.make_node(name, issue_no, config, options=options)         ret.append(node)         <MASK>             sep = nodes.raw(text="", "", format=""html"")             ret.append(sep)     return ret, []",if i != len ( issue_nos ) - 1 :,if i == 0 :,6.011598678897526,6.011598678897526,0.0
"def init_messengers(messengers):     for messenger in messengers:         <MASK>             module_path = messenger[""type""]             messenger[""type""] = messenger[""type""].split(""."")[-1]         else:             module_path = ""oncall.messengers."" + messenger[""type""]         instance = getattr(importlib.import_module(module_path), messenger[""type""])(             messenger         )         for transport in instance.supports:             _active_messengers[transport].append(instance)","if ""."" in messenger [ ""type"" ] :","if ""type"" in messenger :",22.117541221307572,22.117541221307572,0.0
"def _process_enum_definition(self, tok):     fields = []     for field in tok.fields:         <MASK>             expression = self.expression_parser.parse(field.expression)         else:             expression = None         fields.append(c_ast.CEnumField(name=field.name.first, value=expression))     name = tok.enum_name     if name:         name = ""enum %s"" % tok.enum_name.first     else:         name = self._make_anonymous_type(""enum"")     return c_ast.CTypeDefinition(         name=name,         type_definition=c_ast.CEnum(             attributes=tok.attributes, fields=fields, name=name         ),     )",if field . expression :,if field . expression :,100.00000000000004,100.00000000000004,1.0
def result_iterator():     try:         # reverse to keep finishing order         fs.reverse()         while fs:             # Careful not to keep a reference to the popped future             <MASK>                 yield fs.pop().result()             else:                 yield fs.pop().result(end_time - time.time())     finally:         for future in fs:             future.cancel(),if timeout is None :,if future . done ( ) :,7.809849842300637,7.809849842300637,0.0
"def has_encrypted_ssh_key_data(self):     try:         ssh_key_data = self.get_input(""ssh_key_data"")     except AttributeError:         return False     try:         pem_objects = validate_ssh_private_key(ssh_key_data)         for pem_object in pem_objects:             <MASK>                 return True     except ValidationError:         pass     return False","if pem_object . get ( ""key_enc"" , False ) :",if pem_object . key == self . key :,24.70918283591955,24.70918283591955,0.0
"def test_seq_object_transcription_method(self):     for nucleotide_seq in test_seqs:         <MASK>             self.assertEqual(                 repr(Seq.transcribe(nucleotide_seq)),                 repr(nucleotide_seq.transcribe()),             )","if isinstance ( nucleotide_seq , Seq . Seq ) :",if nucleotide_seq . is_seq ( ) :,18.014935004227972,18.014935004227972,0.0
"def max_elevation(self):     max_el = None     for y in xrange(self.height):         for x in xrange(self.width):             el = self.elevation[""data""][y][x]             <MASK>                 max_el = el     return max_el",if max_el is None or el > max_el :,if el is not None :,6.356491690403502,6.356491690403502,0.0
"def stress(mapping, index):     for count in range(OPERATIONS):         function = random.choice(functions)         function(mapping, index)         <MASK>             print(""\r"", len(mapping), "" "" * 7, end="""")     print()",if count % 1000 == 0 :,if count % 2 :,23.4500081062036,23.4500081062036,0.0
"def sync_terminology(self):     if self.is_source:         return     store = self.store     missing = []     for source in self.component.get_all_sources():         if ""terminology"" not in source.all_flags:             continue         try:             _unit, add = store.find_unit(source.context, source.source)         except UnitNotFound:             add = True         # Unit is already present         <MASK>             continue         missing.append((source.context, source.source, """"))     if missing:         self.add_units(None, missing)",if not add :,if not add :,100.00000000000004,100.00000000000004,1.0
"def get_generators(self):     """"""Get a dict with all registered generators, indexed by name""""""     generators = {}     for core in self.db.find():         <MASK>             _generators = core.get_generators({})             if _generators:                 generators[str(core.name)] = _generators     return generators","if hasattr ( core , ""get_generators"" ) :",if core . name in generators :,4.546632359631261,4.546632359631261,0.0
"def act(self, state):     if self.body.env.clock.frame < self.training_start_step:         return policy_util.random(state, self, self.body).cpu().squeeze().numpy()     else:         action = self.action_policy(state, self, self.body)         <MASK>             action = self.scale_action(torch.tanh(action))  # continuous action bound         return action.cpu().squeeze().numpy()",if not self . body . is_discrete :,if action . is_torch ( ) :,18.04438612975343,18.04438612975343,0.0
"def try_open_completions_event(self, event=None):     ""(./) Open completion list after pause with no movement.""     lastchar = self.text.get(""insert-1c"")     if lastchar in TRIGGERS:         args = TRY_A if lastchar == ""."" else TRY_F         self._delayed_completion_index = self.text.index(""insert"")         <MASK>             self.text.after_cancel(self._delayed_completion_id)         self._delayed_completion_id = self.text.after(             self.popupwait, self._delayed_open_completions, args         )",if self . _delayed_completion_id is not None :,if self . _delayed_completion_index != - 1 :,52.960749334062214,52.960749334062214,0.0
"def token_is_available(self):     if self.token:         try:             resp = requests.get(                 ""https://api.shodan.io/account/profile?key={0}"".format(self.token)             )             <MASK>                 return True         except Exception as ex:             logger.error(str(ex))     return False","if resp and resp . status_code == 200 and ""member"" in resp . json ( ) :",if resp . status_code == 200 :,25.020258827408636,25.020258827408636,0.0
"def next_bar_(self, event):     bars = event.bar_dict     self._current_minute = self._minutes_since_midnight(         self.ucontext.now.hour, self.ucontext.now.minute     )     for day_rule, time_rule, func in self._registry:         <MASK>             with ExecutionContext(EXECUTION_PHASE.SCHEDULED):                 with ModifyExceptionFromType(EXC_TYPE.USER_EXC):                     func(self.ucontext, bars)     self._last_minute = self._current_minute",if day_rule ( ) and time_rule ( ) :,if day_rule == time_rule :,29.10042507378281,29.10042507378281,0.0
"def decoder(s):     r = []     decode = []     for c in s:         if c == ""&"" and not decode:             decode.append(""&"")         <MASK>             if len(decode) == 1:                 r.append(""&"")             else:                 r.append(modified_unbase64("""".join(decode[1:])))             decode = []         elif decode:             decode.append(c)         else:             r.append(c)     if decode:         r.append(modified_unbase64("""".join(decode[1:])))     bin_str = """".join(r)     return (bin_str, len(s))","elif c == ""-"" and decode :","if c == ""&"" :",29.797147054518835,29.797147054518835,0.0
"def admin_audit_get(admin_id):     if settings.app.demo_mode:         resp = utils.demo_get_cache()         <MASK>             return utils.jsonify(resp)     if not flask.g.administrator.super_user:         return utils.jsonify(             {                 ""error"": REQUIRES_SUPER_USER,                 ""error_msg"": REQUIRES_SUPER_USER_MSG,             },             400,         )     admin = auth.get_by_id(admin_id)     resp = admin.get_audit_events()     if settings.app.demo_mode:         utils.demo_set_cache(resp)     return utils.jsonify(resp)",if resp :,if settings . app . demo_mode :,5.669791110976001,5.669791110976001,0.0
"def vjp(self, argnum, outgrad, ans, vs, gvs, args, kwargs):     try:         return self.vjps[argnum](outgrad, ans, vs, gvs, *args, **kwargs)     except KeyError:         <MASK>             errstr = ""Gradient of {0} not yet implemented.""         else:             errstr = ""Gradient of {0} w.r.t. arg number {1} not yet implemented.""         raise NotImplementedError(errstr.format(self.fun.__name__, argnum))",if self . vjps == { } :,if argnum == 0 :,11.708995388048026,11.708995388048026,0.0
"def update(self, *args, **kwargs):     assert not self.readonly     longest_key = 0     _dict = self._dict     reverse = self.reverse     casereverse = self.casereverse     for iterable in args + (kwargs,):         <MASK>             iterable = iterable.items()         for key, value in iterable:             longest_key = max(longest_key, len(key))             _dict[key] = value             reverse[value].append(key)             casereverse[value.lower()][value] += 1     self._longest_key = max(self._longest_key, longest_key)","if isinstance ( iterable , ( dict , StenoDictionary ) ) :",if len ( iterable ) > longest_key :,8.639795714750207,8.639795714750207,0.0
"def update_ui(self, window):     view = window.get_active_view()     self.set_status(view)     lang = ""plain_text""     if view:         buf = view.get_buffer()         language = buf.get_language()         <MASK>             lang = language.get_id()         self.setup_smart_indent(view, lang)",if language :,if language :,100.00000000000004,0.0,1.0
"def number_operators(self, a, b, skip=[]):     dict = {""a"": a, ""b"": b}     for name, expr in self.binops.items():         if name not in skip:             name = ""__%s__"" % name             <MASK>                 res = eval(expr, dict)                 self.binop_test(a, b, res, expr, name)     for name, expr in self.unops.items():         if name not in skip:             name = ""__%s__"" % name             if hasattr(a, name):                 res = eval(expr, dict)                 self.unop_test(a, res, expr, name)","if hasattr ( a , name ) :","if hasattr ( b , name ) :",50.000000000000014,50.000000000000014,0.0
"def _getItemHeight(self, item, ctrl=None):     """"""Returns the full height of the item to be inserted in the form""""""     if type(ctrl) == psychopy.visual.TextBox2:         return ctrl.size[1]     if type(ctrl) == psychopy.visual.Slider:         # Set radio button layout         <MASK>             return 0.03 + ctrl.labelHeight * 3         elif item[""layout""] == ""vert"":             # for vertical take into account the nOptions             return ctrl.labelHeight * len(item[""options""])","if item [ ""layout"" ] == ""horiz"" :","if item [ ""layout"" ] == ""horizontal"" :",79.10665071754353,79.10665071754353,0.0
"def test_cleanup_params(self, body, rpc_mock):     res = self._get_resp_post(body)     self.assertEqual(http_client.ACCEPTED, res.status_code)     rpc_mock.assert_called_once_with(self.context, mock.ANY)     cleanup_request = rpc_mock.call_args[0][1]     for key, value in body.items():         if key in (""disabled"", ""is_up""):             <MASK>                 value = value == ""true""         self.assertEqual(value, getattr(cleanup_request, key))     self.assertEqual(self._expected_services(*SERVICES), res.json)",if value is not None :,if value :,23.174952587773145,0.0,0.0
"def _read_json_content(self, body_is_optional=False):     if ""content-length"" not in self.headers:         return self.send_error(411) if not body_is_optional else {}     try:         content_length = int(self.headers.get(""content-length""))         if content_length == 0 and body_is_optional:             return {}         request = json.loads(self.rfile.read(content_length).decode(""utf-8""))         <MASK>             return request     except Exception:         logger.exception(""Bad request"")     self.send_error(400)","if isinstance ( request , dict ) and ( request or body_is_optional ) :",if request is not None :,1.5534791020152603,1.5534791020152603,0.0
"def env_purge_doc(app: Sphinx, env: BuildEnvironment, docname: str) -> None:     modules = getattr(env, ""_viewcode_modules"", {})     for modname, entry in list(modules.items()):         <MASK>             continue         code, tags, used, refname = entry         for fullname in list(used):             if used[fullname] == docname:                 used.pop(fullname)         if len(used) == 0:             modules.pop(modname)",if entry is False :,if not entry :,16.37226966703825,16.37226966703825,0.0
"def frames(self):     """"""an array of all the frames (including iframes) in the current window""""""     from thug.DOM.W3C.HTML.HTMLCollection import HTMLCollection     frames = set()     for frame in self._findAll([""frame"", ""iframe""]):         <MASK>             from thug.DOM.W3C.Core.DOMImplementation import DOMImplementation             DOMImplementation.createHTMLElement(self.window.doc, frame)         frames.add(frame._node)     return HTMLCollection(self.doc, list(frames))","if not getattr ( frame , ""_node"" , None ) :",if frame . _node :,5.380654050345609,5.380654050345609,0.0
"def check(self, **kw):     if not kw:         return exists(self.strpath)     if len(kw) == 1:         <MASK>             return not kw[""dir""] ^ isdir(self.strpath)         if ""file"" in kw:             return not kw[""file""] ^ isfile(self.strpath)     return super(LocalPath, self).check(**kw)","if ""dir"" in kw :","if ""dir"" in kw :",100.00000000000004,100.00000000000004,1.0
"def __init__(self, folders):     self.folders = folders     self.duplicates = {}     for folder, path in folders.items():         duplicates = []         for other_folder, other_path in folders.items():             <MASK>                 continue             if other_path == path:                 duplicates.append(other_folder)         if len(duplicates):             self.duplicates[folder] = duplicates",if other_folder == folder :,if folder == other_path :,28.117066259517458,28.117066259517458,0.0
"def next(self, buf, pos):     if pos >= len(buf):         return EOF, """", pos     mo = self.tokens_re.match(buf, pos)     if mo:         text = mo.group()         type, regexp, test_lit = self.tokens[mo.lastindex - 1]         pos = mo.end()         <MASK>             type = self.literals.get(text, type)         return type, text, pos     else:         c = buf[pos]         return self.symbols.get(c, None), c, pos + 1",if test_lit :,if test_lit :,100.00000000000004,100.00000000000004,1.0
"def step(self, action):     """"""Repeat action, sum reward, and max over last observations.""""""     total_reward = 0.0     done = None     for i in range(self._skip):         obs, reward, done, info = self.env.step(action)         <MASK>             self._obs_buffer[0] = obs         if i == self._skip - 1:             self._obs_buffer[1] = obs         total_reward += reward         if done:             break     # Note that the observation on the done=True frame     # doesn't matter     max_frame = self._obs_buffer.max(axis=0)     return max_frame, total_reward, done, info",if i == self . _skip - 2 :,if i == self . _skip :,71.19674182275,71.19674182275,0.0
"def convert(self, ctx, argument):     arg = argument.replace(""0x"", """").lower()     if arg[0] == ""#"":         arg = arg[1:]     try:         value = int(arg, base=16)         if not (0 <= value <= 0xFFFFFF):             raise BadColourArgument(arg)         return discord.Colour(value=value)     except ValueError:         arg = arg.replace("" "", ""_"")         method = getattr(discord.Colour, arg, None)         <MASK>             raise BadColourArgument(arg)         return method()","if arg . startswith ( ""from_"" ) or method is None or not inspect . ismethod ( method ) :",if not method :,0.1954422280040373,0.1954422280040373,0.0
"def run(self, **inputs):     if self.inputs.copy_inputs:         self.inputs.subjects_dir = os.getcwd()         <MASK>             inputs[""subjects_dir""] = self.inputs.subjects_dir         for originalfile in [self.inputs.in_file, self.inputs.in_norm]:             copy2subjdir(self, originalfile, folder=""mri"")     return super(SegmentCC, self).run(**inputs)","if ""subjects_dir"" in inputs :",if self . inputs . subjects_dir :,21.10534063187263,21.10534063187263,0.0
"def get_queryset(self):     if not hasattr(self, ""_queryset""):         <MASK>             qs = self.queryset         else:             qs = self.model._default_manager.get_queryset()         # If the queryset isn't already ordered we need to add an         # artificial ordering here to make sure that all formsets         # constructed from this queryset have the same form order.         if not qs.ordered:             qs = qs.order_by(self.model._meta.pk.name)         # Removed queryset limiting here. As per discussion re: #13023         # on django-dev, max_num should not prevent existing         # related objects/inlines from being displayed.         self._queryset = qs     return self._queryset",if self . queryset is not None :,"if self . model . _meta . pk . name in [ ""formset"" , ""formset"" ] :",6.908895196867147,6.908895196867147,0.0
"def visit_simple_stmt(self, node: Node) -> Iterator[Line]:     """"""Visit a statement without nested statements.""""""     is_suite_like = node.parent and node.parent.type in STATEMENT     if is_suite_like:         <MASK>             yield from self.visit_default(node)         else:             yield from self.line(+1)             yield from self.visit_default(node)             yield from self.line(-1)     else:         if not self.is_pyi or not node.parent or not is_stub_suite(node.parent):             yield from self.line()         yield from self.visit_default(node)",if self . is_pyi and is_stub_body ( node ) :,if self . is_pyi :,23.246837589676872,23.246837589676872,0.0
"def rawDataReceived(self, data):     if self.timeout > 0:         self.resetTimeout()     self._pendingSize -= len(data)     if self._pendingSize > 0:         self._pendingBuffer.write(data)     else:         passon = b""""         <MASK>             data, passon = data[: self._pendingSize], data[self._pendingSize :]         self._pendingBuffer.write(data)         rest = self._pendingBuffer         self._pendingBuffer = None         self._pendingSize = None         rest.seek(0, 0)         self._parts.append(rest.read())         self.setLineMode(passon.lstrip(b""\r\n""))",if self . _pendingSize < 0 :,if self . timeout > 0 :,26.647313141084275,26.647313141084275,0.0
"def handle(self, *args, **options):     app_name = options.get(""app_name"")     job_name = options.get(""job_name"")     # hack since we are using job_name nargs='?' for -l to work     if app_name and not job_name:         job_name = app_name         app_name = None     if options.get(""list_jobs""):         print_jobs(only_scheduled=False, show_when=True, show_appname=True)     else:         <MASK>             print(""Run a single maintenance job. Please specify the name of the job."")             return         self.runjob(app_name, job_name, options)",if not job_name :,"if options . get ( ""list_jobs"" ) :",4.456882760699063,4.456882760699063,0.0
"def _exportReceived(self, content, error=False, server=None, context={}, **kwargs):     if error:         <MASK>             self.error.emit(content[""message""], True)         else:             self.error.emit(""Can't export the project from the server"", True)         self.finished.emit()         return     self.finished.emit()",if content :,if server :,34.66806371753173,0.0,0.0
"def __iter__(self):     n = self.n     k = self.k     j = int(np.ceil(n / k))     for i in range(k):         test_index = np.zeros(n, dtype=bool)         <MASK>             test_index[i * j : (i + 1) * j] = True         else:             test_index[i * j :] = True         train_index = np.logical_not(test_index)         yield train_index, test_index",if i < k - 1 :,if i % 2 :,15.848738972120703,15.848738972120703,0.0
"def addType(self, graphene_type):     meta = get_meta(graphene_type)     if meta:         <MASK>             self._typeMap[meta.name] = graphene_type         else:             raise Exception(                 ""Type {typeName} already exists in the registry."".format(                     typeName=meta.name                 )             )     else:         raise Exception(""Cannot add unnamed type or a non-type to registry."")",if not graphene_type in self . _typeMap :,if meta . name in self . _typeMap :,48.61555413051454,48.61555413051454,0.0
"def test_len(self):     eq = self.assertEqual     eq(base64MIME.base64_len(""hello""), len(base64MIME.encode(""hello"", eol="""")))     for size in range(15):         if size == 0:             bsize = 0         elif size <= 3:             bsize = 4         <MASK>             bsize = 8         elif size <= 9:             bsize = 12         elif size <= 12:             bsize = 16         else:             bsize = 20         eq(base64MIME.base64_len(""x"" * size), bsize)",elif size <= 6 :,if size <= 4 :,32.46679154750991,32.46679154750991,0.0
"def _asStringList(self, sep=""""):     out = []     for item in self._toklist:         <MASK>             out.append(sep)         if isinstance(item, ParseResults):             out += item._asStringList()         else:             out.append(str(item))     return out",if out and sep :,if sep :,32.34325178227722,0.0,0.0
"def open_file_input(cli_parsed):     files = glob.glob(os.path.join(cli_parsed.d, ""*report.html""))     if len(files) > 0:         print(""\n[*] Done! Report written in the "" + cli_parsed.d + "" folder!"")         print(""Would you like to open the report now? [Y/n]"")         while True:             try:                 response = input().lower()                 <MASK>                     return True                 else:                     return strtobool(response)             except ValueError:                 print(""Please respond with y or n"")     else:         print(""[*] No report files found to open, perhaps no hosts were successful"")         return False","if response == """" :","if response == ""y"" :",59.4603557501361,59.4603557501361,0.0
"def init_values(self):     config = self._raw_config     for valname, value in self.overrides.iteritems():         <MASK>             realvalname, key = valname.split(""."", 1)             config.setdefault(realvalname, {})[key] = value         else:             config[valname] = value     for name in config:         if name in self.values:             self.__dict__[name] = config[name]     del self._raw_config","if ""."" in valname :","if valname . startswith ( ""."" ) :",18.575057999133595,18.575057999133595,0.0
"def get_result(self):     result_list = []     exc_info = None     for f in self.children:         try:             result_list.append(f.get_result())         except Exception as e:             <MASK>                 exc_info = sys.exc_info()             else:                 if not isinstance(e, self.quiet_exceptions):                     app_log.error(""Multiple exceptions in yield list"", exc_info=True)     if exc_info is not None:         raise_exc_info(exc_info)     if self.keys is not None:         return dict(zip(self.keys, result_list))     else:         return list(result_list)",if exc_info is None :,if e is not None :,16.341219448835542,16.341219448835542,0.0
"def test01e_json(self):     ""Testing GeoJSON input/output.""     if not GEOJSON:         return     for g in self.geometries.json_geoms:         geom = OGRGeometry(g.wkt)         <MASK>             self.assertEqual(g.json, geom.json)             self.assertEqual(g.json, geom.geojson)         self.assertEqual(OGRGeometry(g.wkt), OGRGeometry(geom.json))","if not hasattr ( g , ""not_equal"" ) :",if geom . geojson :,2.564755813286796,2.564755813286796,0.0
"def __init__(self, hub=None):  # pylint: disable=unused-argument     if resolver._resolver is None:         _resolver = resolver._resolver = _DualResolver()         <MASK>             _resolver.network_resolver.nameservers[:] = config.resolver_nameservers         if config.resolver_timeout:             _resolver.network_resolver.lifetime = config.resolver_timeout     # Different hubs in different threads could be sharing the same     # resolver.     assert isinstance(resolver._resolver, _DualResolver)     self._resolver = resolver._resolver",if config . resolver_nameservers :,if config . resolver_nameservers :,100.00000000000004,100.00000000000004,1.0
"def __iadd__(self, term):     if isinstance(term, (int, long)):         <MASK>             _gmp.mpz_add_ui(self._mpz_p, self._mpz_p, c_ulong(term))             return self         if -65535 < term < 0:             _gmp.mpz_sub_ui(self._mpz_p, self._mpz_p, c_ulong(-term))             return self         term = Integer(term)     _gmp.mpz_add(self._mpz_p, self._mpz_p, term._mpz_p)     return self",if 0 <= term < 65536 :,if term < 0 :,14.74341205953811,14.74341205953811,0.0
"def copy(dst, src):     for (k, v) in src.iteritems():         <MASK>             d = {}             dst[k] = d             copy(d, v)         else:             dst[k] = v","if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",100.00000000000004,100.00000000000004,1.0
"def generator(self, data):     self.procs = OrderedDict()     for task in data:         self.recurse_task(task, 0, 0, self.procs)     for offset, name, level, pid, ppid, uid, euid, gid in self.procs.values():         <MASK>             yield (                 0,                 [                     Address(offset),                     str(name),                     str(level),                     int(pid),                     int(ppid),                     int(uid),                     int(gid),                     int(euid),                 ],             )",if offset :,if offset < self . max_priority :,10.552670315936318,10.552670315936318,0.0
"def apply(self, db, person):     families = person.get_parent_family_handle_list()     if families == []:         return True     for family_handle in person.get_parent_family_handle_list():         family = db.get_family_from_handle(family_handle)         <MASK>             father_handle = family.get_father_handle()             mother_handle = family.get_mother_handle()             if not father_handle:                 return True             if not mother_handle:                 return True     return False",if family :,if family . is_father ( ) :,10.552670315936318,10.552670315936318,0.0
"def _arctic_task_exec(request):     request.start_time = time.time()     logging.debug(         ""Executing asynchronous request for {}/{}"".format(             request.library, request.symbol         )     )     result = None     try:         request.is_running = True         <MASK>             result = mongo_retry(request.fun)(*request.args, **request.kwargs)         else:             result = request.fun(*request.args, **request.kwargs)     except Exception as e:         request.exception = e     finally:         request.data = result         request.end_time = time.time()         request.is_running = False     return result",if request . mongo_retry :,if request . retry :,33.51600230178196,33.51600230178196,0.0
"def _setup_styles(self):     for ttype, ndef in self.style:         escape = EscapeSequence()         <MASK>             escape.fg = self._color_index(ndef[""color""])         if ndef[""bgcolor""]:             escape.bg = self._color_index(ndef[""bgcolor""])         if self.usebold and ndef[""bold""]:             escape.bold = True         if self.useunderline and ndef[""underline""]:             escape.underline = True         self.style_string[str(ttype)] = (escape.color_string(), escape.reset_string())","if ndef [ ""color"" ] :","if ndef [ ""color"" ] :",100.00000000000004,100.00000000000004,1.0
"def process_string(self, remove_repetitions, sequence):     string = """"     for i, char in enumerate(sequence):         if char != self.int_to_char[self.blank_index]:             # if this char is a repetition and remove_repetitions=true,             # skip.             if remove_repetitions and i != 0 and char == sequence[i - 1]:                 pass             <MASK>                 string += "" ""             else:                 string = string + char     return string",elif char == self . labels [ self . space_index ] :,if char == self . char_to_char :,26.8565108847214,26.8565108847214,0.0
"def arith_expr(self, nodelist):     node = self.com_node(nodelist[0])     for i in range(2, len(nodelist), 2):         right = self.com_node(nodelist[i])         <MASK>             node = Add(node, right, lineno=nodelist[1].context)         elif nodelist[i - 1].type == token.MINUS:             node = Sub(node, right, lineno=nodelist[1].context)         else:             raise ValueError(""unexpected token: %s"" % nodelist[i - 1][0])     return node",if nodelist [ i - 1 ] . type == token . PLUS :,if nodelist [ i - 1 ] . type == token . MINUS :,86.66415730847507,86.66415730847507,0.0
"def invert_index(cls, index, length):     if np.isscalar(index):         return length - index     elif isinstance(index, slice):         start, stop = index.start, index.stop         new_start, new_stop = None, None         <MASK>             new_stop = length - start         if stop is not None:             new_start = length - stop         return slice(new_start - 1, new_stop - 1)     elif isinstance(index, Iterable):         new_index = []         for ind in index:             new_index.append(length - ind)     return new_index",if start is not None :,if start is not None :,100.00000000000004,100.00000000000004,1.0
"def getRoots(job):     if job not in visited:         visited.add(job)         <MASK>             list(map(lambda p: getRoots(p), job._directPredecessors))         else:             roots.add(job)         # The following call ensures we explore all successor edges.         list(map(lambda c: getRoots(c), job._children + job._followOns))",if len ( job . _directPredecessors ) > 0 :,if job . _directPredecessors :,24.439253249722206,24.439253249722206,0.0
"def visit_filter_projection(self, node, value):     base = self.visit(node[""children""][0], value)     if not isinstance(base, list):         return None     comparator_node = node[""children""][2]     collected = []     for element in base:         <MASK>             current = self.visit(node[""children""][1], element)             if current is not None:                 collected.append(current)     return collected","if self . _is_true ( self . visit ( comparator_node , element ) ) :",if comparator_node is not None :,5.454673614807693,5.454673614807693,0.0
"def func(x, y):     try:         if x > y:             z = x + 2 * math.sin(y)             return z ** 2         <MASK>             return 4         else:             return 2 ** 3     except ValueError:         foo = 0         for i in range(4):             foo += i         return foo     except TypeError:         return 42     else:         return 33     finally:         print(""finished"")",elif x == y :,if x < y :,19.3576934939088,19.3576934939088,0.0
"def set_filter(self, dataset_opt):     """"""This function create and set the pre_filter to the obj as attributes""""""     self.pre_filter = None     for key_name in dataset_opt.keys():         <MASK>             new_name = key_name.replace(""filters"", ""filter"")             try:                 filt = instantiate_filters(getattr(dataset_opt, key_name))             except Exception:                 log.exception(                     ""Error trying to create {}, {}"".format(                         new_name, getattr(dataset_opt, key_name)                     )                 )                 continue             setattr(self, new_name, filt)","if ""filter"" in key_name :","if key_name . startswith ( ""filters"" ) :",15.580105704117443,15.580105704117443,0.0
"def _add_states_to_lookup(     self, trackers_as_states, trackers_as_actions, domain, online=False ):     """"""Add states to lookup dict""""""     for states in trackers_as_states:         active_form = self._get_active_form_name(states[-1])         <MASK>             # modify the states             states = self._modified_states(states)             feature_key = self._create_feature_key(states)             # even if there are two identical feature keys             # their form will be the same             # because of `active_form_...` feature             self.lookup[feature_key] = active_form",if active_form and self . _prev_action_listen_in_state ( states [ - 1 ] ) :,if active_form in self . lookup :,6.446447763297845,6.446447763297845,0.0
"def list_loaded_payloads(self):     print(helpers.color(""\n [*] Available Payloads:\n""))     lastBase = None     x = 1     for name in sorted(self.active_payloads.keys()):         parts = name.split(""/"")         <MASK>             print()         lastBase = parts[0]         print(""\t%s)\t%s"" % (x, ""{0: <24}"".format(name)))         x += 1     print(""\n"")     return",if lastBase and parts [ 0 ] != lastBase :,if lastBase == lastBase :,16.51116138719648,16.51116138719648,0.0
"def reprSmart(vw, item):     ptype = type(item)     if ptype is int:         if -1024 < item < 1024:             return str(item)         <MASK>             return vw.reprPointer(item)         else:             return hex(item)     elif ptype in (list, tuple):         return reprComplex(vw, item)  # recurse     elif ptype is dict:         return ""{%s}"" % "","".join(             [""%s:%s"" % (reprSmart(vw, k), reprSmart(vw, v)) for k, v in item.items()]         )     else:         return repr(item)",elif vw . isValidPointer ( item ) :,if - 1024 < item < 1024 :,6.567274736060395,6.567274736060395,0.0
"def ConfigSectionMap(section):     config = ConfigParser.RawConfigParser()     configurations = config_manager()  # Class from mkchromecast.config     configf = configurations.configf     config.read(configf)     dict1 = {}     options = config.options(section)     for option in options:         try:             dict1[option] = config.get(section, option)             <MASK>                 DebugPrint(""skip: %s"" % option)         except:             print(""Exception on %s!"" % option)             dict1[option] = None     return dict1",if dict1 [ option ] == - 1 :,if option in dict1 :,5.557509463743763,5.557509463743763,0.0
"def on_success(result):     subtasks = {}     if result:         subtasks = {             self.nodes_keys.inverse[s[""node_id""]]: s.get(""subtask_id"")             for s in result             <MASK>         }     if subtasks:         print(""subtask finished"")         self.next()     else:         print(""waiting for a subtask to finish"")         time.sleep(10)","if s . get ( ""status"" ) == ""Failure""","if s [ ""node_id"" ] in self . nodes_keys . inverse :",5.401157445454033,5.401157445454033,0.0
"def redirect_aware_commmunicate(p, sys=_sys):     """"""Variant of process.communicate that works with in process I/O redirection.""""""     assert sys is not None     out, err = p.communicate()     if redirecting_io(sys=sys):         if out:             # We don't unicodify in Python2 because sys.stdout may be a             # cStringIO.StringIO object, which does not accept Unicode strings             out = unicodify(out)             sys.stdout.write(out)             out = None         <MASK>             err = unicodify(err)             sys.stderr.write(err)             err = None     return out, err",if err :,if err :,100.00000000000004,0.0,1.0
"def __exit__(self, *args, **kwargs):     self._samples_cache = {}     if is_validation_enabled() and isinstance(self.prior, dict):         extra = set(self.prior) - self._param_hits         <MASK>             warnings.warn(                 ""pyro.module prior did not find params ['{}']. ""                 ""Did you instead mean one of ['{}']?"".format(                     ""', '"".join(extra), ""', '"".join(self._param_misses)                 )             )     return super().__exit__(*args, **kwargs)",if extra :,if extra :,100.00000000000004,0.0,1.0
def __download_thread(self):     while True:         <MASK>             self.__current_download = self.__queue.get()             self.__download_file(self.__current_download)         time.sleep(0.1),if not self . __queue . empty ( ) :,if self . __current_download is not None :,24.384183193426086,24.384183193426086,0.0
"def plot_timer_command(args):     import nnabla.monitor as M     format_unit = dict(         s=""seconds"",         m=""minutes"",         h=""hours"",         d=""days"",     )     if not args.ylabel:         <MASK>             args.ylabel = ""Total elapsed time [{}]"".format(format_unit[args.time_unit])         else:             args.ylabel = ""Elapsed time [{}/iter]"".format(format_unit[args.time_unit])     plot_any_command(         args, M.plot_time_elapsed, dict(elapsed=args.elapsed, unit=args.time_unit)     )     return True",if args . elapsed :,if args . elapsed :,100.00000000000004,100.00000000000004,1.0
"def resolve_page(root: ChannelContext[models.MenuItem], info, **kwargs):     if root.node.page_id:         requestor = get_user_or_app_from_context(info.context)         requestor_has_access_to_all = requestor.is_active and requestor.has_perm(             PagePermissions.MANAGE_PAGES         )         return (             PageByIdLoader(info.context)             .load(root.node.page_id)             .then(                 lambda page: page                 <MASK>                 else None             )         )     return None",if requestor_has_access_to_all or page . is_visible,if requestor_has_access_to_all and page . is_active :,69.64705665515706,69.64705665515706,0.0
"def find(self, pattern):     """"""Find pages in database.""""""     results = self._search_keyword(pattern)     pat = re.compile(""(.*?)(%s)(.*?)( \(.*\))?$"" % re.escape(pattern), re.I)     if results:         for name, keyword, url in results:             <MASK>                 keyword = pat.sub(                     r""\1\033[1;31m\2\033[0m\3\033[1;33m\4\033[0m"", keyword                 )             print(""%s - %s"" % (keyword, name))     else:         raise RuntimeError(""%s: nothing appropriate."" % pattern)",if os . isatty ( sys . stdout . fileno ( ) ) :,if url :,0.8861688619210014,0.0,0.0
"def _certonly_new_request_common(self, mock_client, args=None):     with mock.patch(         ""certbot._internal.main._find_lineage_for_domains_and_certname""     ) as mock_renewal:         mock_renewal.return_value = (""newcert"", None)         with mock.patch(""certbot._internal.main._init_le_client"") as mock_init:             mock_init.return_value = mock_client             <MASK>                 args = []             args += ""-d foo.bar -a standalone certonly"".split()             self._call(args)",if args is None :,if args is None :,100.00000000000004,100.00000000000004,1.0
"def __init__(self, *args, **kw):     if len(args) > 1:         raise TypeError(""MultiDict can only be called with one positional "" ""argument"")     if args:         <MASK>             items = list(args[0].iteritems())         elif hasattr(args[0], ""items""):             items = list(args[0].items())         else:             items = list(args[0])         self._items = items     else:         self._items = []     if kw:         self._items.extend(kw.items())","if hasattr ( args [ 0 ] , ""iteritems"" ) :","if hasattr ( args [ 0 ] , ""iter"" ) :",76.11606003349888,76.11606003349888,0.0
"def test08_ExceptionTypes(self):     self.assertTrue(issubclass(db.DBError, Exception))     for i, j in db.__dict__.items():         <MASK>             self.assertTrue(issubclass(j, db.DBError), msg=i)             if i not in (""DBKeyEmptyError"", ""DBNotFoundError""):                 self.assertFalse(issubclass(j, KeyError), msg=i)     # This two exceptions have two bases     self.assertTrue(issubclass(db.DBKeyEmptyError, KeyError))     self.assertTrue(issubclass(db.DBNotFoundError, KeyError))","if i . startswith ( ""DB"" ) and i . endswith ( ""Error"" ) :","if i not in ( ""DBKeyEmptyError"" , ""DBNotFoundError"" ) :",11.575816250682482,11.575816250682482,0.0
"def _delegate_to_sinks(self, value: Any) -> None:     for sink in self._sinks:         if isinstance(sink, AgentT):             await sink.send(value=value)         <MASK>             await cast(TopicT, sink).send(value=value)         else:             await maybe_async(cast(Callable, sink)(value))","elif isinstance ( sink , ChannelT ) :","if isinstance ( sink , TopicT ) :",41.11336169005198,41.11336169005198,0.0
"def _select_block(str_in, start_tag, end_tag):     """"""Select first block delimited by start_tag and end_tag""""""     start_pos = str_in.find(start_tag)     if start_pos < 0:         raise ValueError(""start_tag not found"")     depth = 0     for pos in range(start_pos, len(str_in)):         if str_in[pos] == start_tag:             depth += 1         <MASK>             depth -= 1         if depth == 0:             break     sel = str_in[start_pos + 1 : pos]     return sel",elif str_in [ pos ] == end_tag :,if pos == end_tag :,37.848698581337665,37.848698581337665,0.0
"def confirm(request):     details = request.session.get(""reauthenticate"")     if not details:         return redirect(""home"")     # Monkey patch request     request.user = User.objects.get(pk=details[""user_pk""])     if request.method == ""POST"":         confirm_form = PasswordConfirmForm(request, request.POST)         <MASK>             request.session.pop(""reauthenticate"")             request.session[""reauthenticate_done""] = True             return redirect(""social:complete"", backend=details[""backend""])     else:         confirm_form = PasswordConfirmForm(request)     context = {""confirm_form"": confirm_form}     context.update(details)     return render(request, ""accounts/confirm.html"", context)",if confirm_form . is_valid ( ) :,if confirm_form . is_valid ( ) :,100.00000000000004,100.00000000000004,1.0
"def verify_credentials(self):     if self.enabled:         response = requests.get(             ""https://api.exotel.com/v1/Accounts/{sid}"".format(sid=self.account_sid),             auth=(self.api_key, self.api_token),         )         <MASK>             frappe.throw(_(""Invalid credentials""))",if response . status_code != 200 :,if not response . status_code :,43.79518644116555,43.79518644116555,0.0
"def pixbufrenderer(self, column, crp, model, it):     tok = model.get_value(it, 0)     if tok.type == ""class"":         icon = ""class""     else:         <MASK>             icon = ""method_priv""         elif tok.visibility == ""protected"":             icon = ""method_prot""         else:             icon = ""method""     crp.set_property(""pixbuf"", imagelibrary.pixbufs[icon])","if tok . visibility == ""private"" :","if tok . visibility == ""private"" :",100.00000000000004,100.00000000000004,1.0
"def _omit_keywords(self, context):     omitted_kws = 0     for event, elem in context:         # Teardowns aren't omitted to allow checking suite teardown status.         omit = elem.tag == ""kw"" and elem.get(""type"") != ""teardown""         start = event == ""start""         if omit and start:             omitted_kws += 1         if not omitted_kws:             yield event, elem         <MASK>             elem.clear()         if omit and not start:             omitted_kws -= 1",elif not start :,"if event == ""stop"" :",5.522397783539471,5.522397783539471,0.0
"def on_double_click(self, event):     # TODO: don't act when the click happens below last item     path = self.get_selected_path()     kind = self.get_selected_kind()     name = self.get_selected_name()     if kind == ""file"":         <MASK>             self.open_file(path)         else:             self.open_path_with_system_app(path)     elif kind == ""dir"":         self.request_focus_into(path)     return ""break""",if self . should_open_name_in_thonny ( name ) :,"if name == ""file"" :",2.673705182447105,2.673705182447105,0.0
"def search_cve(db: DatabaseInterface, product: Product) -> dict:     result = {}     for query_result in db.fetch_multiple(QUERIES[""cve_lookup""]):         cve_entry = CveDbEntry(*query_result)         <MASK>             result[cve_entry.cve_id] = {                 ""score2"": cve_entry.cvss_v2_score,                 ""score3"": cve_entry.cvss_v3_score,                 ""cpe_version"": build_version_string(cve_entry),             }     return result","if _product_matches_cve ( product , cve_entry ) :",if cve_entry . cve_id in result :,11.985825441691155,11.985825441691155,0.0
"def find_go_files_mtime(app_files):     files, mtime = [], 0     for f, mt in app_files.items():         if not f.endswith("".go""):             continue         <MASK>             continue         files.append(f)         mtime = max(mtime, mt)     return files, mtime",if APP_CONFIG . nobuild_files . match ( f ) :,if mt > mtime :,2.099844458473431,2.099844458473431,0.0
"def wrapper(filename):     mtime = getmtime(filename)     with lock:         <MASK>             old_mtime, result = cache.pop(filename)             if old_mtime == mtime:                 # Move to the end                 cache[filename] = old_mtime, result                 return result     result = function(filename)     with lock:         cache[filename] = mtime, result  # at the end         if len(cache) > max_size:             cache.popitem(last=False)     return result",if filename in cache :,if len ( cache ) > max_size :,5.522397783539471,5.522397783539471,0.0
"def Tokenize(s):     # type: (str) -> Iterator[Token]     for item in TOKEN_RE.findall(s):         # The type checker can't know the true type of item!         item = cast(TupleStr4, item)         <MASK>             typ = ""number""             val = item[0]         elif item[1]:             typ = ""name""             val = item[1]         elif item[2]:             typ = item[2]             val = item[2]         elif item[3]:             typ = item[3]             val = item[3]         yield Token(typ, val)",if item [ 0 ] :,if item [ 0 ] :,100.00000000000004,100.00000000000004,1.0
"def _show_encoders(self, *args, **kwargs):     if issubclass(self.current_module.__class__, BasePayload):         encoders = self.current_module.get_encoders()         <MASK>             headers = (""Encoder"", ""Name"", ""Description"")             print_table(headers, *encoders, max_column_length=100)             return     print_error(""No encoders available"")",if encoders :,if not encoders :,35.35533905932737,35.35533905932737,0.0
"def __init__(self):     Builder.__init__(self, commandName=""VCExpress.exe"", formatName=""msvcProject"")     for key in [""VS90COMNTOOLS"", ""VC80COMNTOOLS"", ""VC71COMNTOOLS""]:         <MASK>             self.programDir = os.path.join(os.environ[key], "".."", ""IDE"")     if self.programDir is None:         for version in [""9.0"", ""8"", "".NET 2003""]:             msvcDir = (                 ""C:\\Program Files\\Microsoft Visual Studio %s\\Common7\\IDE"" % version             )             if os.path.exists(msvcDir):                 self.programDir = msvcDir",if os . environ . has_key ( key ) :,if os . environ [ key ] is not None :,23.708987804092644,23.708987804092644,0.0
"def _inner(*args, **kwargs):     component_manager = args[0].component_manager     for condition_name in condition_names:         condition_result, err_msg = component_manager.evaluate_condition(condition_name)         <MASK>             raise ComponentStartConditionNotMetError(err_msg)     if not component_manager.all_components_running(*components):         raise ComponentsNotStartedError(             f""the following required components have not yet started: {json.dumps(components)}""         )     return method(*args, **kwargs)",if not condition_result :,if not condition_result :,100.00000000000004,100.00000000000004,1.0
"def _gridconvvalue(self, value):     if isinstance(value, (str, _tkinter.Tcl_Obj)):         try:             svalue = str(value)             if not svalue:                 return None             <MASK>                 return self.tk.getdouble(svalue)             else:                 return self.tk.getint(svalue)         except (ValueError, TclError):             pass     return value","elif ""."" in svalue :",if self . tk . isdouble ( svalue ) :,5.522397783539471,5.522397783539471,0.0
"def check_songs():     desc = numeric_phrase(""%d song"", ""%d songs"", len(songs))     with Task(_(""Rescan songs""), desc) as task:         task.copool(check_songs)         for i, song in enumerate(songs):             song = song._song             <MASK>                 app.library.reload(song)             task.update((float(i) + 1) / len(songs))             yield",if song in app . library :,if i > 0 :,8.51528917838043,8.51528917838043,0.0
"def initialize(self):     nn.init.xavier_uniform_(self.linear.weight.data)     if self.linear.bias is not None:         self.linear.bias.data.uniform_(-1.0, 1.0)     if self.self_layer:         nn.init.xavier_uniform_(self.linear_self.weight.data)         <MASK>             self.linear_self.bias.data.uniform_(-1.0, 1.0)",if self . linear_self . bias is not None :,if self . self_layer :,14.231728394642222,14.231728394642222,0.0
"def test_row(self, row):     for idx, test in self.patterns.items():         try:             value = row[idx]         except IndexError:             value = """"         result = test(value)         <MASK>             if result:                 return not self.inverse  # True         else:             if not result:                 return self.inverse  # False     if self.any_match:         return self.inverse  # False     else:         return not self.inverse  # True",if self . any_match :,if self . any_match :,100.00000000000004,100.00000000000004,1.0
"def toterminal(self, tw):     for element in self.chain:         element[0].toterminal(tw)         <MASK>             tw.line("""")             tw.line(element[2], yellow=True)     super(ExceptionChainRepr, self).toterminal(tw)",if element [ 2 ] is not None :,"if element [ 1 ] == ""Exception"" :",15.851165692617148,15.851165692617148,0.0
"def runMainLoop(self):     """"""The curses gui main loop.""""""     # pylint: disable=no-member     #     # Do NOT change g.app!     self.curses_app = LeoApp()     stdscr = curses.initscr()     if 1:  # Must follow initscr.         self.dump_keys()     try:         self.curses_app.run()         # run calls CApp.main(), which calls CGui.run().     finally:         curses.nocbreak()         stdscr.keypad(0)         curses.echo()         curses.endwin()         <MASK>             g.pr(""Exiting Leo..."")","if ""shutdown"" in g . app . debug :",if 0 :,2.4088567143060917,0.0,0.0
"def test_chunkcoding(self):     for native, utf8 in zip(*[StringIO(f).readlines() for f in self.tstring]):         u = self.decode(native)[0]         self.assertEqual(u, utf8.decode(""utf-8""))         <MASK>             self.assertEqual(native, self.encode(u)[0])",if self . roundtriptest :,if u :,17.799177396293473,0.0,0.0
"def reload_sanitize_allowlist(self, explicit=True):     self.sanitize_allowlist = []     try:         with open(self.sanitize_allowlist_file) as f:             for line in f.readlines():                 <MASK>                     self.sanitize_allowlist.append(line.strip())     except OSError:         if explicit:             log.warning(                 ""Sanitize log file explicitly specified as '%s' but does not exist, continuing with no tools allowlisted."",                 self.sanitize_allowlist_file,             )","if not line . startswith ( ""#"" ) :",if line :,2.7574525891364066,0.0,0.0
"def get_all_extensions(subtree=None):     if subtree is None:         subtree = full_extension_tree()     result = []     if isinstance(subtree, dict):         for value in subtree.values():             if isinstance(value, dict):                 result += get_all_extensions(value)             <MASK>                 result += value.extensions             elif isinstance(value, (list, tuple)):                 result += value     elif isinstance(subtree, (ContentTypeMapping, ContentTypeDetector)):         result = subtree.extensions     elif isinstance(subtree, (list, tuple)):         result = subtree     return result","elif isinstance ( value , ( ContentTypeMapping , ContentTypeDetector ) ) :","if isinstance ( value , (ContentTypeMapping , ContentTypeDetector ) ) :",90.36020036098445,90.36020036098445,0.0
"def _configuration_dict_to_commandlist(name, config_dict):     command_list = [""config:%s"" % name]     for key, value in config_dict.items():         <MASK>             if value:                 b = ""true""             else:                 b = ""false""             command_list.append(""%s:%s"" % (key, b))         else:             command_list.append(""%s:%s"" % (key, value))     return command_list",if type ( value ) is bool :,"if key == ""true"" :",6.567274736060395,6.567274736060395,0.0
"def _RewriteModinfo(     self,     modinfo,     obj_kernel_version,     this_kernel_version,     info_strings=None,     to_remove=None, ):     new_modinfo = """"     for line in modinfo.split(""\x00""):         <MASK>             continue         if to_remove and line.split(""="")[0] == to_remove:             continue         if info_strings is not None:             info_strings.add(line.split(""="")[0])         if line.startswith(""vermagic""):             line = line.replace(obj_kernel_version, this_kernel_version)         new_modinfo += line + ""\x00""     return new_modinfo",if not line :,if not line :,100.00000000000004,100.00000000000004,1.0
"def zip_random_open_test(self, f, compression):     self.make_test_archive(f, compression)     # Read the ZIP archive     with zipfile.ZipFile(f, ""r"", compression) as zipfp:         zipdata1 = []         with zipfp.open(TESTFN) as zipopen1:             while True:                 read_data = zipopen1.read(randint(1, 1024))                 <MASK>                     break                 zipdata1.append(read_data)         testdata = """".join(zipdata1)         self.assertEqual(len(testdata), len(self.data))         self.assertEqual(testdata, self.data)",if not read_data :,if read_data is None :,27.77619034011791,27.77619034011791,0.0
"def _memoized(*args):     now = time.time()     try:         value, last_update = self.cache[args]         age = now - last_update         <MASK>             self._call_count = 0             raise AttributeError         if self.ctl:             self._call_count += 1         return value     except (KeyError, AttributeError):         value = func(*args)         if value:             self.cache[args] = (value, now)         return value     except TypeError:         return func(*args)",if self . _call_count > self . ctl or age > self . ttl :,if age > self . timeout :,9.452420783005895,9.452420783005895,0.0
"def on_data(res):     if terminate.is_set():         return     if args.strings and not args.no_content:         if type(res) == tuple:             f, v = res             <MASK>                 f = f.encode(""utf-8"")             if type(v) == unicode:                 v = v.encode(""utf-8"")             self.success(""{}: {}"".format(f, v))         elif not args.content_only:             self.success(res)     else:         self.success(res)",if type ( f ) == unicode :,if type ( f ) == unicode :,100.00000000000004,100.00000000000004,1.0
"def _finalize_setup_keywords(self):     for ep in pkg_resources.iter_entry_points(""distutils.setup_keywords""):         value = getattr(self, ep.name, None)         <MASK>             ep.require(installer=self.fetch_build_egg)             ep.load()(self, ep.name, value)",if value is not None :,if value is None :,40.93653765389909,40.93653765389909,0.0
"def test_attributes_types(self):     if not self.connection.strategy.pooled:         <MASK>             self.connection.refresh_server_info()         self.assertEqual(             type(self.connection.server.schema.attribute_types[""cn""]), AttributeTypeInfo         )",if not self . connection . server . info :,"if self . connection . server . schema . attribute_types [ ""cn"" ] is not None :",24.761650580786526,24.761650580786526,0.0
"def to_key(literal_or_identifier):     """"""returns string representation of this object""""""     if literal_or_identifier[""type""] == ""Identifier"":         return literal_or_identifier[""name""]     elif literal_or_identifier[""type""] == ""Literal"":         k = literal_or_identifier[""value""]         if isinstance(k, float):             return unicode(float_repr(k))         elif ""regex"" in literal_or_identifier:             return compose_regex(k)         <MASK>             return ""true"" if k else ""false""         elif k is None:             return ""null""         else:             return unicode(k)","elif isinstance ( k , bool ) :",if k is True :,6.9717291216921975,6.9717291216921975,0.0
"def list2rec(x, test=False):     if test:         vid = ""{}_{:06d}_{:06d}"".format(x[0], int(x[1]), int(x[2]))         label = -1  # label unknown         return vid, label     else:         vid = ""{}_{:06d}_{:06d}"".format(x[1], int(x[2]), int(x[3]))         <MASK>             vid = ""{}/{}"".format(convert_label(x[0]), vid)         else:             assert level == 1         label = class_mapping[convert_label(x[0])]         return vid, label",if level == 2 :,if level == 0 :,53.7284965911771,53.7284965911771,0.0
"def _expand_env(self, snapcraft_yaml):     environment_keys = [""name"", ""version""]     for key in snapcraft_yaml:         <MASK>             continue         replacements = environment_to_replacements(             get_snapcraft_global_environment(self.project)         )         snapcraft_yaml[key] = replace_attr(snapcraft_yaml[key], replacements)     return snapcraft_yaml",if any ( ( key == env_key for env_key in environment_keys ) ) :,if key not in environment_keys :,7.829904572119011,7.829904572119011,0.0
"def enableCtrls(self):     # Check if each ctrl has a requirement or an incompatibility,     # look it up, and enable/disable if so     for data in self.storySettingsData:         name = data[""name""]         <MASK>             if ""requires"" in data:                 set = self.getSetting(data[""requires""])                 for i in self.ctrls[name]:                     i.Enable(set not in [""off"", ""false"", ""0""])",if name in self . ctrls :,"if ""off"" in data :",8.643019616048525,8.643019616048525,0.0
"def __init__(self, *args, **kwargs):     super(ChallengePhaseCreateSerializer, self).__init__(*args, **kwargs)     context = kwargs.get(""context"")     if context:         challenge = context.get(""challenge"")         <MASK>             kwargs[""data""][""challenge""] = challenge.pk         test_annotation = context.get(""test_annotation"")         if test_annotation:             kwargs[""data""][""test_annotation""] = test_annotation",if challenge :,if challenge :,100.00000000000004,0.0,1.0
def set_inactive(self):     for title in self.gramplet_map:         if self.gramplet_map[title].pui:             <MASK>                 self.gramplet_map[title].pui.active = False,"if self . gramplet_map [ title ] . gstate != ""detached"" :",if title . pui . active :,2.3534609554455574,2.3534609554455574,0.0
"def authenticate(username, password):     try:         u = User.objects.get(username=username)         <MASK>             userLogger.info(""User logged in : %s"", username)             return u         else:             userLogger.warn(""Attempt to log in to : %s"", username)             return False     except DoesNotExist:         return False","if check_password_hash ( u . password , password ) :",if u :,1.0144101175482476,0.0,0.0
def _check_date(self):     if not self.value:         return None     if not self.allow_date_in_past:         if self.value < self.date_or_datetime().today():             <MASK>                 self.value = self.date_or_datetime().today()             else:                 self.value = self.date_or_datetime().today() + datetime.timedelta(1),if self . allow_todays_date :,if self . value > self . date_or_datetime ( ) :,12.090340630072072,12.090340630072072,0.0
"def update(self, E=None, **F):     if E:         <MASK>             # Update with `E` dictionary             for k in E:                 self[k] = E[k]         else:             # Update with `E` items             for (k, v) in E:                 self[k] = v     # Update with `F` dictionary     for k in F:         self[k] = F[k]","if hasattr ( E , ""keys"" ) :",if F :,3.361830360737634,0.0,0.0
"def _get_quota_availability(self):     quotas_ok = defaultdict(int)     qa = QuotaAvailability()     qa.queue(*[k for k, v in self._quota_diff.items() if v > 0])     qa.compute(now_dt=self.now_dt)     for quota, count in self._quota_diff.items():         <MASK>             quotas_ok[quota] = 0             break         avail = qa.results[quota]         if avail[1] is not None and avail[1] < count:             quotas_ok[quota] = min(count, avail[1])         else:             quotas_ok[quota] = count     return quotas_ok",if count <= 0 :,if count == 0 :,37.99178428257963,37.99178428257963,0.0
"def gen_env_vars():     for fd_id, fd in zip(STDIO_DESCRIPTORS, (stdin, stdout, stderr)):         is_atty = fd.isatty()         yield (cls.TTY_ENV_TMPL.format(fd_id), cls.encode_env_var_value(int(is_atty)))         <MASK>             yield (cls.TTY_PATH_ENV.format(fd_id), os.ttyname(fd.fileno()) or b"""")",if is_atty :,if os . path . isfile ( fd . fileno ( ) ) :,3.377156414337854,3.377156414337854,0.0
"def _convertDict(self, d):     r = {}     for k, v in d.items():         if isinstance(v, bytes):             v = str(v, ""utf-8"")         elif isinstance(v, list) or isinstance(v, tuple):             v = self._convertList(v)         elif isinstance(v, dict):             v = self._convertDict(v)         <MASK>             k = str(k, ""utf-8"")         r[k] = v     return r","if isinstance ( k , bytes ) :","if isinstance ( v , bytes ) :",50.000000000000014,50.000000000000014,0.0
"def get_attribute_value(self, nodeid, attr):     with self._lock:         self.logger.debug(""get attr val: %s %s"", nodeid, attr)         if nodeid not in self._nodes:             dv = ua.DataValue()             dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown)             return dv         node = self._nodes[nodeid]         <MASK>             dv = ua.DataValue()             dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid)             return dv         attval = node.attributes[attr]         if attval.value_callback:             return attval.value_callback()         return attval.value",if attr not in node . attributes :,if node is None :,7.715486568024961,7.715486568024961,0.0
"def conninfo_parse(dsn):     ret = {}     length = len(dsn)     i = 0     while i < length:         <MASK>             i += 1             continue         param_match = PARAMETER_RE.match(dsn[i:])         if not param_match:             return         param = param_match.group(1)         i += param_match.end()         if i >= length:             return         value, end = read_param_value(dsn[i:])         if value is None:             return         i += end         ret[param] = value     return ret",if dsn [ i ] . isspace ( ) :,if not param :,4.238556455648295,4.238556455648295,0.0
"def connect(self, buttons):     for button in buttons:         assert button is not None         handled = False         for handler_idx in range(0, len(self.__signal_handlers)):             (obj_class, signal, handler, handler_id) = self.__signal_handlers[                 handler_idx             ]             <MASK>                 handler_id = button.connect(signal, handler)                 handled = True             self.__signal_handlers[handler_idx] = (                 obj_class,                 signal,                 handler,                 handler_id,             )         assert handled","if isinstance ( button , obj_class ) :",if button . connect :,5.171845311465849,5.171845311465849,0.0
"def _parse_display(display):     """"""Parse an X11 display value""""""     try:         host, dpynum = display.rsplit("":"", 1)         if host.startswith(""["") and host.endswith(""]""):             host = host[1:-1]         idx = dpynum.find(""."")         <MASK>             screen = int(dpynum[idx + 1 :])             dpynum = dpynum[:idx]         else:             screen = 0     except (ValueError, UnicodeEncodeError):         raise ValueError(""Invalid X11 display"") from None     return host, dpynum, screen",if idx >= 0 :,if idx >= 0 :,100.00000000000004,100.00000000000004,1.0
"def delete_all(path):     ppath = os.getcwd()     os.chdir(path)     for fn in glob.glob(""*""):         fn_full = os.path.join(path, fn)         if os.path.isdir(fn):             delete_all(fn_full)         <MASK>             os.remove(fn_full)         elif fn.endswith("".md""):             os.remove(fn_full)         elif DELETE_ALL_OLD:             os.remove(fn_full)     os.chdir(ppath)     os.rmdir(path)","elif fn . endswith ( "".png"" ) :","if fn . endswith ( "".md"" ) :",58.77283725105324,58.77283725105324,0.0
"def _sync_get(self, identifier, *args, **kw):     self._mutex.acquire()     try:         try:             <MASK>                 return self._values[identifier]             else:                 self._values[identifier] = value = self.creator(identifier, *args, **kw)                 return value         except KeyError:             self._values[identifier] = value = self.creator(identifier, *args, **kw)             return value     finally:         self._mutex.release()",if identifier in self . _values :,if identifier in self . _values :,100.00000000000004,100.00000000000004,1.0
"def _query_fd(self):     if self.stream is None:         self._last_stat = None, None     else:         try:             st = os.stat(self._filename)         except OSError:             e = sys.exc_info()[1]             <MASK>                 raise             self._last_stat = None, None         else:             self._last_stat = st[stat.ST_DEV], st[stat.ST_INO]",if e . errno != errno . ENOENT :,if e != 0 :,12.462989337200145,12.462989337200145,0.0
"def get_place_name(self, place_handle):     """"""Obtain a place name""""""     text = """"     if place_handle:         place = self.dbstate.db.get_place_from_handle(place_handle)         <MASK>             place_title = place_displayer.display(self.dbstate.db, place)             if place_title != """":                 if len(place_title) > 25:                     text = place_title[:24] + ""...""                 else:                     text = place_title     return text",if place :,if place :,100.00000000000004,0.0,1.0
"def test_decoder_state(self):     # Check that getstate() and setstate() handle the state properly     u = ""abc123""     for encoding in all_unicode_encodings:         <MASK>             self.check_state_handling_decode(encoding, u, u.encode(encoding))             self.check_state_handling_encode(encoding, u, u.encode(encoding))",if encoding not in broken_unicode_with_stateful :,if encoding in self . state_handling_decode :,9.00746750211399,9.00746750211399,0.0
"def cleanup(self):     if os.path.exists(self.meta_gui_dir):         for f in os.listdir(self.meta_gui_dir):             <MASK>                 os.remove(os.path.join(self.meta_gui_dir, f))","if os . path . splitext ( f ) [ 1 ] == "".desktop"" :",if os . path . isdir ( f ) :,21.99876340209316,21.99876340209316,0.0
"def _have_applied_incense(self):     for applied_item in inventory.applied_items().all():         self.logger.info(applied_item)         <MASK>             mins = format_time(applied_item.expire_ms * 1000)             self.logger.info(                 ""Not applying incense, currently active: %s, %s minutes remaining"",                 applied_item.item.name,                 mins,             )             return True         else:             self.logger.info("""")             return False     return False",if applied_item . expire_ms > 0 :,if applied_item . expire_ms :,71.19674182275,71.19674182275,0.0
"def get_closest_point(self, point):     point = to_point(point)     cp, cd = None, None     for p0, p1 in iter_pairs(self.pts, self.connected):         diff = p1 - p0         l = diff.length         d = diff / l         pp = p0 + d * max(0, min(l, (point - p0).dot(d)))         dist = (point - pp).length         <MASK>             cp, cd = pp, dist     return cp",if not cp or dist < cd :,if dist > 0 :,7.715486568024961,7.715486568024961,0.0
"def process_return(lines):     for line in lines:         m = re.fullmatch(r""(?P<param>\w+)\s+:\s+(?P<type>[\w.]+)"", line)         <MASK>             # Once this is in scanpydoc, we can use the fancy hover stuff             yield f'**{m[""param""]}** : :class:`~{m[""type""]}`'         else:             yield line",if m :,if m :,100.00000000000004,0.0,1.0
"def _classify(nodes_by_level):     missing, invalid, downloads = [], [], []     for level in nodes_by_level:         for node in level:             if node.binary == BINARY_MISSING:                 missing.append(node)             <MASK>                 invalid.append(node)             elif node.binary in (BINARY_UPDATE, BINARY_DOWNLOAD):                 downloads.append(node)     return missing, invalid, downloads",elif node . binary == BINARY_INVALID :,"if node . binary in ( BINARY_INVALID , BINARY_DOWNLOAD ) :",17.098323692758395,17.098323692758395,0.0
"def safe_parse_date(date_hdr):     """"""Parse a Date: or Received: header into a unix timestamp.""""""     try:         <MASK>             date_hdr = date_hdr.split("";"")[-1].strip()         msg_ts = long(rfc822.mktime_tz(rfc822.parsedate_tz(date_hdr)))         if (msg_ts > (time.time() + 24 * 3600)) or (msg_ts < 1):             return None         else:             return msg_ts     except (ValueError, TypeError, OverflowError):         return None","if "";"" in date_hdr :","if "";"" in date_hdr :",100.00000000000004,100.00000000000004,1.0
"def _on_change(self):     changed = False     self.save()     for key, value in self.data.items():         if isinstance(value, bool):             if value:                 changed = True                 break         if isinstance(value, int):             <MASK>                 changed = True                 break         elif value is None:             continue         elif len(value) != 0:             changed = True             break     self._reset_button.disabled = not changed",if value != 1 :,"if key == ""id"" :",7.267884212102741,7.267884212102741,0.0
"def _rewrite_prepend_append(self, string, prepend, append=None):     if append is None:         append = prepend     if not isinstance(string, StringElem):         string = StringElem(string)     string.sub.insert(0, prepend)     if unicode(string).endswith(u""\n""):         # Try and remove the last character from the tree         try:             lastnode = string.flatten()[-1]             <MASK>                 lastnode.sub[-1] = lastnode.sub[-1].rstrip(u""\n"")         except IndexError:             pass         string.sub.append(append + u""\n"")     else:         string.sub.append(append)     return string","if isinstance ( lastnode . sub [ - 1 ] , unicode ) :",if len ( lastnode ) > 0 :,6.560271639619885,6.560271639619885,0.0
"def parse_indentless_sequence_entry(self):     if self.check_token(BlockEntryToken):         token = self.get_token()         <MASK>             self.states.append(self.parse_indentless_sequence_entry)             return self.parse_block_node()         else:             self.state = self.parse_indentless_sequence_entry             return self.process_empty_scalar(token.end_mark)     token = self.peek_token()     event = SequenceEndEvent(token.start_mark, token.start_mark)     self.state = self.states.pop()     return event","if not self . check_token ( BlockEntryToken , KeyToken , ValueToken , BlockEndToken ) :",if token . end_mark == self . END_MARK :,6.033104985460151,6.033104985460151,0.0
"def walk_directory(directory, verbose=False):     """"""Iterates a directory's text files and their contents.""""""     for dir_path, _, filenames in os.walk(directory):         for filename in filenames:             file_path = os.path.join(dir_path, filename)             if os.path.isfile(file_path) and not filename.startswith("".""):                 with io.open(file_path, ""r"", encoding=""utf-8"") as file:                     <MASK>                         print(""Reading {}"".format(filename))                     doc_text = file.read()                     yield filename, doc_text",if verbose :,if verbose :,100.00000000000004,0.0,1.0
"def set_bounds(self, x, y, width, height):     if self.native:         # Root level widgets may require vertical adjustment to         # account for toolbars, etc.         <MASK>             vertical_shift = self.frame.vertical_shift         else:             vertical_shift = 0         self.native.Size = Size(width, height)         self.native.Location = Point(x, y + vertical_shift)",if self . interface . parent is None :,if self . frame :,19.199242796476852,19.199242796476852,0.0
"def _check_x11(self, command=None, *, exc=None, exit_status=None, **kwargs):     """"""Check requesting X11 forwarding""""""     with (yield from self.connect()) as conn:         <MASK>             with self.assertRaises(exc):                 yield from _create_x11_process(conn, command, **kwargs)         else:             proc = yield from _create_x11_process(conn, command, **kwargs)             yield from proc.wait()             self.assertEqual(proc.exit_status, exit_status)     yield from conn.wait_closed()",if exc :,if exc :,100.00000000000004,0.0,1.0
"def repr(self):     try:         <MASK>             from infogami.infobase.utils import prepr             return prepr(self.obj)         else:             return repr(self.obj)     except:         return ""failed""     return render_template(""admin/memory/object"", self.obj)","if isinstance ( self . obj , ( dict , web . threadeddict ) ) :",if self . obj . __class__ is not None :,10.897689104778708,10.897689104778708,0.0
"def add(self, tag, values):     if tag not in self.different:         if tag not in self:             self[tag] = values         <MASK>             self.different.add(tag)             self[tag] = [""""]     self.counts[tag] += 1",elif self [ tag ] != values :,if tag not in self . counts :,6.413885305524152,6.413885305524152,0.0
"def _on_geturl(self, event):     selected = self._status_list.get_selected()     if selected != -1:         object_id = self._status_list.GetItemData(selected)         download_item = self._download_list.get_item(object_id)         url = download_item.url         <MASK>             clipdata = wx.TextDataObject()             clipdata.SetText(url)             wx.TheClipboard.Open()             wx.TheClipboard.SetData(clipdata)             wx.TheClipboard.Close()",if not wx . TheClipboard . IsOpened ( ) :,if url :,3.361830360737634,0.0,0.0
"def escape2null(text):     """"""Return a string with escape-backslashes converted to nulls.""""""     parts = []     start = 0     while True:         found = text.find(""\\"", start)         <MASK>             parts.append(text[start:])             return """".join(parts)         parts.append(text[start:found])         parts.append(""\x00"" + text[found + 1 : found + 2])         start = found + 2  # skip character after escape",if found == - 1 :,if found == - 1 :,100.00000000000004,100.00000000000004,1.0
def _process_inner_views(self):     for view in self.baseviews:         for inner_class in view.get_uninit_inner_views():             for v in self.baseviews:                 <MASK>                     view.get_init_inner_views().append(v),"if isinstance ( v , inner_class ) and v not in view . get_init_inner_views ( ) :",if inner_class == view :,2.9196763911668104,2.9196763911668104,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             self.set_url(d.getPrefixedString())             continue         <MASK>             self.set_app_version_id(d.getPrefixedString())             continue         if tt == 26:             self.set_method(d.getPrefixedString())             continue         if tt == 34:             self.set_queue(d.getPrefixedString())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 18 :,if tt == 11 :,53.7284965911771,53.7284965911771,0.0
"def test_sample_output():     comment = ""SAMPLE OUTPUT""     skip_files = [""__init__.py""]     errors = []     for _file in sorted(MODULE_PATH.iterdir()):         if _file.suffix == "".py"" and _file.name not in skip_files:             with _file.open() as f:                 <MASK>                     errors.append((comment, _file))     if errors:         line = ""Missing sample error(s) detected!\n\n""         for error in errors:             line += ""`{}` is not in module `{}`\n"".format(*error)         print(line[:-1])         assert False",if comment not in f . read ( ) :,if not f . read ( ) :,56.938938762987256,56.938938762987256,0.0
"def _get_planner(name, path, source):     for klass in _planners:         <MASK>             LOG.debug(""%r accepted %r (filename %r)"", klass, name, path)             return klass         LOG.debug(""%r rejected %r"", klass, name)     raise ansible.errors.AnsibleError(NO_METHOD_MSG + repr(invocation))","if klass . detect ( path , source ) :",if source . is_method ( name ) :,11.044795567078939,11.044795567078939,0.0
"def _to_string_infix(self, ostream, idx, verbose):     if verbose:         ostream.write("" , "")     else:         hasConst = not (             self._const.__class__ in native_numeric_types and self._const == 0         )         <MASK>             idx -= 1         _l = self._coef[id(self._args[idx])]         _lt = _l.__class__         if _lt is _NegationExpression or (_lt in native_numeric_types and _l < 0):             ostream.write("" - "")         else:             ostream.write("" + "")",if hasConst :,if hasConst :,100.00000000000004,0.0,1.0
"def cluster_info_query(self):     if self._major_version >= 90600:         extra = (             "", CASE WHEN latest_end_lsn IS NULL THEN NULL ELSE received_tli END,""             "" slot_name, conninfo FROM pg_catalog.pg_stat_get_wal_receiver()""         )         <MASK>             extra = ""timeline_id"" + extra + "", pg_catalog.pg_control_checkpoint()""         else:             extra = ""0"" + extra     else:         extra = ""0, NULL, NULL, NULL""     return (""SELECT "" + self.TL_LSN + "", {2}"").format(         self.wal_name, self.lsn_name, extra     )","if self . role == ""standby_leader"" :",if self . _major_version >= 90600 :,15.148694266083963,15.148694266083963,0.0
"def __init__(self, *args, **kwargs):     self.country = kwargs.pop(""country"")     self.fields_needed = kwargs.pop(""fields_needed"", [])     super(DynamicManagedAccountForm, self).__init__(*args, **kwargs)     # build our form using the country specific fields and falling     # back to our default set     for f in self.fields_needed:         <MASK>  # pragma: no branch             field_name, field = FIELDS_BY_COUNTRY[self.country][f]             self.fields[field_name] = field","if f in FIELDS_BY_COUNTRY . get ( self . country , { } ) :",if f not in FIELDS_BY_COUNTRY :,24.345633712861616,24.345633712861616,0.0
"def delete_map(self, query=None):     query_map = self.interpolated_map(query=query)     for alias, drivers in six.iteritems(query_map.copy()):         for driver, vms in six.iteritems(drivers.copy()):             for vm_name, vm_details in six.iteritems(vms.copy()):                 <MASK>                     query_map[alias][driver].pop(vm_name)             if not query_map[alias][driver]:                 query_map[alias].pop(driver)         if not query_map[alias]:             query_map.pop(alias)     return query_map","if vm_details == ""Absent"" :",if vm_details . vm_details :,28.24099048856542,28.24099048856542,0.0
"def on_strokes_edited(self):     strokes = self._strokes()     if strokes:         translation = self._engine.raw_lookup(strokes)         <MASK>             fmt = _(""{strokes} maps to {translation}"")         else:             fmt = _(""{strokes} is not in the dictionary"")         info = self._format_label(fmt, (strokes,), translation)     else:         info = """"     self.strokes_info.setText(info)",if translation is not None :,if translation :,23.174952587773145,0.0,0.0
"def release(self):     tid = _thread.get_ident()     with self.lock:         if self.owner != tid:             raise RuntimeError(""cannot release un-acquired lock"")         assert self.count > 0         self.count -= 1         <MASK>             self.owner = None             if self.waiters:                 self.waiters -= 1                 self.wakeup.release()",if self . count == 0 :,if self . count == 0 :,100.00000000000004,100.00000000000004,1.0
"def _cat_blob(self, gcs_uri):     """""":py:meth:`cat_file`, minus decompression.""""""     blob = self._get_blob(gcs_uri)     if not blob:         return  # don't cat nonexistent files     start = 0     while True:         end = start + _CAT_CHUNK_SIZE         try:             chunk = blob.download_as_string(start=start, end=end)         except google.api_core.exceptions.RequestRangeNotSatisfiable:             return         yield chunk         <MASK>             return         start = end",if len ( chunk ) < _CAT_CHUNK_SIZE :,if chunk is None :,2.8383688870107915,2.8383688870107915,0.0
"def device_iter(**kwargs):     for dev in backend.enumerate_devices():         d = Device(dev, backend)         tests = (val == _try_getattr(d, key) for key, val in kwargs.items())         <MASK>             yield d",if _interop . _all ( tests ) and ( custom_match is None or custom_match ( d ) ) :,if tests :,0.025929877904726596,0.0,0.0
"def _get_vtkjs(self):     if self._vtkjs is None and self.object is not None:         if isinstance(self.object, string_types) and self.object.endswith("".vtkjs""):             if isfile(self.object):                 with open(self.object, ""rb"") as f:                     vtkjs = f.read()             else:                 data_url = urlopen(self.object)                 vtkjs = data_url.read()         <MASK>             vtkjs = self.object.read()         self._vtkjs = vtkjs     return self._vtkjs","elif hasattr ( self . object , ""read"" ) :","if isinstance ( vtkjs , list ) :",7.966506956353643,7.966506956353643,0.0
"def _execute_with_error(command, error, message):     try:         cli.invocation = cli.invocation_cls(             cli_ctx=cli,             parser_cls=cli.parser_cls,             commands_loader_cls=cli.commands_loader_cls,             help_cls=cli.help_cls,         )         cli.invocation.execute(command.split())     except CLIError as ex:         <MASK>             raise AssertionError(                 ""{}\nExpected: {}\nActual: {}"".format(message, error, ex)             )         return     except Exception as ex:         raise ex     raise AssertionError(""exception not raised for '{0}'"".format(message))",if error not in str ( ex ) :,if error :,8.525588607164655,0.0,0.0
"def ray_intersection(self, p, line):     p = Vector(center(line.sites))     min_r = BIG_FLOAT     nearest = None     for v_i, v_j in self.edges:         bound = LineEquation2D.from_two_points(v_i, v_j)         intersection = bound.intersect_with_line(line)         if intersection is not None:             r = (p - intersection).length             # info(""INT: [%s - %s] X [%s] => %s (%s)"", v_i, v_j, line, intersection, r)             <MASK>                 nearest = intersection                 min_r = r     return nearest",if r < min_r :,if r < min_r :,100.00000000000004,100.00000000000004,1.0
"def CalculateChecksum(data):     # The checksum is just a sum of all the bytes. I swear.     if isinstance(data, bytearray):         total = sum(data)     elif isinstance(data, bytes):         <MASK>             # Python 2 bytes (str) index as single-character strings.             total = sum(map(ord, data))         else:             # Python 3 bytes index as numbers (and PY2 empty strings sum() to 0)             total = sum(data)     else:         # Unicode strings (should never see?)         total = sum(map(ord, data))     return total & 0xFFFFFFFF","if data and isinstance ( data [ 0 ] , bytes ) :","if isinstance ( data , int ) :",15.049981051832416,15.049981051832416,0.0
"def __mul__(self, other: Union[""Tensor"", float]) -> ""Tensor"":     if isinstance(other, Tensor):         <MASK>             errstr = (                 f""Given backens are inconsistent. Found '{self.backend.name}'""                 f""and '{other.backend.name}'""             )             raise ValueError(errstr)         other = other.array     array = self.backend.multiply(self.array, other)     return Tensor(array, backend=self.backend)",if self . backend . name != other . backend . name :,if self . backend . name != other . backend . name :,100.00000000000004,100.00000000000004,1.0
"def next_item(self, direction):     """"""Selects next menu item, based on self._direction""""""     start, i = -1, 0     try:         start = self.items.index(self._selected)         i = start + direction     except:         pass     while True:         <MASK>             # Cannot find valid menu item             self.select(start)             break         if i >= len(self.items):             i = 0             continue         if i < 0:             i = len(self.items) - 1             continue         if self.select(i):             break         i += direction         if start < 0:             start = 0",if i == start :,if start < 0 :,11.51015341649912,11.51015341649912,0.0
"def resolve_none(self, data):     # replace None to '_'     for tok_idx in range(len(data)):         for feat_idx in range(len(data[tok_idx])):             <MASK>                 data[tok_idx][feat_idx] = ""_""     return data",if data [ tok_idx ] [ feat_idx ] is None :,if data [ tok_idx ] [ feat_idx ] == None :,76.70387248467661,76.70387248467661,0.0
"def distinct(expr, *on):     fields = frozenset(expr.fields)     _on = []     append = _on.append     for n in on:         if isinstance(n, Field):             if n._child.isidentical(expr):                 n = n._name             else:                 raise ValueError(""{0} is not a field of {1}"".format(n, expr))         if not isinstance(n, _strtypes):             raise TypeError(""on must be a name or field, not: {0}"".format(n))         <MASK>             raise ValueError(""{0} is not a field of {1}"".format(n, expr))         append(n)     return Distinct(expr, tuple(_on))",elif n not in fields :,if not n . _child . isidentical ( expr ) :,4.456882760699063,4.456882760699063,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         <MASK>             length = d.getVarInt32()             tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)             d.skip(length)             self.mutable_cost().TryMerge(tmp)             continue         if tt == 24:             self.add_version(d.getVarInt64())             continue         if tt == 0:             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 10 :,if tt == 1 :,53.7284965911771,53.7284965911771,0.0
"def func_std_string(func_name):  # match what old profile produced     if func_name[:2] == (""~"", 0):         # special case for built-in functions         name = func_name[2]         <MASK>             return ""{%s}"" % name[1:-1]         else:             return name     else:         return ""%s:%d(%s)"" % func_name","if name . startswith ( ""<"" ) and name . endswith ( "">"" ) :","if name [ 0 ] == ""~"" :",4.5544734701106,4.5544734701106,0.0
"def f():     try:         # Intra-buffer read then buffer-flushing read         for n in cycle([1, 19]):             s = bufio.read(n)             <MASK>                 break             # list.append() is atomic             results.append(s)     except Exception as e:         errors.append(e)         raise",if not s :,if s is None :,14.058533129758727,14.058533129758727,0.0
"def stop(self):     # Try to shut the connection down, but if we get any sort of     # errors, go ahead and ignore them.. as we're shutting down anyway     try:         self.rpcserver.stop()         if self.backend_rpcserver:             self.backend_rpcserver.stop()         <MASK>             self.cluster_rpcserver.stop()     except Exception:         pass     if self.coordination:         try:             coordination.COORDINATOR.stop()         except Exception:             pass     super(Service, self).stop(graceful=True)",if self . cluster_rpcserver :,if self . cluster_rpcserver :,100.00000000000004,100.00000000000004,1.0
"def download(cls, architecture, path=""./""):     if cls.sanity_check(architecture):         architecture_file = download_file(             cls.architecture_map[architecture], directory=path         )         <MASK>             return None         print(""Coreml model {} is saved in [{}]"".format(architecture, path))         return architecture_file     else:         return None",if not architecture_file :,if architecture_file is None :,27.77619034011791,27.77619034011791,0.0
"def opps_output_converter(kpt_list):     kpts = []     mpii_keys = to_opps_converter.keys()     for mpii_idx in range(0, 16):         <MASK>             model_idx = to_opps_converter[mpii_idx]             x, y = kpt_list[model_idx]             if x < 0 or y < 0:                 kpts += [0.0, 0.0, -1.0]             else:                 kpts += [x, y, 1.0]         else:             kpts += [0.0, 0.0, -1.0]     return kpts",if mpii_idx in mpii_keys :,if mpii_keys [ mpii_idx ] == mpii_idx :,20.68720601025941,20.68720601025941,0.0
"def _get_headers(self, headers=None):     request_headers = headers or {}     # Auth headers if access_token is present     if self._client.client.config:         config = self._client.client.config         if ""Authorization"" not in request_headers and config.token:             request_headers.update(                 {                     ""Authorization"": ""{} {}"".format(                         config.authentication_type, config.token                     )                 }             )         <MASK>             request_headers.update({config.header: config.header_service})     return request_headers",if config . header and config . header_service :,if config . header_service :,56.47181220077595,56.47181220077595,0.0
"def get_last_traded_prices(cls, trading_pairs: List[str]) -> Dict[str, float]:     results = dict()     async with aiohttp.ClientSession() as client:         resp = await client.get(f""{constants.REST_URL}/tickers"")         resp_json = await resp.json()         for trading_pair in trading_pairs:             resp_record = [                 o                 for o in resp_json                 <MASK>             ][0]             results[trading_pair] = float(resp_record[""price""])     return results","if o [ ""symbol"" ] == convert_to_exchange_trading_pair ( trading_pair )","if o [ ""price"" ] > 0 :",8.513729523394375,8.513729523394375,0.0
"def reset_two_factor_hotp():     uid = request.form[""uid""]     otp_secret = request.form.get(""otp_secret"", None)     if otp_secret:         user = Journalist.query.get(uid)         <MASK>             return render_template(""admin_edit_hotp_secret.html"", uid=uid)         db.session.commit()         return redirect(url_for(""admin.new_user_two_factor"", uid=uid))     else:         return render_template(""admin_edit_hotp_secret.html"", uid=uid)","if not validate_hotp_secret ( user , otp_secret ) :",if user :,0.7268566109861576,0.0,0.0
"def ctx_for_video(self, vurl):     ""Get a context dict for a given video URL""     ctx = self.get_context_dict()     for portal, match, context_fn in self.PORTALS:         <MASK>             try:                 ctx.update(context_fn(vurl))                 ctx[""portal""] = portal                 break             except AttributeError:                 continue     return ctx",if match . search ( vurl ) :,if match :,11.898417391331403,0.0,0.0
"def get(self):     name = request.args.get(""filename"")     if name is not None:         opts = dict()         opts[""type""] = ""episode""         result = guessit(name, options=opts)         res = dict()         if ""episode"" in result:             res[""episode""] = result[""episode""]         else:             res[""episode""] = 0         if ""season"" in result:             res[""season""] = result[""season""]         else:             res[""season""] = 0         <MASK>             res[""subtitle_language""] = str(result[""subtitle_language""])         return jsonify(data=res)     else:         return """", 400","if ""subtitle_language"" in result :","if ""subtitle_language"" in result :",100.00000000000004,100.00000000000004,1.0
"def package_files(package_path, directory_name):     paths = []     directory_path = os.path.join(package_path, directory_name)     for (path, directories, filenames) in os.walk(directory_path):         relative_path = os.path.relpath(path, package_path)         for filename in filenames:             <MASK>                 continue             paths.append(os.path.join(relative_path, filename))     return paths","if filename [ 0 ] == ""."" :",if not relative_path :,4.194930905450255,4.194930905450255,0.0
"def parse_simple(d, data):     units = {}     for v in data[d]:         key = v[""name""]         if not key:             continue         key_to_insert = make_key(key)         <MASK>             index = 2             tmp = f""{key_to_insert}_{index}""             while tmp in units:                 index += 1                 tmp = f""{key_to_insert}_{index}""             key_to_insert = tmp         units[key_to_insert] = v[""id""]     return units",if key_to_insert in units :,"if v [ ""id"" ] == ""id"" :",3.673526562988939,3.673526562988939,0.0
"def parse_clademodelc(branch_type_no, line_floats, site_classes):     """"""Parse results specific to the clade model C.""""""     if not site_classes or len(line_floats) == 0:         return     for n in range(len(line_floats)):         <MASK>             site_classes[n][""branch types""] = {}         site_classes[n][""branch types""][branch_type_no] = line_floats[n]     return site_classes","if site_classes [ n ] . get ( ""branch types"" ) is None :",if n not in site_classes :,7.003939561149077,7.003939561149077,0.0
"def track_modules(self, *modules):     """"""Add module names to the tracked list.""""""     already_tracked = self.session.GetParameter(""autodetect_build_local_tracked"") or []     needed = set(modules)     if not needed.issubset(already_tracked):         needed.update(already_tracked)         with self.session as session:             session.SetParameter(""autodetect_build_local_tracked"", needed)             for module_name in modules:                 module_obj = self.GetModuleByName(module_name)                 <MASK>                     # Clear the module's profile. This will force it to                     # reload a new profile.                     module_obj.profile = None",if module_obj :,if module_obj :,100.00000000000004,100.00000000000004,1.0
"def set_job_on_hold(self, value, blocking=True):     trigger = False     # don't run any locking code beyond this...     if not self._job_on_hold.acquire(blocking=blocking):         return False     try:         <MASK>             self._job_on_hold.set()         else:             self._job_on_hold.clear()             if self._job_on_hold.counter == 0:                 trigger = True     finally:         self._job_on_hold.release()     # locking code is now safe to run again     if trigger:         self._continue_sending()     return True",if value :,if value :,100.00000000000004,0.0,1.0
"def moveToThreadNext(self):     """"""Move a position to threadNext position.""""""     p = self     if p.v:         <MASK>             p.moveToFirstChild()         elif p.hasNext():             p.moveToNext()         else:             p.moveToParent()             while p:                 if p.hasNext():                     p.moveToNext()                     break  # found                 p.moveToParent()             # not found.     return p",if p . v . children :,if p . hasFirstChild ( ) :,26.269098944241588,26.269098944241588,0.0
"def best_image(width, height):     # A heuristic for finding closest sized image to required size.     image = images[0]     for img in images:         <MASK>             # Exact match always used             return img         elif img.width >= width and img.width * img.height > image.width * image.height:             # At least wide enough, and largest area             image = img     return image",if img . width == width and img . height == height :,if img . width == width :,36.24372413507827,36.24372413507827,0.0
"def _check_input_types(self):     if len(self.base_features) == 0:         return True     input_types = self.primitive.input_types     if input_types is not None:         <MASK>             input_types = [input_types]         for t in input_types:             zipped = list(zip(t, self.base_features))             if all([issubclass(f.variable_type, v) for v, f in zipped]):                 return True     else:         return True     return False",if type ( input_types [ 0 ] ) != list :,"if isinstance ( input_types , list ) :",20.940395623289575,20.940395623289575,0.0
"def get_result(self):     result_list = []     exc_info = None     for f in self.children:         try:             result_list.append(f.get_result())         except Exception as e:             if exc_info is None:                 exc_info = sys.exc_info()             else:                 <MASK>                     app_log.error(""Multiple exceptions in yield list"", exc_info=True)     if exc_info is not None:         raise_exc_info(exc_info)     if self.keys is not None:         return dict(zip(self.keys, result_list))     else:         return list(result_list)","if not isinstance ( e , self . quiet_exceptions ) :",if e . args :,3.0500258614052496,3.0500258614052496,0.0
"def _update_learning_params(self):     model = self.model     hparams = self.hparams     fd = self.runner.feed_dict     step_num = self.step_num     if hparams.model_type == ""resnet_tf"":         if step_num < hparams.lrn_step:             lrn_rate = hparams.mom_lrn         <MASK>             lrn_rate = hparams.mom_lrn / 10         elif step_num < 35000:             lrn_rate = hparams.mom_lrn / 100         else:             lrn_rate = hparams.mom_lrn / 1000         fd[model.lrn_rate] = lrn_rate",elif step_num < 30000 :,if step_num < 10000 :,43.47208719449914,43.47208719449914,0.0
"def topic_exists(self, arn):     response = self._conn.get_all_topics()     topics = response[""ListTopicsResponse""][""ListTopicsResult""][""Topics""]     current_topics = []     if len(topics) > 0:         for topic in topics:             topic_arn = topic[""TopicArn""]             current_topics.append(topic_arn)         <MASK>             return True     return False",if arn in current_topics :,if current_topics :,47.39878501170795,47.39878501170795,0.0
"def assertStartsWith(self, expectedPrefix, text, msg=None):     if not text.startswith(expectedPrefix):         <MASK>             text = text[: len(expectedPrefix) + 5] + ""...""         standardMsg = ""{} not found at the start of {}"".format(             repr(expectedPrefix), repr(text)         )         self.fail(self._formatMessage(msg, standardMsg))",if len ( expectedPrefix ) + 5 < len ( text ) :,if not text . startswith ( expectedPrefix ) :,14.975985500507548,14.975985500507548,0.0
"def validate_memory(self, value):     for k, v in value.viewitems():         <MASK>  # use NoneType to unset a value             continue         if not re.match(PROCTYPE_MATCH, k):             raise serializers.ValidationError(""Process types can only contain [a-z]"")         if not re.match(MEMLIMIT_MATCH, str(v)):             raise serializers.ValidationError(                 ""Limit format: <number><unit>, where unit = B, K, M or G""             )     return value",if v is None :,if v is None :,100.00000000000004,100.00000000000004,1.0
"def open(self) -> ""KeyValueJsonDb"":     """"""Create a new data base or open existing one""""""     if os.path.exists(self._name):         <MASK>             raise IOError(""%s exists and is not a file"" % self._name)         try:             with open(self._name, ""r"") as _in:                 self.set_records(json.load(_in))         except json.JSONDecodeError:             # file corrupted, reset it.             self.commit()     else:         # make sure path exists         mkpath(os.path.dirname(self._name))         self.commit()     return self",if not os . path . isfile ( self . _name ) :,if not os . path . isfile ( self . _name ) :,100.00000000000004,100.00000000000004,1.0
"def _calculate(self):     before = self.before.data     after = self.after.data     self.deleted = {}     self.updated = {}     self.created = after.copy()     for path, f in before.items():         <MASK>             self.deleted[path] = f             continue         del self.created[path]         if f.mtime < after[path].mtime:             self.updated[path] = after[path]",if path not in after :,if f . mtime < before [ path ] . mtime :,4.456882760699063,4.456882760699063,0.0
"def cache_sqs_queues_across_accounts() -> bool:     function: str = f""{__name__}.{sys._getframe().f_code.co_name}""     # First, get list of accounts     accounts_d: list = async_to_sync(get_account_id_to_name_mapping)()     # Second, call tasks to enumerate all the roles across all accounts     for account_id in accounts_d.keys():         <MASK>             cache_sqs_queues_for_account.delay(account_id)         else:             if account_id in config.get(""celery.test_account_ids"", []):                 cache_sqs_queues_for_account.delay(account_id)     stats.count(f""{function}.success"")     return True","if config . get ( ""environment"" ) == ""prod"" :","if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :",17.855149299161603,17.855149299161603,0.0
"def remove(self, path, config=None, error_on_path=False, defaults=None):     if not path:         if error_on_path:             raise NoSuchSettingsPath()         return     if config is not None or defaults is not None:         <MASK>             config = self._config         if defaults is None:             defaults = dict(self._map.parents)         chain = HierarchicalChainMap(config, defaults)     else:         chain = self._map     try:         chain.del_by_path(path)         self._mark_dirty()     except KeyError:         if error_on_path:             raise NoSuchSettingsPath()         pass",if config is None :,if config is None :,100.00000000000004,100.00000000000004,1.0
"def PopulateProjectId(project_id=None):     """"""Fills in a project_id from the boto config file if one is not provided.""""""     if not project_id:         default_id = boto.config.get_value(""GSUtil"", ""default_project_id"")         <MASK>             raise ProjectIdException(""MissingProjectId"")         return default_id     return project_id",if not default_id :,if not default_id :,100.00000000000004,100.00000000000004,1.0
"def set(self, name, value):     with self._object_cache_lock:         old_value = self._object_cache.get(name)         ret = not old_value or int(old_value.metadata.resource_version) < int(             value.metadata.resource_version         )         <MASK>             self._object_cache[name] = value     return ret, old_value",if ret :,if ret :,100.00000000000004,0.0,1.0
"def remove(self, url):     try:         i = self.items.index(url)     except (ValueError, IndexError):         pass     else:         was_selected = i in self.selectedindices()         self.list.delete(i)         del self.items[i]         if not self.items:             self.mp.hidepanel(self.name)         <MASK>             if i >= len(self.items):                 i = len(self.items) - 1             self.list.select_set(i)",elif was_selected :,if was_selected :,66.87403049764218,66.87403049764218,0.0
"def add_directory_csv_files(dir_path, paths=None):     if not paths:         paths = []     for p in listdir(dir_path):         path = join(dir_path, p)         <MASK>             # call recursively for each dir             paths = add_directory_csv_files(path, paths)         elif isfile(path) and path.endswith("".csv""):             # add every file to the list             paths.append(path)     return paths",if isdir ( path ) :,"if isdir ( path ) and path . endswith ( "".csv"" ) :",25.33654946448646,25.33654946448646,0.0
"def _get_client(rp_mapping, resource_provider):     for key, value in rp_mapping.items():         <MASK>             if isinstance(value, dict):                 return GeneralPrivateEndpointClient(                     key,                     value[""api_version""],                     value[""support_list_or_not""],                     value[""resource_get_api_version""],                 )             return value()     raise CLIError(         ""Resource type must be one of {}"".format("", "".join(rp_mapping.keys()))     )",if str . lower ( key ) == str . lower ( resource_provider ) :,if key in resource_provider . resource_types :,9.124813543640702,9.124813543640702,0.0
"def compute_rule_hash(self, rule):     buf = ""%d-%d-%s-"" % (         rule.get(""FromPort"", 0) or 0,         rule.get(""ToPort"", 0) or 0,         rule.get(""IpProtocol"", ""-1"") or ""-1"",     )     for a, ke in self.RULE_ATTRS:         <MASK>             continue         ev = [e[ke] for e in rule[a]]         ev.sort()         for e in ev:             buf += ""%s-"" % e     # mask to generate the same numeric value across all Python versions     return zlib.crc32(buf.encode(""ascii"")) & 0xFFFFFFFF",if a not in rule :,if a == ke :,17.965205598154213,17.965205598154213,0.0
"def analysis_sucess_metrics(analysis_time: float, allow_exception=False):     try:         anchore_engine.subsys.metrics.counter_inc(name=""anchore_analysis_success"")         anchore_engine.subsys.metrics.histogram_observe(             ""anchore_analysis_time_seconds"",             analysis_time,             buckets=ANALYSIS_TIME_SECONDS_BUCKETS,             status=""success"",         )     except:         <MASK>             raise         else:             logger.exception(                 ""Unexpected exception during metrics update for a successful analysis. Swallowing error and continuing""             )",if allow_exception :,if allow_exception :,100.00000000000004,100.00000000000004,1.0
"def decide_file_icon(file):     if file.state == File.ERROR:         return FileItem.icon_error     elif isinstance(file.parent, Track):         <MASK>             return FileItem.icon_saved         elif file.state == File.PENDING:             return FileItem.match_pending_icons[int(file.similarity * 5 + 0.5)]         else:             return FileItem.match_icons[int(file.similarity * 5 + 0.5)]     elif file.state == File.PENDING:         return FileItem.icon_file_pending     else:         return FileItem.icon_file",if file . state == File . NORMAL :,if file . state == File . SAVE :,78.25422900366438,78.25422900366438,0.0
"def deleteMenu(self, menuName):     try:         menu = self.getMenu(menuName)         <MASK>             self.destroy(menu)             self.destroyMenu(menuName)         else:             g.es(""can't delete menu:"", menuName)     except Exception:         g.es(""exception deleting"", menuName, ""menu"")         g.es_exception()",if menu :,if menu :,100.00000000000004,0.0,1.0
"def parser(cls, buf):     (type_, code, csum) = struct.unpack_from(cls._PACK_STR, buf)     msg = cls(type_, code, csum)     offset = cls._MIN_LEN     if len(buf) > offset:         cls_ = cls._ICMPV6_TYPES.get(type_, None)         <MASK>             msg.data = cls_.parser(buf, offset)         else:             msg.data = buf[offset:]     return msg, None, None",if cls_ :,if cls_ :,100.00000000000004,100.00000000000004,1.0
"def _load_dataset_area(self, dsid, file_handlers, coords):     """"""Get the area for *dsid*.""""""     try:         return self._load_area_def(dsid, file_handlers)     except NotImplementedError:         if any(x is None for x in coords):             logger.warning(""Failed to load coordinates for '{}'"".format(dsid))             return None         area = self._make_area_from_coords(coords)         <MASK>             logger.debug(""No coordinates found for %s"", str(dsid))         return area",if area is None :,if area is None :,100.00000000000004,100.00000000000004,1.0
"def __getattr__(self, name):     if Popen.verbose:         sys.stdout.write(""Getattr: %s..."" % name)     if name in Popen.__slots__:         return object.__getattribute__(self, name)     else:         if self.popen is not None:             if Popen.verbose:                 print(""from Popen"")             return getattr(self.popen, name)         else:             <MASK>                 return self.emu_wait             else:                 raise Exception(""subprocess emulation: not implemented: %s"" % name)","if name == ""wait"" :",if self . emulation_wait is not None :,5.522397783539471,5.522397783539471,0.0
"def update(self, time_delta):     super().update(time_delta)     n = self.menu.selected_option     if n == self.last:         return     self.last = n     s = """"     for i in range(len(self.files)):         <MASK>             for l in open(self.files[i][1]):                 x = l.strip()                 if len(x) > 1 and x[0] == ""#"":                     x = ""<b><u>"" + x[1:] + "" </u></b>""                 s += x + ""<br>""     self.set_text(s)",if self . files [ i ] [ 0 ] == n :,if i == n :,14.823156396438126,14.823156396438126,0.0
"def wrapper(*args, **kwargs):     list_args, empty = _apply_defaults(func, args, kwargs)     if len(dimensions) > len(list_args):         raise TypeError(             ""%s takes %i parameters, but %i dimensions were passed""             % (func.__name__, len(list_args), len(dimensions))         )     for dim, value in zip(dimensions, list_args):         if dim is None:             continue         <MASK>             val_dim = ureg.get_dimensionality(value)             raise DimensionalityError(value, ""a quantity of"", val_dim, dim)     return func(*args, **kwargs)",if not ureg . Quantity ( value ) . check ( dim ) :,if not empty :,2.9021429580323628,2.9021429580323628,0.0
"def _check(self, name, size=None, *extra):     func = getattr(imageop, name)     for height in VALUES:         for width in VALUES:             strlen = abs(width * height)             if size:                 strlen *= size             <MASK>                 data = ""A"" * strlen             else:                 data = AAAAA             if size:                 arguments = (data, size, width, height) + extra             else:                 arguments = (data, width, height) + extra             try:                 func(*arguments)             except (ValueError, imageop.error):                 pass",if strlen < MAX_LEN :,if strlen > 0 :,15.848738972120703,15.848738972120703,0.0
"def wait_send_all_might_not_block(self) -> None:     with self._send_conflict_detector:         <MASK>             raise trio.ClosedResourceError(""file was already closed"")         try:             await trio.lowlevel.wait_writable(self._fd_holder.fd)         except BrokenPipeError as e:             # kqueue: raises EPIPE on wait_writable instead             # of sending, which is annoying             raise trio.BrokenResourceError from e",if self . _fd_holder . closed :,if self . _fd_holder . fd is not None :,57.60844201603898,57.60844201603898,0.0
"def parse_win_proxy(val):     proxies = []     for p in val.split("";""):         if ""="" in p:             tab = p.split(""="", 1)             <MASK>                 tab[0] = ""SOCKS4""             proxies.append(                 (tab[0].upper(), tab[1], None, None)             )  # type, addr:port, username, password         else:             proxies.append((""HTTP"", p, None, None))     return proxies","if tab [ 0 ] == ""socks"" :","if tab [ 0 ] == ""SOCKS4"" :",74.19446627365011,74.19446627365011,0.0
"def _super_function(args):     passed_class, passed_self = args.get_arguments([""type"", ""self""])     if passed_self is None:         return passed_class     else:         # pyclass = passed_self.get_type()         pyclass = passed_class         if isinstance(pyclass, pyobjects.AbstractClass):             supers = pyclass.get_superclasses()             <MASK>                 return pyobjects.PyObject(supers[0])         return passed_self",if supers :,if len ( supers ) == 1 :,6.27465531099474,6.27465531099474,0.0
"def update_output_mintime(job):     try:         return output_mintime[job]     except KeyError:         for job_ in chain([job], self.depending[job]):             try:                 t = output_mintime[job_]             except KeyError:                 t = job_.output_mintime             <MASK>                 output_mintime[job] = t                 return         output_mintime[job] = None",if t is not None :,if t is not None :,100.00000000000004,100.00000000000004,1.0
"def get_list_of_strings_to_mongo_objects(self, notifications_list=None):     result = []     if len(notifications_list) > 0:         for x in notifications_list:             split_provider_id = x.split("":"")  # email:id             if len(split_provider_id) == 2:                 _id = split_provider_id[1]                 cursor = self.get_by_id(_id)                 <MASK>  # Append if exists                     result.append(cursor)     return result",if cursor :,if cursor :,100.00000000000004,0.0,1.0
"def stop(self):     with self.lock:         <MASK>             return         self.task_queue.put(None)         self.result_queue.put(None)         process = self.process         self.process = None         self.task_queue = None         self.result_queue = None     process.join(timeout=0.1)     if process.exitcode is None:         os.kill(process.pid, signal.SIGKILL)         process.join()",if not self . process :,if not self . process :,100.00000000000004,100.00000000000004,1.0
"def on_api_command(self, command, data):     if command == ""select"":         if not Permissions.PLUGIN_ACTION_COMMAND_PROMPT_INTERACT.can():             return flask.abort(403, ""Insufficient permissions"")         <MASK>             return flask.abort(409, ""No active prompt"")         choice = data[""choice""]         if not isinstance(choice, int) or not self._prompt.validate_choice(choice):             return flask.abort(                 400, ""{!r} is not a valid value for choice"".format(choice)             )         self._answer_prompt(choice)",if self . _prompt is None :,if not self . _prompt . active :,33.03164318013809,33.03164318013809,0.0
"def application_openFiles_(self, nsapp, filenames):     # logging.info('[osx] file open')     # logging.info('[osx] file : %s' % (filenames))     for filename in filenames:         logging.info(""[osx] receiving from macOS : %s"", filename)         <MASK>             if sabnzbd.filesystem.get_ext(filename) in VALID_ARCHIVES + VALID_NZB_FILES:                 sabnzbd.add_nzbfile(filename, keep=True)",if os . path . exists ( filename ) :,if nsapp . is_macOS ( ) :,10.729256185679601,10.729256185679601,0.0
"def test_error_through_destructor(self):     # Test that the exception state is not modified by a destructor,     # even if close() fails.     rawio = self.CloseFailureIO()     with support.catch_unraisable_exception() as cm:         with self.assertRaises(AttributeError):             self.tp(rawio).xyzzy         <MASK>             self.assertIsNone(cm.unraisable)         elif cm.unraisable is not None:             self.assertEqual(cm.unraisable.exc_type, OSError)",if not IOBASE_EMITS_UNRAISABLE :,if cm . unraisable is not None :,7.267884212102741,7.267884212102741,0.0
"def http_wrapper(self, url, postdata={}):     try:         <MASK>             f = urllib.urlopen(url, postdata)         else:             f = urllib.urlopen(url)         response = f.read()     except:         import traceback         import logging, sys         cla, exc, tb = sys.exc_info()         logging.error(url)         if postdata:             logging.error(""with post data"")         else:             logging.error(""without post data"")         logging.error(exc.args)         logging.error(traceback.format_tb(tb))         response = """"     return response",if postdata != { } :,if postdata :,16.605579150202516,0.0,0.0
"def check_single_file(fn, fetchuri):     """"""Determine if a single downloaded file is something we can't handle""""""     with open(fn, ""r"", errors=""surrogateescape"") as f:         <MASK>             logger.error(                 'Fetching ""%s"" returned a single HTML page - check the URL is correct and functional'                 % fetchuri             )             sys.exit(1)","if ""<html"" in f . read ( 100 ) . lower ( ) :",if not f . read ( ) :,16.23262336791749,16.23262336791749,0.0
"def update_properties(self, update_dict):     signed_attribute_changed = False     for k, value in update_dict.items():         if getattr(self, k) != value:             setattr(self, k, value)             signed_attribute_changed = signed_attribute_changed or (                 k in self.payload_arguments             )     if signed_attribute_changed:         <MASK>             self.status = UPDATED         self.timestamp = clock.tick()         self.sign()     return self",if self . status != NEW :,if self . status == UPDATED :,38.260294162784454,38.260294162784454,0.0
"def clean_items(event, items, variations):     for item in items:         if event != item.event:             raise ValidationError(_(""One or more items do not belong to this event.""))         if item.has_variations:             <MASK>                 raise ValidationError(                     _(                         ""One or more items has variations but none of these are in the variations list.""                     )                 )",if not any ( var . item == item for var in variations ) :,if variations != item . variations :,5.34741036489421,5.34741036489421,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             length = d.getVarInt32()             tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)             d.skip(length)             self.add_status().TryMerge(tmp)             continue         if tt == 18:             self.add_doc_id(d.getPrefixedString())             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt == 17 :,53.7284965911771,53.7284965911771,0.0
"def connections(self):     # Connections look something like this:     # socket:[102422]     fds = self.open_files     socket = ""socket:[""     result = []     functions = [pwndbg.net.tcp, pwndbg.net.unix, pwndbg.net.netlink]     for fd, path in fds.items():         if socket not in path:             continue         inode = path[len(socket) : -1]         inode = int(inode)         for func in functions:             for x in func():                 <MASK>                     x.fd = fd                     result.append(x)     return tuple(result)",if x . inode == inode :,if x . inode == inode :,100.00000000000004,100.00000000000004,1.0
"def _movement_finished(self):     if self.in_ship_map:         # if the movement somehow stops, the position sticks, and the unit isn't at next_target any more         <MASK>             ship = self.session.world.ship_map.get(self._next_target.to_tuple())             if ship is not None and ship() is self:                 del self.session.world.ship_map[self._next_target.to_tuple()]     super()._movement_finished()",if self . _next_target is not None :,if self . next_target is not None :,71.0866788975034,71.0866788975034,0.0
"def print_addresses(self):     p = 3     tmp_str = ""[""     if self.get_len() >= 7:  # at least one complete IP address         while 1:             if p + 1 == self.get_ptr():                 tmp_str += ""#""             tmp_str += self.get_ip_address(p)             p += 4             <MASK>                 break             else:                 tmp_str += "", ""     tmp_str += ""] ""     if self.get_ptr() % 4:  # ptr field should be a multiple of 4         tmp_str += ""nonsense ptr field: %d "" % self.get_ptr()     return tmp_str",if p >= self . get_len ( ) :,if p == 0 :,7.1018646972849515,7.1018646972849515,0.0
"def source_shapes(self):     """"""Prints debug information about the sources in this provider.""""""     if logger.isEnabledFor(logging.DEBUG):         for i, source in enumerate(self.sources):             <MASK>                 name = ""anonymous""             else:                 name = self.keys[i]             try:                 shape = source.shape()             except NotImplementedError:                 shape = ""N/A""             logger.debug(                 'Data source ""%s"": entries=%s, shape=%s', name, len(source), shape             )",if self . keys is None :,if i == 0 :,8.170609724417774,8.170609724417774,0.0
def swap_actions(actions):     for mutexgroup in mutex_groups:         mutex_actions = mutexgroup._group_actions         <MASK>             # make a best guess as to where we should store the group             targetindex = actions.index(mutexgroup._group_actions[0])             # insert the _ArgumentGroup container             actions[targetindex] = mutexgroup             # remove the duplicated individual actions             actions = [action for action in actions if action not in mutex_actions]     return actions,"if contains_actions ( mutex_actions , actions ) :",if targetindex in actions :,3.466791587270993,3.466791587270993,0.0
"def rec_deps(services, container_by_name, cnt, init_service):     deps = cnt[""_deps""]     for dep in deps.copy():         dep_cnts = services.get(dep)         if not dep_cnts:             continue         dep_cnt = container_by_name.get(dep_cnts[0])         if dep_cnt:             # TODO: avoid creating loops, A->B->A             <MASK>                 continue             new_deps = rec_deps(services, container_by_name, dep_cnt, init_service)             deps.update(new_deps)     return deps","if init_service and init_service in dep_cnt [ ""_deps"" ] :",if not dep_cnt :,3.932742381668659,3.932742381668659,0.0
"def make_dump_list_by_name_list(name_list):     info_list = []     for info_name in name_list:         info = next((x for x in DUMP_LIST if x.info_name == info_name), None)         <MASK>             raise RuntimeError('Unknown info name: ""{}""'.format(info_name))         info_list.append(info)     return info_list",if not info :,if info is None :,14.058533129758727,14.058533129758727,0.0
"def create(self, private=False):     try:         if private:             log.info(""Creating private channel %s."", self)             self._bot.api_call(                 ""conversations.create"", data={""name"": self.name, ""is_private"": True}             )         else:             log.info(""Creating channel %s."", self)             self._bot.api_call(""conversations.create"", data={""name"": self.name})     except SlackAPIResponseError as e:         <MASK>             raise RoomError(f""Unable to create channel. {USER_IS_BOT_HELPTEXT}"")         else:             raise RoomError(e)","if e . error == ""user_is_bot"" :",if e . status == 404 :,12.779458309114789,12.779458309114789,0.0
"def talk(self, words):     if self.writeSentence(words) == 0:         return     r = []     while 1:         i = self.readSentence()         <MASK>             continue         reply = i[0]         attrs = {}         for w in i[1:]:             j = w.find(""="", 1)             if j == -1:                 attrs[w] = """"             else:                 attrs[w[:j]] = w[j + 1 :]         r.append((reply, attrs))         if reply == ""!done"":             return r",if len ( i ) == 0 :,"if i == """" :",12.411264901419441,12.411264901419441,0.0
"def _load_logfile(self, lfn):     enc_key = self.decryption_key_func()     with open(os.path.join(self.logdir, lfn)) as fd:         <MASK>             with DecryptingStreamer(                 fd, mep_key=enc_key, name=""EventLog/DS(%s)"" % lfn             ) as streamer:                 lines = streamer.read()                 streamer.verify(_raise=IOError)         else:             lines = fd.read()         if lines:             for line in lines.splitlines():                 event = Event.Parse(line.strip())                 self._events[event.event_id] = event",if enc_key :,if os . path . isfile ( lfn ) :,4.990049701936832,4.990049701936832,0.0
"def set_ok_port(self, cookie, request):     if cookie.port_specified:         req_port = request_port(request)         if req_port is None:             req_port = ""80""         else:             req_port = str(req_port)         for p in cookie.port.split("",""):             try:                 int(p)             except ValueError:                 debug(""   bad port %s (not numeric)"", p)                 return False             <MASK>                 break         else:             debug(""   request port (%s) not found in %s"", req_port, cookie.port)             return False     return True",if p == req_port :,if req_port == req_port :,53.7284965911771,53.7284965911771,0.0
"def get_attribute_value(self, nodeid, attr):     with self._lock:         self.logger.debug(""get attr val: %s %s"", nodeid, attr)         <MASK>             dv = ua.DataValue()             dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadNodeIdUnknown)             return dv         node = self._nodes[nodeid]         if attr not in node.attributes:             dv = ua.DataValue()             dv.StatusCode = ua.StatusCode(ua.StatusCodes.BadAttributeIdInvalid)             return dv         attval = node.attributes[attr]         if attval.value_callback:             return attval.value_callback()         return attval.value",if nodeid not in self . _nodes :,if nodeid not in self . _nodes :,100.00000000000004,100.00000000000004,1.0
"def data_logging_status(self, trail_name, trail_details, api_client):     for es in api_client.get_event_selectors(TrailName=trail_name)[""EventSelectors""]:         has_wildcard = {             u""Values"": [u""arn:aws:s3:::""],             u""Type"": u""AWS::S3::Object"",         } in es[""DataResources""]         is_logging = trail_details[""IsLogging""]         <MASK>             return True     return False",if has_wildcard and is_logging and self . is_fresh ( trail_details ) :,if has_wildcard :,3.520477365831487,3.520477365831487,0.0
"def pytest_deselected(items):     if sb_config.dashboard:         sb_config.item_count -= len(items)         for item in items:             test_id, display_id = _get_test_ids_(item)             <MASK>                 sb_config._results.pop(test_id)",if test_id in sb_config . _results . keys ( ) :,if display_id :,2.815135668143185,2.815135668143185,0.0
"def _visit(self, func):     fname = func[0]     if fname in self._flags:         if self._flags[fname] == 1:             logger.critical(""Fatal error! network ins not Dag."")             import sys             sys.exit(-1)         else:             return     else:         if fname not in self._flags:             self._flags[fname] = 1         for output in func[3]:             for f in self._orig:                 for input in f[2]:                     <MASK>                         self._visit(f)     self._flags[fname] = 2     self._sorted.insert(0, func)",if output == input :,if input == output :,21.3643503198117,21.3643503198117,0.0
"def printWiki():     firstHeading = False     for m in protocol:         <MASK>             if firstHeading:                 output(""|}"")             __printWikiHeader(m[1], m[2])             firstHeading = True         else:             output(""|-"")             output(                 '| <span style=""white-space:nowrap;""><tt>'                 + m[0]                 + ""</tt></span> || || ""                 + m[1]             )     output(""|}"")","if m [ 0 ] == """" :","if m [ 0 ] == ""heading"" :",74.19446627365011,74.19446627365011,0.0
"def test_getitem(self):     n = 200     d = deque(range(n))     l = list(range(n))     for i in range(n):         d.popleft()         l.pop(0)         <MASK>             d.append(i)             l.append(i)         for j in range(1 - len(l), len(l)):             assert d[j] == l[j]     d = deque(""superman"")     self.assertEqual(d[0], ""s"")     self.assertEqual(d[-1], ""n"")     d = deque()     self.assertRaises(IndexError, d.__getitem__, 0)     self.assertRaises(IndexError, d.__getitem__, -1)",if random . random ( ) < 0.5 :,if i in d :,5.70796903405875,5.70796903405875,0.0
"def get_num(line, char_ptr, num_chars):     char_ptr = char_ptr + 1     numstr = """"     good = ""-.0123456789""     while char_ptr < num_chars:         digit = line[char_ptr]         <MASK>             numstr = numstr + digit             char_ptr = char_ptr + 1         else:             break     return numstr",if good . find ( digit ) != - 1 :,if digit in good :,3.7253099995802206,3.7253099995802206,0.0
"def read_digits(source, start, first_code):     body = source.body     position = start     code = first_code     if code is not None and 48 <= code <= 57:  # 0 - 9         while True:             position += 1             code = char_code_at(body, position)             <MASK>                 break         return position     raise GraphQLSyntaxError(         source,         position,         u""Invalid number, expected digit but got: {}."".format(print_char_code(code)),     )",if not ( code is not None and 48 <= code <= 57 ) :,if code == 0 :,1.9405075296086907,1.9405075296086907,0.0
"def get_aws_metadata(headers, provider=None):     if not provider:         provider = boto.provider.get_default()     metadata_prefix = provider.metadata_prefix     metadata = {}     for hkey in headers.keys():         <MASK>             val = urllib.unquote_plus(headers[hkey])             try:                 metadata[hkey[len(metadata_prefix) :]] = unicode(val, ""utf-8"")             except UnicodeDecodeError:                 metadata[hkey[len(metadata_prefix) :]] = val             del headers[hkey]     return metadata",if hkey . lower ( ) . startswith ( metadata_prefix ) :,if hkey in metadata_prefix :,11.835764736093042,11.835764736093042,0.0
"def _process_rtdest(self):     LOG.debug(""Processing RT NLRI destination..."")     if self._rtdest_queue.is_empty():         return     else:         processed_any = False         while not self._rtdest_queue.is_empty():             # We process the first destination in the queue.             next_dest = self._rtdest_queue.pop_first()             if next_dest:                 next_dest.process()                 processed_any = True         <MASK>             # Since RT destination were updated we update RT filters             self._core_service.update_rtfilters()",if processed_any :,if processed_any :,100.00000000000004,100.00000000000004,1.0
"def _get_header(self, requester, header_name):     hits = sum([header_name in headers for _, headers in requester.requests])     self.assertEquals(hits, 2 if self.revs_enabled else 1)     for url, headers in requester.requests:         if header_name in headers:             <MASK>                 self.assertTrue(url.endswith(""/latest""), msg=url)             else:                 self.assertTrue(url.endswith(""/download_urls""), msg=url)             return headers.get(header_name)",if self . revs_enabled :,"if url . startswith ( ""latest"" ) :",5.522397783539471,5.522397783539471,0.0
"def add_external_deps(self, deps):     for dep in deps:         if hasattr(dep, ""el""):             dep = dep.el         <MASK>             raise InvalidArguments(""Argument is not an external dependency"")         self.external_deps.append(dep)         if isinstance(dep, dependencies.Dependency):             self.process_sourcelist(dep.get_sources())","if not isinstance ( dep , dependencies . Dependency ) :","if not isinstance ( dep , dependencies . ExternalDependency ) :",74.19446627365011,74.19446627365011,0.0
"def _consume_msg(self):     ws = self._ws     try:         while True:             r = await ws.recv()             if isinstance(r, bytes):                 r = r.decode(""utf-8"")             msg = json.loads(r)             stream = msg.get(""stream"")             <MASK>                 await self._dispatch(stream, msg)     except websockets.WebSocketException as wse:         logging.warn(wse)         await self.close()         asyncio.ensure_future(self._ensure_ws())",if stream is not None :,if stream :,23.174952587773145,0.0,0.0
"def generate_and_check_random():     random_size = 256     while True:         random = os.urandom(random_size)         a = int.from_bytes(random, ""big"")         A = pow(g, a, p)         <MASK>             a_for_hash = big_num_for_hash(A)             u = int.from_bytes(sha256(a_for_hash, b_for_hash), ""big"")             if u > 0:                 return (a, a_for_hash, u)","if is_good_mod_exp_first ( A , p ) :",if A > 0 :,1.5577298727187734,1.5577298727187734,0.0
"def write(self, datagram, address):     """"""Write a datagram.""""""     try:         return self.socket.sendto(datagram, address)     except OSError as se:         no = se.args[0]         if no == EINTR:             return self.write(datagram, address)         elif no == EMSGSIZE:             raise error.MessageLengthError(""message too long"")         <MASK>             # oh, well, drop the data. The only difference from UDP             # is that UDP won't ever notice.             # TODO: add TCP-like buffering             pass         else:             raise",elif no == EAGAIN :,if no == EADDRINUSE :,32.46679154750991,32.46679154750991,0.0
"def doDir(elem):     for child in elem.childNodes:         <MASK>             continue         if child.tagName == ""Directory"":             doDir(child)         elif child.tagName == ""Component"":             for grandchild in child.childNodes:                 if not isinstance(grandchild, minidom.Element):                     continue                 if grandchild.tagName != ""File"":                     continue                 files.add(grandchild.getAttribute(""Source"").replace(os.sep, ""/""))","if not isinstance ( child , minidom . Element ) :","if child . tagName != ""File"" :",5.369488567517933,5.369488567517933,0.0
"def add_reversed_tensor(i, X, reversed_X):     # Do not keep tensors that should stop the mapping.     if X in stop_mapping_at_tensors:         return     if X not in reversed_tensors:         reversed_tensors[X] = {""id"": (nid, i), ""tensor"": reversed_X}     else:         tmp = reversed_tensors[X]         if ""tensor"" in tmp and ""tensors"" in tmp:             raise Exception(""Wrong order, tensors already aggregated!"")         <MASK>             tmp[""tensors""] = [tmp[""tensor""], reversed_X]             del tmp[""tensor""]         else:             tmp[""tensors""].append(reversed_X)","if ""tensor"" in tmp :","if ""tensors"" in tmp :",48.892302243490086,48.892302243490086,0.0
"def walk(source, path, default, delimiter="".""):     """"""Walk the sourch hash given the path and return the value or default if not found""""""     if not isinstance(source, dict):         raise RuntimeError(             ""The source is not a walkable dict: {} path: {}"".format(source, path)         )     keys = path.split(delimiter)     max_depth = len(keys)     cur_depth = 0     while cur_depth < max_depth:         <MASK>             source = source[keys[cur_depth]]             cur_depth = cur_depth + 1         else:             return default     return source",if keys [ cur_depth ] in source :,if keys [ cur_depth ] in source :,100.00000000000004,100.00000000000004,1.0
"def _from_txt_get_vulns(self):     file_vulns = []     vuln_regex = (         'SQL injection in a .*? was found at: ""(.*?)""'         ', using HTTP method (.*?). The sent .*?data was: ""(.*?)""'     )     vuln_re = re.compile(vuln_regex)     for line in file(self.OUTPUT_FILE):         mo = vuln_re.search(line)         <MASK>             v = MockVuln(""TestCase"", None, ""High"", 1, ""plugin"")             v.set_url(URL(mo.group(1)))             v.set_method(mo.group(2))             file_vulns.append(v)     return file_vulns",if mo :,if mo :,100.00000000000004,0.0,1.0
"def __get__(self, instance, instance_type=None):     if instance:         if self.att_name not in instance._obj_cache:             rel_obj = self.get_obj(instance)             <MASK>                 instance._obj_cache[self.att_name] = rel_obj         return instance._obj_cache.get(self.att_name)     return self",if rel_obj :,if rel_obj :,100.00000000000004,100.00000000000004,1.0
"def get_ranges_from_func_set(support_set):     pos_start = 0     pos_end = 0     ranges = []     for pos, func in enumerate(network.function):         <MASK>             pos_end = pos         else:             if pos_end >= pos_start:                 ranges.append((pos_start, pos_end))             pos_start = pos + 1     if pos_end >= pos_start:         ranges.append((pos_start, pos_end))     return ranges",if func . type in support_set :,if func . support_set == support_set :,30.66148710292676,30.66148710292676,0.0
"def get_all_active_plugins(self) -> List[BotPlugin]:     """"""This returns the list of plugins in the callback ordered defined from the config.""""""     all_plugins = []     for name in self.plugins_callback_order:         # None is a placeholder for any plugin not having a defined order         if name is None:             all_plugins += [                 plugin                 for name, plugin in self.plugins.items()                 <MASK>             ]         else:             plugin = self.plugins[name]             if plugin.is_activated:                 all_plugins.append(plugin)     return all_plugins",if name not in self . plugins_callback_order and plugin . is_activated,if plugin . is_activated :,15.42055884967951,15.42055884967951,0.0
"def render_token_list(self, tokens):     result = []     vars = []     for token in tokens:         <MASK>             result.append(token.contents.replace(""%"", ""%%""))         elif token.token_type == TOKEN_VAR:             result.append(""%%(%s)s"" % token.contents)             vars.append(token.contents)     msg = """".join(result)     if self.trimmed:         msg = translation.trim_whitespace(msg)     return msg, vars",if token . token_type == TOKEN_TEXT :,if token . token_type == TOKEN_STRING :,82.651681837938,82.651681837938,0.0
"def test_build_root_config_overwrite(self):     cfg = build_root_config(""tests.files.settings_overwrite"")     for key, val in DEFAULT_SPIDER_GLOBAL_CONFIG.items():         <MASK>             self.assertEqual(cfg[""global""][key], [""zzz""])         else:             self.assertEqual(cfg[""global""][key], val)","if key == ""spider_modules"" :","if val == ""zzz"" :",21.069764742263047,21.069764742263047,0.0
"def get_limit(self, request):     if self.limit_query_param:         try:             limit = int(request.query_params[self.limit_query_param])             if limit < 0:                 raise ValueError()             # Enforce maximum page size, if defined             <MASK>                 if limit == 0:                     return settings.MAX_PAGE_SIZE                 else:                     return min(limit, settings.MAX_PAGE_SIZE)             return limit         except (KeyError, ValueError):             pass     return self.default_limit",if settings . MAX_PAGE_SIZE :,if limit > settings . MAX_PAGE_SIZE :,69.89307622784945,69.89307622784945,0.0
"def track_handler(handler):     tid = handler.request.tid     for event in events_monitored:         <MASK>             e = Event(event, handler.request.execution_time)             State.tenant_state[tid].RecentEventQ.append(e)             State.tenant_state[tid].EventQ.append(e)             break","if event [ ""handler_check"" ] ( handler ) :",if event . type == EventType . Event :,6.8803707079889325,6.8803707079889325,0.0
"def TryMerge(self, d):     while d.avail() > 0:         tt = d.getVarInt32()         if tt == 10:             length = d.getVarInt32()             tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)             d.skip(length)             self.add_subscription().TryMerge(tmp)             continue         <MASK>             raise ProtocolBuffer.ProtocolBufferDecodeError         d.skipData(tt)",if tt == 0 :,if tt != 0 :,37.99178428257963,37.99178428257963,0.0
"def GetCreateInstanceBinder(self, info):     with self._lock:         <MASK>             return self._createInstanceBinders[info]         b = runtime.SymplCreateInstanceBinder(info)         self._createInstanceBinders[info] = b     return b",if self . _createInstanceBinders . ContainsKey ( info ) :,if info in self . _createInstanceBinders :,27.329052280893862,27.329052280893862,0.0
"def process_task(self, body, message):     if ""control"" in body:         try:             return self.control(body, message)         except Exception:             logger.exception(""Exception handling control message:"")             return     if len(self.pool):         <MASK>             try:                 queue = UUID(body[""uuid""]).int % len(self.pool)             except Exception:                 queue = self.total_messages % len(self.pool)         else:             queue = self.total_messages % len(self.pool)     else:         queue = 0     self.pool.write(queue, body)     self.total_messages += 1     message.ack()","if ""uuid"" in body and body [ ""uuid"" ] :","if ""uuid"" in body :",30.93485033266056,30.93485033266056,0.0
"def is_defined_in_base_class(self, var: Var) -> bool:     if var.info:         for base in var.info.mro[1:]:             if base.get(var.name) is not None:                 return True         <MASK>             return True     return False",if var . info . fallback_to_any :,if base . get ( var . name ) is not None :,7.768562846380176,7.768562846380176,0.0
"def ant_map(m):     tmp = ""rows %s\ncols %s\n"" % (len(m), len(m[0]))     players = {}     for row in m:         tmp += ""m ""         for col in row:             if col == LAND:                 tmp += "".""             <MASK>                 tmp += ""%""             elif col == FOOD:                 tmp += ""*""             elif col == UNSEEN:                 tmp += ""?""             else:                 players[col] = True                 tmp += chr(col + 97)         tmp += ""\n""     tmp = (""players %s\n"" % len(players)) + tmp     return tmp",elif col == BARRIER :,if col == QUANT :,32.46679154750991,32.46679154750991,0.0
"def prompt_for_resume(config):     logger = logging.getLogger(""changeme"")     logger.error(         ""A previous scan was interrupted. Type R to resume or F to start a fresh scan""     )     answer = """"     while not (answer == ""R"" or answer == ""F""):         prompt = ""(R/F)> ""         answer = """"         try:             answer = raw_input(prompt)         except NameError:             answer = input(prompt)         <MASK>             logger.debug(""Forcing a fresh scan"")         elif answer.upper() == ""R"":             logger.debug(""Resuming previous scan"")             config.resume = True     return config.resume","if answer . upper ( ) == ""F"" :","if answer . upper ( ) == ""F"" :",100.00000000000004,100.00000000000004,1.0
"def f(view, s):     if mode == modes.INTERNAL_NORMAL:         <MASK>             if view.line(s.b).size() > 0:                 eol = view.line(s.b).b                 return R(s.b, eol)             return s     return s",if count == 1 :,"if s . b == ""\n"" :",8.913765521398126,8.913765521398126,0.0
"def flush(self):     if not self.cuts:         return     for move, (x, y, z), cent in douglas(self.cuts, self.tolerance, self.plane):         <MASK>             self.write(""%s X%.4f Y%.4f Z%.4f %s"" % (move, x, y, z, cent))             self.lastgcode = None             self.lastx = x             self.lasty = y             self.lastz = z         else:             self.move_common(x, y, z, gcode=""G1"")     self.cuts = []",if cent :,if move :,34.66806371753173,0.0,0.0
"def copy_shell(self):     cls = self.__class__     old_id = cls.id     new_i = cls()  # create a new group     new_i.id = self.id  # with the same id     cls.id = old_id  # Reset the Class counter     # Copy all properties     for prop in cls.properties:         <MASK>             if self.has(prop):                 val = getattr(self, prop)                 setattr(new_i, prop, val)     # but no members     new_i.members = []     return new_i","if prop is not ""members"" :",if prop in new_i . members :,11.339582221952005,11.339582221952005,0.0
"def find_region_by_value(key, value):     for region in cognitoidp_backends:         backend = cognitoidp_backends[region]         for user_pool in backend.user_pools.values():             if key == ""client_id"" and value in user_pool.clients:                 return region             <MASK>                 return region     # If we can't find the `client_id` or `access_token`, we just pass     # back a default backend region, which will raise the appropriate     # error message (e.g. NotAuthorized or NotFound).     return list(cognitoidp_backends)[0]","if key == ""access_token"" and value in user_pool . access_tokens :","if key == ""access_token"" and value in user_pool . access_tokens :",100.00000000000004,100.00000000000004,1.0
"def __init__(     self, fixed: MQTTFixedHeader = None, variable_header: PacketIdVariableHeader = None ):     if fixed is None:         header = MQTTFixedHeader(PUBREL, 0x02)  # [MQTT-3.6.1-1]     else:         <MASK>             raise HBMQTTException(                 ""Invalid fixed packet type %s for PubrelPacket init"" % fixed.packet_type             )         header = fixed     super().__init__(header)     self.variable_header = variable_header     self.payload = None",if fixed . packet_type is not PUBREL :,if fixed . packet_type != MQTTFixedHeader . PUBREL :,46.92470064105597,46.92470064105597,0.0
"def _on_event_MetadataStatisticsUpdated(self, event, data):     with self._selectedFileMutex:         <MASK>             self._setJobData(                 self._selectedFile[""filename""],                 self._selectedFile[""filesize""],                 self._selectedFile[""sd""],                 self._selectedFile[""user""],             )",if self . _selectedFile :,if self . _selectedFile :,100.00000000000004,100.00000000000004,1.0
"def _validate_parameter_range(self, value_hp, parameter_range):     """"""Placeholder docstring""""""     for (         parameter_range_key,         parameter_range_value,     ) in parameter_range.__dict__.items():         <MASK>             continue         # Categorical ranges         if isinstance(parameter_range_value, list):             for categorical_value in parameter_range_value:                 value_hp.validate(categorical_value)         # Continuous, Integer ranges         else:             value_hp.validate(parameter_range_value)","if parameter_range_key == ""scaling_type"" :",if parameter_range_key not in self . _parameter_range_range_dict :,26.801651563557776,26.801651563557776,0.0
"def visit_filter_projection(self, node, value):     base = self.visit(node[""children""][0], value)     if not isinstance(base, list):         return None     comparator_node = node[""children""][2]     collected = []     for element in base:         if self._is_true(self.visit(comparator_node, element)):             current = self.visit(node[""children""][1], element)             <MASK>                 collected.append(current)     return collected",if current is not None :,if current is not None :,100.00000000000004,100.00000000000004,1.0
"def _getSubstrings(self, va, size, ltyp):     # rip through the desired memory range to populate any substrings     subs = set()     end = va + size     for offs in range(va, end, 1):         loc = self.getLocation(offs, range=True)         <MASK>             subs.add((loc[L_VA], loc[L_SIZE]))             if loc[L_TINFO]:                 subs = subs.union(set(loc[L_TINFO]))     return list(subs)",if loc and loc [ L_LTYPE ] == LOC_STRING and loc [ L_VA ] > va :,if loc :,0.05744496417108234,0.0,0.0
"def run(self):     while not self._stopped:         try:             try:                 test_name = next(self.pending)             except StopIteration:                 break             mp_result = self._runtest(test_name)             self.output.put((False, mp_result))             <MASK>                 break         except ExitThread:             break         except BaseException:             self.output.put((True, traceback.format_exc()))             break","if must_stop ( mp_result . result , self . ns ) :",if mp_result is None :,7.678812443288274,7.678812443288274,0.0
"def get_in_inputs(key, data):     if isinstance(data, dict):         for k, v in data.items():             if k == key:                 return v             elif isinstance(v, (list, tuple, dict)):                 out = get_in_inputs(key, v)                 <MASK>                     return out     elif isinstance(data, (list, tuple)):         out = [get_in_inputs(key, x) for x in data]         out = [x for x in out if x]         if out:             return out[0]",if out :,if out :,100.00000000000004,0.0,1.0
"def act_mapping(self, items, actions, mapping):     """"""Executes all the actions on the list of pods.""""""     success = True     for action in actions:         for key, method in mapping.items():             if key in action:                 params = action.get(key)                 ret = method(items, params)                 <MASK>                     success = False     return success",if not ret :,if ret is not None :,11.478744233307168,11.478744233307168,0.0
"def _apply(self, plan):     desired = plan.desired     changes = plan.changes     self.log.debug(""_apply: zone=%s, len(changes)=%d"", desired.name, len(changes))     domain_name = desired.name[:-1]     try:         nsone_zone = self._client.loadZone(domain_name)     except ResourceException as e:         <MASK>             raise         self.log.debug(""_apply:   no matching zone, creating"")         nsone_zone = self._client.createZone(domain_name)     for change in changes:         class_name = change.__class__.__name__         getattr(self, ""_apply_{}"".format(class_name))(nsone_zone, change)",if e . message != self . ZONE_NOT_FOUND_MESSAGE :,if nsone_zone is None :,2.389389104935703,2.389389104935703,0.0
"def split_artists(self, json):     if len(json) == 0:         ([], [])     elif len(json) == 1:         artist = Artist.query.filter_by(name=json[0][""name""]).first()         return ([artist], [])     my_artists = []     other_artists = []     for artist_dict in json:         artist = Artist.query.filter_by(name=artist_dict[""name""])         <MASK>             my_artists.append(artist.first())         else:             del artist_dict[""thumb_url""]             other_artists.append(artist_dict)     return (my_artists, other_artists)",if artist . count ( ) :,if artist . first ( ) :,41.11336169005196,41.11336169005196,0.0
"def update_metadata(self):     for attrname in dir(self):         if attrname.startswith(""__""):             continue         attrvalue = getattr(self, attrname, None)         <MASK>             continue         if attrname == ""salt_version"":             attrname = ""version""         if hasattr(self.metadata, ""set_{0}"".format(attrname)):             getattr(self.metadata, ""set_{0}"".format(attrname))(attrvalue)         elif hasattr(self.metadata, attrname):             try:                 setattr(self.metadata, attrname, attrvalue)             except AttributeError:                 pass",if attrvalue == 0 :,if attrvalue is None :,19.3576934939088,19.3576934939088,0.0
"def close(self, code=errno.ECONNRESET):     with self.shutdown_lock:         <MASK>             super(RemoteIPRoute, self).close(code=code)             self.closed = True             try:                 self._mitogen_call.get()             except mitogen.core.ChannelError:                 pass             if self._mitogen_broker is not None:                 self._mitogen_broker.shutdown()                 self._mitogen_broker.join()",if not self . closed :,if self . closed :,57.89300674674101,57.89300674674101,0.0
"def untokenize(self, iterable):     for t in iterable:         <MASK>             self.compat(t, iterable)             break         tok_type, token, start, end, line = t         self.add_whitespace(start)         self.tokens.append(token)         self.prev_row, self.prev_col = end         if tok_type in (NEWLINE, NL):             self.prev_row += 1             self.prev_col = 0     return """".join(self.tokens)",if len ( t ) == 2 :,"if tok_type in ( NL , NL ) :",5.300156689756295,5.300156689756295,0.0
"def __call__(self, x, uttid=None):     if self.utt2spk is not None:         spk = self.utt2spk[uttid]     else:         spk = uttid     if not self.reverse:         if self.norm_means:             x = np.add(x, self.bias[spk])         <MASK>             x = np.multiply(x, self.scale[spk])     else:         if self.norm_vars:             x = np.divide(x, self.scale[spk])         if self.norm_means:             x = np.subtract(x, self.bias[spk])     return x",if self . norm_vars :,if self . norm_vars :,100.00000000000004,100.00000000000004,1.0
"def get_party_total(self, args):     self.party_total = frappe._dict()     for d in self.receivables:         self.init_party_total(d)         # Add all amount columns         for k in list(self.party_total[d.party]):             <MASK>                 self.party_total[d.party][k] += d.get(k, 0.0)         # set territory, customer_group, sales person etc         self.set_party_details(d)","if k not in [ ""currency"" , ""sales_person"" ] :",if k not in self . party_total :,15.9547977015442,15.9547977015442,0.0
"def get_databases(request):     dbs = {}     global_env = globals()     for (key, value) in global_env.items():         try:             cond = isinstance(value, GQLDB)         except:             cond = isinstance(value, SQLDB)         <MASK>             dbs[key] = value     return dbs",if cond :,if not cond :,35.35533905932737,35.35533905932737,0.0
"def check_twobit_file(dbkey, GALAXY_DATA_INDEX_DIR):     twobit_file = ""%s/twobit.loc"" % GALAXY_DATA_INDEX_DIR     twobit_path = """"     twobits = {}     for i, line in enumerate(open(twobit_file)):         line = line.rstrip(""\r\n"")         if line and not line.startswith(""#""):             fields = line.split(""\t"")             <MASK>                 continue             twobits[(fields[0])] = fields[1]     if dbkey in twobits:         twobit_path = twobits[(dbkey)]     return twobit_path",if len ( fields ) < 2 :,if i != 0 :,6.916271812933183,6.916271812933183,0.0
"def action(scheduler, _):     nonlocal state     nonlocal has_result     nonlocal result     nonlocal first     nonlocal time     <MASK>         observer.on_next(result)     try:         if first:             first = False         else:             state = iterate(state)         has_result = condition(state)         if has_result:             result = state             time = time_mapper(state)     except Exception as e:  # pylint: disable=broad-except         observer.on_error(e)         return     if has_result:         mad.disposable = scheduler.schedule_relative(time, action)     else:         observer.on_completed()",if has_result :,if has_result :,100.00000000000004,100.00000000000004,1.0
def orthogonalEnd(self):     if self.type == Segment.LINE:         O = self.AB.orthogonal()         O.norm()         return O     else:         O = self.B - self.C         O.norm()         <MASK>             return -O         else:             return O,if self . type == Segment . CCW :,if self . type == Segment . LINE :,78.25422900366438,78.25422900366438,0.0
"def remove(self, values):     if not isinstance(values, (list, tuple, set)):         values = [values]     for v in values:         v = str(v)         if isinstance(self._definition, dict):             self._definition.pop(v, None)         elif self._definition == ""ANY"":             <MASK>                 self._definition = []         elif v in self._definition:             self._definition.remove(v)     if (         self._value is not None         and self._value not in self._definition         and self._not_any()     ):         raise ConanException(bad_value_msg(self._name, self._value, self.values_range))","if v == ""ANY"" :",if v in self . _definition :,12.22307556087252,12.22307556087252,0.0
"def __enter__(self) -> None:     try:         <MASK>             signal.signal(signal.SIGALRM, self.handle_timeout)             signal.alarm(self.seconds)     except ValueError as ex:         logger.warning(""timeout can't be used in the current context"")         logger.exception(ex)",if threading . current_thread ( ) == threading . main_thread ( ) :,if self . seconds :,1.044177559991939,1.044177559991939,0.0
"def __init__(self, fixed: MQTTFixedHeader = None):     if fixed is None:         header = MQTTFixedHeader(PINGRESP, 0x00)     else:         <MASK>             raise HBMQTTException(                 ""Invalid fixed packet type %s for PingRespPacket init""                 % fixed.packet_type             )         header = fixed     super().__init__(header)     self.variable_header = None     self.payload = None",if fixed . packet_type is not PINGRESP :,if fixed . packet_type != MQTTFixedHeader . PINGRESP :,46.92470064105597,46.92470064105597,0.0
"def _put_nowait(self, data, *, sender):     if not self._running:         logger.warning(""Pub/Sub listener message after stop: %r, %r"", sender, data)         return     self._queue.put_nowait((sender, data))     if self._waiter is not None:         fut, self._waiter = self._waiter, None         <MASK>             assert fut.cancelled(), (""Waiting future is in wrong state"", self, fut)             return         fut.set_result(None)",if fut . done ( ) :,if fut . done ( ) :,100.00000000000004,100.00000000000004,1.0
"def OnAssignBuiltin(self, cmd_val):     # type: (cmd_value__Assign) -> None     buf = self._ShTraceBegin()     if not buf:         return     for i, arg in enumerate(cmd_val.argv):         <MASK>             buf.write("" "")         buf.write(arg)     for pair in cmd_val.pairs:         buf.write("" "")         buf.write(pair.var_name)         buf.write(""="")         if pair.rval:             _PrintShValue(pair.rval, buf)     buf.write(""\n"")     self.f.write(buf.getvalue())",if i != 0 :,if i == 0 :,37.99178428257963,37.99178428257963,0.0
"def convertDict(obj):     obj = dict(obj)     for k, v in obj.items():         del obj[k]         if not (isinstance(k, str) or isinstance(k, unicode)):             k = dumps(k)             # Keep track of which keys need to be decoded when loading.             <MASK>                 obj[Types.KEYS] = []             obj[Types.KEYS].append(k)         obj[k] = convertObjects(v)     return obj",if Types . KEYS not in obj :,if k not in obj :,38.49815007763549,38.49815007763549,0.0
"def _ArgumentListHasDictionaryEntry(self, token):     """"""Check if the function argument list has a dictionary as an arg.""""""     if _IsArgumentToFunction(token):         while token:             if token.value == ""{"":                 length = token.matching_bracket.total_length - token.total_length                 return length + self.stack[-2].indent > self.column_limit             if token.ClosesScope():                 break             <MASK>                 token = token.matching_bracket             token = token.next_token     return False",if token . OpensScope ( ) :,"if token . value == ""}"" :",16.784459625186194,16.784459625186194,0.0
"def get_editable_dict(self):     ret = {}     for ref, ws_package in self._workspace_packages.items():         path = ws_package.root_folder         <MASK>             path = os.path.join(path, CONANFILE)         ret[ref] = {""path"": path, ""layout"": ws_package.layout}     return ret",if os . path . isdir ( path ) :,if os . path . isdir ( path ) :,100.00000000000004,100.00000000000004,1.0
"def serialize(self, name=None):     data = super(WebLink, self).serialize(name)     data[""contentType""] = self.contentType     if self.width:         <MASK>             raise InvalidWidthException(self.width)         data[""inputOptions""] = {}         data[""width""] = self.width     data.update({""content"": {""url"": self.linkUrl, ""text"": self.linkText}})     return data","if self . width not in [ 100 , 50 , 33 , 25 ] :",if self . width not in self . width_choices :,31.070152518033044,31.070152518033044,0.0
"def callback(lexer, match, context):     text = match.group()     extra = """"     if start:         context.next_indent = len(text)         <MASK>             while context.next_indent < context.indent:                 context.indent = context.indent_stack.pop()             if context.next_indent > context.indent:                 extra = text[context.indent :]                 text = text[: context.indent]     else:         context.next_indent += len(text)     if text:         yield match.start(), TokenClass, text     if extra:         yield match.start() + len(text), TokenClass.Error, extra     context.pos = match.end()",if context . next_indent < context . indent :,if context . indent_stack :,21.606281467072083,21.606281467072083,0.0
"def _handle_unsubscribe(self, web_sock):     index = None     with await self._subscriber_lock:         for i, (subscriber_web_sock, _) in enumerate(self._subscribers):             if subscriber_web_sock == web_sock:                 index = i                 break         <MASK>             del self._subscribers[index]         if not self._subscribers:             asyncio.ensure_future(self._unregister_subscriptions())",if index is not None :,if i > 0 :,10.400597689005304,10.400597689005304,0.0
"def test_missing_dict_param():     expected_err = ""params dictionary did not contain value for placeholder""     try:         substitute_params(             ""SELECT * FROM cust WHERE salesrep = %(name)s"", {""foobar"": ""John Doe""}         )         assert False, ""expected exception b/c dict did not contain replacement value""     except ValueError as exc:         <MASK>             raise",if expected_err not in str ( exc ) :,if exc . status_code == 404 :,5.369488567517933,5.369488567517933,0.0
"def one_gpr_reg_one_mem_scalable(ii):     n, r = 0, 0     for op in _gen_opnds(ii):         <MASK>             n += 1         elif op_gprv(op):             r += 1         else:             return False     return n == 1 and r == 1","if op_agen ( op ) or ( op_mem ( op ) and op . oc2 in [ ""v"" ] ) :",if op_n ( op ) :,3.5337864092458613,3.5337864092458613,0.0
"def on_enter(self):     """"""Fired when mouse enter the bbox of the widget.""""""     if hasattr(self, ""md_bg_color"") and self.focus_behavior:         if hasattr(self, ""theme_cls"") and not self.focus_color:             self.md_bg_color = self.theme_cls.bg_normal         else:             <MASK>                 self.md_bg_color = App.get_running_app().theme_cls.bg_normal             else:                 self.md_bg_color = self.focus_color",if not self . focus_color :,"if self . focus_behavior == ""normal"" :",23.462350320527996,23.462350320527996,0.0
"def __init__(self, *args, **kwargs):     BaseCellExporter.__init__(self, *args, **kwargs)     self.comment = ""#""     for key in [""cell_marker""]:         <MASK>             self.metadata[key] = self.unfiltered_metadata[key]     if self.fmt.get(""rst2md""):         raise ValueError(             ""The 'rst2md' option is a read only option. The reverse conversion is not ""             ""implemented. Please either deactivate the option, or save to another format.""         )  # pragma: no cover",if key in self . unfiltered_metadata :,if key in self . unfiltered_metadata :,100.00000000000004,100.00000000000004,1.0
"def sendQueryQueueByAfterNate(self):     for i in range(10):         queryQueueByAfterNateRsp = self.session.httpClint.send(urls.get(""queryQueue""))         <MASK>             print(                 """".join(queryQueueByAfterNateRsp.get(""messages""))                 or queryQueueByAfterNateRsp.get(""validateMessages"")             )             time.sleep(1)         else:             sendEmail(ticket.WAIT_ORDER_SUCCESS)             sendServerChan(ticket.WAIT_ORDER_SUCCESS)             raise ticketIsExitsException(ticket.WAIT_AFTER_NATE_SUCCESS)","if not queryQueueByAfterNateRsp . get ( ""status"" ) :",if queryQueueByAfterNateRsp :,2.7574525891364066,0.0,0.0
"def filter_errors(self, errors: List[str]) -> List[str]:     real_errors: List[str] = list()     current_file = __file__     current_path = os.path.split(current_file)     for line in errors:         line = line.strip()         <MASK>             continue         fn, lno, lvl, msg = self.parse_trace_line(line)         if fn is not None:             _path = os.path.split(fn)             if _path[-1] != current_path[-1]:                 continue         real_errors.append(line)     return real_errors",if not line :,if not line :,100.00000000000004,100.00000000000004,1.0
"def pretty(self, n, comment=True):     if isinstance(n, (str, bytes, list, tuple, dict)):         r = repr(n)         if not comment:  # then it can be inside a comment!             r = r.replace(""*/"", r""\x2a/"")         return r     if not isinstance(n, six.integer_types):         return n     if isinstance(n, constants.Constant):         <MASK>             return ""%s /* %s */"" % (n, self.pretty(int(n)))         else:             return ""%s (%s)"" % (n, self.pretty(int(n)))     elif abs(n) < 10:         return str(n)     else:         return hex(n)",if comment :,if n < 0 :,12.703318703865365,12.703318703865365,0.0
"def get_pricings(self, subscription_id: str):     try:         client = self.get_client(subscription_id)         pricings_list = await run_concurrently(lambda: client.pricings.list())         <MASK>             return pricings_list.value         else:             return []     except Exception as e:         print_exception(f""Failed to retrieve pricings: {e}"")         return []","if hasattr ( pricings_list , ""value"" ) :",if pricings_list . status == 200 :,14.530346490115708,14.530346490115708,0.0
"def add_doc(target, variables, body_lines):     if isinstance(target, ast.Name):         # if it is a variable name add it to the doc         name = target.id         if name not in variables:             doc = find_doc_for(target, body_lines)             <MASK>                 variables[name] = doc     elif isinstance(target, ast.Tuple):         # if it is a tuple then iterate the elements         # this can happen like this:         # a, b = 1, 2         for e in target.elts:             add_doc(e, variables, body_lines)",if doc is not None :,if doc :,23.174952587773145,0.0,0.0
"def find_word_bounds(self, text, index, allowed_chars):     right = left = index     done = False     while not done:         if left == 0:             done = True         elif not self.word_boundary_char(text[left - 1]):             left -= 1         else:             done = True     done = False     while not done:         <MASK>             done = True         elif not self.word_boundary_char(text[right]):             right += 1         else:             done = True     return left, right",if right == len ( text ) :,if right == 0 :,32.58798048281462,32.58798048281462,0.0
"def pxrun_nodes(self, *args, **kwargs):     cell = self._px_cell     if re.search(r""^\s*%autopx\b"", cell):         self._disable_autopx()         return False     else:         try:             result = self.view.execute(cell, silent=False, block=False)         except:             self.shell.showtraceback()             return True         else:             <MASK>                 try:                     result.get()                 except:                     self.shell.showtraceback()                     return True                 else:                     result.display_outputs()             return False",if self . view . block :,if not result . is_success ( ) :,5.522397783539471,5.522397783539471,0.0
"def candidates() -> Generator[""Symbol"", None, None]:     s = self     if Symbol.debug_lookup:         Symbol.debug_print(""searching in self:"")         print(s.to_string(Symbol.debug_indent + 1), end="""")     while True:         if matchSelf:             yield s         if recurseInAnon:             yield from s.children_recurse_anon         else:             yield from s._children         <MASK>             break         s = s.siblingAbove         if Symbol.debug_lookup:             Symbol.debug_print(""searching in sibling:"")             print(s.to_string(Symbol.debug_indent + 1), end="""")",if s . siblingAbove is None :,if s . siblingAbove :,47.39878501170795,47.39878501170795,0.0
"def decTaskGen():     cnt = intbv(0, min=-n, max=n)     while 1:         yield clock.posedge, reset.negedge         <MASK>             cnt[:] = 0             count.next = 0         else:             # print count             decTaskFunc(cnt, enable, reset, n)             count.next = cnt",if reset == ACTIVE_LOW :,if count . next == n :,13.134549472120788,13.134549472120788,0.0
"def __call__(self, *args, **kwargs):     if not NET_INITTED:         return self.raw(*args, **kwargs)     for stack in traceback.walk_stack(None):         <MASK>             layer = stack[0].f_locals[""self""]             if layer in layer_names:                 log.pytorch_layer_name = layer_names[layer]                 print(layer_names[layer])                 break     out = self.obj(self.raw, *args, **kwargs)     # if isinstance(out,Variable):     #     out=[out]     return out","if ""self"" in stack [ 0 ] . f_locals :","if stack [ 0 ] . f_code == ""pytorch"" :",44.80304273880272,44.80304273880272,0.0
"def to_json_dict(self):     d = super().to_json_dict()     d[""bullet_list""] = RenderedContent.rendered_content_list_to_json(self.bullet_list)     if self.header is not None:         if isinstance(self.header, RenderedContent):             d[""header""] = self.header.to_json_dict()         else:             d[""header""] = self.header     if self.subheader is not None:         <MASK>             d[""subheader""] = self.subheader.to_json_dict()         else:             d[""subheader""] = self.subheader     return d","if isinstance ( self . subheader , RenderedContent ) :","if isinstance ( self . subheader , RenderedContent ) :",100.00000000000004,100.00000000000004,1.0
"def add(request):     form_type = ""servers""     if request.method == ""POST"":         form = BookMarkForm(request.POST)         <MASK>             form_type = form.save()             messages.add_message(request, messages.INFO, ""Bookmark created"")         else:             messages.add_message(request, messages.INFO, form.errors)         if form_type == ""server"":             url = reverse(""servers"")         else:             url = reverse(""metrics"")         return redirect(url)     else:         return redirect(reverse(""servers""))",if form . is_valid ( ) :,if form . is_valid ( ) :,100.00000000000004,100.00000000000004,1.0
"def fee_amount_in_quote(self, trading_pair: str, price: Decimal, order_amount: Decimal):     fee_amount = Decimal(""0"")     if self.percent > 0:         fee_amount = (price * order_amount) * self.percent     base, quote = trading_pair.split(""-"")     for flat_fee in self.flat_fees:         if interchangeable(flat_fee[0], base):             fee_amount += flat_fee[1] * price         <MASK>             fee_amount += flat_fee[1]     return fee_amount","elif interchangeable ( flat_fee [ 0 ] , quote ) :","if interchangeable ( flat_fee [ 0 ] , quote ) :",91.21679090703874,91.21679090703874,0.0
"def load_batch(fpath):     with open(fpath, ""rb"") as f:         <MASK>             # Python3             d = pickle.load(f, encoding=""latin1"")         else:             # Python2             d = pickle.load(f)     data = d[""data""]     labels = d[""labels""]     return data, labels","if sys . version_info > ( 3 , 0 ) :","if isinstance ( f , bytes ) :",7.433761660133445,7.433761660133445,0.0
"def clear_entries(options):     """"""Clear pending entries""""""     with Session() as session:         query = session.query(db.PendingEntry).filter(db.PendingEntry.approved == False)         <MASK>             query = query.filter(db.PendingEntry.task_name == options.task_name)         deleted = query.delete()         console(""Successfully deleted %i pending entries"" % deleted)",if options . task_name :,if options . task_name :,100.00000000000004,100.00000000000004,1.0
"def attribute_table(self, attribute):     """"""Return a tuple (schema, table) for attribute.""""""     dimension = attribute.dimension     if dimension:         schema = self.naming.dimension_schema or self.naming.schema         <MASK>             table = self.fact_name         else:             table = self.naming.dimension_table_name(dimension)     else:         table = self.fact_name         schema = self.naming.schema     return (schema, table)",if dimension . is_flat and not dimension . has_details :,"if dimension . name == ""fact"" :",11.25095974863814,11.25095974863814,0.0
"def remove_rating(self, songs, librarian):     count = len(songs)     if count > 1 and config.getboolean(""browsers"", ""rating_confirm_multiple""):         parent = qltk.get_menu_item_top_parent(self)         dialog = ConfirmRateMultipleDialog(parent, _(""_Remove Rating""), count, None)         if dialog.run() != Gtk.ResponseType.YES:             return     reset = []     for song in songs:         <MASK>             del song[""~#rating""]             reset.append(song)     librarian.changed(reset)","if ""~#rating"" in song :","if song [ ""~#rating"" ] == librarian . ID :",27.668736912821906,27.668736912821906,0.0
"def find_word_bounds(self, text, index, allowed_chars):     right = left = index     done = False     while not done:         if left == 0:             done = True         elif not self.word_boundary_char(text[left - 1]):             left -= 1         else:             done = True     done = False     while not done:         if right == len(text):             done = True         <MASK>             right += 1         else:             done = True     return left, right",elif not self . word_boundary_char ( text [ right ] ) :,if not self . word_boundary_char ( text [ right - 1 ] ) :,71.60350546947924,71.60350546947924,0.0
"def handle_read(self):     """"""Called when there is data waiting to be read.""""""     try:         chunk = self.recv(self.ac_in_buffer_size)     except RetryError:         pass     except socket.error:         self.handle_error()     else:         self.tot_bytes_received += len(chunk)         if not chunk:             self.transfer_finished = True             # self.close()  # <-- asyncore.recv() already do that...             return         <MASK>             chunk = self._data_wrapper(chunk)         try:             self.file_obj.write(chunk)         except OSError as err:             raise _FileReadWriteError(err)",if self . _data_wrapper is not None :,if self . data_wrapper :,24.12967158103988,24.12967158103988,0.0
"def toggle(self, event=None):     if self.absolute:         if self.save == self.split:             self.save = 100         if self.split > 20:             self.save = self.split             self.split = 1         else:             self.split = self.save     else:         if self.save == self.split:             self.save = 0.3         if self.split <= self.min or self.split >= self.max:             self.split = self.save         <MASK>             self.split = self.min         else:             self.split = self.max     self.placeChilds()",elif self . split < 0.5 :,if self . split < self . min :,31.55984539112946,31.55984539112946,0.0
"def readAtOffset(self, offset, size, shortok=False):     ret = b""""     self.fd.seek(offset)     while len(ret) != size:         rlen = size - len(ret)         x = self.fd.read(rlen)         if x == b"""":             <MASK>                 return None             return ret         ret += x     return ret",if not shortok :,if not shortok :,100.00000000000004,100.00000000000004,1.0
"def webfinger(environ, start_response, _):     query = parse_qs(environ[""QUERY_STRING""])     try:         rel = query[""rel""]         resource = query[""resource""][0]     except KeyError:         resp = BadRequest(""Missing parameter in request"")     else:         <MASK>             resp = BadRequest(""Bad issuer in request"")         else:             wf = WebFinger()             resp = Response(wf.response(subject=resource, base=OAS.baseurl))     return resp(environ, start_response)",if rel != [ OIC_ISSUER ] :,"if rel != ""issuer"" :",28.46946938149361,28.46946938149361,0.0
"def _tokenize(self, text):     if format_text(text) == EMPTY_TEXT:         return [self.additional_special_tokens[0]]     split_tokens = []     if self.do_basic_tokenize:         for token in self.basic_tokenizer.tokenize(             text, never_split=self.all_special_tokens         ):             # If the token is part of the never_split set             <MASK>                 split_tokens.append(token)             else:                 split_tokens += self.wordpiece_tokenizer.tokenize(token)     else:         split_tokens = self.wordpiece_tokenizer.tokenize(text)     return split_tokens",if token in self . basic_tokenizer . never_split :,if token in self . never_split :,55.74689950645963,55.74689950645963,0.0
"def send_packed_command(self, command, check_health=True):     if not self._sock:         self.connect()     try:         if isinstance(command, str):             command = [command]         for item in command:             self._sock.sendall(item)     except socket.error as e:         self.disconnect()         <MASK>             _errno, errmsg = ""UNKNOWN"", e.args[0]         else:             _errno, errmsg = e.args         raise ConnectionError(             ""Error %s while writing to socket. %s."" % (_errno, errmsg)         )     except Exception:         self.disconnect()         raise",if len ( e . args ) == 1 :,if check_health :,3.8261660656802645,3.8261660656802645,0.0
"def to_value(self, value):     # Tip: 'value' is the object returned by     #      taiga.projects.history.models.HistoryEntry.values_diff()     ret = {}     for key, val in value.items():         if key in [""attachments"", ""custom_attributes"", ""description_diff""]:             ret[key] = val         <MASK>             ret[key] = {k: {""from"": v[0], ""to"": v[1]} for k, v in val.items()}         else:             ret[key] = {""from"": val[0], ""to"": val[1]}     return ret","elif key == ""points"" :","if key in [ ""attachments"" , ""custom_attributes"" , ""description_diff"" ] :",2.568331954752977,2.568331954752977,0.0
"def to_child(cls, key=None, process=None):     if process is not None:         if type(process) is not dict:             raise ValueError(                 'Invalid value provided for ""process"" parameter, expected a dictionary'             )         <MASK>             # Merge class `__process__` parameters with provided parameters             result = {}             result.update(deepcopy(cls.__process__))             result.update(process)             process = result     class Child(cls):         __key__ = key         __process__ = process         __root__ = False     Child.__name__ = cls.__name__     return Child",if cls . __process__ :,if key is not None :,5.854497694024015,5.854497694024015,0.0
"def _super_function(args):     passed_class, passed_self = args.get_arguments([""type"", ""self""])     if passed_self is None:         return passed_class     else:         # pyclass = passed_self.get_type()         pyclass = passed_class         <MASK>             supers = pyclass.get_superclasses()             if supers:                 return pyobjects.PyObject(supers[0])         return passed_self","if isinstance ( pyclass , pyobjects . AbstractClass ) :",if pyclass is pyclass :,5.171845311465849,5.171845311465849,0.0
"def get_data(row):     data = []     for field_name, field_xpath in fields:         result = row.xpath(field_xpath)         <MASK>             result = "" "".join(                 text                 for text in map(                     six.text_type.strip, map(six.text_type, map(unescape, result))                 )                 if text             )         else:             result = None         data.append(result)     return data",if result :,if result :,100.00000000000004,0.0,1.0
"def say(jarvis, s):     """"""Reads what is typed.""""""     if not s:         jarvis.say(""What should I say?"")     else:         voice_state = jarvis.is_voice_enabled()         jarvis.enable_voice()         jarvis.say(s)         <MASK>             jarvis.disable_voice()",if not voice_state :,if voice_state :,57.89300674674101,57.89300674674101,0.0
"def __import__(name, globals=None, locals=None, fromlist=(), level=0):     module = orig___import__(name, globals, locals, fromlist, level)     if fromlist and module.__name__ in modules:         <MASK>             fromlist = list(fromlist)             fromlist.remove(""*"")             fromlist.extend(getattr(module, ""__all__"", []))         for x in fromlist:             if isinstance(getattr(module, x, None), types.ModuleType):                 from_name = ""{}.{}"".format(module.__name__, x)                 if from_name in modules:                     importlib.import_module(from_name)     return module","if ""*"" in fromlist :","if not hasattr ( module , ""__all__"" ) :",3.716499092256817,3.716499092256817,0.0
"def _read_pricing_file(self, region=None, pricing_file=None):     if not self.__pricing_file_cache:         <MASK>             logging.info(""Reading pricing file..."")             with open(pricing_file) as data_file:                 self.__pricing_file_cache = json.load(data_file)         else:             self.__pricing_file_cache = self._download_pricing_file(region)     return self.__pricing_file_cache",if pricing_file :,if pricing_file :,100.00000000000004,100.00000000000004,1.0
